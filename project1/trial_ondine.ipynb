{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **TODO**\n",
    "* Features cross products\n",
    "* Square and cube roots\n",
    "* phi features overfit?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import numpy as np   # generic stuff\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from lib.proj1_helpers import * #the helper provided for the project\n",
    "\n",
    "from implementations import * #our implementations of the functions done by us\n",
    "from helpers import *\n",
    "import datetime\n",
    "import operator\n",
    "# Useful starting lines\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the training and the testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_FOLDER = 'data/'\n",
    "\n",
    "y_train, tx_train, ids_train = load_csv_data(DATA_FOLDER+'train.csv',sub_sample=False)\n",
    "\n",
    "y_test, tx_test, ids_test = load_csv_data(DATA_FOLDER+'test.csv',sub_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1., -1., -1., ...,  1., -1., -1.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 138.47 ,   51.655,   97.827, ...,    1.24 ,   -2.475,  113.497],\n",
       "       [ 160.937,   68.768,  103.235, ..., -999.   , -999.   ,   46.226],\n",
       "       [-999.   ,  162.172,  125.953, ..., -999.   , -999.   ,   44.251],\n",
       "       ..., \n",
       "       [ 105.457,   60.526,   75.839, ..., -999.   , -999.   ,   41.992],\n",
       "       [  94.951,   19.362,   68.812, ..., -999.   , -999.   ,    0.   ],\n",
       "       [-999.   ,   72.756,   70.831, ..., -999.   , -999.   ,    0.   ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tx_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling missing values (-999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_missing_values(tx):\n",
    "    nan_values = (tx==-999)*1\n",
    "    for col in range(tx.shape[1]):\n",
    "        column = tx[:,col][tx[:,col]!=-999]\n",
    "        median = np.median(column)\n",
    "        tx[:,col][tx[:,col]==-999] = median\n",
    "    return tx, nan_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tx_train, nan_values_train = clean_missing_values(tx_train)\n",
    "tx_test, nan_values_test = clean_missing_values(tx_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_nan_feature(tx, nan_values):\n",
    "    # Keep in nan_values only the columns containing ones (i.e. keep only columns that used to contain -999)\n",
    "    cols_wo_nan_ids = [i for i in range(nan_values.shape[1]) if np.prod((nan_values.T[i]==0)*1)==1]\n",
    "    nan_values = np.delete(nan_values, cols_wo_nan_ids, axis=1)\n",
    "\n",
    "    # If two (or more) columns of nan_values are equal, keep only one of them\n",
    "    unique_nan_cols_ids = [0]\n",
    "    erase_cols_ids = []\n",
    "    for i in range(1,nan_values.shape[1]):\n",
    "        id_loop = unique_nan_cols_ids\n",
    "        erase = False\n",
    "        for j in id_loop:\n",
    "            if np.sum(nan_values.T[i]-nan_values.T[j])==0:\n",
    "                erase_cols_ids.append(i)\n",
    "                erase = True\n",
    "                break\n",
    "        if erase == False:\n",
    "            unique_nan_cols_ids.append(i)\n",
    "    nan_values = nan_values[:,unique_nan_cols_ids]\n",
    "    \n",
    "    # Concatenate nan_values with the original data matrix tx\n",
    "    return np.concatenate((tx, nan_values), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 30)\n",
      "(250000, 33)\n"
     ]
    }
   ],
   "source": [
    "print(tx_train.shape)\n",
    "tx_train = add_nan_feature(tx_train, nan_values_train)\n",
    "print(tx_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tx_test = add_nan_feature(tx_test, nan_values_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(568238, 33)\n"
     ]
    }
   ],
   "source": [
    "print(tx_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manage categorical data (feature 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.,  1.,  1., ...,  1.,  0.,  0.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tx_train[:,22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def manage_categorical_data(tx):\n",
    "    idx_cat = 22\n",
    "    larger_tx = np.zeros([tx.shape[0],4])\n",
    "    larger_tx[:,0] = (tx[:,idx_cat]==0)*1\n",
    "    larger_tx[:,1] = (tx[:,idx_cat]==1)*1\n",
    "    larger_tx[:,2] = (tx[:,idx_cat]==2)*1\n",
    "    larger_tx[:,3] = (tx[:,idx_cat]==3)*1\n",
    "    tx = np.delete(tx, idx_cat, axis=1)\n",
    "    tx = np.concatenate((tx, larger_tx), axis=1)\n",
    "    return tx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 33)\n",
      "(250000, 36)\n"
     ]
    }
   ],
   "source": [
    "print(tx_train.shape)\n",
    "tx_train = manage_categorical_data(tx_train)\n",
    "print(tx_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tx_test = manage_categorical_data(tx_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardization of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tx_train = standardize(tx_train)\n",
    "tx_train = tx_train[0]\n",
    "tx_test = standardize(tx_test)\n",
    "tx_test = tx_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add a column of all ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_ones(tx):\n",
    "    return np.concatenate((tx, np.ones([tx.shape[0],1])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tx_train = add_ones(tx_train)\n",
    "tx_test = add_ones(tx_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add powers of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_poly(x, degree):\n",
    "    \"\"\"polynomial basis functions for input data x, for j=1 up to j=degree.\"\"\"\n",
    "    return np.array([x**p for p in range(1,degree+1)]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_powers(tx, degree):\n",
    "    for col in range(tx.shape[1]):\n",
    "        tx = np.concatenate((tx, build_poly(tx[:,col], degree)), axis=1)\n",
    "    return tx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DONE IN CROSS VALIDATION\n",
    "#deg = 11\n",
    "#tx_train = add_powers(tx_train, deg)\n",
    "#tx_test = add_powers(tx_test, deg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 37)\n",
      "(568238, 37)\n"
     ]
    }
   ],
   "source": [
    "print(tx_train.shape)\n",
    "print(tx_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validation_ridge_regression(y, tx, k_indices, k, lambda_, degree, fct='rmse'):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    # get k'th subgroup in test, others in train\n",
    "    tx = add_powers(tx, degree)\n",
    "\n",
    "    k_index = k_indices[k]\n",
    "    test_y = y[k_index]\n",
    "    test_tx = tx[k_index,:]\n",
    "\n",
    "    mask = np.ones(len(y), dtype=bool) # set all elements to True\n",
    "    mask[k_index] = False              # set test elements to False\n",
    "    train_tx = tx[mask,:]              # select only True elements (ie train elements)\n",
    "    train_y = y[mask]\n",
    "    \n",
    "    # ridge regression and\n",
    "    # calculate the loss for train and test data\n",
    "    weights = ridge_regression(train_y, train_tx, lambda_)\n",
    "\n",
    "    y_pred_train = predict_labels(weights, train_tx)\n",
    "    y_pred_test = predict_labels(weights, test_tx)\n",
    "    right_train = np.sum(y_pred_train == train_y)/len(train_y)*100\n",
    "    right_test = np.sum(y_pred_test == test_y)/len(test_y)*100\n",
    "    \n",
    "    return right_train, right_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** !!!!!! ATTENTION! Long loop. !!!!!! **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seed = 1\n",
    "k_fold = 4\n",
    "k_indices = build_k_indices(y_train, k_fold, seed)\n",
    "\n",
    "lambdas = np.logspace(-15,-3,40)\n",
    "degrees = range(5,15)\n",
    "\n",
    "right_train_l = np.zeros([len(lambdas), len(degrees)])\n",
    "right_test_l = np.zeros([len(lambdas), len(degrees)])\n",
    "\n",
    "for j,degree in enumerate(degrees):\n",
    "    print('j = ', j, ' over ', len(degrees), ' ; Degree = ', degree)\n",
    "    for i,lambda_ in enumerate(lambdas):\n",
    "        print('i = ', i, ' over ', len(lambdas), ' ; Lambda_ = ', lambda_)\n",
    "        right_train = 0\n",
    "        right_test = 0\n",
    "        for k in range(k_fold):\n",
    "            right_train_temp, right_test_temp = cross_validation_ridge_regression(y_train, tx_train, k_indices, k, lambda_, degree)\n",
    "            right_train += right_train_temp\n",
    "            right_test += right_test_temp\n",
    "        right_train_l[i,j] = right_train/k_fold\n",
    "        right_test_l[i,j] = right_test/k_fold\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogx(lambdas, right_train_l[:,6], label='train', color='r')\n",
    "plt.semilogx(lambdas, right_test_l[:,6], label='test')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.semilogx(degrees, right_train_l[20,:], label='train', color='r')\n",
    "plt.semilogx(degrees, right_test_l[20,:], label='test')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "surf = ax.plot_surface(lambdas, degrees, right_test_l) #, rstride=1, cstride=1, cmap=cm.coolwarm,\n",
    "        #linewidth=0, antialiased=False)\n",
    "\n",
    "#ax.zaxis.set_major_locator(LinearLocator(10))\n",
    "#ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n",
    "\n",
    "#fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, let us compute the results on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_id_i, max_id_j = np.unravel_index(right_test_l.argmax(), right_test_l.shape)\n",
    "print(right_test_l[max_id_i, max_id_j])\n",
    "lambda_ = lambdas[max_id_i]\n",
    "deg = degrees[max_id_j]\n",
    "tx_train = add_powers(tx_train, deg)\n",
    "tx_test = add_powers(tx_test, deg)\n",
    "weights = ridge_regression(y_train, tx_train, lambda_);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(weights: \\n',weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = predict_labels(weights, tx_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name = 'output/ridge_regression_ondine'\n",
    "create_csv_submission(ids_test, y_pred, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton regularized logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
