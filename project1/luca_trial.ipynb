{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Project 1\n",
    "\n",
    "    We begin by loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline \n",
    "import numpy as np   # generic stuff\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#### REMOVE THIS LINE BEFORE SUBMISSION\n",
    "import pandas as pd\n",
    "#######################################################################\n",
    "\n",
    "from lib.proj1_helpers import * #the helper provided for the project\n",
    "from lib.costs import *\n",
    "\n",
    "# choose which implementations you would like\n",
    "from lib.implementations import *\n",
    "#from implementations import * #our implementations of the functions done by us\n",
    "\n",
    "\n",
    "import datetime\n",
    "from helpers import * #helpers of exo 2\n",
    "# Useful starting lines\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = 'data/'\n",
    "\n",
    "y_train, tx_train, ids_train = load_csv_data(DATA_FOLDER+'train.csv',sub_sample=False)\n",
    "\n",
    "y_test, tx_test, ids_test = load_csv_data(DATA_FOLDER+'test.csv',sub_sample=False)\n",
    "AAA = 0;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note that here we are only considering a sub_sample as the \"True\" value indicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1., -1., -1., ...,  1., -1., -1.])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -4.90230794e+01,   4.92398193e+01,   8.11819816e+01,\n",
       "         5.78959617e+01,  -7.08420675e+02,  -6.01237051e+02,\n",
       "        -7.09356603e+02,   2.37309984e+00,   1.89173324e+01,\n",
       "         1.58432217e+02,   1.43760943e+00,  -1.28304708e-01,\n",
       "        -7.08985189e+02,   3.87074191e+01,  -1.09730480e-02,\n",
       "        -8.17107200e-03,   4.66602072e+01,  -1.95074680e-02,\n",
       "         4.35429640e-02,   4.17172345e+01,  -1.01191920e-02,\n",
       "         2.09797178e+02,   9.79176000e-01,  -3.48329567e+02,\n",
       "        -3.99254314e+02,  -3.99259788e+02,  -6.92381204e+02,\n",
       "        -7.09121609e+02,  -7.09118631e+02,   7.30645914e+01])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids_train\n",
    "np.mean(tx_train,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>138.470</td>\n",
       "      <td>51.655</td>\n",
       "      <td>97.827</td>\n",
       "      <td>27.980</td>\n",
       "      <td>0.91</td>\n",
       "      <td>124.711</td>\n",
       "      <td>2.666</td>\n",
       "      <td>3.064</td>\n",
       "      <td>41.928</td>\n",
       "      <td>197.760</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.277</td>\n",
       "      <td>258.733</td>\n",
       "      <td>2.0</td>\n",
       "      <td>67.435</td>\n",
       "      <td>2.150</td>\n",
       "      <td>0.444</td>\n",
       "      <td>46.062</td>\n",
       "      <td>1.24</td>\n",
       "      <td>-2.475</td>\n",
       "      <td>113.497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>160.937</td>\n",
       "      <td>68.768</td>\n",
       "      <td>103.235</td>\n",
       "      <td>48.146</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.473</td>\n",
       "      <td>2.078</td>\n",
       "      <td>125.157</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.916</td>\n",
       "      <td>164.546</td>\n",
       "      <td>1.0</td>\n",
       "      <td>46.226</td>\n",
       "      <td>0.725</td>\n",
       "      <td>1.158</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>46.226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-999.000</td>\n",
       "      <td>162.172</td>\n",
       "      <td>125.953</td>\n",
       "      <td>35.635</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.148</td>\n",
       "      <td>9.336</td>\n",
       "      <td>197.814</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.186</td>\n",
       "      <td>260.414</td>\n",
       "      <td>1.0</td>\n",
       "      <td>44.251</td>\n",
       "      <td>2.053</td>\n",
       "      <td>-2.028</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>44.251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>143.905</td>\n",
       "      <td>81.417</td>\n",
       "      <td>80.943</td>\n",
       "      <td>0.414</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.310</td>\n",
       "      <td>0.414</td>\n",
       "      <td>75.968</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060</td>\n",
       "      <td>86.062</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>175.864</td>\n",
       "      <td>16.915</td>\n",
       "      <td>134.805</td>\n",
       "      <td>16.405</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.891</td>\n",
       "      <td>16.405</td>\n",
       "      <td>57.983</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.871</td>\n",
       "      <td>53.131</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0        1        2       3       4        5        6      7       8   \\\n",
       "0  138.470   51.655   97.827  27.980    0.91  124.711    2.666  3.064  41.928   \n",
       "1  160.937   68.768  103.235  48.146 -999.00 -999.000 -999.000  3.473   2.078   \n",
       "2 -999.000  162.172  125.953  35.635 -999.00 -999.000 -999.000  3.148   9.336   \n",
       "3  143.905   81.417   80.943   0.414 -999.00 -999.000 -999.000  3.310   0.414   \n",
       "4  175.864   16.915  134.805  16.405 -999.00 -999.000 -999.000  3.891  16.405   \n",
       "\n",
       "        9    ...        20       21   22       23       24       25       26  \\\n",
       "0  197.760   ...    -0.277  258.733  2.0   67.435    2.150    0.444   46.062   \n",
       "1  125.157   ...    -1.916  164.546  1.0   46.226    0.725    1.158 -999.000   \n",
       "2  197.814   ...    -2.186  260.414  1.0   44.251    2.053   -2.028 -999.000   \n",
       "3   75.968   ...     0.060   86.062  0.0 -999.000 -999.000 -999.000 -999.000   \n",
       "4   57.983   ...    -0.871   53.131  0.0 -999.000 -999.000 -999.000 -999.000   \n",
       "\n",
       "       27       28       29  \n",
       "0    1.24   -2.475  113.497  \n",
       "1 -999.00 -999.000   46.226  \n",
       "2 -999.00 -999.000   44.251  \n",
       "3 -999.00 -999.000    0.000  \n",
       "4 -999.00 -999.000    0.000  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# illegal: \n",
    "pd.DataFrame(tx_train).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_999_median(tx):\n",
    "    nan_values = (tx==-999)*1\n",
    "    for col in range(tx.shape[1]):\n",
    "        column = tx[:,col][tx[:,col]!=-999]\n",
    "        median = np.median(column)\n",
    "        tx[:,col][tx[:,col]==-999] = median\n",
    "    return tx, nan_values\n",
    "\n",
    "def replace_999_mean(tx):\n",
    "    nan_values = (tx==-999)*1\n",
    "    for col in range(tx.shape[1]):\n",
    "        column = tx[:,col][tx[:,col]!=-999]\n",
    "        median = np.median(column)\n",
    "        tx[:,col][tx[:,col]==-999] = median\n",
    "    return tx, nan_values\n",
    "\n",
    "def add_ones(tx):\n",
    "    return np.concatenate((np.ones([tx.shape[0],1]),tx), axis=1)\n",
    "\n",
    "def standardize(x):\n",
    "    \"\"\"Standardize the original data set.\"\"\"\n",
    "    mean_x = np.mean(x, axis=0)\n",
    "    x = x - mean_x\n",
    "    std_x = np.std(x, axis=0)\n",
    "    x = x / std_x\n",
    "    return x, mean_x, std_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-472073c1bfc1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpercentile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# return 50th percentile, e.g median.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "p = np.percentile(a, 50) # return 50th percentile, e.g median.\n",
    "print (p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tx_train, nan_values_train = replace_999_median(tx_train)\n",
    "tx_test, nan_values_test = replace_999_median(tx_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_train = standardize(tx_train)\n",
    "tx_train = tx_train[0]\n",
    "tx_test = standardize(tx_test)\n",
    "tx_test = tx_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (AAA!=1):\n",
    "    tx_train = add_ones(tx_train)\n",
    "    tx_test = add_ones(tx_test)\n",
    "    AAA = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.00000000e+00,   9.27252671e-13,   4.50019089e-15,\n",
       "        -3.48448848e-15,   7.19675786e-15,  -3.68138116e-12,\n",
       "         2.13543213e-12,   5.95433581e-13,   2.16429719e-14,\n",
       "         6.39742126e-15,   2.86409207e-15,  -7.00447966e-15,\n",
       "         4.45924897e-15,   4.29197803e-12,  -5.96492045e-15,\n",
       "         1.35646161e-16,   7.13136217e-17,   2.58030370e-14,\n",
       "        -1.06327391e-16,  -1.87188487e-16,   8.24369382e-15,\n",
       "         1.41040513e-16,  -9.00283004e-15,  -6.01698247e-16,\n",
       "        -2.95741174e-12,   1.46076658e-15,   2.17857554e-14,\n",
       "        -4.93485594e-12,   5.44553736e-15,  -2.56949984e-15,\n",
       "        -8.76751116e-16])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(tx_train,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 31)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tx_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_w = np.zeros(tx_train.shape[1])\n",
    "initial_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gamma = 0.05\n",
    "max_iters = 1000\n",
    "lambda_ = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for methods: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def func_least_squares (y, tx, test_set, fct='mse'):\n",
    "    name = 'least_squares'\n",
    "    w,loss = least_squares(y,tx,fct)\n",
    "    y_pred = predict_labels(w, test_set)\n",
    "    create_csv_submission(ids_test, y_pred, name)\n",
    "    \n",
    "    plt.plot(w, 'go')\n",
    "    plt.title('least squares weights for loss:  '+str(loss),fontsize=15, fontweight='bold');\n",
    "    plt.show()\n",
    "    \n",
    "    return w, loss\n",
    "\n",
    "def func_GD (y, tx, test_set, max_iters, gamma, initial_w):\n",
    "    name = 'Gradient_descent'\n",
    "    w,loss = least_squares_GD(y, tx, initial_w, max_iters, gamma,fct='mse');\n",
    "    y_pred = predict_labels(w, test_set)\n",
    "    create_csv_submission(ids_test, y_pred, name)\n",
    "    \n",
    "    plt.plot(w, 'go')\n",
    "    plt.title('GD: weights with loss:  '+str(loss),fontsize=15, fontweight='bold');\n",
    "    plt.show()\n",
    "    \n",
    "    return w, loss\n",
    "\n",
    "########################  RIDGE REGRESSION #######################################################\n",
    "def func_ridge_regression (y, tx, test_set, lambda_):\n",
    "    name = 'Ridge_regression'\n",
    "    \n",
    "    w,loss = ridge_regression(y, tx, lambda_=lambda_, fct='mse');\n",
    "    \n",
    "    y_pred = predict_labels(w, test_set)\n",
    "    create_csv_submission(ids_test, y_pred, name)\n",
    "    \n",
    "    plt.plot(w, 'go')\n",
    "    plt.title('Ridge regression: weights ;loss:  '+str(loss),fontsize=15, fontweight='bold');\n",
    "    plt.show()\n",
    "    \n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *LEAST-SQUARES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaoAAAEKCAYAAABDkxEYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3X+cXFV9//HXO7+AACb8iEiBZFHjDxREXTF+9auxqPyo\nCH6/asEoxGrX9ovfilUrmrYEayq2iilWaVfFQF1LabUVWgpiNEVbIy4UCQhIgBAiCEEg/IhAAp/+\ncc6wJ5OZ3dnNTubO7vv5eMxjZ8587r3n3Hvnfu4998ysIgIzM7OqmtLpCpiZmQ3HicrMzCrNicrM\nzCrNicrMzCrNicrMzCrNicrMzCptxEQlaZWkkLRuJ9RnuHrMlrQ0PxZ2si7WGkkr8r4zpu9ASFqX\np1/VQmxt3zhhLMsaZr6fkHSbpCdyXVaM5/wbLG9xbZ11+34uaW9JX5J0l6THJd0k6aOSprYw7VxJ\n/yjpVkmP5PW/Pu9Tz66L/W1J35f0yxz3qKT/lvQxSdPqYmdL+oKkO3Odbpf0aUkzG9ThWEk/yPN7\nWNL3JL2mQVxtP230OLwu9mRJP5J0X57vzXn5e9XF/Wlu06PFvN7VYNkfkPRfku6VtEXSQ5JWS+qr\niztM0hclXSfp/rxOr8vraJcibldJf5yX/QtJj+X2fUPScxos/1WSLpe0SdLm3LbjG8TNkfRZST/P\nbdoo6T8lnVgf21BEDPsAVgEBrBsptp0PoCfXI4ClnayLHy1vsxW1bTbG6dfl6Ve1EFvbN1aMY/1/\nq5jvuM+/yTIXF8ta2OltuAPt2A24rsH6C+DLLUy/oMm0AdwNzCpi/2aY2HOLuJnD1OkKYEoRezLw\nVIO4J4Ajm+ynjR6HF3EfHSZudd08H2wQ864G6+myYeb5sSLu9GHi/r2Ie9YwcZuA5xWxv5nXR6PY\nk4u4KcOs9wB+f6T9wV1/FSRp107XYTxExOKIUESo03UZo/Js+HW5LYt3dKaSZkia6J+904BD8/OP\nA3OAf86v3yfpVSNMfx/w/4HnkJLeS4Bb83vPIh0ka74PHAvsB+wO/G7x3juL531FnU4H9gT+OL9+\nA3ASQL4K+xwg4Dbg4FyPO4DpwLmSGu3T76nt78Xj2uL92hXRk8DrgH2Bq3LZKyUdUsR+DfgdYGmD\n5ZS+BSwE9gFm1cWXbQ/gm8CrSQn7daTEA3C0pFcUsbeQTpj2Bn4DuDyXPwP4YBF3Nml9PAgcRtou\ng/m9v5K0R37+EobW+1Wkdr+WtB4A3jNCG8d+RQW8ErgY+BUpq/4c+BNgehHzfNKKvB14BHgcWAv8\nBbB7Ebc7ace4Ffh1bvga4CuknXQxzbNx07NO0s73fWBjXvYvSGcgJ9Qt+yvAQ6QPx2dJO/Q286fJ\nmW6j9QMcB3wXuCsv91HgauD9dfVbUcyzF/hRjj8tvz+7WC9P5HZcCMyvm887gB8D9+f1tx74NvC/\nh1k37yyW/ZJc9sai7Lm57K1F2YK66X+Ut+uvSTvgbzdrX4Nlr83TfS/vJ7VlrGhwprqKdCD6aV6X\nPwZenmMWDrNvLN6B9bNqhHnuA/xVrmNt2/wT8OK6+TzdLuDDedlPAbObLHcxjfezVpc3YltbjGm4\n7Vp9kD6/QfpcTc1lRxRtO2cM8/xcMf2bR4j9VY67ryj7l2L6mbnsGUXZpbns8KLsL4rpzy7Kj2iw\nny4eoU7X5LgNRdmninm+fIT9Ybsrqgbxexbxg0X5Hg1iv1DEnpTLpgO71MW9vIi7vDg21couKmL/\noCh/Ry47rCj7VBG7IZetGbFdLTR8FdsfiI+i+SXfJUXc0U1iAviHIu7cYeL2ZQyJCphH+hA2muav\ni7iBBu/fVT9/Rpeozhqmvr9fxK0oyu8vnp+Wd7jrm8zjfnKyAl5F4y6KAD4yzHY9sIj7vVz2p0XZ\nu3PZX+bXj5JPQoBPDtO+jzRqX1F2ZIP6/qJ43ihR/QrYWjfNetKHauEwdVm8A+tn1TDznAXc3OT9\nRykOOE22bzCKRNXq8lppa6vro9G2a/UB7EI6Ww7g2qK8PLj9YBTzm0ZKHmvztLcAuzWJ3YN0RVVb\nTnlgvLwob5So7i7WUa3sM8X0ZaJ6f1G+LpfdRzouPgBcCryqrm6/l+OeJF1R7EM6Yah9BnZp0J5y\nfxg2UeX1u7SIf98I8V8uYl87TNyri7jzctn+RVl5LC8T1adzmUgntZHbuw9DV1QBfG7EfaCFnWQV\n2x+IazvMf5ISwq6kg2utgkfnuB7gTaRL8umkS8n+HPMUsE+Oq519XUS6wpkNvAI4E9izmFdt/ktb\nqPf/LeJ7gRnAXNLl/Yk55vkMfWivIR28XwTcWUw7lkS1INd/b9KHbC7piiqA6xodDIAfkroY9gIO\nIl2dBukK6yjSh//FwD25/MI8jw/n1w8Bz85xzwHeCxw1wjq6PU97fn59WbE+vpTL/jO/XplfH8xQ\n0vjrXN/ZwDdy2WZgr2YHu9zO2of1uDx9ebLQKFEF8Im8nK8VZa8pYrebfhzWz9Jivj1Nys8iHeze\nytAHb1WDegWpC+wZwAsoeh6GOTAtHM3yWmlrq+uj0bZr9cG2B7Eri/IpRfnNLc5rsG4d/gyY2yDu\nxXVxAXyhLuazxXsfIx1rlhRlT+S4vRg6Eb+VdIw7mLr9scl+Wj6eoC4BkLrO6k8UrqG4lzXM/tAw\nUQFvrpvfU8AfjbBe55NOcoJ0EjSlSdwUtk3wr8nlYuhY9ACpa28/4CdFbH8xn2eQerfKem4l5YNd\nR9wPWthRVlEciIHnNdko5eMzOXYX4M+Bm4DHGsQtyHH/ytBZxadJfbmH1NWjp5iulUTVW8R/C/gQ\nKWnuWcScXMS8uyg/sygfS6I6ELiAdGm7pa7NjzU6GACvqKv/f42wju/JcW9jaOdcAXyA1P888sZP\ndQxSt63yDrca+CVwLSm517bbGXmavhHqFcAxjQ52wFSGDgAri3o8p5i2UaK6m/xBAo4pYk8qYreb\nfhzWz9JivmWiqp0d/priLJihD+JW8hl/Mf31LR6Yt9vPWl1eK23dkfXR6oN0X6PWhjJRTS3Kb2px\nXvWJKoAbKAZT5LhGiSqAZUXMPNI+3my/fbSIPXuYuAA+WsSeDvwv0sF4P7Yd3LGqiHsn2x8PgtSD\nc0oL+0Oriaq2ffuaxM8l3XsL0glLsyQphi4sAvizuvf/oMFyy8cXc9wU4JImMVeSbzMMux+0sKOs\nYttE9eoRKhfkUT3AOSPE1T6IL6DxqJCryF0kjGHUH6n/d3PdPDeTL4lJZ1W18t8spnt/gzouri/L\n5T+oWz9TSAf5pu0upl1RlO9aV/dbRlh3W4rlfZXtPwAPAL81wvp5XxH/2vz3bFJf/la2vWf1+jzN\nkhHqFcCi+vbl1/sVMRcU9dilKG+UqH5YlL2+iF1clG83/Tisn6VFfE+DbbO+Lv7vivgD6up1YYv7\n7Hb7WavLa6WtO7I+Wn0w/l1/04FDSPd9a9N/uEnsnsBbGBo1txXYr3j/UODfSfdWN5J6Am7MsTcV\ncVNIV8C3k07W1gB/Wyz/xGHqO42hq5VHi/nV7pttAF6Y67qCocTSO8L+0ErX32KGTgYfpO5KiW2T\n1MM0uU9LSlJl1+DyJnG/m9ff43k/XV5Mc3qOOaEoW8FQr0LL96jGMvLovuL58th+lItIZ90Ab89/\nryddros0kmcbEXFTRBxGOrM+jnRF8ySp++zUWthoKxoRf0y6x/Vq0gZcTTrzPCeP7LmrCD+geP4b\nDWb3ePG8HJU3ry5uPmmUC6QDyezc7m+OUNfH6opq6/lXpJvR9et4Rp7uqYh4L6mdC0nr/ibSTrt8\nuGWSkmzNh/LfH+XHVFJ3LqSD2uq6ekEalFJfrykRMdBkeffleUHqHqo5aIR6bi2ej2o/2MH100xt\nHcwpv4NCupKGdNB5oG6a+u077strpa1tWh/biIjHSV10AM8uvjf1vCLsv0cxvy0R8TPSzf+a+U1i\nH46Ii0lXm5D244OL99dExDERsUdEzCFdFdT2v/8o4p6KiE9HxMERsWtEHMrQNn2K1IVNk9GbtYNy\nLRbgmaRbAZCuMm+MiIdJiRJSYnh987Uwsoh4MCJWkK44Id3bfGbtfUnzSBceB5NG/L0xIn5QNxvy\niMYvk05kIfWQnVYfl5f55Yh4YUTsEhHzSftSTW19vqAo+0ZEPBQRN5GupgBeLGnOcG0bS6L6Oeks\nA9Iw02Pyl8TmSDpR0lUMHbxn5L9bgEclPZ+hxPM0SX8k6a057nLSvaraB7vWgPuLSV4gafpwlZR0\nqKQ/IX04biCNkKp9OHYjnc2sZmhH+pCkA/IQ0d9pMMsNxfOj8jIWs/1Bdkbx/NfAE5LeSBq1NhqX\n5b/7AGdL2lfSbpJeKek8UncDkl4v6Q9JifZq4B9J2wiG1l1DEXEzcG9++Zb8979IiQpSNxuk0UO/\nzs+vYGidfSp/kXCGpB5JH6L4sDdY3pOkm6kACyW9SdJs0uCMHVU7iDxX0m61wh1ZP8P4Tv67K3CG\npGfkLzm+Npf/MCI2j3HeY15eK21tdX1oB7+szdABeE/go5L2Je+zde83/GK30heDT5I0L+9fz2Pb\nY8dtOW6P/EXW1yh9wXimpGMZOugH6cocSVPyF2R78jHrMNIJ5O6kE+MvFct/g6TX5XW9j6T3M3Qy\n962IqB0P3izpHyQtzMveD/hiniekzxOk/bN2THutpBdI2pNth5A/WCx/r7zO9ije3yMfB2blmOdK\n+oyk3lzPPZW+FPyiHF8byVyfpO4nfRdsNXVykvoK6Z4lwCcj4vT6uBz7inz83zsv/x2kWzeQjhm1\n48jdxWTvzLEvYGj/3Uq6Am2uhcvuVRRdW7nsWBr3tW7TTcLQPZDysbZ4vrBuGY0exxTLbTTyaVqT\nei8cZp6ri7hGdby7QR1nsO0gi4fz31rX4roY6qa4tW5+TzF0uR3FslfUlxXvzWKoS6LRY2mDroH6\nx4jdTaQEXotfn8tm1m3fs+qm+fNhlrluuPbReNRfub6/VsSuy2WrmmzXxUX55Wxfl+fuyPqhedff\nbJp3zW6muN9YlK8YaVs02J4LR7O8Vtra6vpotO1G82AUX/htsp3/ZZh63g7sXaybZnEB/FUxz2nD\nxH2krk6fahJ3C/CsIu6EYeb5CPDSIna4+16/JA8uq1snjR6rcszhw8QE8IdN9uVGj9rxpGeEuPLz\n/b4mMfcCLyzi9iR9B63ZPEf+AvhYElUuX0D67sV9pG6x9aQhmb8LzMgxtRFdD5D6gz9DytT1H8TF\npAPNL/K8fkU6E6n/Xs4C0n2r8r5Ts0R1IKlP+XrSmcpjpB38b9l2R9uddJn7UF7uchp8jyrHvox0\nRbCZlEQWNVo/pBu7V+a4tcC7aXzQ3q6srg2zSSOV1hbrZZD0IXp2jnkRcD7prPjhvMxbSMPK92w0\n37plfLBoa3mgKm9iH9tgukWkEYHlMv+ujG3WvjztrXmb/AfwmmJZZ49wAFtYxJaJ6vmk7p6Hivef\nuyPrhyaJKr+3L2nU4x2kpH4fadDOYXVxtenHnKhaXV4rbW11fTTbdqN5kHoDvkQ6EXmcdKL5UfL3\nqkbYzu8CVuZpnyCdcV9P+g7mnCJuF9IVzE9Jx5mtpM/J94BTABWxU4C/z8t7jNT9tZLG+/dxpJ6F\n+3PdbyN9j2vvurj98n6ymnSA3kK6pTAAPL8udirp83ZNbs8WUk/NBeTPc4N1MlyieibpfuONuS1b\nSSPx/g04bph9ebwS1QLS8W9j3kYbSMfSg5ocj79CyhNbSPvdT0lX2Q1HwJYP5ZlYIXfpfS2/fH1E\nrOpcbSaW/G31l5G6q56SNIN0AlPrAz8+0j0GMzMgXQqb7UyzSVdRj0naSLovUhucchlpGKuZ2dMm\n+u+NWfU8RPoZqHtJXRdPkbpCPgK8JXyJb2Z12tL1l39U9UpS//E04J8i4gxJB5MOUnuTDk7vjogn\n8pDbC0i/KfUr0r2pdeNeMTMz6zrtuqJ6nPQF2peQRqYcLWkB6V7E5yONt3+AoSGQ7yV9D+S5wOdz\nnJmZWfsHUyj9Q7IfAr9PGo3yrIjYmn/mf2lEHCXp8vz8R/mLuL8kjexpWrl99903enp62lp3M7OJ\n5uqrr74v0pedu0bbBlPkb6NfTRoi/EXScOQHI6L2KwMbGPo1iANI31EiJ7FNpKGt99XNs4/8qxdz\n585lcHAQMzNrnaQ7Ol2H0WrbYIqIeDIiDieNnz+C9NtW24Xlv43+Cdl2V1MR0R8RvRHRO2dOV50Q\nmJnZGLV91F9EPEj6UtgCYHbu2oOUwGq/tbeB/FNE+f1ZbPuTSWZmNkm1JVHl3/2bnZ/vRvpPuzeS\nfjngbTnsFNIvW0D6T8Gn5OdvA77nYcpmZgbtu0e1P3B+vk81hfSviv9V0s+ACyV9ivQDsV/N8V8F\n/k7SWtKV1IltqpeZmXWZtiSqiLgOeGmD8ttI96vqyx9j6F+CmJmZPc2/TGFm1gUG1gzQs7yHKWdO\noWd5DwNrmv3bt4nHv/VnZlZxA2sG6Lukj81b0r85u2PTHfRdkv4/7aJDF3WyajuFr6jMzCpuycol\nTyepms1bNrNk5ZIO1WjncqIyM6u49ZvWj6p8onGiMjOruLmz5o6qfKJxojIzq7hlRy5j5vSZ25TN\nnD6TZUcu61CNdi4nKjOzilt06CL6j+tn3qx5CDFv1jz6j+ufFAMpYCf8enq79Pb2hn+U1sxsdCRd\nHRG9na7HaPiKyszMKs2JyszMKs2JyszMKs2JyszMKs2JyszMKs2JyszMKs2JyszMKs2JyszMKs2J\nyszMKs2JyszMKs2JyszMKs2JyszMKs2JyszMKs2JyszMKs2JyszMKs2JyszMKq0tiUrSQZK+L+lG\nSTdI+mAu31vSFZJuyX/3yuWSdI6ktZKuk/SydtTLzMy6T7uuqLYCH46IFwILgFMlHQKcDqyMiPnA\nyvwa4Bhgfn70Aee2qV5mZtZl2pKoIuLuiLgmP38YuBE4ADgeOD+HnQ+ckJ8fD1wQyWpgtqT921E3\nMzPrLm2/RyWpB3gp8GNgv4i4G1IyA56Zww4A7iwm25DL6ufVJ2lQ0uDGjRvbWW0zM6uItiYqSXsA\n3wROi4iHhgttUBbbFUT0R0RvRPTOmTNnvKppZmYV1rZEJWk6KUkNRMS3cvE9tS69/PfeXL4BOKiY\n/EDgrnbVzczMuke7Rv0J+CpwY0ScXbx1MXBKfn4K8O2i/OQ8+m8BsKnWRWhmZpPbtDbN99XAu4E1\nkq7NZZ8AzgIukvReYD3w9vzepcCxwFpgM/CeNtXLzMy6TFsSVUT8kMb3nQCObBAfwKntqIuZmXU3\n/zKFmZlVmhOVmZlVmhOVmZlVmhOVmZlVmhOVmZlVmhOVmZlVmhOVmZlVmhOVmZlVmhOVmZlVmhOV\nmZlVmhOVmZlVmhOVmZlVmhOVmZlVmhOVmZlVmhOVmZlVmhOVmZlVmhOVmZlVmhOVmZlVmhOVmZlV\nmhOVmZlVmhOVmZlVmhOVmZlVmhOVmZlVmhOVmZlVWlsSlaTzJN0r6fqibG9JV0i6Jf/dK5dL0jmS\n1kq6TtLL2lEnMzPrTu26oloBHF1XdjqwMiLmAyvza4BjgPn50Qec26Y6mZlZF2pLooqIK4H764qP\nB87Pz88HTijKL4hkNTBb0v7tqJeZmXWfnXmPar+IuBsg/31mLj8AuLOI25DLzMzMKjGYQg3KomGg\n1CdpUNLgxo0b21wtMzOrgp2ZqO6pdenlv/fm8g3AQUXcgcBdjWYQEf0R0RsRvXPmzGlrZc3MrBp2\nZqK6GDglPz8F+HZRfnIe/bcA2FTrIjQzM5vWjplK+ntgIbCvpA3AGcBZwEWS3gusB96ewy8FjgXW\nApuB97SjTmZm1p3akqgi4qQmbx3ZIDaAU9tRDzMz635VGExhZmbWlBOVmZlVmhOVmZlVmhOVmZlV\nmhOV2QgG1gzQs7yHKWdOoWd5DwNrBib0cs2qpi2j/swmioE1A/Rd0sfmLZsBuGPTHfRd0gfAokMX\nTbjlmlWRr6jMhrFk5ZKnk0XN5i2bWbJyyYRcrlkVOVGZDWP9pvWjKu/25ZpVkROV2TDmzpo7qvJu\nX65ZFTlRmQ1j2ZHLmDl95jZlM6fPZNmRyybkcs2qyInKbBiLDl1E/3H9zJs1DyHmzZpH/3H9bR/Q\n0KnlmlWR0k/tdZ/e3t4YHBzsdDXMzLqKpKsjorfT9RgNX1GZmVmlOVGZmVmlOVGZmVmlOVGZmVml\nOVGZmVmlOVGZmVmlOVGZmVmlOVGZmVmlOVGZmVmlOVGZmVmlOVGZmVmlOVGZmVmlOVGZmVmlVSZR\nSTpa0s2S1ko6vR3LGFgzQM/yHqacOYWe5T0MrBmoVFw31LEdbW5V1es4GdeNPyvV2w8noqlLly7t\ndB2QNBW4DDgK+DRwzplnnnnl0qVLNzabpr+/f2lfX1/LyxhYM0DfJX3ct/k+ADY9vonL1l5Gz+we\nDtvvsI7HdUMd29HmVlW9jpNx3fizUr39sBVnnnnm3UuXLu1vy8zbpBL/j0rSq4ClEXFUfv1xgIj4\ndLNpRvv/qHqW93DHpju2K583ax7rTlvX8bhuqGM72tyqqtdxMq4bf1Z2Xtx48v+jGrsDgDuL1xty\n2TYk9UkalDS4cWPTi62G1m9a31J5p+K6oY7taHOrql7Hybhu/FnZeXGTXVUSlRqUbXepFxH9EdEb\nEb1z5swZ1QLmzprbUnmn4rqhju1oc6uqXsfJuG78Wdl5cZNdVRLVBuCg4vWBwF3juYBlRy5j5vSZ\n25TNnD6TZUcuq0RcN9SxHW1uVdXrOBnXjT8rOy9u0ouIjj+AacBtwMHADOCnwIuGm+blL395jNbX\nr/t6zPv8vNBSxbzPz4uvX/f1SsV1Qx3b0eZWVb2Ok3Hd+LNSvf1wJMBgVOC4P5pHJQZTAEg6FlgO\nTAXOi4hhTylGO5jCzMy6czDFtE5XoCYiLgUu7XQ9zMysWqpyj8rMzKwhJyozM6s0JyozM6s0Jyoz\nM6s0JyozM6s0JyozM6s0JyozM6s0JyozM6s0JyozM6s0JyozM6s0JyozM6s0JyozM6s0JyozM6s0\nJyozM6s0JyozM6s0JyozM6s0JyozM6s0JyozM6s0JyozM6s0JyozM6s0JyozM6s0JyozM6s0Jyoz\nM6s0JyozM6u0cU9Ukt4u6QZJT0nqrXvv45LWSrpZ0lFF+dG5bK2k08e7TmZm1r3acUV1PfB/gCvL\nQkmHACcCLwKOBr4kaaqkqcAXgWOAQ4CTcqyZmRnTxnuGEXEjgKT6t44HLoyIx4HbJa0FjsjvrY2I\n2/J0F+bYn4133czMrPvszHtUBwB3Fq835LJm5duR1CdpUNLgxo0b21ZRMzOrjjFdUUn6LvCsBm8t\niYhvN5usQVnQOFlGoxlERD/QD9Db29swxszMJpYxJaqIeMMYJtsAHFS8PhC4Kz9vVm5mZpPczuz6\nuxg4UdIukg4G5gNXAT8B5ks6WNIM0oCLi3divczMrMLGfTCFpLcCXwDmAP8m6dqIOCoibpB0EWmQ\nxFbg1Ih4Mk/zAeByYCpwXkTcMN71MjOz7qSI7rzV09vbG4ODg52uhplZV5F0dUT0jhxZHf5lCjMz\nqzQnKjMzqzQnKjMzqzQnKjMzqzQnKjMzqzQnKjMzqzQnKjMzqzQnKjMzqzQnKjMzqzQnKjMzqzQn\nKjMzqzQnKjMzqzQnKjMzqzQnKjMzqzQnKjMzqzQnKjMzqzQnKjMzqzQnKjMzqzQnKjMzqzQnKjMz\nqzQnKjMzqzQnKjMzqzQnKjMzqzQnKjMzq7RxT1SS/lLSTZKuk/TPkmYX731c0lpJN0s6qig/Opet\nlXT6eNfJzMy6VzuuqK4AXhwRhwE/Bz4OIOkQ4ETgRcDRwJckTZU0FfgicAxwCHBSjjUzMxv/RBUR\n34mIrfnlauDA/Px44MKIeDwibgfWAkfkx9qIuC0ingAuzLFmZmZtv0f1O8C/5+cHAHcW723IZc3K\nzczMmDaWiSR9F3hWg7eWRMS3c8wSYCswUJusQXzQOFlGk+X2AX0Ac+fOHWWtzcysG40pUUXEG4Z7\nX9IpwJuBIyOilnQ2AAcVYQcCd+Xnzcrrl9sP9AP09vY2TGZmZjaxtGPU39HAx4C3RMTm4q2LgRMl\n7SLpYGA+cBXwE2C+pIMlzSANuLh4vOtlZmbdaUxXVCP4a2AX4ApJAKsj4vci4gZJFwE/I3UJnhoR\nTwJI+gBwOTAVOC8ibmhDvczMrAtpqGeuu/T29sbg4GCnq2Fm1lUkXR0RvZ2ux2j4lynMzKzSnKjM\nzKzSnKjMzKzSnKjMzKzSnKjMzKzSnKjMzKzSnKjMzKzSnKjMzKzSnKjMzKzSnKjMzKzSnKjMzKzS\nnKjMzKzSnKjMzKzSnKjMzKzSnKjMzKzSnKjMzKzSnKjMzKzSnKjMzKzSnKjMzKzSnKjMzKzSnKjM\nzKzSnKjMzKzSnKjMzKzSnKjMzKzSnKjMzKzSxj1RSfozSddJulbSdyT9Ri6XpHMkrc3vv6yY5hRJ\nt+THKeNdJzMz617tuKL6y4g4LCIOB/4V+NNcfgwwPz/6gHMBJO0NnAG8EjgCOEPSXm2ol5mZdaFx\nT1QR8VDxcncg8vPjgQsiWQ3MlrQ/cBRwRUTcHxEPAFcAR493vczMrDtNa8dMJS0DTgY2Aa/PxQcA\ndxZhG3JZs/JG8+0jXY0xd+7c8a20mZlV0piuqCR9V9L1DR7HA0TEkog4CBgAPlCbrMGsYpjy7Qsj\n+iOiNyJ658yZM5aqm5lZlxnTFVVEvKHF0G8A/0a6B7UBOKh470Dgrly+sK581VjqZWZmE087Rv3N\nL16+BbgpP78YODmP/lsAbIqIu4HLgTdJ2isPonhTLjMzM2vLqL+zcjfgdaSk88FcfilwG7AW+DLw\n/wAi4n7gz4Cf5Mcnc5lNAgNrBuhZ3sOUM6fQs7yHgTUDna6SmVWMIhreDqq83t7eGBwc7HQ1bAcM\nrBmg75I+Nm/Z/HTZzOkz6T+un0WHLupgzcwmLklXR0Rvp+sxGv5lCuuYJSuXbJOkADZv2cySlUs6\nVCMzqyJk0ejGAAAEUElEQVQnKuuY9ZvWj6rczCYnJyrrmLmzGn8Xrlm5mU1OTlTWMcuOXMbM6TO3\nKZs5fSbLjlzWoRqZWRU5UVnHLDp0Ef3H9TNv1jyEmDdrngdSmNl2POrPzGwS8ag/MzOzceZEZWZm\nleZEZWZmleZEZWZmleZEZWZmlda1o/4kbQTuGOPk+wL3jWN1OmmitGWitAPclqqaKG3Z0XbMi4iu\n+od+XZuodoSkwW4bntnMRGnLRGkHuC1VNVHaMlHaMRru+jMzs0pzojIzs0qbrImqv9MVGEcTpS0T\npR3gtlTVRGnLRGlHyyblPSozM+sek/WKyszMuoQTlZmZVdqkS1SSjpZ0s6S1kk7vdH3GStI6SWsk\nXSupq35GXtJ5ku6VdH1RtrekKyTdkv/u1ck6tqpJW5ZK+kXeNtdKOraTdWyFpIMkfV/SjZJukPTB\nXN5122WYtnTjdtlV0lWSfprbcmYuP1jSj/N2+QdJMzpd13aaVPeoJE0Ffg68EdgA/AQ4KSJ+1tGK\njYGkdUBvRHTdFxglvRZ4BLggIl6cy/4CuD8izsonEHtFxMc6Wc9WNGnLUuCRiPhsJ+s2GpL2B/aP\niGsk7QlcDZwALKbLtsswbXkH3bddBOweEY9Img78EPgg8IfAtyLiQkl/A/w0Is7tZF3babJdUR0B\nrI2I2yLiCeBC4PgO12nSiYgrgfvrio8Hzs/PzycdWCqvSVu6TkTcHRHX5OcPAzcCB9CF22WYtnSd\nSB7JL6fnRwC/CfxTLu+K7bIjJluiOgC4s3i9gS7dgUk763ckXS2pr9OVGQf7RcTdkA40wDM7XJ8d\n9QFJ1+Wuwcp3l5Uk9QAvBX5Ml2+XurZAF24XSVMlXQvcC1wB3Ao8GBFbc0g3H8daMtkSlRqUdWvf\n56sj4mXAMcCpuQvKquFc4DnA4cDdwOc6W53WSdoD+CZwWkQ81On67IgGbenK7RIRT0bE4cCBpF6h\nFzYK27m12rkmW6LaABxUvD4QuKtDddkhEXFX/nsv8M+kHbib3ZPvLdTuMdzb4fqMWUTckw8uTwFf\npku2Tb4H8k1gICK+lYu7crs0aku3bpeaiHgQWAUsAGZLmpbf6trjWKsmW6L6CTA/j5iZAZwIXNzh\nOo2apN3zTWIk7Q68Cbh++Kkq72LglPz8FODbHazLDqkd2LO30gXbJt+0/ypwY0ScXbzVddulWVu6\ndLvMkTQ7P98NeAPpntv3gbflsK7YLjtiUo36A8hDUpcDU4HzImJZh6s0apKeTbqKApgGfKOb2iHp\n74GFpH9XcA9wBvAvwEXAXGA98PaIqPwghSZtWUjqXgpgHfD+2n2eqpL0GuAHwBrgqVz8CdK9na7a\nLsO05SS6b7scRhosMZV0YXFRRHwyHwMuBPYG/ht4V0Q83rmattekS1RmZtZdJlvXn5mZdRknKjMz\nqzQnKjMzqzQnKjMzqzQnKjMzqzQnKjMzqzQnKjMzq7T/ASaI2ZHu1AGQAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f99a97896a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w, loss = func_least_squares (y=y_train, tx=tx_train, test_set=tx_test, fct='mse');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74.499200000000002"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_train = predict_labels(w, tx_train)\n",
    "right_train = np.sum(y_pred_train == y_train)/len(y_train)*100\n",
    "right_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Gradient descent     -------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/499): loss=0.5000000000000249, w0=-0.031466400000014945, w1=0.002504206625732244\n",
      "Gradient Descent(1/499): loss=0.4494343967425655, w0=-0.0597861600000126, w1=0.004438498929019952\n",
      "Gradient Descent(2/499): loss=0.4237550966541934, w0=-0.08527394400001406, w1=0.006020285830294936\n",
      "Gradient Descent(3/499): loss=0.4070187761061894, w0=-0.108212949600019, w1=0.0072840399711404345\n",
      "Gradient Descent(4/499): loss=0.3951946905073953, w0=-0.12885805464002748, w1=0.008234547940797575\n",
      "Gradient Descent(5/499): loss=0.38648993849775537, w0=-0.14743864917603822, w1=0.008883836358338948\n",
      "Gradient Descent(6/499): loss=0.3798896347830327, w0=-0.16416118425845092, w1=0.009255089133456313\n",
      "Gradient Descent(7/499): loss=0.37476690775624144, w0=-0.17921146583262498, w1=0.009378902879218826\n",
      "Gradient Descent(8/499): loss=0.37071472549275486, w0=-0.19275671924938384, w1=0.00928903699482596\n",
      "Gradient Descent(9/499): loss=0.3674580691602446, w0=-0.20494744732446882, w1=0.009019283091522143\n",
      "Gradient Descent(10/499): loss=0.36480472393657815, w0=-0.21591910259204714, w1=0.008601519728930907\n",
      "Gradient Descent(11/499): loss=0.36261648374655586, w0=-0.22579359233286914, w1=0.008064668317610603\n",
      "Gradient Descent(12/499): loss=0.36079163177278045, w0=-0.23468063309961046, w1=0.007434256491318512\n",
      "Gradient Descent(13/499): loss=0.3592538737238359, w0=-0.24267896978967884, w1=0.006732362016934813\n",
      "Gradient Descent(14/499): loss=0.3579450962900165, w0=-0.24987747281074155, w1=0.00597777882561493\n",
      "Gradient Descent(15/499): loss=0.35682047490212365, w0=-0.25635612552969916, w1=0.0051863002497362\n",
      "Gradient Descent(16/499): loss=0.35584507472245464, w0=-0.262186912976762, w1=0.00437105261960692\n",
      "Gradient Descent(17/499): loss=0.35499143157761803, w0=-0.26743462167911947, w1=0.0035428382968072975\n",
      "Gradient Descent(18/499): loss=0.35423779479871625, w0=-0.27215755951124215, w1=0.002710464396935861\n",
      "Gradient Descent(19/499): loss=0.3535668286601078, w0=-0.2764082035601534, w1=0.0018810445772838272\n",
      "Gradient Descent(20/499): loss=0.3529646386535219, w0=-0.2802337832041743, w1=0.0010602682903819998\n",
      "Gradient Descent(21/499): loss=0.3524200322832165, w0=-0.2836768048837939, w1=0.00025263619990395936\n",
      "Gradient Descent(22/499): loss=0.35192395198892207, w0=-0.28677552439545223, w1=-0.0005383370412434682\n",
      "Gradient Descent(23/499): loss=0.3514690362130231, w0=-0.2895643719559454, w1=-0.0013099501045215718\n",
      "Gradient Descent(24/499): loss=0.35104927705013034, w0=-0.2920743347603899, w1=-0.002060169229780778\n",
      "Gradient Descent(25/499): loss=0.35065975147295947, w0=-0.2943333012843906, w1=-0.0027875060716798417\n",
      "Gradient Descent(26/499): loss=0.3502964091307315, w0=-0.29636637115599185, w1=-0.0034909159166575904\n",
      "Gradient Descent(27/499): loss=0.34995590399724485, w0=-0.2981961340404335, w1=-0.004169713277853097\n",
      "Gradient Descent(28/499): loss=0.3496354602447124, w0=-0.29984292063643153, w1=-0.004823502181765157\n",
      "Gradient Descent(29/499): loss=0.3493327649931605, w0=-0.30132502857283033, w1=-0.005452118787255545\n",
      "Gradient Descent(30/499): loss=0.3490458822739176, w0=-0.3026589257155898, w1=-0.006055584297156151\n",
      "Gradient Descent(31/499): loss=0.34877318381384237, w0=-0.30385943314407377, w1=-0.006634066420066071\n",
      "Gradient Descent(32/499): loss=0.34851329320881186, w0=-0.3049398898297099, w1=-0.007187847907624596\n",
      "Gradient Descent(33/499): loss=0.3482650407910446, w0=-0.30591230084678284, w1=-0.007717300928190996\n",
      "Gradient Descent(34/499): loss=0.3480274270626065, w0=-0.30678747076214896, w1=-0.008222866241914055\n",
      "Gradient Descent(35/499): loss=0.34779959300848823, w0=-0.30757512368597895, w1=-0.00870503631670072\n",
      "Gradient Descent(36/499): loss=0.34758079594734814, w0=-0.3082840113174264, w1=-0.009164341672451323\n",
      "Gradient Descent(37/499): loss=0.34737038984888213, w0=-0.30892201018572957, w1=-0.009601339865259552\n",
      "Gradient Descent(38/499): loss=0.34716780926062063, w0=-0.30949620916720283, w1=-0.01001660662719802\n",
      "Gradient Descent(39/499): loss=0.34697255615646283, w0=-0.3100129882505292, w1=-0.010410728763756885\n",
      "Gradient Descent(40/499): loss=0.3467841891540662, w0=-0.31047808942552335, w1=-0.010784298482628574\n",
      "Gradient Descent(41/499): loss=0.34660231465579394, w0=-0.3108966804830185, w1=-0.011137908886685415\n",
      "Gradient Descent(42/499): loss=0.3464265795539457, w0=-0.31127341243476453, w1=-0.011472150412720133\n",
      "Gradient Descent(43/499): loss=0.346256665209985, w0=-0.31161247119133634, w1=-0.011787608037560224\n",
      "Gradient Descent(44/499): loss=0.3460922824728801, w0=-0.3119176240722514, w1=-0.012084859106013864\n",
      "Gradient Descent(45/499): loss=0.345933167546257, w0=-0.3121922616650753, w1=-0.012364471662006231\n",
      "Gradient Descent(46/499): loss=0.3457790785500007, w0=-0.31243943549861725, w1=-0.012627003186269232\n",
      "Gradient Descent(47/499): loss=0.34562979265092386, w0=-0.31266189194880534, w1=-0.012872999661926081\n",
      "Gradient Descent(48/499): loss=0.34548510366056673, w0=-0.312862102753975, w1=-0.013102994903987579\n",
      "Gradient Descent(49/499): loss=0.3453448200171256, w0=-0.3130422924786281, w1=-0.01331751010074708\n",
      "Gradient Descent(50/499): loss=0.34520876308387344, w0=-0.3132044632308162, w1=-0.013517053524818022\n",
      "Gradient Descent(51/499): loss=0.3450767657088646, w0=-0.31335041690778587, w1=-0.013702120379506253\n",
      "Gradient Descent(52/499): loss=0.3449486710008176, w0=-0.3134817752170589, w1=-0.013873192752681522\n",
      "Gradient Descent(53/499): loss=0.34482433128427203, w0=-0.313599997695405, w1=-0.014030739655580028\n",
      "Gradient Descent(54/499): loss=0.3447036072037792, w0=-0.3137063979259168, w1=-0.014175217128255967\n",
      "Gradient Descent(55/499): loss=0.34458636695231337, w0=-0.3138021581333778, w1=-0.014307068396885834\n",
      "Gradient Descent(56/499): loss=0.3444724856035177, w0=-0.313888342320093, w1=-0.014426724070963766\n",
      "Gradient Descent(57/499): loss=0.34436184453099616, w0=-0.313965908088137, w1=-0.014534602370730303\n",
      "Gradient Descent(58/499): loss=0.34425433090080926, w0=-0.31403571727937696, w1=-0.014631109377049416\n",
      "Gradient Descent(59/499): loss=0.344149837225743, w0=-0.31409854555149325, w1=-0.0147166392974698\n",
      "Gradient Descent(60/499): loss=0.34404826097188024, w0=-0.3141550909963982, w1=-0.014791574743442027\n",
      "Gradient Descent(61/499): loss=0.343949504209624, w0=-0.314205981896813, w1=-0.014856287014666195\n",
      "Gradient Descent(62/499): loss=0.3438534733026436, w0=-0.3142517837071866, w1=-0.014911136387358881\n",
      "Gradient Descent(63/499): loss=0.3437600786293, w0=-0.31429300533652316, w1=-0.014956472403888437\n",
      "Gradient Descent(64/499): loss=0.34366923433200813, w0=-0.3143301048029264, w1=-0.014992634161763274\n",
      "Gradient Descent(65/499): loss=0.34358085809072125, w0=-0.31436349432268956, w1=-0.015019950600391406\n",
      "Gradient Descent(66/499): loss=0.34349487091734343, w0=-0.3143935448904767, w1=-0.015038740784380727\n",
      "Gradient Descent(67/499): loss=0.3434111969683685, w0=-0.31442059040148546, w1=-0.01504931418243348\n",
      "Gradient Descent(68/499): loss=0.34332976337346977, w0=-0.3144449313613936, w1=-0.01505197094111771\n",
      "Gradient Descent(69/499): loss=0.3432505000781074, w0=-0.31446683822531124, w1=-0.015047002152983496\n",
      "Gradient Descent(70/499): loss=0.34317333969850744, w0=-0.3144865544028374, w1=-0.015034690118640651\n",
      "Gradient Descent(71/499): loss=0.3430982173876096, w0=-0.31450429896261123, w1=-0.015015308602534116\n",
      "Gradient Descent(72/499): loss=0.3430250707107826, w0=-0.31452026906640795, w1=-0.014989123082249086\n",
      "Gradient Descent(73/499): loss=0.3429538395302699, w0=-0.31453464215982524, w1=-0.014956390991254278\n",
      "Gradient Descent(74/499): loss=0.34288446589747545, w0=-0.3145475779439011, w1=-0.014917361955052458\n",
      "Gradient Descent(75/499): loss=0.342816893952313, w0=-0.3145592201495696, w1=-0.014872278020755293\n",
      "Gradient Descent(76/499): loss=0.3427510698289486, w0=-0.31456969813467156, w1=-0.014821373880137202\n",
      "Gradient Descent(77/499): loss=0.34268694156734736, w0=-0.31457912832126356, w1=-0.01476487708625214\n",
      "Gradient Descent(78/499): loss=0.3426244590301106, w0=-0.3145876154891966, w1=-0.014703008263719638\n",
      "Gradient Descent(79/499): loss=0.3425635738241476, w0=-0.31459525394033666, w1=-0.01463598131280349\n",
      "Gradient Descent(80/499): loss=0.34250423922678397, w0=-0.31460212854636294, w1=-0.014564003607418883\n",
      "Gradient Descent(81/499): loss=0.34244641011595006, w0=-0.3146083156917868, w1=-0.01448727618721283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(82/499): loss=0.34239004290413655, w0=-0.31461388412266855, w1=-0.014405993943868886\n",
      "Gradient Descent(83/499): loss=0.34233509547582847, w0=-0.31461889571046237, w1=-0.014320345801790816\n",
      "Gradient Descent(84/499): loss=0.3422815271281734, w0=-0.31462340613947704, w1=-0.014230514893322001\n",
      "Gradient Descent(85/499): loss=0.34222929851464656, w0=-0.3146274655255905, w1=-0.014136678728657745\n",
      "Gradient Descent(86/499): loss=0.34217837159151865, w0=-0.31463111897309287, w1=-0.014039009360607143\n",
      "Gradient Descent(87/499): loss=0.34212870956692654, w0=-0.3146344070758452, w1=-0.013937673544359494\n",
      "Gradient Descent(88/499): loss=0.3420802768523936, w0=-0.31463736636832257, w1=-0.013832832892408138\n",
      "Gradient Descent(89/499): loss=0.3420330390166304, w0=-0.3146400297315524, w1=-0.01372464402478169\n",
      "Gradient Descent(90/499): loss=0.3419869627414883, w0=-0.3146424267584595, w1=-0.013613258714729596\n",
      "Gradient Descent(91/499): loss=0.3419420157799274, w0=-0.3146445840826761, w1=-0.013498824030005345\n",
      "Gradient Descent(92/499): loss=0.3418981669158857, w0=-0.3146465256744712, w1=-0.013381482469887074\n",
      "Gradient Descent(93/499): loss=0.3418553859259397, w0=-0.31464827310708704, w1=-0.013261372098071449\n",
      "Gradient Descent(94/499): loss=0.3418136435426556, w0=-0.3146498457964415, w1=-0.013138626671572779\n",
      "Gradient Descent(95/499): loss=0.34177291141953503, w0=-0.31465126121686077, w1=-0.013013375765755411\n",
      "Gradient Descent(96/499): loss=0.34173316209747634, w0=-0.3146525350952383, w1=-0.0128857448956235\n",
      "Gradient Descent(97/499): loss=0.3416943689726636, w0=-0.3146536815857783, w1=-0.012755855633488303\n",
      "Gradient Descent(98/499): loss=0.3416565062658123, w0=-0.3146547134272645, w1=-0.012623825723129289\n",
      "Gradient Descent(99/499): loss=0.34161954899270497, w0=-0.3146556420846022, w1=-0.012489769190561559\n",
      "Gradient Descent(100/499): loss=0.3415834729359442, w0=-0.31465647787620643, w1=-0.012353796451518273\n",
      "Gradient Descent(101/499): loss=0.34154825461787175, w0=-0.31465723008865043, w1=-0.012216014415753245\n",
      "Gradient Descent(102/499): loss=0.34151387127458815, w0=-0.3146579070798502, w1=-0.012076526588265182\n",
      "Gradient Descent(103/499): loss=0.3414803008310273, w0=-0.3146585163719302, w1=-0.011935433167541677\n",
      "Gradient Descent(104/499): loss=0.3414475218770272, w0=-0.3146590647348024, w1=-0.011792831140917679\n",
      "Gradient Descent(105/499): loss=0.3414155136443596, w0=-0.31465955826138753, w1=-0.011648814377139917\n",
      "Gradient Descent(106/499): loss=0.34138425598466615, w0=-0.31466000243531433, w1=-0.011503473716225603\n",
      "Gradient Descent(107/499): loss=0.3413537293482621, w0=-0.31466040219184865, w1=-0.011356897056700685\n",
      "Gradient Descent(108/499): loss=0.3413239147637689, w0=-0.31466076197272974, w1=-0.011209169440300036\n",
      "Gradient Descent(109/499): loss=0.3412947938185382, w0=-0.3146610857755229, w1=-0.011060373134209005\n",
      "Gradient Descent(110/499): loss=0.34126634863982885, w0=-0.3146613771980369, w1=-0.0109105877109232\n",
      "Gradient Descent(111/499): loss=0.34123856187670776, w0=-0.31466163947829967, w1=-0.01075989012580051\n",
      "Gradient Descent(112/499): loss=0.3412114166826409, w0=-0.3146618755305364, w1=-0.010608354792377062\n",
      "Gradient Descent(113/499): loss=0.34118489669874463, w0=-0.3146620879775496, w1=-0.010456053655516105\n",
      "Gradient Descent(114/499): loss=0.34115898603766714, w0=-0.3146622791798616, w1=-0.010303056262456704\n",
      "Gradient Descent(115/499): loss=0.34113366926807703, w0=-0.3146624512619426, w1=-0.010149429831826623\n",
      "Gradient Descent(116/499): loss=0.34110893139972986, w0=-0.31466260613581565, w1=-0.009995239320681734\n",
      "Gradient Descent(117/499): loss=0.34108475786908843, w0=-0.31466274552230156, w1=-0.009840547489632109\n",
      "Gradient Descent(118/499): loss=0.3410611345254764, w0=-0.31466287097013906, w1=-0.009685414966112917\n",
      "Gradient Descent(119/499): loss=0.34103804761773976, w0=-0.31466298387319297, w1=-0.009529900305856343\n",
      "Gradient Descent(120/499): loss=0.34101548378139623, w0=-0.31466308548594163, w1=-0.009374060052618757\n",
      "Gradient Descent(121/499): loss=0.34099343002625454, w0=-0.3146631769374156, w1=-0.009217948796215673\n",
      "Gradient Descent(122/499): loss=0.34097187372448046, w0=-0.3146632592437423, w1=-0.009061619228915168\n",
      "Gradient Descent(123/499): loss=0.3409508025990954, w0=-0.31466333331943647, w1=-0.00890512220023883\n",
      "Gradient Descent(124/499): loss=0.3409302047128881, w0=-0.31466339998756143, w1=-0.008748506770217651\n",
      "Gradient Descent(125/499): loss=0.3409100684577202, w0=-0.314663459988874, w1=-0.008591820261148728\n",
      "Gradient Descent(126/499): loss=0.3408903825442181, w0=-0.3146635139900555, w1=-0.0084351083078971\n",
      "Gradient Descent(127/499): loss=0.34087113599182517, w0=-0.314663562591119, w1=-0.008278414906785644\n",
      "Gradient Descent(128/499): loss=0.3408523181192087, w0=-0.3146636063320763, w1=-0.008121782463114523\n",
      "Gradient Descent(129/499): loss=0.34083391853500256, w0=-0.31466364569893795, w1=-0.007965251837350374\n",
      "Gradient Descent(130/499): loss=0.34081592712887326, w0=-0.31466368112911364, w1=-0.007808862390024077\n",
      "Gradient Descent(131/499): loss=0.3407983340629002, w0=-0.3146637130162719, w1=-0.0076526520253747095\n",
      "Gradient Descent(132/499): loss=0.3407811297632528, w0=-0.31466374171471445, w1=-0.007496657233776151\n",
      "Gradient Descent(133/499): loss=0.3407643049121562, w0=-0.3146637675433129, w1=-0.007340913132981525\n",
      "Gradient Descent(134/499): loss=0.3407478504401334, w0=-0.3146637907890516, w1=-0.007185453508219668\n",
      "Gradient Descent(135/499): loss=0.3407317575185148, w0=-0.31466381171021657, w1=-0.007030310851176648\n",
      "Gradient Descent(136/499): loss=0.3407160175522008, w0=-0.31466383053926517, w1=-0.006875516397894339\n",
      "Gradient Descent(137/499): loss=0.3407006221726736, w0=-0.31466384748540904, w1=-0.006721100165617111\n",
      "Gradient Descent(138/499): loss=0.34068556323124377, w0=-0.31466386273693864, w1=-0.00656709098861662\n",
      "Gradient Descent(139/499): loss=0.3406708327925259, w0=-0.3146638764633154, w1=-0.00641351655302385\n",
      "Gradient Descent(140/499): loss=0.3406564231281317, w0=-0.31466388881705465, w1=-0.006260403430696573\n",
      "Gradient Descent(141/499): loss=0.34064232671057637, w0=-0.3146638999354201, w1=-0.006107777112149621\n",
      "Gradient Descent(142/499): loss=0.340628536207388, w0=-0.3146639099419491, w1=-0.005955662038574398\n",
      "Gradient Descent(143/499): loss=0.34061504447540836, w0=-0.3146639189478253, w1=-0.0058040816329734\n",
      "Gradient Descent(144/499): loss=0.3406018445552905, w0=-0.314663927053114, w1=-0.005653058330434565\n",
      "Gradient Descent(145/499): loss=0.3405889296661675, w0=-0.314663934347874, w1=-0.005502613607569661\n",
      "Gradient Descent(146/499): loss=0.34057629320050486, w0=-0.3146639409131581, w1=-0.005352768011140102\n",
      "Gradient Descent(147/499): loss=0.34056392871911567, w0=-0.31466394682191384, w1=-0.005203541185892918\n",
      "Gradient Descent(148/499): loss=0.34055182994634037, w0=-0.31466395213979415, w1=-0.005054951901628896\n",
      "Gradient Descent(149/499): loss=0.34053999076538133, w0=-0.31466395692588656, w1=-0.004907018079524276\n",
      "Gradient Descent(150/499): loss=0.3405284052137909, w0=-0.3146639612333698, w1=-0.004759756817726728\n",
      "Gradient Descent(151/499): loss=0.34051706747910027, w0=-0.31466396511010486, w1=-0.0046131844162457455\n",
      "Gradient Descent(152/499): loss=0.3405059718945912, w0=-0.3146639685991665, w1=-0.00446731640115696\n",
      "Gradient Descent(153/499): loss=0.3404951129352034, w0=-0.3146639717393221, w1=-0.004322167548139331\n",
      "Gradient Descent(154/499): loss=0.34048448521356933, w0=-0.31466397456546225, w1=-0.004177751905363628\n",
      "Gradient Descent(155/499): loss=0.34047408347617625, w0=-0.31466397710898847, w1=-0.004034082815750022\n",
      "Gradient Descent(156/499): loss=0.340463902599649, w0=-0.31466397939816215, w1=-0.003891172938612129\n",
      "Gradient Descent(157/499): loss=0.3404539375871493, w0=-0.3146639814584186, w1=-0.0037490342707043717\n",
      "Gradient Descent(158/499): loss=0.3404441835648876, w0=-0.3146639833126495, w1=-0.0036076781666889443\n",
      "Gradient Descent(159/499): loss=0.3404346357787438, w0=-0.3146639849814574, w1=-0.003467115359038301\n",
      "Gradient Descent(160/499): loss=0.3404252895909926, w0=-0.3146639864833846, w1=-0.0033273559773885335\n",
      "Gradient Descent(161/499): loss=0.34041614047713187, w0=-0.3146639878351192, w1=-0.0031884095673586524\n",
      "Gradient Descent(162/499): loss=0.34040718402280423, w0=-0.3146639890516804, w1=-0.0030502851088502884\n",
      "Gradient Descent(163/499): loss=0.3403984159208181, w0=-0.3146639901465856, w1=-0.0029129910338419416\n",
      "Gradient Descent(164/499): loss=0.34038983196825695, w0=-0.31466399113200033, w1=-0.0027765352436915024\n",
      "Gradient Descent(165/499): loss=0.34038142806367766, w0=-0.31466399201887374, w1=-0.0026409251259604054\n",
      "Gradient Descent(166/499): loss=0.3403732002043914, w0=-0.31466399281705987, w1=-0.002506167570772335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(167/499): loss=0.34036514448383137, w0=-0.31466399353542746, w1=-0.0023722689867191013\n",
      "Gradient Descent(168/499): loss=0.3403572570889932, w0=-0.3146639941819584, w1=-0.0022392353163259225\n",
      "Gradient Descent(169/499): loss=0.3403495342979599, w0=-0.31466399476383633, w1=-0.002107072051088\n",
      "Gradient Descent(170/499): loss=0.3403419724774923, w0=-0.31466399528752653, w1=-0.001975784246089955\n",
      "Gradient Descent(171/499): loss=0.3403345680806975, w0=-0.3146639957588478, w1=-0.0018453765342193655\n",
      "Gradient Descent(172/499): loss=0.340327317644765, w0=-0.31466399618303703, w1=-0.0017158531399853452\n",
      "Gradient Descent(173/499): loss=0.3403202177887653, w0=-0.3146639965648074, w1=-0.0015872178929527423\n",
      "Gradient Descent(174/499): loss=0.3403132652115199, w0=-0.31466399690840086, w1=-0.0014594742408023378\n",
      "Gradient Descent(175/499): loss=0.3403064566895287, w0=-0.31466399721763505, w1=-0.0013326252620270804\n",
      "Gradient Descent(176/499): loss=0.34029978907496017, w0=-0.31466399749594587, w1=-0.0012066736782740842\n",
      "Gradient Descent(177/499): loss=0.3402932592936979, w0=-0.31466399774642567, w1=-0.0010816218663419258\n",
      "Gradient Descent(178/499): loss=0.3402868643434471, w0=-0.3146639979718576, w1=-0.0009574718698424799\n",
      "Gradient Descent(179/499): loss=0.3402806012918911, w0=-0.31466399817474644, w1=-0.000834225410536249\n",
      "Gradient Descent(180/499): loss=0.3402744672749063, w0=-0.31466399835734643, w1=-0.0007118838993499554\n",
      "Gradient Descent(181/499): loss=0.3402684594948217, w0=-0.3146639985216865, w1=-0.0005904484470848594\n",
      "Gradient Descent(182/499): loss=0.3402625752187345, w0=-0.31466399866959266, w1=-0.0004699198748241369\n",
      "Gradient Descent(183/499): loss=0.3402568117768683, w0=-0.31466399880270823, w1=-0.0003502987240472819\n",
      "Gradient Descent(184/499): loss=0.34025116656098175, w0=-0.31466399892251234, w1=-0.00023158526645941727\n",
      "Gradient Descent(185/499): loss=0.34024563702281896, w0=-0.3146639990303361, w1=-0.00011377951354309621\n",
      "Gradient Descent(186/499): loss=0.34024022067260584, w0=-0.3146639991273775, w1=3.118774159960292e-06\n",
      "Gradient Descent(187/499): loss=0.34023491507758674, w0=-0.3146639992147149, w1=0.00011911007803002232\n",
      "Gradient Descent(188/499): loss=0.34022971786060324, w0=-0.3146639992933186, w1=0.00023419511260638594\n",
      "Gradient Descent(189/499): loss=0.34022462669871023, w0=-0.31466399936406203, w1=0.00034837481707340734\n",
      "Gradient Descent(190/499): loss=0.34021963932183413, w0=-0.31466399942773116, w1=0.0004616503469889725\n",
      "Gradient Descent(191/499): loss=0.34021475351146463, w0=-0.31466399948503343, w1=0.0005740230662488683\n",
      "Gradient Descent(192/499): loss=0.3402099670993831, w0=-0.31466399953660557, w1=0.0006854945392808066\n",
      "Gradient Descent(193/499): loss=0.34020527796642785, w0=-0.31466399958302055, w1=0.0007960665234619554\n",
      "Gradient Descent(194/499): loss=0.3402006840412899, w0=-0.3146639996247941, w1=0.0009057409617540339\n",
      "Gradient Descent(195/499): loss=0.3401961832993439, w0=-0.31466399966239034, w1=0.0010145199755501322\n",
      "Gradient Descent(196/499): loss=0.3401917737615104, w0=-0.314663999696227, w1=0.001122405857727663\n",
      "Gradient Descent(197/499): loss=0.34018745349314794, w0=-0.3146639997266801, w1=0.0012294010659019204\n",
      "Gradient Descent(198/499): loss=0.3401832206029747, w0=-0.3146639997540879, w1=0.0013355082158748638\n",
      "Gradient Descent(199/499): loss=0.34017907324202207, w0=-0.314663999778755, w1=0.0014407300752739887\n",
      "Gradient Descent(200/499): loss=0.34017500960261127, w0=-0.31466399980095544, w1=0.0015450695573761338\n",
      "Gradient Descent(201/499): loss=0.3401710279173612, w0=-0.3146639998209359, w1=0.001648529715111342\n",
      "Gradient Descent(202/499): loss=0.3401671264582222, w0=-0.31466399983891835, w1=0.0017511137352419653\n",
      "Gradient Descent(203/499): loss=0.34016330353553237, w0=-0.3146639998551026, w1=0.0018528249327122823\n",
      "Gradient Descent(204/499): loss=0.3401595574971027, w0=-0.31466399986966853, w1=0.0019536667451641493\n",
      "Gradient Descent(205/499): loss=0.3401558867273239, w0=-0.3146639998827779, w1=0.0020536427276141678\n",
      "Gradient Descent(206/499): loss=0.3401522896462958, w0=-0.3146639998945764, w1=0.002152756547288121\n",
      "Gradient Descent(207/499): loss=0.3401487647089845, w0=-0.31466399990519506, w1=0.0022510119786084116\n",
      "Gradient Descent(208/499): loss=0.34014531040439333, w0=-0.3146639999147519, w1=0.002348412898330451\n",
      "Gradient Descent(209/499): loss=0.34014192525476333, w0=-0.31466399992335314, w1=0.002444963280823982\n",
      "Gradient Descent(210/499): loss=0.3401386078147891, w0=-0.3146639999310943, w1=0.0025406671934954665\n",
      "Gradient Descent(211/499): loss=0.3401353566708565, w0=-0.3146639999380614, w1=0.0026355287923477575\n",
      "Gradient Descent(212/499): loss=0.3401321704403024, w0=-0.3146639999443318, w1=0.0027295523176733435\n",
      "Gradient Descent(213/499): loss=0.3401290477706884, w0=-0.31466399994997524, w1=0.002822742089877628\n",
      "Gradient Descent(214/499): loss=0.3401259873390963, w0=-0.3146639999550544, w1=0.0029151025054287064\n",
      "Gradient Descent(215/499): loss=0.34012298785144346, w0=-0.3146639999596257, w1=0.00300663803293026\n",
      "Gradient Descent(216/499): loss=0.34012004804180984, w0=-0.3146639999637399, w1=0.0030973532093142592\n",
      "Gradient Descent(217/499): loss=0.34011716667178776, w0=-0.3146639999674427, w1=0.003187252636150236\n",
      "Gradient Descent(218/499): loss=0.3401143425298446, w0=-0.31466399997077527, w1=0.003276340976067979\n",
      "Gradient Descent(219/499): loss=0.34011157443070217, w0=-0.31466399997377464, w1=0.003364622949290622\n",
      "Gradient Descent(220/499): loss=0.34010886121473427, w0=-0.3146639999764741, w1=0.003452103330275101\n",
      "Gradient Descent(221/499): loss=0.34010620174737405, w0=-0.31466399997890365, w1=0.0035387869444571283\n",
      "Gradient Descent(222/499): loss=0.34010359491854225, w0=-0.3146639999810903, w1=0.003624678665097803\n",
      "Gradient Descent(223/499): loss=0.3401010396420847, w0=-0.31466399998305833, w1=0.0037097834102291457\n",
      "Gradient Descent(224/499): loss=0.3400985348552268, w0=-0.3146639999848296, w1=0.0037941061396958627\n",
      "Gradient Descent(225/499): loss=0.3400960795180398, w0=-0.31466399998642375, w1=0.0038776518522907087\n",
      "Gradient Descent(226/499): loss=0.34009367261292145, w0=-0.31466399998785854, w1=0.0039604255829809604\n",
      "Gradient Descent(227/499): loss=0.3400913131440893, w0=-0.3146639999891499, w1=0.0040424324002234285\n",
      "Gradient Descent(228/499): loss=0.34008900013708504, w0=-0.31466399999031214, w1=0.0041236774033657\n",
      "Gradient Descent(229/499): loss=0.3400867326382919, w0=-0.3146639999913582, w1=0.004204165720131192\n",
      "Gradient Descent(230/499): loss=0.34008450971446597, w0=-0.3146639999922997, w1=0.004283902504185744\n",
      "Gradient Descent(231/499): loss=0.34008233045227354, w0=-0.3146639999931471, w1=0.0043628929327835355\n",
      "Gradient Descent(232/499): loss=0.34008019395784683, w0=-0.31466399999390976, w1=0.004441142204490127\n",
      "Gradient Descent(233/499): loss=0.34007809935634326, w0=-0.3146639999945962, w1=0.004518655536980537\n",
      "Gradient Descent(234/499): loss=0.3400760457915212, w0=-0.31466399999521405, w1=0.0045954381649102915\n",
      "Gradient Descent(235/499): loss=0.34007403242532225, w0=-0.31466399999577016, w1=0.0046714953378574125\n",
      "Gradient Descent(236/499): loss=0.34007205843746535, w0=-0.3146639999962707, w1=0.004746832318333426\n",
      "Gradient Descent(237/499): loss=0.34007012302505013, w0=-0.3146639999967212, w1=0.004821454379861463\n",
      "Gradient Descent(238/499): loss=0.3400682254021695, w0=-0.3146639999971267, w1=0.0048953668051196116\n",
      "Gradient Descent(239/499): loss=0.3400663647995323, w0=-0.3146639999974917, w1=0.004968574884147703\n",
      "Gradient Descent(240/499): loss=0.34006454046409357, w0=-0.3146639999978202, w1=0.0050410839126157885\n",
      "Gradient Descent(241/499): loss=0.3400627516586952, w0=-0.3146639999981159, w1=0.00511289919015258\n",
      "Gradient Descent(242/499): loss=0.3400609976617137, w0=-0.314663999998382, w1=0.005184026018732187\n",
      "Gradient Descent(243/499): loss=0.3400592777667176, w0=-0.31466399999862155, w1=0.005254469701117536\n",
      "Gradient Descent(244/499): loss=0.34005759128213187, w0=-0.3146639999988372, w1=0.005324235539358867\n",
      "Gradient Descent(245/499): loss=0.34005593753091046, w0=-0.31466399999903133, w1=0.005393328833345811\n",
      "Gradient Descent(246/499): loss=0.3400543158502167, w0=-0.314663999999206, w1=0.005461754879411492\n",
      "Gradient Descent(247/499): loss=0.34005272559111216, w0=-0.3146639999993633, w1=0.005529518968987213\n",
      "Gradient Descent(248/499): loss=0.3400511661182509, w0=-0.31466399999950484, w1=0.00559662638730633\n",
      "Gradient Descent(249/499): loss=0.3400496368095813, w0=-0.3146639999996323, w1=0.005663082412155882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(250/499): loss=0.34004813705605696, w0=-0.31466399999974703, w1=0.005728892312674654\n",
      "Gradient Descent(251/499): loss=0.3400466662613513, w0=-0.31466399999985034, w1=0.005794061348196355\n",
      "Gradient Descent(252/499): loss=0.34004522384158087, w0=-0.3146639999999433, w1=0.005858594767136622\n",
      "Gradient Descent(253/499): loss=0.3400438092250343, w0=-0.31466400000002703, w1=0.0059224978059226035\n",
      "Gradient Descent(254/499): loss=0.34004242185190625, w0=-0.3146640000001024, w1=0.005985775687963906\n",
      "Gradient Descent(255/499): loss=0.34004106117404187, w0=-0.31466400000017025, w1=0.006048433622663741\n",
      "Gradient Descent(256/499): loss=0.34003972665468096, w0=-0.3146640000002313, w1=0.006110476804469093\n",
      "Gradient Descent(257/499): loss=0.34003841776821264, w0=-0.3146640000002863, w1=0.006171910411958805\n",
      "Gradient Descent(258/499): loss=0.3400371339999343, w0=-0.31466400000033584, w1=0.006232739606968469\n",
      "Gradient Descent(259/499): loss=0.34003587484581566, w0=-0.3146640000003804, w1=0.0062929695337510866\n",
      "Gradient Descent(260/499): loss=0.34003463981226745, w0=-0.3146640000004206, w1=0.00635260531817243\n",
      "Gradient Descent(261/499): loss=0.34003342841591844, w0=-0.3146640000004568, w1=0.0064116520669401334\n",
      "Gradient Descent(262/499): loss=0.3400322401833942, w0=-0.3146640000004894, w1=0.006470114866865503\n",
      "Gradient Descent(263/499): loss=0.34003107465110133, w0=-0.3146640000005187, w1=0.006527998784157083\n",
      "Gradient Descent(264/499): loss=0.34002993136501963, w0=-0.3146640000005451, w1=0.006585308863745084\n",
      "Gradient Descent(265/499): loss=0.34002880988049533, w0=-0.31466400000056893, w1=0.006642050128635744\n",
      "Gradient Descent(266/499): loss=0.34002770976204033, w0=-0.31466400000059036, w1=0.006698227579294717\n",
      "Gradient Descent(267/499): loss=0.3400266305831372, w0=-0.3146640000006097, w1=0.006753846193058684\n",
      "Gradient Descent(268/499): loss=0.340025571926046, w0=-0.3146640000006271, w1=0.006808910923574293\n",
      "Gradient Descent(269/499): loss=0.3400245333816181, w0=-0.3146640000006428, w1=0.006863426700263654\n",
      "Gradient Descent(270/499): loss=0.34002351454911245, w0=-0.314664000000657, w1=0.00691739842781558\n",
      "Gradient Descent(271/499): loss=0.34002251503601566, w0=-0.3146640000006697, w1=0.006970830985701805\n",
      "Gradient Descent(272/499): loss=0.34002153445786965, w0=-0.3146640000006812, w1=0.007023729227717411\n",
      "Gradient Descent(273/499): loss=0.340020572438097, w0=-0.31466400000069156, w1=0.007076097981544768\n",
      "Gradient Descent(274/499): loss=0.34001962860783574, w0=-0.3146640000007009, w1=0.0071279420483402476\n",
      "Gradient Descent(275/499): loss=0.3400187026057753, w0=-0.3146640000007093, w1=0.007179266202343018\n",
      "Gradient Descent(276/499): loss=0.3400177940779967, w0=-0.3146640000007169, w1=0.007230075190505256\n",
      "Gradient Descent(277/499): loss=0.3400169026778147, w0=-0.31466400000072375, w1=0.007280373732143121\n",
      "Gradient Descent(278/499): loss=0.34001602806562714, w0=-0.3146640000007299, w1=0.007330166518607834\n",
      "Gradient Descent(279/499): loss=0.340015169908764, w0=-0.3146640000007355, w1=0.00737945821297627\n",
      "Gradient Descent(280/499): loss=0.34001432788134034, w0=-0.3146640000007406, w1=0.007428253449760421\n",
      "Gradient Descent(281/499): loss=0.34001350166411537, w0=-0.3146640000007451, w1=0.0074765568346351705\n",
      "Gradient Descent(282/499): loss=0.34001269094435055, w0=-0.31466400000074923, w1=0.007524372944183791\n",
      "Gradient Descent(283/499): loss=0.3400118954156737, w0=-0.31466400000075295, w1=0.007571706325660623\n",
      "Gradient Descent(284/499): loss=0.3400111147779451, w0=-0.3146640000007563, w1=0.0076185614967703595\n",
      "Gradient Descent(285/499): loss=0.3400103487371271, w0=-0.31466400000075934, w1=0.007664942945463439\n",
      "Gradient Descent(286/499): loss=0.34000959700515465, w0=-0.31466400000076206, w1=0.0077108551297470455\n",
      "Gradient Descent(287/499): loss=0.3400088592998114, w0=-0.31466400000076455, w1=0.007756302477511145\n",
      "Gradient Descent(288/499): loss=0.34000813534460694, w0=-0.31466400000076683, w1=0.007801289386369165\n",
      "Gradient Descent(289/499): loss=0.34000742486865687, w0=-0.3146640000007689, w1=0.007845820223512786\n",
      "Gradient Descent(290/499): loss=0.34000672760656525, w0=-0.3146640000007707, w1=0.007889899325580369\n",
      "Gradient Descent(291/499): loss=0.3400060432983107, w0=-0.3146640000007724, w1=0.00793353099853864\n",
      "Gradient Descent(292/499): loss=0.3400053716891335, w0=-0.3146640000007739, w1=0.007976719517577134\n",
      "Gradient Descent(293/499): loss=0.34000471252942693, w0=-0.31466400000077527, w1=0.008019469127014988\n",
      "Gradient Descent(294/499): loss=0.34000406557462837, w0=-0.31466400000077654, w1=0.008061784040219708\n",
      "Gradient Descent(295/499): loss=0.3400034305851157, w0=-0.3146640000007777, w1=0.008103668439537444\n",
      "Gradient Descent(296/499): loss=0.340002807326104, w0=-0.31466400000077877, w1=0.00814512647623443\n",
      "Gradient Descent(297/499): loss=0.3400021955675445, w0=-0.3146640000007797, w1=0.008186162270449214\n",
      "Gradient Descent(298/499): loss=0.34000159508402794, w0=-0.31466400000078054, w1=0.008226779911155268\n",
      "Gradient Descent(299/499): loss=0.34000100565468605, w0=-0.3146640000007813, w1=0.008266983456133673\n",
      "Gradient Descent(300/499): loss=0.3400004270630989, w0=-0.31466400000078204, w1=0.00830677693195546\n",
      "Gradient Descent(301/499): loss=0.33999985909720215, w0=-0.3146640000007827, w1=0.008346164333973359\n",
      "Gradient Descent(302/499): loss=0.339999301549198, w0=-0.3146640000007833, w1=0.00838514962632254\n",
      "Gradient Descent(303/499): loss=0.33999875421546544, w0=-0.31466400000078387, w1=0.008423736741930078\n",
      "Gradient Descent(304/499): loss=0.33999821689647425, w0=-0.31466400000078437, w1=0.008461929582532805\n",
      "Gradient Descent(305/499): loss=0.3399976893967016, w0=-0.3146640000007848, w1=0.008499732018703264\n",
      "Gradient Descent(306/499): loss=0.339997171524548, w0=-0.31466400000078526, w1=0.008537147889883464\n",
      "Gradient Descent(307/499): loss=0.3399966630922579, w0=-0.31466400000078565, w1=0.008574181004426144\n",
      "Gradient Descent(308/499): loss=0.33999616391583887, w0=-0.314664000000786, w1=0.008610835139643263\n",
      "Gradient Descent(309/499): loss=0.33999567381498474, w0=-0.3146640000007863, w1=0.00864711404186148\n",
      "Gradient Descent(310/499): loss=0.3399951926130006, w0=-0.31466400000078665, w1=0.008683021426484293\n",
      "Gradient Descent(311/499): loss=0.339994720136727, w0=-0.3146640000007869, w1=0.008718560978060683\n",
      "Gradient Descent(312/499): loss=0.3399942562164692, w0=-0.3146640000007872, w1=0.008753736350359899\n",
      "Gradient Descent(313/499): loss=0.3399938006859242, w0=-0.3146640000007874, w1=0.008788551166452246\n",
      "Gradient Descent(314/499): loss=0.3399933533821121, w0=-0.31466400000078765, w1=0.008823009018795566\n",
      "Gradient Descent(315/499): loss=0.3399929141453082, w0=-0.31466400000078787, w1=0.008857113469327238\n",
      "Gradient Descent(316/499): loss=0.33999248281897576, w0=-0.3146640000007881, w1=0.008890868049561436\n",
      "Gradient Descent(317/499): loss=0.3399920592497007, w0=-0.31466400000078826, w1=0.008924276260691458\n",
      "Gradient Descent(318/499): loss=0.33999164328712794, w0=-0.3146640000007884, w1=0.008957341573696907\n",
      "Gradient Descent(319/499): loss=0.3399912347838997, w0=-0.3146640000007886, w1=0.008990067429455515\n",
      "Gradient Descent(320/499): loss=0.3399908335955923, w0=-0.31466400000078876, w1=0.00902245723885944\n",
      "Gradient Descent(321/499): loss=0.3399904395806575, w0=-0.3146640000007889, w1=0.009054514382935802\n",
      "Gradient Descent(322/499): loss=0.3399900526003643, w0=-0.31466400000078903, w1=0.009086242212971317\n",
      "Gradient Descent(323/499): loss=0.3399896725187407, w0=-0.31466400000078915, w1=0.00911764405064082\n",
      "Gradient Descent(324/499): loss=0.3399892992025172, w0=-0.31466400000078926, w1=0.009148723188139515\n",
      "Gradient Descent(325/499): loss=0.3399889325210725, w0=-0.31466400000078937, w1=0.009179482888318786\n",
      "Gradient Descent(326/499): loss=0.3399885723463794, w0=-0.3146640000007895, w1=0.009209926384825377\n",
      "Gradient Descent(327/499): loss=0.3399882185529517, w0=-0.3146640000007896, w1=0.009240056882243837\n",
      "Gradient Descent(328/499): loss=0.33998787101779293, w0=-0.3146640000007897, w1=0.009269877556242015\n",
      "Gradient Descent(329/499): loss=0.3399875296203456, w0=-0.3146640000007898, w1=0.009299391553719488\n",
      "Gradient Descent(330/499): loss=0.3399871942424413, w0=-0.31466400000078987, w1=0.009328601992958777\n",
      "Gradient Descent(331/499): loss=0.33998686476825346, w0=-0.3146640000007899, w1=0.0093575119637792\n",
      "Gradient Descent(332/499): loss=0.33998654108424786, w0=-0.31466400000079, w1=0.009386124527693208\n",
      "Gradient Descent(333/499): loss=0.33998622307913856, w0=-0.31466400000079003, w1=0.009414442718065139\n",
      "Gradient Descent(334/499): loss=0.33998591064383965, w0=-0.3146640000007901, w1=0.009442469540272162\n",
      "Gradient Descent(335/499): loss=0.3399856036714238, w0=-0.31466400000079014, w1=0.009470207971867366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(336/499): loss=0.33998530205707567, w0=-0.3146640000007902, w1=0.009497660962744838\n",
      "Gradient Descent(337/499): loss=0.33998500569805107, w0=-0.31466400000079026, w1=0.009524831435306618\n",
      "Gradient Descent(338/499): loss=0.33998471449363443, w0=-0.3146640000007903, w1=0.009551722284631423\n",
      "Gradient Descent(339/499): loss=0.33998442834509773, w0=-0.31466400000079037, w1=0.009578336378645021\n",
      "Gradient Descent(340/499): loss=0.33998414715566055, w0=-0.3146640000007904, w1=0.009604676558292157\n",
      "Gradient Descent(341/499): loss=0.33998387083045145, w0=-0.3146640000007905, w1=0.009630745637709917\n",
      "Gradient Descent(342/499): loss=0.3399835992764677, w0=-0.31466400000079053, w1=0.009656546404402448\n",
      "Gradient Descent(343/499): loss=0.3399833324025394, w0=-0.3146640000007906, w1=0.009682081619416923\n",
      "Gradient Descent(344/499): loss=0.3399830701192921, w0=-0.31466400000079064, w1=0.009707354017520646\n",
      "Gradient Descent(345/499): loss=0.3399828123391097, w0=-0.3146640000007907, w1=0.00973236630737924\n",
      "Gradient Descent(346/499): loss=0.3399825589761006, w0=-0.31466400000079076, w1=0.009757121171735796\n",
      "Gradient Descent(347/499): loss=0.3399823099460614, w0=-0.3146640000007908, w1=0.009781621267590933\n",
      "Gradient Descent(348/499): loss=0.33998206516644436, w0=-0.3146640000007908, w1=0.009805869226383627\n",
      "Gradient Descent(349/499): loss=0.33998182455632336, w0=-0.31466400000079087, w1=0.00982986765417281\n",
      "Gradient Descent(350/499): loss=0.33998158803636136, w0=-0.31466400000079087, w1=0.0098536191318196\n",
      "Gradient Descent(351/499): loss=0.3399813555287795, w0=-0.31466400000079087, w1=0.009877126215170113\n",
      "Gradient Descent(352/499): loss=0.3399811269573242, w0=-0.31466400000079087, w1=0.00990039143523878\n",
      "Gradient Descent(353/499): loss=0.3399809022472373, w0=-0.31466400000079087, w1=0.009923417298392107\n",
      "Gradient Descent(354/499): loss=0.3399806813252265, w0=-0.3146640000007909, w1=0.009946206286532772\n",
      "Gradient Descent(355/499): loss=0.33998046411943594, w0=-0.3146640000007909, w1=0.009968760857284072\n",
      "Gradient Descent(356/499): loss=0.3399802505594173, w0=-0.3146640000007909, w1=0.009991083444174573\n",
      "Gradient Descent(357/499): loss=0.3399800405761003, w0=-0.3146640000007909, w1=0.01001317645682297\n",
      "Gradient Descent(358/499): loss=0.33997983410176846, w0=-0.3146640000007909, w1=0.01003504228112305\n",
      "Gradient Descent(359/499): loss=0.3399796310700279, w0=-0.3146640000007909, w1=0.010056683279428713\n",
      "Gradient Descent(360/499): loss=0.33997943141578396, w0=-0.3146640000007909, w1=0.010078101790739046\n",
      "Gradient Descent(361/499): loss=0.33997923507521466, w0=-0.3146640000007909, w1=0.010099300130883289\n",
      "Gradient Descent(362/499): loss=0.33997904198574413, w0=-0.3146640000007909, w1=0.010120280592705772\n",
      "Gradient Descent(363/499): loss=0.3399788520860186, w0=-0.3146640000007909, w1=0.010141045446250677\n",
      "Gradient Descent(364/499): loss=0.3399786653158823, w0=-0.3146640000007909, w1=0.010161596938946615\n",
      "Gradient Descent(365/499): loss=0.3399784816163526, w0=-0.3146640000007909, w1=0.010181937295790996\n",
      "Gradient Descent(366/499): loss=0.33997830092959824, w0=-0.3146640000007909, w1=0.010202068719534075\n",
      "Gradient Descent(367/499): loss=0.3399781231989143, w0=-0.3146640000007909, w1=0.01022199339086273\n",
      "Gradient Descent(368/499): loss=0.33997794836870204, w0=-0.3146640000007909, w1=0.010241713468583836\n",
      "Gradient Descent(369/499): loss=0.33997777638444493, w0=-0.3146640000007909, w1=0.010261231089807277\n",
      "Gradient Descent(370/499): loss=0.3399776071926878, w0=-0.3146640000007909, w1=0.010280548370128496\n",
      "Gradient Descent(371/499): loss=0.33997744074101677, w0=-0.3146640000007909, w1=0.01029966740381059\n",
      "Gradient Descent(372/499): loss=0.33997727697803687, w0=-0.3146640000007909, w1=0.010318590263965897\n",
      "Gradient Descent(373/499): loss=0.3399771158533528, w0=-0.3146640000007909, w1=0.010337319002737034\n",
      "Gradient Descent(374/499): loss=0.339976957317549, w0=-0.3146640000007909, w1=0.010355855651477374\n",
      "Gradient Descent(375/499): loss=0.33997680132217045, w0=-0.3146640000007909, w1=0.010374202220930915\n",
      "Gradient Descent(376/499): loss=0.33997664781970216, w0=-0.3146640000007909, w1=0.010392360701411515\n",
      "Gradient Descent(377/499): loss=0.3399764967635529, w0=-0.3146640000007909, w1=0.010410333062981496\n",
      "Gradient Descent(378/499): loss=0.3399763481080351, w0=-0.3146640000007909, w1=0.010428121255629502\n",
      "Gradient Descent(379/499): loss=0.3399762018083472, w0=-0.3146640000007909, w1=0.01044572720944772\n",
      "Gradient Descent(380/499): loss=0.33997605782055657, w0=-0.3146640000007909, w1=0.0104631528348083\n",
      "Gradient Descent(381/499): loss=0.3399759161015824, w0=-0.3146640000007909, w1=0.010480400022539067\n",
      "Gradient Descent(382/499): loss=0.33997577660917844, w0=-0.3146640000007909, w1=0.010497470644098405\n",
      "Gradient Descent(383/499): loss=0.33997563930191643, w0=-0.3146640000007909, w1=0.010514366551749359\n",
      "Gradient Descent(384/499): loss=0.3399755041391701, w0=-0.3146640000007909, w1=0.01053108957873292\n",
      "Gradient Descent(385/499): loss=0.3399753710810996, w0=-0.3146640000007909, w1=0.010547641539440464\n",
      "Gradient Descent(386/499): loss=0.3399752400886362, w0=-0.3146640000007909, w1=0.010564024229585309\n",
      "Gradient Descent(387/499): loss=0.33997511112346546, w0=-0.3146640000007909, w1=0.010580239426373402\n",
      "Gradient Descent(388/499): loss=0.33997498414801375, w0=-0.3146640000007909, w1=0.01059628888867312\n",
      "Gradient Descent(389/499): loss=0.339974859125434, w0=-0.31466400000079087, w1=0.010612174357184145\n",
      "Gradient Descent(390/499): loss=0.33997473601959016, w0=-0.31466400000079087, w1=0.010627897554605421\n",
      "Gradient Descent(391/499): loss=0.3399746147950442, w0=-0.3146640000007908, w1=0.01064346018580215\n",
      "Gradient Descent(392/499): loss=0.33997449541704194, w0=-0.3146640000007908, w1=0.010658863937971861\n",
      "Gradient Descent(393/499): loss=0.33997437785149814, w0=-0.31466400000079076, w1=0.010674110480809473\n",
      "Gradient Descent(394/499): loss=0.33997426206498693, w0=-0.31466400000079076, w1=0.010689201466671405\n",
      "Gradient Descent(395/499): loss=0.3399741480247255, w0=-0.3146640000007907, w1=0.010704138530738688\n",
      "Gradient Descent(396/499): loss=0.339974035698562, w0=-0.31466400000079064, w1=0.010718923291179063\n",
      "Gradient Descent(397/499): loss=0.3399739250549648, w0=-0.3146640000007906, w1=0.010733557349308092\n",
      "Gradient Descent(398/499): loss=0.339973816063009, w0=-0.3146640000007906, w1=0.010748042289749196\n",
      "Gradient Descent(399/499): loss=0.3399737086923642, w0=-0.31466400000079053, w1=0.01076237968059272\n",
      "Gradient Descent(400/499): loss=0.3399736029132838, w0=-0.31466400000079053, w1=0.010776571073553915\n",
      "Gradient Descent(401/499): loss=0.3399734986965932, w0=-0.3146640000007905, w1=0.010790618004129898\n",
      "Gradient Descent(402/499): loss=0.33997339601367826, w0=-0.3146640000007905, w1=0.010804521991755538\n",
      "Gradient Descent(403/499): loss=0.33997329483647437, w0=-0.3146640000007904, w1=0.01081828453995831\n",
      "Gradient Descent(404/499): loss=0.3399731951374555, w0=-0.31466400000079037, w1=0.010831907136512046\n",
      "Gradient Descent(405/499): loss=0.3399730968896254, w0=-0.3146640000007903, w1=0.010845391253589638\n",
      "Gradient Descent(406/499): loss=0.339973000066504, w0=-0.31466400000079026, w1=0.010858738347914653\n",
      "Gradient Descent(407/499): loss=0.33997290464212027, w0=-0.3146640000007902, w1=0.010871949860911884\n",
      "Gradient Descent(408/499): loss=0.3399728105910006, w0=-0.31466400000079014, w1=0.010885027218856784\n",
      "Gradient Descent(409/499): loss=0.3399727178881597, w0=-0.31466400000079014, w1=0.010897971833023842\n",
      "Gradient Descent(410/499): loss=0.3399726265090902, w0=-0.3146640000007901, w1=0.010910785099833838\n",
      "Gradient Descent(411/499): loss=0.33997253642975495, w0=-0.31466400000079003, w1=0.010923468401000053\n",
      "Gradient Descent(412/499): loss=0.3399724476265765, w0=-0.31466400000079, w1=0.010936023103673306\n",
      "Gradient Descent(413/499): loss=0.3399723600764279, w0=-0.3146640000007899, w1=0.01094845056058595\n",
      "Gradient Descent(414/499): loss=0.3399722737566253, w0=-0.31466400000078987, w1=0.010960752110194767\n",
      "Gradient Descent(415/499): loss=0.33997218864491785, w0=-0.3146640000007898, w1=0.010972929076822726\n",
      "Gradient Descent(416/499): loss=0.33997210471948003, w0=-0.31466400000078976, w1=0.010984982770799647\n",
      "Gradient Descent(417/499): loss=0.33997202195890325, w0=-0.3146640000007897, w1=0.010996914488601803\n",
      "Gradient Descent(418/499): loss=0.3399719403421877, w0=-0.3146640000007897, w1=0.011008725512990364\n",
      "Gradient Descent(419/499): loss=0.3399718598487338, w0=-0.31466400000078965, w1=0.011020417113148773\n",
      "Gradient Descent(420/499): loss=0.33997178045833576, w0=-0.3146640000007896, w1=0.011031990544818987\n",
      "Gradient Descent(421/499): loss=0.3399717021511722, w0=-0.31466400000078953, w1=0.011043447050436675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(422/499): loss=0.33997162490779986, w0=-0.3146640000007895, w1=0.01105478785926525\n",
      "Gradient Descent(423/499): loss=0.33997154870914603, w0=-0.3146640000007894, w1=0.011066014187528835\n",
      "Gradient Descent(424/499): loss=0.33997147353650115, w0=-0.31466400000078937, w1=0.01107712723854413\n",
      "Gradient Descent(425/499): loss=0.3399713993715112, w0=-0.3146640000007893, w1=0.011088128202851173\n",
      "Gradient Descent(426/499): loss=0.3399713261961724, w0=-0.31466400000078926, w1=0.011099018258343018\n",
      "Gradient Descent(427/499): loss=0.3399712539928227, w0=-0.3146640000007892, w1=0.01110979857039431\n",
      "Gradient Descent(428/499): loss=0.33997118274413574, w0=-0.31466400000078915, w1=0.011120470291988784\n",
      "Gradient Descent(429/499): loss=0.33997111243311495, w0=-0.3146640000007891, w1=0.011131034563845673\n",
      "Gradient Descent(430/499): loss=0.33997104304308545, w0=-0.31466400000078903, w1=0.01114149251454503\n",
      "Gradient Descent(431/499): loss=0.3399709745576909, w0=-0.314664000000789, w1=0.011151845260651964\n",
      "Gradient Descent(432/499): loss=0.33997090696088283, w0=-0.3146640000007889, w1=0.011162093906839813\n",
      "Gradient Descent(433/499): loss=0.3399708402369194, w0=-0.31466400000078887, w1=0.011172239546012245\n",
      "Gradient Descent(434/499): loss=0.33997077437035583, w0=-0.3146640000007888, w1=0.01118228325942427\n",
      "Gradient Descent(435/499): loss=0.3399707093460411, w0=-0.31466400000078876, w1=0.011192226116802208\n",
      "Gradient Descent(436/499): loss=0.33997064514911013, w0=-0.3146640000007887, w1=0.011202069176462586\n",
      "Gradient Descent(437/499): loss=0.33997058176497996, w0=-0.31466400000078865, w1=0.011211813485429978\n",
      "Gradient Descent(438/499): loss=0.3399705191793436, w0=-0.3146640000007886, w1=0.011221460079553793\n",
      "Gradient Descent(439/499): loss=0.3399704573781645, w0=-0.31466400000078854, w1=0.011231009983624021\n",
      "Gradient Descent(440/499): loss=0.33997039634767146, w0=-0.3146640000007885, w1=0.011240464211485932\n",
      "Gradient Descent(441/499): loss=0.33997033607435423, w0=-0.3146640000007884, w1=0.01124982376615372\n",
      "Gradient Descent(442/499): loss=0.3399702765449569, w0=-0.31466400000078837, w1=0.011259089639923144\n",
      "Gradient Descent(443/499): loss=0.3399702177464746, w0=-0.3146640000007883, w1=0.011268262814483123\n",
      "Gradient Descent(444/499): loss=0.3399701596661475, w0=-0.31466400000078826, w1=0.011277344261026301\n",
      "Gradient Descent(445/499): loss=0.33997010229145647, w0=-0.3146640000007882, w1=0.011286334940358604\n",
      "Gradient Descent(446/499): loss=0.33997004561011873, w0=-0.31466400000078815, w1=0.011295235803007786\n",
      "Gradient Descent(447/499): loss=0.3399699896100825, w0=-0.3146640000007881, w1=0.011304047789330965\n",
      "Gradient Descent(448/499): loss=0.33996993427952404, w0=-0.31466400000078804, w1=0.011312771829621154\n",
      "Gradient Descent(449/499): loss=0.3399698796068415, w0=-0.314664000000788, w1=0.011321408844212795\n",
      "Gradient Descent(450/499): loss=0.33996982558065114, w0=-0.3146640000007879, w1=0.011329959743586308\n",
      "Gradient Descent(451/499): loss=0.3399697721897844, w0=-0.31466400000078787, w1=0.011338425428471646\n",
      "Gradient Descent(452/499): loss=0.3399697194232814, w0=-0.3146640000007878, w1=0.01134680678995089\n",
      "Gradient Descent(453/499): loss=0.3399696672703896, w0=-0.31466400000078776, w1=0.011355104709559848\n",
      "Gradient Descent(454/499): loss=0.33996961572055706, w0=-0.3146640000007877, w1=0.011363320059388698\n",
      "Gradient Descent(455/499): loss=0.33996956476343165, w0=-0.31466400000078765, w1=0.011371453702181672\n",
      "Gradient Descent(456/499): loss=0.3399695143888539, w0=-0.3146640000007876, w1=0.01137950649143579\n",
      "Gradient Descent(457/499): loss=0.339969464586856, w0=-0.31466400000078754, w1=0.011387479271498625\n",
      "Gradient Descent(458/499): loss=0.33996941534765623, w0=-0.3146640000007875, w1=0.011395372877665148\n",
      "Gradient Descent(459/499): loss=0.3399693666616568, w0=-0.3146640000007874, w1=0.01140318813627363\n",
      "Gradient Descent(460/499): loss=0.33996931851943935, w0=-0.31466400000078737, w1=0.011410925864800602\n",
      "Gradient Descent(461/499): loss=0.33996927091176243, w0=-0.3146640000007873, w1=0.011418586871954912\n",
      "Gradient Descent(462/499): loss=0.3399692238295563, w0=-0.31466400000078726, w1=0.011426171957770853\n",
      "Gradient Descent(463/499): loss=0.3399691772639214, w0=-0.3146640000007872, w1=0.011433681913700373\n",
      "Gradient Descent(464/499): loss=0.3399691312061255, w0=-0.31466400000078715, w1=0.011441117522704403\n",
      "Gradient Descent(465/499): loss=0.33996908564759803, w0=-0.3146640000007871, w1=0.011448479559343269\n",
      "Gradient Descent(466/499): loss=0.33996904057992944, w0=-0.31466400000078704, w1=0.011455768789866216\n",
      "Gradient Descent(467/499): loss=0.3399689959948665, w0=-0.314664000000787, w1=0.011462985972300051\n",
      "Gradient Descent(468/499): loss=0.33996895188431014, w0=-0.3146640000007869, w1=0.011470131856536909\n",
      "Gradient Descent(469/499): loss=0.3399689082403126, w0=-0.31466400000078687, w1=0.011477207184421141\n",
      "Gradient Descent(470/499): loss=0.3399688650550736, w0=-0.3146640000007868, w1=0.011484212689835342\n",
      "Gradient Descent(471/499): loss=0.33996882232093883, w0=-0.31466400000078676, w1=0.01149114909878552\n",
      "Gradient Descent(472/499): loss=0.33996878003039577, w0=-0.3146640000007867, w1=0.011498017129485418\n",
      "Gradient Descent(473/499): loss=0.33996873817607176, w0=-0.31466400000078665, w1=0.011504817492439975\n",
      "Gradient Descent(474/499): loss=0.3399686967507316, w0=-0.3146640000007866, w1=0.011511550890527956\n",
      "Gradient Descent(475/499): loss=0.3399686557472744, w0=-0.31466400000078654, w1=0.011518218019083778\n",
      "Gradient Descent(476/499): loss=0.33996861515873134, w0=-0.3146640000007865, w1=0.011524819565978455\n",
      "Gradient Descent(477/499): loss=0.3399685749782616, w0=-0.3146640000007864, w1=0.011531356211699779\n",
      "Gradient Descent(478/499): loss=0.33996853519915343, w0=-0.31466400000078637, w1=0.011537828629431642\n",
      "Gradient Descent(479/499): loss=0.3399684958148179, w0=-0.31466400000078626, w1=0.011544237485132594\n",
      "Gradient Descent(480/499): loss=0.3399684568187886, w0=-0.3146640000007862, w1=0.011550583437613575\n",
      "Gradient Descent(481/499): loss=0.339968418204719, w0=-0.3146640000007861, w1=0.011556867138614858\n",
      "Gradient Descent(482/499): loss=0.33996837996637996, w0=-0.31466400000078604, w1=0.011563089232882217\n",
      "Gradient Descent(483/499): loss=0.33996834209765725, w0=-0.3146640000007859, w1=0.011569250358242317\n",
      "Gradient Descent(484/499): loss=0.33996830459255023, w0=-0.31466400000078587, w1=0.011575351145677303\n",
      "Gradient Descent(485/499): loss=0.3399682674451689, w0=-0.3146640000007858, w1=0.011581392219398662\n",
      "Gradient Descent(486/499): loss=0.33996823064973186, w0=-0.3146640000007857, w1=0.011587374196920306\n",
      "Gradient Descent(487/499): loss=0.33996819420056434, w0=-0.31466400000078565, w1=0.011593297689130894\n",
      "Gradient Descent(488/499): loss=0.33996815809209624, w0=-0.3146640000007856, w1=0.011599163300365416\n",
      "Gradient Descent(489/499): loss=0.3399681223188602, w0=-0.31466400000078554, w1=0.011604971628476048\n",
      "Gradient Descent(490/499): loss=0.3399680868754897, w0=-0.3146640000007854, w1=0.011610723264902253\n",
      "Gradient Descent(491/499): loss=0.33996805175671674, w0=-0.31466400000078537, w1=0.011616418794740171\n",
      "Gradient Descent(492/499): loss=0.3399680169573702, w0=-0.31466400000078526, w1=0.011622058796811269\n",
      "Gradient Descent(493/499): loss=0.33996798247237414, w0=-0.3146640000007852, w1=0.011627643843730311\n",
      "Gradient Descent(494/499): loss=0.3399679482967459, w0=-0.31466400000078515, w1=0.011633174501972578\n",
      "Gradient Descent(495/499): loss=0.33996791442559393, w0=-0.3146640000007851, w1=0.011638651331940414\n",
      "Gradient Descent(496/499): loss=0.3399678808541173, w0=-0.314664000000785, w1=0.01164407488802907\n",
      "Gradient Descent(497/499): loss=0.33996784757760223, w0=-0.3146640000007849, w1=0.011649445718691847\n",
      "Gradient Descent(498/499): loss=0.3399678145914208, w0=-0.3146640000007848, w1=0.01165476436650458\n",
      "Gradient Descent(499/499): loss=0.3399677818910316, w0=-0.3146640000007847, w1=0.011660031368229426\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAEKCAYAAAD6q1UVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIQd/M7MS+i+Q43vLgfrGYgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f999c41a470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w, loss = func_GD (y=y_train, tx=tx_train, test_set=tx_test, max_iters=500,gamma=0.1, initial_w=initial_w);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 31)"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.shape\n",
    "tx_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74.499200000000002"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_train = predict_labels(w, tx_train)\n",
    "right_train = np.sum(y_pred_train == y_train)/len(y_train)*100\n",
    "right_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Ridge regression    -------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'func_ridge_regression' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-1a008bd5aaef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfunc_ridge_regression\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'func_ridge_regression' is not defined"
     ]
    }
   ],
   "source": [
    "func_ridge_regression (y=y_train, tx=tx_train, test_set=tx_test, lambda_=lambda_);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## * SGD -------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def func_SGD (y, tx, test_set, max_iters, gamma, initial_w, batch_size):\n",
    "    name = 'Stochastic_Gradient_descent'\n",
    "    \n",
    "    w,loss = least_squares_SGD(y, tx, initial_w, max_iters,\\\n",
    "                               gamma, batch_size)\n",
    "    \n",
    "    y_pred = predict_labels(w, test_set)\n",
    "    create_csv_submission(ids_test, y_pred, name)\n",
    "    \n",
    "    plt.plot(w, 'go')\n",
    "    plt.title('SGD: weights with loss:  '+str(loss),fontsize=15, fontweight='bold');\n",
    "    plt.show()\n",
    "    \n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEKCAYAAAA8QgPpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xu8XFV99/HPNxei8RITCBcJyaEVW9EI1lMo6mOpIQGt\nGLResLEmPrapT0srtbZio00Aj0WfqvFuj2iJ9ij6eCOx1TQGYr2VEhSJgBrUJARiCCZEMAKB/J4/\n1ppkZzJzztzOmZkz3/frNa/Ze83ae699W7+99mW2IgIzM7N6TWh3AczMrDs5gJiZWUMcQMzMrCEO\nIGZm1hAHEDMza4gDiJmZNaQjAoikZ0n6qqS7JD2Qv78l6X2SJlbIP0/S5yTdKelBSfdIulnS+yWd\nUci3RFIUPg9KulvSf0taLumYsZ3T+kjaksu9oYFh+wrzvaKGvCvy5/RGy9sK1cqd1+UKSRdXGGZD\nzr+lwWk2vJw7jaSFkr4jaZ+kPZKukfTUOoZ/jqRrJd2XP9dKek6VfGskbc/77BZJ/ybptwt5jpb0\nNknflvTznG+zpI9IOr5sfPNyHVAa368l3Srp7ZIeO0x5hwrby+1V8rxA0npJ9+bl8hNJV0lS/n1F\nWT1R/tlQZbznlOV7TtnvW4YZ5+mFfK+W9EVJP8vl25nL+wdl47tqhHJeVZb/aZJW5+1gX94uFlaZ\nl6dIujqvpweV6uBrJP1mtWUPQES09QPMBx4BosrnUWX53z9M3gC+VMi7ZIS8dwGntXsZDLNstuRy\nbmhg2L7CfK4YIe/ZhbxL2jzPFcsNbMhpWyoMU/W30V7OnfQB/qTKdr4XOLWG4Z8HPFRh+IeAPyjk\nOwt4uMq09gAn5Hy/N8y+tx04ujDOS4bJ+5Uq5f1fZflur5DnjcOMd1LOs2KEeuLLFcY7GbitLN9z\nqmxXlT6nF/L9cJh8FxbyXTVCOT9QyPvUvN4r5XtVWTmfC/yqSt5zhttmOqEF8rekltAWYC7wKOBk\n4OXAGtJMACDpr4GLcu9W4ALgccCjgWcA/wjsqDKdS4GJwG8A785pJwBrJD2uZXPTQhHRFxGKiLPb\nXZaxEhFb8jwrIla0uzzdQtJU4L2592ekfegcUuX/eA5t88P5EKli3AOcnj97ctqHCvkuJO1LAG8G\nHgt8MPc/AXhxIe93gT/KZTgl9wOcCPzvQr6bgJfl9KnAQuDB/Nt5kmaUze9E4APAAeCBSjMj6TTg\nitx7HXBaHvcppHrnAEBErChsc4oIAX9VGNWnKoz+b4DfBvZVmnaZ15SPPyJuKvx+L/AW0sHT44G3\nF357a6kjIpZUKOe7qpTz3XlcD5G2g5NJ2wXA+/L2gqRHAUOk5bIVOJdUpz4ReBVwx7Bz1gFHTT8i\nBYm1I+SbBOzKeR8Bnl7DuJdQ5Sgc+LfCbxcX0jdQw9EsMJjz7QaU0wZy2vZCvvfmtPs5dMQzhRTs\nbiNt/PcC/w48s8oRzIZC2kTgn/Ky+CXwceBFhXlZkvP1FdIuBf4ubwz3Al8EZtZw9NWXp/fWXNZf\nAfeRjpg+CTyx1cuHCi2QYcp3Vfk6A84Evk3asTcBC2rYTo5Yzjn9fODreTk/kMf3RmBiIc+xwEdJ\nO98DwC+AG4H31JnniPmucz/6o8Lwf19I/w8O7TPHDDP8MwvDf6iQ/qFC+u/ktHcX0p6U084ppP1t\nTpsKTBimnP8ywjzdWMj7uLLf/ro0jsL6u70sT2kbvA+YUefy3JSH3QVMKfvtiXmcO4GVhTJWa4Es\nGWFajy3rF4daDw8OM9wU4J6c7+ZC+kwOndX5j0L63xfK+pKc9seFtBH3lSPKUO8Arf6QjgxKM3AD\ncDmwAHh0Wb4zC/m+WuO4l1TbKYEzKo2P2gPIqwrD/3ZOu7aQdlJOuyH3r8v9k8ryFT8PAM+usAFu\nKKRdVmG4uwrdS3K+vkLazgrDXJ3zrahSllIAedMwv/ePwvIplntFTqs2/avK1tn9HNkUv5/CqZIq\nZa20nP/PMNP9TCHfV6rkub/OPEfMd5370UBh+AsK6cUKbt4ww/9ZIV/xgOriQvprc9ozgF/ntEtI\ngeKDuf8Aw5wWBhYVxvePVfI8mtQCeSDn+7ey348lHQjdAxxN9QBSOji9Ffh0zn8f6czGbw1TxmcX\nyvjOCr9/Ov+2hMP3n2oB5B5SS2APKaCfNcK6nJK32wB+WuOy/ItC+vxC+spC+gWF9Mtz2r/k/odJ\nrZnSQc7/AM8fabvrhFNYHy5095OacmuBn0t6c+G3OYXuH5U6JF1Y4WLSrBqm++NC9+wGyv2NQvdZ\nuUn9u6SVUUp7NKnZDPBf+fuVQOni2GLSznIy6Qh/Coc3SQ8j6QmkpjOk1sRTSRXP7hHKOg14PnAc\n6cgK4CWSJkQ6TVS8WFdsbm8BShcGvw1MJzVvTyNVHMNNt9Hlc4RITfWv596thfItKcv6GGAVMIN0\nIFJKe/4w5TxCPqX5jtx7Zy7jcaQACPBySWfn7tLyeQ9pXc4knVN+T2GUteRpVvGGkF9W6T62FcNH\nxPdIB3n3klrDvwL+gnT6+JUR8f1KE8jr+0259yHSWYDi74+VFKTW45dI+8MXOPxUF8A7Sdv0P0TE\nL4aZp5Py91NIp92OJp1ueyHwDUknVBnudfk7SBVssYy/n8f1bdK2VoujSacBn0DaFr8u6bnD5H8j\nabsF+Ngw+UrlvJ/Dl2U920JpGU0E3kCqC6eQ9tUvSzp3mOm3P4BExGeBl5Caq0WPB94u6RW5X8XB\nWjBpVUqMiLNz5dQ33MARsRXYlnvPAp5O2jjX5LRnkVo5k3N/qYIsVmarSEdyPyNt5ABnlM5PVlCa\nBsBHI+LWXI6RKqJrIuKrEXE36WiYXK7jRhgO0hEJwKmk024vI21s74yIn1YbqInl04yHgTdFxB7S\nUWLJSVXyV/MsUqCEtJxvzsvuskKeBfm7tHxeQLoe8Dzgzoh4ayHviHmi+Ws/Fbdnat9vah5e0tOA\nz5EqxKJpwO9KmnTESKQpeZi5Oekvh9t+Cl4C/GthPGcBrwY2AleOMOzkQvdbSOv0Lbl/JoeupxbL\nOQN4ae5dFxE/Kfw2iXTd5ZFc/pHqoY+QWjPTgOM5FIwmc/i2VJz+qwu/XUcKlpXyPZVDByafiohi\ncKhnWyguoy+TDhLPIbUkJ5D2+araHkAAIuKLEdFPin6vAf678HPptrOthbQnF4a9Oh+h1no0UHJK\noXtr1VzDKx1l/x6pkoR0TWJP7n9WTnsIuD53zxxhnCKtxEqKR0x3Frq3jzDOzYXu4gXHKSMMB+lI\n/pukyuJvSPP3XeCHkvpGGLaR5dOMnRFxX+6udz6LikdwxYuIxeVcWo9/TjoA+C3SzvYZ4CeSvixp\nch15mrWr0P34QvfjquRpZvi3kY5gD5Cuv00lHQ1PJV2cfn1xxDl4fIEUQAHeEBFHVP4RcX/elx9D\nujOwtOz/WNIzc/cy0j4yCDw93w57VP7tKEmnFy64F1sn74+I+0l3cZacxpGWkG7kgRQAii4AnkY+\nCMvTLt6O/CRJTyrMzxUR8e2I+GVE7CQFrNJF998tn7CkxaRgOYG0zy2MiP0VyghpmyopL2c967K4\njD4aEfdGxHqg1IqstIwOansAKd4BFRF3RMRVHDq6g3Q6AlIL5Z7cvUDSbzU56eJG/tUGx1GqIJ8K\nnJe7v02qDJ9BOtIEuCEiShVaaR4OkC7sld9VMSEiisGh6K5CdzGYjHSE/XChu9JRU9UjqYjYGRH/\nC5hFaj39PanJ/GTSzjycRpZP1aKM8DuMPJ+1uqfQPatK9z0AEfGtiPgNUgvtJRy6E+oPSXcS1pSn\nBb5b6H5yhe4DHKoUGhke4Hv5u/Ssx86IWBMRv6bQSuDQei0Fjy+SgkeQjtyHbTFHxL6I+Drw+UJy\n6YCv1AIfzOX5Hof2hZNy/4vKylvNryukLc3fd3KotVxSmvYLC9MuVuT/Sm4VSapUt5auP0C+A6xE\n0hLSwdUE0qnS8woHQ5TlfTTplm2A6/MpxaKbCuMfaV0Wh620z1RaRge1PYCQbqMdlHS2pMfl0zeL\nCr//CCBH4n/KaROBayTNl/QoSdMZ/vwukFaqpDmS/pl09wGk0ywfK+Sp56G0UgU5gVQZ3B4Ru4Dv\nkJqG8/LvxdMzXy0M8xFJJ0qaIuk0Se8B3jfM9G4mVd4Ar5X0ZElzSBc6m7Gn0H2qCg9vSloqaRHp\nKO860tFz6drHSK2pRpbPSGU8RmUPoY2C73BoOf+ZpLmSZnLo9AfAfwJIGsjnifeSTgF8oZBnZh15\nan7ws4qvcGgZvS6Pbx6HlvG6iLgnT6v4gO3ZABFxI4euLV6Yt8fTSOf7AX4YEaUgU7pV/jhJL8wV\n2msKZbk3T2cK6VrG80kV2p9GRPF24IMkfSDvz8fmffrZpDu2Smo53VXu6kL3Xyk9kFi8PffrxcxK\nD+6VDkyvjIjiAUm9XijpM7lemyrpONKNBqVrG98uTPc1pDpoAql++MOI+NUw476QQ6cPy1sf5H3s\na7l3ntJDmn0cumaym0P10OdIp+QAlkqaJul5HGp5HLaMjjDSVfbR/pBOV1W72+V+CndLkJquHx0m\nf+kzK+dfMkK+Oym7Y4Q6H0rj0K3FAXwijrwLIkhHE6X8k/NKqVamqwp5t3Dk3UGV7sLaUehenPP1\nFdJWFIZfUUjvy2lTSRtVcZzb829XDVPW143C8qlW7jdXmP6fVltn1cZTpYyVlvNFw8z3/6swbPnn\nIfKt5jXmqbm8w8xHTQ8Scvh+cXYhvdYHCReSAkKlaT0M/H7Od/Ywy7B8ed87TL5ralx/5XdhTeDw\nuzyLn+9z5J2enynMw6wal/mKwjifU0i/oMp0g1SvPaOGbeiw/bSQ//qcvrt8Hgp5hnuQ8E/K8laq\nU4J00f0pw81/J7RA3kKKoptIpwUeJlU6XwKeGxEH77iK5M9I9+d/OefbD/ycdDvou4AzIqLaNYH9\neRrXA8uBuVHljpE6fLPQXTqquJ5DTchHgG8V5mE/6WGd5aQ7rx4krehNpFsuR3rg61LSw1G/IN2S\n+EkOP5W0p9JAw4mIfaTK5xZSZVH0eWA16Xz0A7ms3wMuiogjjn4qqGv5DOP9pHm9Z6SMrRARHyA9\nEPcN0g7/IOl20DeR7qQrluvrpFul95O2yfWko8ib68jTijJ/klRxXU869bCXdBrm2RFxaw3DX0sK\nIteR7qz6Ve5+XkRcV8h3DWkfvJZU8T9CqszWAvMjnX6q1wdJF8Z/kce3l9QSfD2HLmrXJSIOkE43\n/V/S9rufdB3rA6Qgd/D0jKRjScsO0pPnI11XHMl3SPvq9aT1/TDpQO9TpOe9Rjq9VlG+7lL6u6ZP\nFOehKCJuIV3AX0Nalr/OZbkgbyfFvP9Iap38gEO3G38BODMibhu2PDkCWZfI/00zOSJ+mPuPIVXy\nzyWt/FmRmrBmZqPqiNvtrOOdBXxS0i9JLZDjOfS3EssdPMxsrHTCKSyrzy2kC7gPkJ7juI90OuTF\nEXHFcAOambWST2GZmVlD3AIxM7OGdOU1kGOOOSb6+vraXQwzs65y44033hMRIz2/VbOuDCB9fX1s\n3Lix3cUwM+sqkhr926aKfArLzMwa4gBiZmYNcQAxM7OGOICYmVlDHEDMzKwhDiBmVrOhTUP0rexj\nwqUT6FvZx9CmoXYXydqoK2/jNbOxN7RpiKVrlrJvf3qp3ta9W1m6Jr1/adHcRcMNauOUWyDWdXwU\n3B7L1i87GDxK9u3fx7L1I72Y0qrp9m3ZAcS6SukoeOverQRx8Ch4LHe8du707Zz2tr3b6krvBqOx\nPGsdZydsy81yALGuUu9RcKsriHp2+nZOezSmP3va7LrS26mWeR+NCryecY6HFp0DSIdp5xFRN6jn\nKHg0Kohad/p2Tnu0pj8wb4Cpk6celjZ18lQG5g00PM5SWdsRaEejAq9nnOOhRecA0kHafUTUbrVU\nJPUcBY9GBVHrTt/OaY/W9BfNXcTg+YPMmTYHIeZMm8Pg+YNNXUBvZ6AdjQq8nnF2U4uuGgeQDtLu\nI6J2n9uvpSKp5yh4NCqIWnf6dk57tKYPKYhsuXgLB5YfYMvFW5q++6qdgXY0KvB6xjlaLbqx5ADS\nQdp5RNTulkqtFUk9R8GjUUHUutO3c9qjNf3R0M5AOxoVeD3jHI0W3VhzAOkg7TwiavcFvXoqklqP\ngkejgqh1p2/ntEdr+qOhnYF2NCrwesfZ6hbdWOvKV9r29/fHeHwfSPmDWpA2/GY26lrHOeHSCQRH\nbgtCHFh+oKFp16NvZR9b9x75qoI50+aw5eItDY93aNMQy9YvY9vebcyeNpuBeQNjtpO2c9qdMP1a\njMY2Xxpvp897O0i6MSL6WzY+B5DOMhobfi3jHK0KvJ4yjkZFYp3Plf3Y6cgAIuk84L3ARODKiLii\n7PcpwCeAZwK/AF4REVskzQeuAI4CHgL+LiKuHWl64zmAtEsnVOCuSMxGV8cFEEkTgR8D84HtwA3A\nKyPi1kKevwCeHhGvk3Qh8OKIeIWkZwA7I+IuSU8D1kbEiSNN0wFkdLgCNxvfOjGAnAWsiIhzc/+b\nASLinwp51uY835E0Cfg5MDMKE5ck4B7giRHx4HDTdAAxM6tfqwNIK+7COhG4o9C/PadVzBMRDwN7\ngaPL8vwR8L1qwUPSUkkbJW3ctWtXC4ptZmbNaEUAUYW08mbNsHkkPRV4B/Dn1SYSEYMR0R8R/TNn\nzmyooGZm1jqtCCDbgZMK/bOAu6rlyaewpgG7c/8s4IvAqyPiJy0oj5mZjYFWBJAbgFMknSzpKOBC\nYHVZntXA4tz9UuDaiAhJTwD+HXhzRHyrBWUxM7Mx0nQAydc0LgLWArcBn42IWyRdJulFOdvHgKMl\n3Q68Abgkp18EPAl4q6Sb8ufYZstkZmajzw8Smpn1iE68C8vMzHqQA4iZmTXEAcTMzBriAGJmZg1x\nABkD4+md5GZmJZPaXYDxrvxfbktv+gP8R4Vm1tXcAhll7X7Tn9XGrUSz+rkFMspG453P1lpuJZo1\nxi2QUTYa73y21nIr0awxDiCjbGDeAFMnTz0sberkqQzMG2hTiaycW4lmjXEAGWWL5i5i8PxB5kyb\ngxBzps3xe747jFuJZo3xNZAxsGjuIgeMDjYwb6Di++DdSjQbnlsg1vPcSjRrjP+N18ysR/jfeM3M\nrCM4gJiZWUNaEkAknSfpR5Jul3RJhd+nSPpM/v16SX05/WhJ10m6X9IHWlEWMzMbG00HEEkTgQ8C\nzwdOBV4p6dSybK8F9kTEk4D3AO/I6Q8AbwXe2Gw5zMxsbLWiBXIGcHtE/DQiHgKuBhaW5VkIrMrd\nnwPmSVJE/CoivkkKJGZm1kVaEUBOBO4o9G/PaRXzRMTDwF7g6BZM28zM2qQVAUQV0srvDa4lz/AT\nkZZK2ihp465du+oZ1MzMRkErAsh24KRC/yzgrmp5JE0CpgG765lIRAxGRH9E9M+cObOJ4pqZWSu0\nIoDcAJwi6WRJRwEXAqvL8qwGFufulwLXRjc+wWhmbdPL72zp1Hlv+r+wIuJhSRcBa4GJwMcj4hZJ\nlwEbI2I18DHgk5JuJ7U8LiwNL2kL8HjgKEkXAAsi4tZmy2Vm40cvv7Olk+fdf2ViZh2vb2UfW/du\nPSJ9zrQ5bLl4y9gXaAy1ct79VyZm1nN6+Z0tnTzvDiBm1vF6+Z0tnTzvDiBm1vF6+c2enTzvDiBm\n1vF6+Z0tnTzvvohuZtYjfBHdzMw6ggOImZk1xAHEzMwa4gBiZmYNcQAxM7OGOIBYQzr1z93MbOw0\n/WeK1ns6+c/dzGzsuAVidVu2ftnB4FGyb/8+lq1f1qYSmVk7OIBY3Tr5z93MbOw4gFjdOvnP3cxs\n7DiAWN06+c/dzGzsOIBY3Tr5z93MbOy05M8UJZ0HvJf0StsrI+KKst+nAJ8Angn8AnhFRGzJv70Z\neC3wCPDXEbF2pOn5zxTNzOrXcX+mKGki8EHg+cCpwCslnVqW7bXAnoh4EvAe4B152FNJ70d/KnAe\n8KE8PjOzjuJnn47UilNYZwC3R8RPI+Ih4GpgYVmehcCq3P05YJ4k5fSrI+LBiPgZcHsen5mNEVeM\nIys9+7R171aCOPjsU68vq1YEkBOBOwr923NaxTwR8TCwFzi6xmEBkLRU0kZJG3ft2tWCYpuZK8ba\n+NmnyloRQFQhrfzCSrU8tQybEiMGI6I/IvpnzpxZZxHNrBJXjLXxs0+VtSKAbAdOKvTPAu6qlkfS\nJGAasLvGYc3GvXadRnLFWBs/+1RZKwLIDcApkk6WdBTpovjqsjyrgcW5+6XAtZFu/1oNXChpiqST\ngVOA/2lBmcy6RjtPI7lirI2ffaqs6QCSr2lcBKwFbgM+GxG3SLpM0otyto8BR0u6HXgDcEke9hbg\ns8CtwFeBv4yIR5otk1k3aedpJFeMtfGzT5W15DmQsebnQGw8mXDpBKLCpT8hDiw/MOrTH9o0xLL1\ny9i2dxuzp81mYN5Az1eM41WrnwPx37mbtdnsabPZundrxfSxsGjuIgcMa4j/ysSszXwaybqVA4hZ\nm/n8unUrXwMxM+sRHfdfWGZm1pscQMzMrCEOIGZm1hAHEDMza4gDiJmZNcQBxMzMGuIAYmZmDXEA\nMTOzhjiAmJlZQxxAzMysIQ4gZmbWEAcQMzNriAOImZk1pKkAImmGpHWSNufv6VXyLc55NktaXEgf\nkHSHpPubKYeZmY29ZlsglwDrI+IUYH3uP4ykGcBy4EzgDGB5IdCsyWlmZtZlmg0gC4FVuXsVcEGF\nPOcC6yJid0TsAdYB5wFExH9HxI4my2BmZm3QbAA5rhQA8vexFfKcCNxR6N+e0+oiaamkjZI27tq1\nq6HCmplZ60waKYOkrwHHV/hpWY3TUIW0ul+DGBGDwCCkNxLWO7yZmbXWiAEkIs6p9puknZJOiIgd\nkk4A7q6QbTtwdqF/FrChznKamVmHafYU1mqgdFfVYuCaCnnWAgskTc8XzxfkNDMz62LNBpArgPmS\nNgPzcz+S+iVdCRARu4HLgRvy57KchqR3StoOTJW0XdKKJstjZmZjRBHddzmhv78/Nm7c2O5imJl1\nFUk3RkR/q8bnJ9HNzKwhDiBmZtYQBxAzM2uIA4iZmTXEAcTMzBriAGJmZg1xADEzs4Y4gJiZWUMc\nQMzMrCEOIGZm1hAHEDMza4gDSJmhTUP0rexjwqUT6FvZx9CmoXYXycysI434PpBeMrRpiKVrlrJv\n/z4Atu7dytI1SwFYNHdRO4tmZtZx3AIpWLZ+2cHgUbJv/z6Wra/15YtmZr3DAaRg295tdaWbmfUy\nB5CC2dNm15VuZtbLHEAKBuYNMHXy1MPSpk6eysC8gTaVyMysczUVQCTNkLRO0ub8Pb1KvsU5z2ZJ\ni3PaVEn/LumHkm6RdEUzZWmFRXMXMXj+IHOmzUGIOdPmMHj+oC+gm5lV0NQrbSW9E9gdEVdIugSY\nHhFvKsszA9gI9AMB3Ag8E3gQODMirpN0FLAeeHtEfGWk6fqVtmZm9eu0V9ouBFbl7lXABRXynAus\ni4jdEbEHWAecFxH7IuI6gIh4CPguMKvJ8piZ2RhpNoAcFxE7APL3sRXynAjcUejfntMOkvQE4HxS\nK6QiSUslbZS0cdeuXU0W28zMmjXig4SSvgYcX+GnWh+OUIW0g+fNJE0CPg28LyJ+Wm0kETEIDEI6\nhVXjtM3MbJSMGEAi4pxqv0naKemEiNgh6QTg7grZtgNnF/pnARsK/YPA5ohYWVOJzcysIzR7Cms1\nsDh3LwauqZBnLbBA0vR8l9aCnIaktwHTgIubLIeZmY2xZgPIFcB8SZuB+bkfSf2SrgSIiN3A5cAN\n+XNZROyWNIt0GuxU4LuSbpL0p02Wx8zMxkhTt/G2i2/jNTOrX6fdxmtmZj3KAcTMzBriAGJmZg1x\nADEzs4Y4gJiZWUMcQMzMrCEOIGZm1hAHEBtVQ5uG6FvZx4RLJ9C3so+hTUPtLpKZtciI/4Vl1qih\nTUMsXbOUffv3AbB171aWrlkK4Jd0mY0DboHYqFm2ftnB4FGyb/8+lq2v9Y+czayTOYDYqNm2d1td\n6WbWXRxAbNTMnja7rnQz6y4OIDZqBuYNMHXy1MPSpk6eysC8gTaVyMxayQHERs2iuYsYPH+QOdPm\nIMScaXMYPH/QF9DNxgn/nbuZWY/w37mbmVlHaCqASJohaZ2kzfl7epV8i3OezZIWF9K/Kun7km6R\n9BFJE5spj5mZjZ1mWyCXAOsj4hRgfe4/jKQZwHLgTOAMYHkh0Lw8Ik4DngbMBF7WZHnMzGyMNBtA\nFgKrcvcq4IIKec4F1kXE7ojYA6wDzgOIiF/mPJOAo4DuuyBjZtajmg0gx0XEDoD8fWyFPCcCdxT6\nt+c0ACStBe4G7gM+12R5zMxsjIz4X1iSvgYcX+GnWv+PQhXSDrY0IuJcSY8ChoDnkVoolcqxFFgK\nMHu2H0QzM2u3EQNIRJxT7TdJOyWdEBE7JJ1AakmU2w6cXeifBWwom8YDklaTTolVDCARMQgMQrqN\nd6Rym5nZ6Gr2FNZqoHRX1WLgmgp51gILJE3PF88XAGslPTYHHSRNAl4A/LDJ8piZ2RhpNoBcAcyX\ntBmYn/uR1C/pSoCI2A1cDtyQP5fltMcAqyXdDHyf1Hr5SJPlMTOzMeIn0c3MeoSfRO8gftuemfUy\nv5GwQX7bnpn1OrdAGuS37ZlZr3MAaZDftmdmvc4BpEF+256Z9ToHkAb5bXtm1uscQBrkt+2ZWa/z\ncyBmZj3Cz4GYmVlHcAAxM7OGOICYmVlDHEDMzKwhDiBmZtYQBxAzM2uIA4iZmTXEAcTMzBriAGJm\nZg1pKoBImiFpnaTN+Xt6lXyLc57NkhZX+H21pB80UxYzMxtbzbZALgHWR8QpwPrcfxhJM4DlwJnA\nGcDyYqCR9BLg/ibLYWZmY6zZALIQWJW7VwEXVMhzLrAuInZHxB5gHXAegKTHAm8A3tZkOczMbIw1\nG0COi4gdAPn72Ap5TgTuKPRvz2kAlwPvAvaVD1RO0lJJGyVt3LVrV3OlNjOzpo34TnRJXwOOr/BT\nre9uVYW2OiahAAAHe0lEQVS0kHQ68KSI+BtJfSONJCIGgUFI/8Zb47TNzGyUjBhAIuKcar9J2inp\nhIjYIekE4O4K2bYDZxf6ZwEbgLOAZ0rakstxrKQNEXE2ZmbW8Zo9hbUaKN1VtRi4pkKetcACSdPz\nxfMFwNqI+HBEPDEi+oDnAD928DAz6x7NBpArgPmSNgPzcz+S+iVdCRARu0nXOm7In8tympmZdTG/\nkdDMrEf4jYRmZtYRHEDMzKwhDiBmZtYQBxAzM2uIA4iZmTXEAcTMzBriAGJmZg1xADEzs4Y4gJiZ\nWUMcQMzMrCEOIGZm1hAHEDMza4gDiJmZNaRnAsjQpiH6VvYx4dIJ9K3sY2jTULuLZGbW1UZ8I+F4\nMLRpiKVrlrJvf3r1+ta9W1m6ZikAi+YuamfRzMy6Vk+0QJatX3YweJTs27+PZetrfa27mZmV64kA\nsm3vtrrSzcxsZE0FEEkzJK2TtDl/T6+Sb3HOs1nS4kL6Bkk/knRT/hzbTHmqmT1tdl3pZmY2smZb\nIJcA6yPiFGB97j+MpBnAcuBM4AxgeVmgWRQRp+fP3U2Wp6KBeQNMnTz1sLSpk6cyMG9gNCZnZtYT\nmg0gC4FVuXsVcEGFPOcC6yJid0TsAdYB5zU53bosmruIwfMHmTNtDkLMmTaHwfMHfQHdzKwJzd6F\ndVxE7ACIiB1VTkGdCNxR6N+e00r+VdIjwOeBt0VEVJqQpKXAUoDZs+s/9bRo7iIHDLMxMrRpiGXr\nl7Ft7zZmT5vNwLwB73/j0IgBRNLXgOMr/FTrLUyqkFYKEosi4k5JjyMFkD8BPlFpJBExCAwC9Pf3\nVwwyZtZ+vm2+d4x4CisizomIp1X4XAPslHQCQP6udA1jO3BSoX8WcFce9535+z7gU6RrJGbWxXzb\nfO9o9hrIaqB0V9Vi4JoKedYCCyRNzxfPFwBrJU2SdAyApMnAC4EfNFmenuEn661T+bb53tFsALkC\nmC9pMzA/9yOpX9KVABGxG7gcuCF/LstpU0iB5GbgJuBO4KNNlqcnlE4RbN27lSAOniJwELFO4Nvm\ne4eqXLPuaP39/bFx48Z2F6Nt+lb2sXXv1iPS50ybw5aLt4x9gcwKyq+BQLpt3nc+tp+kGyOiv1Xj\n64kn0ccbnyKwTubb5ntHT/yZ4ngze9rsii0QnyKwTuHb5nuDWyBdyE/Wm1kncADpQj5FYGadwBfR\nzcx6hC+im5lZR3AAMTOzhjiAmJlZQxxAzMysIQ4gZmbWkK68C0vSLuDIJ+lqcwxwTwuL027jbX5g\n/M3TeJsfGH/zNN7mByrP05yImNmqCXRlAGmGpI2tvI2t3cbb/MD4m6fxNj8w/uZpvM0PjM08+RSW\nmZk1xAHEzMwa0osBZLDdBWix8TY/MP7mabzND4y/eRpv8wNjME89dw3EzMxaoxdbIGZm1gIOIGZm\n1pCeCSCSzpP0I0m3S7qk3eVpBUlbJG2SdJOkrvx7Ykkfl3S3pB8U0mZIWidpc/6e3s4y1qPK/KyQ\ndGdeTzdJekE7y1gPSSdJuk7SbZJukfT6nN7N66jaPHXlepL0KEn/I+n7eX4uzeknS7o+r6PPSDqq\n5dPuhWsgkiYCPwbmA9uBG4BXRsStbS1YkyRtAfojomsfgJL0XOB+4BMR8bSc9k5gd0RckYP99Ih4\nUzvLWasq87MCuD8i/rmdZWuEpBOAEyLiu5IeB9wIXAAsoXvXUbV5ejlduJ4kCXhMRNwvaTLwTeD1\nwBuAL0TE1ZI+Anw/Ij7cymn3SgvkDOD2iPhpRDwEXA0sbHOZDIiI/wJ2lyUvBFbl7lWknbsrVJmf\nrhUROyLiu7n7PuA24ES6ex1Vm6euFMn9uXdy/gTwPOBzOX1U1lGvBJATgTsK/dvp4g2mIID/lHSj\npKXtLkwLHRcROyDt7MCxbS5PK1wk6eZ8iqtrTvcUSeoDngFczzhZR2XzBF26niRNlHQTcDewDvgJ\ncG9EPJyzjEqd1ysBRBXSxsO5u2dHxO8Azwf+Mp8+sc7zYeA3gdOBHcC72luc+kl6LPB54OKI+GW7\ny9MKFeapa9dTRDwSEacDs0hnXJ5SKVurp9srAWQ7cFKhfxZwV5vK0jIRcVf+vhv4ImnDGQ925vPU\npfPVd7e5PE2JiJ15Bz8AfJQuW0/5vPrngaGI+EJO7up1VGmeun09AUTEvcAG4PeAJ0ialH8alTqv\nVwLIDcAp+a6Eo4ALgdVtLlNTJD0mXwBE0mOABcAPhh+qa6wGFufuxcA1bSxL00oVbfZiumg95Qu0\nHwNui4h3F37q2nVUbZ66dT1JminpCbn70cA5pOs61wEvzdlGZR31xF1YAPmWvJXARODjETHQ5iI1\nRdJvkFodAJOAT3XjPEn6NHA26a+ndwLLgS8BnwVmA9uAl0VEV1yYrjI/Z5NOiwSwBfjz0vWDTifp\nOcA3gE3AgZz8D6RrBt26jqrN0yvpwvUk6emki+QTSY2Cz0bEZbmOuBqYAXwPeFVEPNjSafdKADEz\ns9bqlVNYZmbWYg4gZmbWEAcQMzNriAOImZk1xAHEzMwa4gBiZmYNcQAxM7OG/H9q8CcBiqAk6gAA\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3899eda2b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w, loss = func_SGD (y=y_train, tx=tx_train, test_set=tx_test, max_iters=10,gamma=gamma, initial_w=initial_w,\\\n",
    "                   batch_size=5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *LOGISTIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def func_logistic (y, tx, test_set, max_iters, gamma, initial_w):\n",
    "    name = 'Logistic regression'\n",
    "    \n",
    "    #w,loss = logistic_regression(y, tx, initial_w, max_iters, gamma)\n",
    "    w,loss = logistic_regression_mat (y, tx, initial_w, max_iters, gamma)\n",
    "    \n",
    "    y_pred = predict_labels(w, test_set)\n",
    "    create_csv_submission(ids_test, y_pred, name)\n",
    "    \n",
    "    plt.plot(w, 'go')\n",
    "    plt.title('logistic reg: weights with loss:  '+str(loss),fontsize=15, fontweight='bold');\n",
    "    plt.show()\n",
    "    \n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent logistic (1/49): loss=[ 173286.7951405]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zampieri/Documents/EPFL/ML/LMO_ML/project1/lib/implementations.py:185: RuntimeWarning: overflow encountered in exp\n",
      "  log_like=log_like+(np.log(1+np.exp(tx[i,:].T.dot(initial_w)))-y[i,:].dot(tx[i,:].dot(initial_w)));\n",
      "/home/zampieri/Documents/EPFL/ML/LMO_ML/project1/lib/implementations.py:223: RuntimeWarning: overflow encountered in exp\n",
      "  sigma[i,:]= np.exp(v[i,:])/((1+np.exp(v[i,:])));\n",
      "/home/zampieri/Documents/EPFL/ML/LMO_ML/project1/lib/implementations.py:223: RuntimeWarning: invalid value encountered in true_divide\n",
      "  sigma[i,:]= np.exp(v[i,:])/((1+np.exp(v[i,:])));\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent logistic (2/49): loss=[ inf]\n",
      "Gradient Descent logistic (3/49): loss=[ nan]\n",
      "Gradient Descent logistic (4/49): loss=[ nan]\n",
      "Gradient Descent logistic (5/49): loss=[ nan]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-b8969dfd2e6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_logistic\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m                         \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-73-cec0c042ddca>\u001b[0m in \u001b[0;36mfunc_logistic\u001b[0;34m(y, tx, test_set, max_iters, gamma, initial_w)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#w,loss = logistic_regression(y, tx, initial_w, max_iters, gamma)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogistic_regression_mat\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/EPFL/ML/LMO_ML/project1/lib/implementations.py\u001b[0m in \u001b[0;36mlogistic_regression_mat\u001b[0;34m(y, tx, initial_w, max_iters, gamma)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m     \u001b[0;34m[\u001b[0m\u001b[0mw_opt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlog_like\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mlogistic_gradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog_like\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mw_opt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlog_like\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/EPFL/ML/LMO_ML/project1/lib/implementations.py\u001b[0m in \u001b[0;36mlogistic_gradient_descent\u001b[0;34m(y, tx, initial_w, max_iters, gamma)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0msigma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m                 \u001b[0msigma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "w, loss = func_logistic (y=y_train, tx=tx_train, test_set=tx_test, max_iters=50,\\\n",
    "                         gamma=0.0001, initial_w=initial_w);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70.532399999999996"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_train = predict_labels(w, tx_train)\n",
    "right_train = np.sum(y_pred_train == y_train)/len(y_train)*100\n",
    "right_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
