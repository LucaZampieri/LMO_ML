{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Project 1\n",
    "\n",
    "    We begin by loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import numpy as np   # generic stuff\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#### REMOVE THIS LINE BEFORE SUBMISSION\n",
    "import pandas as pd\n",
    "#######################################################################\n",
    "\n",
    "from lib.proj1_helpers import * #the helper provided for the project\n",
    "from lib.costs import *\n",
    "\n",
    "# choose which implementations you would like\n",
    "from lib.implementations import *\n",
    "#from implementations import * #our implementations of the functions done by us\n",
    "\n",
    "\n",
    "import datetime\n",
    "from helpers import * #helpers of exo 2\n",
    "# Useful starting lines\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_FOLDER = 'data/'\n",
    "\n",
    "y_train, tx_train, ids_train = load_csv_data(DATA_FOLDER+'train.csv',sub_sample=False)\n",
    "\n",
    "y_test, tx_test, ids_test = load_csv_data(DATA_FOLDER+'test.csv',sub_sample=False)\n",
    "AAA = 0;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note that here we are only considering a sub_sample as the \"True\" value indicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1., -1., -1., ...,  1., -1., -1.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -4.90230794e+01,   4.92398193e+01,   8.11819816e+01,\n",
       "         5.78959617e+01,  -7.08420675e+02,  -6.01237051e+02,\n",
       "        -7.09356603e+02,   2.37309984e+00,   1.89173324e+01,\n",
       "         1.58432217e+02,   1.43760943e+00,  -1.28304708e-01,\n",
       "        -7.08985189e+02,   3.87074191e+01,  -1.09730480e-02,\n",
       "        -8.17107200e-03,   4.66602072e+01,  -1.95074680e-02,\n",
       "         4.35429640e-02,   4.17172345e+01,  -1.01191920e-02,\n",
       "         2.09797178e+02,   9.79176000e-01,  -3.48329567e+02,\n",
       "        -3.99254314e+02,  -3.99259788e+02,  -6.92381204e+02,\n",
       "        -7.09121609e+02,  -7.09118631e+02,   7.30645914e+01])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids_train\n",
    "np.mean(tx_train,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>138.470</td>\n",
       "      <td>51.655</td>\n",
       "      <td>97.827</td>\n",
       "      <td>27.980</td>\n",
       "      <td>0.91</td>\n",
       "      <td>124.711</td>\n",
       "      <td>2.666</td>\n",
       "      <td>3.064</td>\n",
       "      <td>41.928</td>\n",
       "      <td>197.760</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.277</td>\n",
       "      <td>258.733</td>\n",
       "      <td>2.0</td>\n",
       "      <td>67.435</td>\n",
       "      <td>2.150</td>\n",
       "      <td>0.444</td>\n",
       "      <td>46.062</td>\n",
       "      <td>1.24</td>\n",
       "      <td>-2.475</td>\n",
       "      <td>113.497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>160.937</td>\n",
       "      <td>68.768</td>\n",
       "      <td>103.235</td>\n",
       "      <td>48.146</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.473</td>\n",
       "      <td>2.078</td>\n",
       "      <td>125.157</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.916</td>\n",
       "      <td>164.546</td>\n",
       "      <td>1.0</td>\n",
       "      <td>46.226</td>\n",
       "      <td>0.725</td>\n",
       "      <td>1.158</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>46.226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-999.000</td>\n",
       "      <td>162.172</td>\n",
       "      <td>125.953</td>\n",
       "      <td>35.635</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.148</td>\n",
       "      <td>9.336</td>\n",
       "      <td>197.814</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.186</td>\n",
       "      <td>260.414</td>\n",
       "      <td>1.0</td>\n",
       "      <td>44.251</td>\n",
       "      <td>2.053</td>\n",
       "      <td>-2.028</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>44.251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>143.905</td>\n",
       "      <td>81.417</td>\n",
       "      <td>80.943</td>\n",
       "      <td>0.414</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.310</td>\n",
       "      <td>0.414</td>\n",
       "      <td>75.968</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060</td>\n",
       "      <td>86.062</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>175.864</td>\n",
       "      <td>16.915</td>\n",
       "      <td>134.805</td>\n",
       "      <td>16.405</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.891</td>\n",
       "      <td>16.405</td>\n",
       "      <td>57.983</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.871</td>\n",
       "      <td>53.131</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0        1        2       3       4        5        6      7       8   \\\n",
       "0  138.470   51.655   97.827  27.980    0.91  124.711    2.666  3.064  41.928   \n",
       "1  160.937   68.768  103.235  48.146 -999.00 -999.000 -999.000  3.473   2.078   \n",
       "2 -999.000  162.172  125.953  35.635 -999.00 -999.000 -999.000  3.148   9.336   \n",
       "3  143.905   81.417   80.943   0.414 -999.00 -999.000 -999.000  3.310   0.414   \n",
       "4  175.864   16.915  134.805  16.405 -999.00 -999.000 -999.000  3.891  16.405   \n",
       "\n",
       "        9    ...        20       21   22       23       24       25       26  \\\n",
       "0  197.760   ...    -0.277  258.733  2.0   67.435    2.150    0.444   46.062   \n",
       "1  125.157   ...    -1.916  164.546  1.0   46.226    0.725    1.158 -999.000   \n",
       "2  197.814   ...    -2.186  260.414  1.0   44.251    2.053   -2.028 -999.000   \n",
       "3   75.968   ...     0.060   86.062  0.0 -999.000 -999.000 -999.000 -999.000   \n",
       "4   57.983   ...    -0.871   53.131  0.0 -999.000 -999.000 -999.000 -999.000   \n",
       "\n",
       "       27       28       29  \n",
       "0    1.24   -2.475  113.497  \n",
       "1 -999.00 -999.000   46.226  \n",
       "2 -999.00 -999.000   44.251  \n",
       "3 -999.00 -999.000    0.000  \n",
       "4 -999.00 -999.000    0.000  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# illegal: \n",
    "pd.DataFrame(tx_train).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_999_median(tx):\n",
    "    nan_values = (tx==-999)*1\n",
    "    for col in range(tx.shape[1]):\n",
    "        column = tx[:,col][tx[:,col]!=-999]\n",
    "        median = np.median(column)\n",
    "        tx[:,col][tx[:,col]==-999] = median\n",
    "    return tx, nan_values\n",
    "\n",
    "def replace_999_mean(tx):\n",
    "    nan_values = (tx==-999)*1\n",
    "    for col in range(tx.shape[1]):\n",
    "        column = tx[:,col][tx[:,col]!=-999]\n",
    "        median = np.median(column)\n",
    "        tx[:,col][tx[:,col]==-999] = median\n",
    "    return tx, nan_values\n",
    "\n",
    "def add_ones(tx):\n",
    "    return np.concatenate((np.ones([tx.shape[0],1]),tx), axis=1)\n",
    "\n",
    "def standardize(x):\n",
    "    \"\"\"Standardize the original data set.\"\"\"\n",
    "    mean_x = np.mean(x, axis=0)\n",
    "    x = x - mean_x\n",
    "    std_x = np.std(x, axis=0)\n",
    "    x = x / std_x\n",
    "    return x, mean_x, std_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tx_train, nan_values_train = replace_999_median(tx_train)\n",
    "tx_test, nan_values_test = replace_999_median(tx_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tx_train = standardize(tx_train)\n",
    "tx_train = tx_train[0]\n",
    "tx_test = standardize(tx_test)\n",
    "tx_test = tx_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if (AAA!=1):\n",
    "    tx_train = add_ones(tx_train)\n",
    "    tx_test = add_ones(tx_test)\n",
    "    AAA = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.00000000e+00,   9.27252671e-13,   4.50019089e-15,\n",
       "        -3.48448848e-15,   7.19675786e-15,  -3.68138116e-12,\n",
       "         2.13543213e-12,   5.95433581e-13,   2.16429719e-14,\n",
       "         6.39742126e-15,   2.86409207e-15,  -7.00447966e-15,\n",
       "         4.45924897e-15,   4.29197803e-12,  -5.96492045e-15,\n",
       "         1.35646161e-16,   7.13136217e-17,   2.58030370e-14,\n",
       "        -1.06327391e-16,  -1.87188487e-16,   8.24369382e-15,\n",
       "         1.41040513e-16,  -9.00283004e-15,  -6.01698247e-16,\n",
       "        -2.95741174e-12,   1.46076658e-15,   2.17857554e-14,\n",
       "        -4.93485594e-12,   5.44553736e-15,  -2.56949984e-15,\n",
       "        -8.76751116e-16])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(tx_train,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 31)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tx_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_w = np.zeros(tx_train.shape[1])\n",
    "initial_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gamma = 0.05\n",
    "max_iters = 1000\n",
    "lambda_ = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for methods: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def func_least_squares (y, tx, test_set, fct='mse'):\n",
    "    name = 'least_squares'\n",
    "    w,loss = least_squares(y,tx,fct)\n",
    "    y_pred = predict_labels(w, test_set)\n",
    "    create_csv_submission(ids_test, y_pred, name)\n",
    "    \n",
    "    plt.plot(w, 'go')\n",
    "    plt.title('least squares weights for loss:  '+str(loss),fontsize=15, fontweight='bold');\n",
    "    plt.show()\n",
    "    \n",
    "    return w, loss\n",
    "\n",
    "def func_GD (y, tx, test_set, max_iters, gamma, initial_w):\n",
    "    name = 'Gradient_descent'\n",
    "    w,loss = least_squares_GD(y, tx, initial_w, max_iters, gamma,fct='mse');\n",
    "    y_pred = predict_labels(w, test_set)\n",
    "    create_csv_submission(ids_test, y_pred, name)\n",
    "    \n",
    "    plt.plot(w, 'go')\n",
    "    plt.title('GD: weights with loss:  '+str(loss),fontsize=15, fontweight='bold');\n",
    "    plt.show()\n",
    "    \n",
    "    return w, loss\n",
    "\n",
    "########################  RIDGE REGRESSION #######################################################\n",
    "def func_ridge_regression (y, tx, test_set, lambda_):\n",
    "    name = 'Ridge_regression'\n",
    "    \n",
    "    w,loss = ridge_regression(y, tx, lambda_=lambda_, fct='mse');\n",
    "    \n",
    "    y_pred = predict_labels(w, test_set)\n",
    "    create_csv_submission(ids_test, y_pred, name)\n",
    "    \n",
    "    plt.plot(w, 'go')\n",
    "    plt.title('Ridge regression: weights ;loss:  '+str(loss),fontsize=15, fontweight='bold');\n",
    "    plt.show()\n",
    "    \n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *LEAST-SQUARES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaoAAAEKCAYAAABDkxEYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9af46f5d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w, loss = func_least_squares (y=y_train, tx=tx_train, test_set=tx_test, fct='mse');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74.499200000000002"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_train = predict_labels(w, tx_train)\n",
    "right_train = np.sum(y_pred_train == y_train)/len(y_train)*100\n",
    "right_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Gradient descent     -------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/499): loss=0.5000000000000249, w0=-0.031466400000014945, w1=0.002504206625732244\n",
      "Gradient Descent(1/499): loss=0.4494343967425655, w0=-0.0597861600000126, w1=0.004438498929019952\n",
      "Gradient Descent(2/499): loss=0.4237550966541934, w0=-0.08527394400001406, w1=0.006020285830294936\n",
      "Gradient Descent(3/499): loss=0.4070187761061894, w0=-0.108212949600019, w1=0.0072840399711404345\n",
      "Gradient Descent(4/499): loss=0.3951946905073953, w0=-0.12885805464002748, w1=0.008234547940797575\n",
      "Gradient Descent(5/499): loss=0.38648993849775537, w0=-0.14743864917603822, w1=0.008883836358338948\n",
      "Gradient Descent(6/499): loss=0.3798896347830327, w0=-0.16416118425845092, w1=0.009255089133456313\n",
      "Gradient Descent(7/499): loss=0.37476690775624144, w0=-0.17921146583262498, w1=0.009378902879218826\n",
      "Gradient Descent(8/499): loss=0.37071472549275486, w0=-0.19275671924938384, w1=0.00928903699482596\n",
      "Gradient Descent(9/499): loss=0.3674580691602446, w0=-0.20494744732446882, w1=0.009019283091522143\n",
      "Gradient Descent(10/499): loss=0.36480472393657815, w0=-0.21591910259204714, w1=0.008601519728930907\n",
      "Gradient Descent(11/499): loss=0.36261648374655586, w0=-0.22579359233286914, w1=0.008064668317610603\n",
      "Gradient Descent(12/499): loss=0.36079163177278045, w0=-0.23468063309961046, w1=0.007434256491318512\n",
      "Gradient Descent(13/499): loss=0.3592538737238359, w0=-0.24267896978967884, w1=0.006732362016934813\n",
      "Gradient Descent(14/499): loss=0.3579450962900165, w0=-0.24987747281074155, w1=0.00597777882561493\n",
      "Gradient Descent(15/499): loss=0.35682047490212365, w0=-0.25635612552969916, w1=0.0051863002497362\n",
      "Gradient Descent(16/499): loss=0.35584507472245464, w0=-0.262186912976762, w1=0.00437105261960692\n",
      "Gradient Descent(17/499): loss=0.35499143157761803, w0=-0.26743462167911947, w1=0.0035428382968072975\n",
      "Gradient Descent(18/499): loss=0.35423779479871625, w0=-0.27215755951124215, w1=0.002710464396935861\n",
      "Gradient Descent(19/499): loss=0.3535668286601078, w0=-0.2764082035601534, w1=0.0018810445772838272\n",
      "Gradient Descent(20/499): loss=0.3529646386535219, w0=-0.2802337832041743, w1=0.0010602682903819998\n",
      "Gradient Descent(21/499): loss=0.3524200322832165, w0=-0.2836768048837939, w1=0.00025263619990395936\n",
      "Gradient Descent(22/499): loss=0.35192395198892207, w0=-0.28677552439545223, w1=-0.0005383370412434682\n",
      "Gradient Descent(23/499): loss=0.3514690362130231, w0=-0.2895643719559454, w1=-0.0013099501045215718\n",
      "Gradient Descent(24/499): loss=0.35104927705013034, w0=-0.2920743347603899, w1=-0.002060169229780778\n",
      "Gradient Descent(25/499): loss=0.35065975147295947, w0=-0.2943333012843906, w1=-0.0027875060716798417\n",
      "Gradient Descent(26/499): loss=0.3502964091307315, w0=-0.29636637115599185, w1=-0.0034909159166575904\n",
      "Gradient Descent(27/499): loss=0.34995590399724485, w0=-0.2981961340404335, w1=-0.004169713277853097\n",
      "Gradient Descent(28/499): loss=0.3496354602447124, w0=-0.29984292063643153, w1=-0.004823502181765157\n",
      "Gradient Descent(29/499): loss=0.3493327649931605, w0=-0.30132502857283033, w1=-0.005452118787255545\n",
      "Gradient Descent(30/499): loss=0.3490458822739176, w0=-0.3026589257155898, w1=-0.006055584297156151\n",
      "Gradient Descent(31/499): loss=0.34877318381384237, w0=-0.30385943314407377, w1=-0.006634066420066071\n",
      "Gradient Descent(32/499): loss=0.34851329320881186, w0=-0.3049398898297099, w1=-0.007187847907624596\n",
      "Gradient Descent(33/499): loss=0.3482650407910446, w0=-0.30591230084678284, w1=-0.007717300928190996\n",
      "Gradient Descent(34/499): loss=0.3480274270626065, w0=-0.30678747076214896, w1=-0.008222866241914055\n",
      "Gradient Descent(35/499): loss=0.34779959300848823, w0=-0.30757512368597895, w1=-0.00870503631670072\n",
      "Gradient Descent(36/499): loss=0.34758079594734814, w0=-0.3082840113174264, w1=-0.009164341672451323\n",
      "Gradient Descent(37/499): loss=0.34737038984888213, w0=-0.30892201018572957, w1=-0.009601339865259552\n",
      "Gradient Descent(38/499): loss=0.34716780926062063, w0=-0.30949620916720283, w1=-0.01001660662719802\n",
      "Gradient Descent(39/499): loss=0.34697255615646283, w0=-0.3100129882505292, w1=-0.010410728763756885\n",
      "Gradient Descent(40/499): loss=0.3467841891540662, w0=-0.31047808942552335, w1=-0.010784298482628574\n",
      "Gradient Descent(41/499): loss=0.34660231465579394, w0=-0.3108966804830185, w1=-0.011137908886685415\n",
      "Gradient Descent(42/499): loss=0.3464265795539457, w0=-0.31127341243476453, w1=-0.011472150412720133\n",
      "Gradient Descent(43/499): loss=0.346256665209985, w0=-0.31161247119133634, w1=-0.011787608037560224\n",
      "Gradient Descent(44/499): loss=0.3460922824728801, w0=-0.3119176240722514, w1=-0.012084859106013864\n",
      "Gradient Descent(45/499): loss=0.345933167546257, w0=-0.3121922616650753, w1=-0.012364471662006231\n",
      "Gradient Descent(46/499): loss=0.3457790785500007, w0=-0.31243943549861725, w1=-0.012627003186269232\n",
      "Gradient Descent(47/499): loss=0.34562979265092386, w0=-0.31266189194880534, w1=-0.012872999661926081\n",
      "Gradient Descent(48/499): loss=0.34548510366056673, w0=-0.312862102753975, w1=-0.013102994903987579\n",
      "Gradient Descent(49/499): loss=0.3453448200171256, w0=-0.3130422924786281, w1=-0.01331751010074708\n",
      "Gradient Descent(50/499): loss=0.34520876308387344, w0=-0.3132044632308162, w1=-0.013517053524818022\n",
      "Gradient Descent(51/499): loss=0.3450767657088646, w0=-0.31335041690778587, w1=-0.013702120379506253\n",
      "Gradient Descent(52/499): loss=0.3449486710008176, w0=-0.3134817752170589, w1=-0.013873192752681522\n",
      "Gradient Descent(53/499): loss=0.34482433128427203, w0=-0.313599997695405, w1=-0.014030739655580028\n",
      "Gradient Descent(54/499): loss=0.3447036072037792, w0=-0.3137063979259168, w1=-0.014175217128255967\n",
      "Gradient Descent(55/499): loss=0.34458636695231337, w0=-0.3138021581333778, w1=-0.014307068396885834\n",
      "Gradient Descent(56/499): loss=0.3444724856035177, w0=-0.313888342320093, w1=-0.014426724070963766\n",
      "Gradient Descent(57/499): loss=0.34436184453099616, w0=-0.313965908088137, w1=-0.014534602370730303\n",
      "Gradient Descent(58/499): loss=0.34425433090080926, w0=-0.31403571727937696, w1=-0.014631109377049416\n",
      "Gradient Descent(59/499): loss=0.344149837225743, w0=-0.31409854555149325, w1=-0.0147166392974698\n",
      "Gradient Descent(60/499): loss=0.34404826097188024, w0=-0.3141550909963982, w1=-0.014791574743442027\n",
      "Gradient Descent(61/499): loss=0.343949504209624, w0=-0.314205981896813, w1=-0.014856287014666195\n",
      "Gradient Descent(62/499): loss=0.3438534733026436, w0=-0.3142517837071866, w1=-0.014911136387358881\n",
      "Gradient Descent(63/499): loss=0.3437600786293, w0=-0.31429300533652316, w1=-0.014956472403888437\n",
      "Gradient Descent(64/499): loss=0.34366923433200813, w0=-0.3143301048029264, w1=-0.014992634161763274\n",
      "Gradient Descent(65/499): loss=0.34358085809072125, w0=-0.31436349432268956, w1=-0.015019950600391406\n",
      "Gradient Descent(66/499): loss=0.34349487091734343, w0=-0.3143935448904767, w1=-0.015038740784380727\n",
      "Gradient Descent(67/499): loss=0.3434111969683685, w0=-0.31442059040148546, w1=-0.01504931418243348\n",
      "Gradient Descent(68/499): loss=0.34332976337346977, w0=-0.3144449313613936, w1=-0.01505197094111771\n",
      "Gradient Descent(69/499): loss=0.3432505000781074, w0=-0.31446683822531124, w1=-0.015047002152983496\n",
      "Gradient Descent(70/499): loss=0.34317333969850744, w0=-0.3144865544028374, w1=-0.015034690118640651\n",
      "Gradient Descent(71/499): loss=0.3430982173876096, w0=-0.31450429896261123, w1=-0.015015308602534116\n",
      "Gradient Descent(72/499): loss=0.3430250707107826, w0=-0.31452026906640795, w1=-0.014989123082249086\n",
      "Gradient Descent(73/499): loss=0.3429538395302699, w0=-0.31453464215982524, w1=-0.014956390991254278\n",
      "Gradient Descent(74/499): loss=0.34288446589747545, w0=-0.3145475779439011, w1=-0.014917361955052458\n",
      "Gradient Descent(75/499): loss=0.342816893952313, w0=-0.3145592201495696, w1=-0.014872278020755293\n",
      "Gradient Descent(76/499): loss=0.3427510698289486, w0=-0.31456969813467156, w1=-0.014821373880137202\n",
      "Gradient Descent(77/499): loss=0.34268694156734736, w0=-0.31457912832126356, w1=-0.01476487708625214\n",
      "Gradient Descent(78/499): loss=0.3426244590301106, w0=-0.3145876154891966, w1=-0.014703008263719638\n",
      "Gradient Descent(79/499): loss=0.3425635738241476, w0=-0.31459525394033666, w1=-0.01463598131280349\n",
      "Gradient Descent(80/499): loss=0.34250423922678397, w0=-0.31460212854636294, w1=-0.014564003607418883\n",
      "Gradient Descent(81/499): loss=0.34244641011595006, w0=-0.3146083156917868, w1=-0.01448727618721283\n",
      "Gradient Descent(82/499): loss=0.34239004290413655, w0=-0.31461388412266855, w1=-0.014405993943868886\n",
      "Gradient Descent(83/499): loss=0.34233509547582847, w0=-0.31461889571046237, w1=-0.014320345801790816\n",
      "Gradient Descent(84/499): loss=0.3422815271281734, w0=-0.31462340613947704, w1=-0.014230514893322001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(85/499): loss=0.34222929851464656, w0=-0.3146274655255905, w1=-0.014136678728657745\n",
      "Gradient Descent(86/499): loss=0.34217837159151865, w0=-0.31463111897309287, w1=-0.014039009360607143\n",
      "Gradient Descent(87/499): loss=0.34212870956692654, w0=-0.3146344070758452, w1=-0.013937673544359494\n",
      "Gradient Descent(88/499): loss=0.3420802768523936, w0=-0.31463736636832257, w1=-0.013832832892408138\n",
      "Gradient Descent(89/499): loss=0.3420330390166304, w0=-0.3146400297315524, w1=-0.01372464402478169\n",
      "Gradient Descent(90/499): loss=0.3419869627414883, w0=-0.3146424267584595, w1=-0.013613258714729596\n",
      "Gradient Descent(91/499): loss=0.3419420157799274, w0=-0.3146445840826761, w1=-0.013498824030005345\n",
      "Gradient Descent(92/499): loss=0.3418981669158857, w0=-0.3146465256744712, w1=-0.013381482469887074\n",
      "Gradient Descent(93/499): loss=0.3418553859259397, w0=-0.31464827310708704, w1=-0.013261372098071449\n",
      "Gradient Descent(94/499): loss=0.3418136435426556, w0=-0.3146498457964415, w1=-0.013138626671572779\n",
      "Gradient Descent(95/499): loss=0.34177291141953503, w0=-0.31465126121686077, w1=-0.013013375765755411\n",
      "Gradient Descent(96/499): loss=0.34173316209747634, w0=-0.3146525350952383, w1=-0.0128857448956235\n",
      "Gradient Descent(97/499): loss=0.3416943689726636, w0=-0.3146536815857783, w1=-0.012755855633488303\n",
      "Gradient Descent(98/499): loss=0.3416565062658123, w0=-0.3146547134272645, w1=-0.012623825723129289\n",
      "Gradient Descent(99/499): loss=0.34161954899270497, w0=-0.3146556420846022, w1=-0.012489769190561559\n",
      "Gradient Descent(100/499): loss=0.3415834729359442, w0=-0.31465647787620643, w1=-0.012353796451518273\n",
      "Gradient Descent(101/499): loss=0.34154825461787175, w0=-0.31465723008865043, w1=-0.012216014415753245\n",
      "Gradient Descent(102/499): loss=0.34151387127458815, w0=-0.3146579070798502, w1=-0.012076526588265182\n",
      "Gradient Descent(103/499): loss=0.3414803008310273, w0=-0.3146585163719302, w1=-0.011935433167541677\n",
      "Gradient Descent(104/499): loss=0.3414475218770272, w0=-0.3146590647348024, w1=-0.011792831140917679\n",
      "Gradient Descent(105/499): loss=0.3414155136443596, w0=-0.31465955826138753, w1=-0.011648814377139917\n",
      "Gradient Descent(106/499): loss=0.34138425598466615, w0=-0.31466000243531433, w1=-0.011503473716225603\n",
      "Gradient Descent(107/499): loss=0.3413537293482621, w0=-0.31466040219184865, w1=-0.011356897056700685\n",
      "Gradient Descent(108/499): loss=0.3413239147637689, w0=-0.31466076197272974, w1=-0.011209169440300036\n",
      "Gradient Descent(109/499): loss=0.3412947938185382, w0=-0.3146610857755229, w1=-0.011060373134209005\n",
      "Gradient Descent(110/499): loss=0.34126634863982885, w0=-0.3146613771980369, w1=-0.0109105877109232\n",
      "Gradient Descent(111/499): loss=0.34123856187670776, w0=-0.31466163947829967, w1=-0.01075989012580051\n",
      "Gradient Descent(112/499): loss=0.3412114166826409, w0=-0.3146618755305364, w1=-0.010608354792377062\n",
      "Gradient Descent(113/499): loss=0.34118489669874463, w0=-0.3146620879775496, w1=-0.010456053655516105\n",
      "Gradient Descent(114/499): loss=0.34115898603766714, w0=-0.3146622791798616, w1=-0.010303056262456704\n",
      "Gradient Descent(115/499): loss=0.34113366926807703, w0=-0.3146624512619426, w1=-0.010149429831826623\n",
      "Gradient Descent(116/499): loss=0.34110893139972986, w0=-0.31466260613581565, w1=-0.009995239320681734\n",
      "Gradient Descent(117/499): loss=0.34108475786908843, w0=-0.31466274552230156, w1=-0.009840547489632109\n",
      "Gradient Descent(118/499): loss=0.3410611345254764, w0=-0.31466287097013906, w1=-0.009685414966112917\n",
      "Gradient Descent(119/499): loss=0.34103804761773976, w0=-0.31466298387319297, w1=-0.009529900305856343\n",
      "Gradient Descent(120/499): loss=0.34101548378139623, w0=-0.31466308548594163, w1=-0.009374060052618757\n",
      "Gradient Descent(121/499): loss=0.34099343002625454, w0=-0.3146631769374156, w1=-0.009217948796215673\n",
      "Gradient Descent(122/499): loss=0.34097187372448046, w0=-0.3146632592437423, w1=-0.009061619228915168\n",
      "Gradient Descent(123/499): loss=0.3409508025990954, w0=-0.31466333331943647, w1=-0.00890512220023883\n",
      "Gradient Descent(124/499): loss=0.3409302047128881, w0=-0.31466339998756143, w1=-0.008748506770217651\n",
      "Gradient Descent(125/499): loss=0.3409100684577202, w0=-0.314663459988874, w1=-0.008591820261148728\n",
      "Gradient Descent(126/499): loss=0.3408903825442181, w0=-0.3146635139900555, w1=-0.0084351083078971\n",
      "Gradient Descent(127/499): loss=0.34087113599182517, w0=-0.314663562591119, w1=-0.008278414906785644\n",
      "Gradient Descent(128/499): loss=0.3408523181192087, w0=-0.3146636063320763, w1=-0.008121782463114523\n",
      "Gradient Descent(129/499): loss=0.34083391853500256, w0=-0.31466364569893795, w1=-0.007965251837350374\n",
      "Gradient Descent(130/499): loss=0.34081592712887326, w0=-0.31466368112911364, w1=-0.007808862390024077\n",
      "Gradient Descent(131/499): loss=0.3407983340629002, w0=-0.3146637130162719, w1=-0.0076526520253747095\n",
      "Gradient Descent(132/499): loss=0.3407811297632528, w0=-0.31466374171471445, w1=-0.007496657233776151\n",
      "Gradient Descent(133/499): loss=0.3407643049121562, w0=-0.3146637675433129, w1=-0.007340913132981525\n",
      "Gradient Descent(134/499): loss=0.3407478504401334, w0=-0.3146637907890516, w1=-0.007185453508219668\n",
      "Gradient Descent(135/499): loss=0.3407317575185148, w0=-0.31466381171021657, w1=-0.007030310851176648\n",
      "Gradient Descent(136/499): loss=0.3407160175522008, w0=-0.31466383053926517, w1=-0.006875516397894339\n",
      "Gradient Descent(137/499): loss=0.3407006221726736, w0=-0.31466384748540904, w1=-0.006721100165617111\n",
      "Gradient Descent(138/499): loss=0.34068556323124377, w0=-0.31466386273693864, w1=-0.00656709098861662\n",
      "Gradient Descent(139/499): loss=0.3406708327925259, w0=-0.3146638764633154, w1=-0.00641351655302385\n",
      "Gradient Descent(140/499): loss=0.3406564231281317, w0=-0.31466388881705465, w1=-0.006260403430696573\n",
      "Gradient Descent(141/499): loss=0.34064232671057637, w0=-0.3146638999354201, w1=-0.006107777112149621\n",
      "Gradient Descent(142/499): loss=0.340628536207388, w0=-0.3146639099419491, w1=-0.005955662038574398\n",
      "Gradient Descent(143/499): loss=0.34061504447540836, w0=-0.3146639189478253, w1=-0.0058040816329734\n",
      "Gradient Descent(144/499): loss=0.3406018445552905, w0=-0.314663927053114, w1=-0.005653058330434565\n",
      "Gradient Descent(145/499): loss=0.3405889296661675, w0=-0.314663934347874, w1=-0.005502613607569661\n",
      "Gradient Descent(146/499): loss=0.34057629320050486, w0=-0.3146639409131581, w1=-0.005352768011140102\n",
      "Gradient Descent(147/499): loss=0.34056392871911567, w0=-0.31466394682191384, w1=-0.005203541185892918\n",
      "Gradient Descent(148/499): loss=0.34055182994634037, w0=-0.31466395213979415, w1=-0.005054951901628896\n",
      "Gradient Descent(149/499): loss=0.34053999076538133, w0=-0.31466395692588656, w1=-0.004907018079524276\n",
      "Gradient Descent(150/499): loss=0.3405284052137909, w0=-0.3146639612333698, w1=-0.004759756817726728\n",
      "Gradient Descent(151/499): loss=0.34051706747910027, w0=-0.31466396511010486, w1=-0.0046131844162457455\n",
      "Gradient Descent(152/499): loss=0.3405059718945912, w0=-0.3146639685991665, w1=-0.00446731640115696\n",
      "Gradient Descent(153/499): loss=0.3404951129352034, w0=-0.3146639717393221, w1=-0.004322167548139331\n",
      "Gradient Descent(154/499): loss=0.34048448521356933, w0=-0.31466397456546225, w1=-0.004177751905363628\n",
      "Gradient Descent(155/499): loss=0.34047408347617625, w0=-0.31466397710898847, w1=-0.004034082815750022\n",
      "Gradient Descent(156/499): loss=0.340463902599649, w0=-0.31466397939816215, w1=-0.003891172938612129\n",
      "Gradient Descent(157/499): loss=0.3404539375871493, w0=-0.3146639814584186, w1=-0.0037490342707043717\n",
      "Gradient Descent(158/499): loss=0.3404441835648876, w0=-0.3146639833126495, w1=-0.0036076781666889443\n",
      "Gradient Descent(159/499): loss=0.3404346357787438, w0=-0.3146639849814574, w1=-0.003467115359038301\n",
      "Gradient Descent(160/499): loss=0.3404252895909926, w0=-0.3146639864833846, w1=-0.0033273559773885335\n",
      "Gradient Descent(161/499): loss=0.34041614047713187, w0=-0.3146639878351192, w1=-0.0031884095673586524\n",
      "Gradient Descent(162/499): loss=0.34040718402280423, w0=-0.3146639890516804, w1=-0.0030502851088502884\n",
      "Gradient Descent(163/499): loss=0.3403984159208181, w0=-0.3146639901465856, w1=-0.0029129910338419416\n",
      "Gradient Descent(164/499): loss=0.34038983196825695, w0=-0.31466399113200033, w1=-0.0027765352436915024\n",
      "Gradient Descent(165/499): loss=0.34038142806367766, w0=-0.31466399201887374, w1=-0.0026409251259604054\n",
      "Gradient Descent(166/499): loss=0.3403732002043914, w0=-0.31466399281705987, w1=-0.002506167570772335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(167/499): loss=0.34036514448383137, w0=-0.31466399353542746, w1=-0.0023722689867191013\n",
      "Gradient Descent(168/499): loss=0.3403572570889932, w0=-0.3146639941819584, w1=-0.0022392353163259225\n",
      "Gradient Descent(169/499): loss=0.3403495342979599, w0=-0.31466399476383633, w1=-0.002107072051088\n",
      "Gradient Descent(170/499): loss=0.3403419724774923, w0=-0.31466399528752653, w1=-0.001975784246089955\n",
      "Gradient Descent(171/499): loss=0.3403345680806975, w0=-0.3146639957588478, w1=-0.0018453765342193655\n",
      "Gradient Descent(172/499): loss=0.340327317644765, w0=-0.31466399618303703, w1=-0.0017158531399853452\n",
      "Gradient Descent(173/499): loss=0.3403202177887653, w0=-0.3146639965648074, w1=-0.0015872178929527423\n",
      "Gradient Descent(174/499): loss=0.3403132652115199, w0=-0.31466399690840086, w1=-0.0014594742408023378\n",
      "Gradient Descent(175/499): loss=0.3403064566895287, w0=-0.31466399721763505, w1=-0.0013326252620270804\n",
      "Gradient Descent(176/499): loss=0.34029978907496017, w0=-0.31466399749594587, w1=-0.0012066736782740842\n",
      "Gradient Descent(177/499): loss=0.3402932592936979, w0=-0.31466399774642567, w1=-0.0010816218663419258\n",
      "Gradient Descent(178/499): loss=0.3402868643434471, w0=-0.3146639979718576, w1=-0.0009574718698424799\n",
      "Gradient Descent(179/499): loss=0.3402806012918911, w0=-0.31466399817474644, w1=-0.000834225410536249\n",
      "Gradient Descent(180/499): loss=0.3402744672749063, w0=-0.31466399835734643, w1=-0.0007118838993499554\n",
      "Gradient Descent(181/499): loss=0.3402684594948217, w0=-0.3146639985216865, w1=-0.0005904484470848594\n",
      "Gradient Descent(182/499): loss=0.3402625752187345, w0=-0.31466399866959266, w1=-0.0004699198748241369\n",
      "Gradient Descent(183/499): loss=0.3402568117768683, w0=-0.31466399880270823, w1=-0.0003502987240472819\n",
      "Gradient Descent(184/499): loss=0.34025116656098175, w0=-0.31466399892251234, w1=-0.00023158526645941727\n",
      "Gradient Descent(185/499): loss=0.34024563702281896, w0=-0.3146639990303361, w1=-0.00011377951354309621\n",
      "Gradient Descent(186/499): loss=0.34024022067260584, w0=-0.3146639991273775, w1=3.118774159960292e-06\n",
      "Gradient Descent(187/499): loss=0.34023491507758674, w0=-0.3146639992147149, w1=0.00011911007803002232\n",
      "Gradient Descent(188/499): loss=0.34022971786060324, w0=-0.3146639992933186, w1=0.00023419511260638594\n",
      "Gradient Descent(189/499): loss=0.34022462669871023, w0=-0.31466399936406203, w1=0.00034837481707340734\n",
      "Gradient Descent(190/499): loss=0.34021963932183413, w0=-0.31466399942773116, w1=0.0004616503469889725\n",
      "Gradient Descent(191/499): loss=0.34021475351146463, w0=-0.31466399948503343, w1=0.0005740230662488683\n",
      "Gradient Descent(192/499): loss=0.3402099670993831, w0=-0.31466399953660557, w1=0.0006854945392808066\n",
      "Gradient Descent(193/499): loss=0.34020527796642785, w0=-0.31466399958302055, w1=0.0007960665234619554\n",
      "Gradient Descent(194/499): loss=0.3402006840412899, w0=-0.3146639996247941, w1=0.0009057409617540339\n",
      "Gradient Descent(195/499): loss=0.3401961832993439, w0=-0.31466399966239034, w1=0.0010145199755501322\n",
      "Gradient Descent(196/499): loss=0.3401917737615104, w0=-0.314663999696227, w1=0.001122405857727663\n",
      "Gradient Descent(197/499): loss=0.34018745349314794, w0=-0.3146639997266801, w1=0.0012294010659019204\n",
      "Gradient Descent(198/499): loss=0.3401832206029747, w0=-0.3146639997540879, w1=0.0013355082158748638\n",
      "Gradient Descent(199/499): loss=0.34017907324202207, w0=-0.314663999778755, w1=0.0014407300752739887\n",
      "Gradient Descent(200/499): loss=0.34017500960261127, w0=-0.31466399980095544, w1=0.0015450695573761338\n",
      "Gradient Descent(201/499): loss=0.3401710279173612, w0=-0.3146639998209359, w1=0.001648529715111342\n",
      "Gradient Descent(202/499): loss=0.3401671264582222, w0=-0.31466399983891835, w1=0.0017511137352419653\n",
      "Gradient Descent(203/499): loss=0.34016330353553237, w0=-0.3146639998551026, w1=0.0018528249327122823\n",
      "Gradient Descent(204/499): loss=0.3401595574971027, w0=-0.31466399986966853, w1=0.0019536667451641493\n",
      "Gradient Descent(205/499): loss=0.3401558867273239, w0=-0.3146639998827779, w1=0.0020536427276141678\n",
      "Gradient Descent(206/499): loss=0.3401522896462958, w0=-0.3146639998945764, w1=0.002152756547288121\n",
      "Gradient Descent(207/499): loss=0.3401487647089845, w0=-0.31466399990519506, w1=0.0022510119786084116\n",
      "Gradient Descent(208/499): loss=0.34014531040439333, w0=-0.3146639999147519, w1=0.002348412898330451\n",
      "Gradient Descent(209/499): loss=0.34014192525476333, w0=-0.31466399992335314, w1=0.002444963280823982\n",
      "Gradient Descent(210/499): loss=0.3401386078147891, w0=-0.3146639999310943, w1=0.0025406671934954665\n",
      "Gradient Descent(211/499): loss=0.3401353566708565, w0=-0.3146639999380614, w1=0.0026355287923477575\n",
      "Gradient Descent(212/499): loss=0.3401321704403024, w0=-0.3146639999443318, w1=0.0027295523176733435\n",
      "Gradient Descent(213/499): loss=0.3401290477706884, w0=-0.31466399994997524, w1=0.002822742089877628\n",
      "Gradient Descent(214/499): loss=0.3401259873390963, w0=-0.3146639999550544, w1=0.0029151025054287064\n",
      "Gradient Descent(215/499): loss=0.34012298785144346, w0=-0.3146639999596257, w1=0.00300663803293026\n",
      "Gradient Descent(216/499): loss=0.34012004804180984, w0=-0.3146639999637399, w1=0.0030973532093142592\n",
      "Gradient Descent(217/499): loss=0.34011716667178776, w0=-0.3146639999674427, w1=0.003187252636150236\n",
      "Gradient Descent(218/499): loss=0.3401143425298446, w0=-0.31466399997077527, w1=0.003276340976067979\n",
      "Gradient Descent(219/499): loss=0.34011157443070217, w0=-0.31466399997377464, w1=0.003364622949290622\n",
      "Gradient Descent(220/499): loss=0.34010886121473427, w0=-0.3146639999764741, w1=0.003452103330275101\n",
      "Gradient Descent(221/499): loss=0.34010620174737405, w0=-0.31466399997890365, w1=0.0035387869444571283\n",
      "Gradient Descent(222/499): loss=0.34010359491854225, w0=-0.3146639999810903, w1=0.003624678665097803\n",
      "Gradient Descent(223/499): loss=0.3401010396420847, w0=-0.31466399998305833, w1=0.0037097834102291457\n",
      "Gradient Descent(224/499): loss=0.3400985348552268, w0=-0.3146639999848296, w1=0.0037941061396958627\n",
      "Gradient Descent(225/499): loss=0.3400960795180398, w0=-0.31466399998642375, w1=0.0038776518522907087\n",
      "Gradient Descent(226/499): loss=0.34009367261292145, w0=-0.31466399998785854, w1=0.0039604255829809604\n",
      "Gradient Descent(227/499): loss=0.3400913131440893, w0=-0.3146639999891499, w1=0.0040424324002234285\n",
      "Gradient Descent(228/499): loss=0.34008900013708504, w0=-0.31466399999031214, w1=0.0041236774033657\n",
      "Gradient Descent(229/499): loss=0.3400867326382919, w0=-0.3146639999913582, w1=0.004204165720131192\n",
      "Gradient Descent(230/499): loss=0.34008450971446597, w0=-0.3146639999922997, w1=0.004283902504185744\n",
      "Gradient Descent(231/499): loss=0.34008233045227354, w0=-0.3146639999931471, w1=0.0043628929327835355\n",
      "Gradient Descent(232/499): loss=0.34008019395784683, w0=-0.31466399999390976, w1=0.004441142204490127\n",
      "Gradient Descent(233/499): loss=0.34007809935634326, w0=-0.3146639999945962, w1=0.004518655536980537\n",
      "Gradient Descent(234/499): loss=0.3400760457915212, w0=-0.31466399999521405, w1=0.0045954381649102915\n",
      "Gradient Descent(235/499): loss=0.34007403242532225, w0=-0.31466399999577016, w1=0.0046714953378574125\n",
      "Gradient Descent(236/499): loss=0.34007205843746535, w0=-0.3146639999962707, w1=0.004746832318333426\n",
      "Gradient Descent(237/499): loss=0.34007012302505013, w0=-0.3146639999967212, w1=0.004821454379861463\n",
      "Gradient Descent(238/499): loss=0.3400682254021695, w0=-0.3146639999971267, w1=0.0048953668051196116\n",
      "Gradient Descent(239/499): loss=0.3400663647995323, w0=-0.3146639999974917, w1=0.004968574884147703\n",
      "Gradient Descent(240/499): loss=0.34006454046409357, w0=-0.3146639999978202, w1=0.0050410839126157885\n",
      "Gradient Descent(241/499): loss=0.3400627516586952, w0=-0.3146639999981159, w1=0.00511289919015258\n",
      "Gradient Descent(242/499): loss=0.3400609976617137, w0=-0.314663999998382, w1=0.005184026018732187\n",
      "Gradient Descent(243/499): loss=0.3400592777667176, w0=-0.31466399999862155, w1=0.005254469701117536\n",
      "Gradient Descent(244/499): loss=0.34005759128213187, w0=-0.3146639999988372, w1=0.005324235539358867\n",
      "Gradient Descent(245/499): loss=0.34005593753091046, w0=-0.31466399999903133, w1=0.005393328833345811\n",
      "Gradient Descent(246/499): loss=0.3400543158502167, w0=-0.314663999999206, w1=0.005461754879411492\n",
      "Gradient Descent(247/499): loss=0.34005272559111216, w0=-0.3146639999993633, w1=0.005529518968987213\n",
      "Gradient Descent(248/499): loss=0.3400511661182509, w0=-0.31466399999950484, w1=0.00559662638730633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(249/499): loss=0.3400496368095813, w0=-0.3146639999996323, w1=0.005663082412155882\n",
      "Gradient Descent(250/499): loss=0.34004813705605696, w0=-0.31466399999974703, w1=0.005728892312674654\n",
      "Gradient Descent(251/499): loss=0.3400466662613513, w0=-0.31466399999985034, w1=0.005794061348196355\n",
      "Gradient Descent(252/499): loss=0.34004522384158087, w0=-0.3146639999999433, w1=0.005858594767136622\n",
      "Gradient Descent(253/499): loss=0.3400438092250343, w0=-0.31466400000002703, w1=0.0059224978059226035\n",
      "Gradient Descent(254/499): loss=0.34004242185190625, w0=-0.3146640000001024, w1=0.005985775687963906\n",
      "Gradient Descent(255/499): loss=0.34004106117404187, w0=-0.31466400000017025, w1=0.006048433622663741\n",
      "Gradient Descent(256/499): loss=0.34003972665468096, w0=-0.3146640000002313, w1=0.006110476804469093\n",
      "Gradient Descent(257/499): loss=0.34003841776821264, w0=-0.3146640000002863, w1=0.006171910411958805\n",
      "Gradient Descent(258/499): loss=0.3400371339999343, w0=-0.31466400000033584, w1=0.006232739606968469\n",
      "Gradient Descent(259/499): loss=0.34003587484581566, w0=-0.3146640000003804, w1=0.0062929695337510866\n",
      "Gradient Descent(260/499): loss=0.34003463981226745, w0=-0.3146640000004206, w1=0.00635260531817243\n",
      "Gradient Descent(261/499): loss=0.34003342841591844, w0=-0.3146640000004568, w1=0.0064116520669401334\n",
      "Gradient Descent(262/499): loss=0.3400322401833942, w0=-0.3146640000004894, w1=0.006470114866865503\n",
      "Gradient Descent(263/499): loss=0.34003107465110133, w0=-0.3146640000005187, w1=0.006527998784157083\n",
      "Gradient Descent(264/499): loss=0.34002993136501963, w0=-0.3146640000005451, w1=0.006585308863745084\n",
      "Gradient Descent(265/499): loss=0.34002880988049533, w0=-0.31466400000056893, w1=0.006642050128635744\n",
      "Gradient Descent(266/499): loss=0.34002770976204033, w0=-0.31466400000059036, w1=0.006698227579294717\n",
      "Gradient Descent(267/499): loss=0.3400266305831372, w0=-0.3146640000006097, w1=0.006753846193058684\n",
      "Gradient Descent(268/499): loss=0.340025571926046, w0=-0.3146640000006271, w1=0.006808910923574293\n",
      "Gradient Descent(269/499): loss=0.3400245333816181, w0=-0.3146640000006428, w1=0.006863426700263654\n",
      "Gradient Descent(270/499): loss=0.34002351454911245, w0=-0.314664000000657, w1=0.00691739842781558\n",
      "Gradient Descent(271/499): loss=0.34002251503601566, w0=-0.3146640000006697, w1=0.006970830985701805\n",
      "Gradient Descent(272/499): loss=0.34002153445786965, w0=-0.3146640000006812, w1=0.007023729227717411\n",
      "Gradient Descent(273/499): loss=0.340020572438097, w0=-0.31466400000069156, w1=0.007076097981544768\n",
      "Gradient Descent(274/499): loss=0.34001962860783574, w0=-0.3146640000007009, w1=0.0071279420483402476\n",
      "Gradient Descent(275/499): loss=0.3400187026057753, w0=-0.3146640000007093, w1=0.007179266202343018\n",
      "Gradient Descent(276/499): loss=0.3400177940779967, w0=-0.3146640000007169, w1=0.007230075190505256\n",
      "Gradient Descent(277/499): loss=0.3400169026778147, w0=-0.31466400000072375, w1=0.007280373732143121\n",
      "Gradient Descent(278/499): loss=0.34001602806562714, w0=-0.3146640000007299, w1=0.007330166518607834\n",
      "Gradient Descent(279/499): loss=0.340015169908764, w0=-0.3146640000007355, w1=0.00737945821297627\n",
      "Gradient Descent(280/499): loss=0.34001432788134034, w0=-0.3146640000007406, w1=0.007428253449760421\n",
      "Gradient Descent(281/499): loss=0.34001350166411537, w0=-0.3146640000007451, w1=0.0074765568346351705\n",
      "Gradient Descent(282/499): loss=0.34001269094435055, w0=-0.31466400000074923, w1=0.007524372944183791\n",
      "Gradient Descent(283/499): loss=0.3400118954156737, w0=-0.31466400000075295, w1=0.007571706325660623\n",
      "Gradient Descent(284/499): loss=0.3400111147779451, w0=-0.3146640000007563, w1=0.0076185614967703595\n",
      "Gradient Descent(285/499): loss=0.3400103487371271, w0=-0.31466400000075934, w1=0.007664942945463439\n",
      "Gradient Descent(286/499): loss=0.34000959700515465, w0=-0.31466400000076206, w1=0.0077108551297470455\n",
      "Gradient Descent(287/499): loss=0.3400088592998114, w0=-0.31466400000076455, w1=0.007756302477511145\n",
      "Gradient Descent(288/499): loss=0.34000813534460694, w0=-0.31466400000076683, w1=0.007801289386369165\n",
      "Gradient Descent(289/499): loss=0.34000742486865687, w0=-0.3146640000007689, w1=0.007845820223512786\n",
      "Gradient Descent(290/499): loss=0.34000672760656525, w0=-0.3146640000007707, w1=0.007889899325580369\n",
      "Gradient Descent(291/499): loss=0.3400060432983107, w0=-0.3146640000007724, w1=0.00793353099853864\n",
      "Gradient Descent(292/499): loss=0.3400053716891335, w0=-0.3146640000007739, w1=0.007976719517577134\n",
      "Gradient Descent(293/499): loss=0.34000471252942693, w0=-0.31466400000077527, w1=0.008019469127014988\n",
      "Gradient Descent(294/499): loss=0.34000406557462837, w0=-0.31466400000077654, w1=0.008061784040219708\n",
      "Gradient Descent(295/499): loss=0.3400034305851157, w0=-0.3146640000007777, w1=0.008103668439537444\n",
      "Gradient Descent(296/499): loss=0.340002807326104, w0=-0.31466400000077877, w1=0.00814512647623443\n",
      "Gradient Descent(297/499): loss=0.3400021955675445, w0=-0.3146640000007797, w1=0.008186162270449214\n",
      "Gradient Descent(298/499): loss=0.34000159508402794, w0=-0.31466400000078054, w1=0.008226779911155268\n",
      "Gradient Descent(299/499): loss=0.34000100565468605, w0=-0.3146640000007813, w1=0.008266983456133673\n",
      "Gradient Descent(300/499): loss=0.3400004270630989, w0=-0.31466400000078204, w1=0.00830677693195546\n",
      "Gradient Descent(301/499): loss=0.33999985909720215, w0=-0.3146640000007827, w1=0.008346164333973359\n",
      "Gradient Descent(302/499): loss=0.339999301549198, w0=-0.3146640000007833, w1=0.00838514962632254\n",
      "Gradient Descent(303/499): loss=0.33999875421546544, w0=-0.31466400000078387, w1=0.008423736741930078\n",
      "Gradient Descent(304/499): loss=0.33999821689647425, w0=-0.31466400000078437, w1=0.008461929582532805\n",
      "Gradient Descent(305/499): loss=0.3399976893967016, w0=-0.3146640000007848, w1=0.008499732018703264\n",
      "Gradient Descent(306/499): loss=0.339997171524548, w0=-0.31466400000078526, w1=0.008537147889883464\n",
      "Gradient Descent(307/499): loss=0.3399966630922579, w0=-0.31466400000078565, w1=0.008574181004426144\n",
      "Gradient Descent(308/499): loss=0.33999616391583887, w0=-0.314664000000786, w1=0.008610835139643263\n",
      "Gradient Descent(309/499): loss=0.33999567381498474, w0=-0.3146640000007863, w1=0.00864711404186148\n",
      "Gradient Descent(310/499): loss=0.3399951926130006, w0=-0.31466400000078665, w1=0.008683021426484293\n",
      "Gradient Descent(311/499): loss=0.339994720136727, w0=-0.3146640000007869, w1=0.008718560978060683\n",
      "Gradient Descent(312/499): loss=0.3399942562164692, w0=-0.3146640000007872, w1=0.008753736350359899\n",
      "Gradient Descent(313/499): loss=0.3399938006859242, w0=-0.3146640000007874, w1=0.008788551166452246\n",
      "Gradient Descent(314/499): loss=0.3399933533821121, w0=-0.31466400000078765, w1=0.008823009018795566\n",
      "Gradient Descent(315/499): loss=0.3399929141453082, w0=-0.31466400000078787, w1=0.008857113469327238\n",
      "Gradient Descent(316/499): loss=0.33999248281897576, w0=-0.3146640000007881, w1=0.008890868049561436\n",
      "Gradient Descent(317/499): loss=0.3399920592497007, w0=-0.31466400000078826, w1=0.008924276260691458\n",
      "Gradient Descent(318/499): loss=0.33999164328712794, w0=-0.3146640000007884, w1=0.008957341573696907\n",
      "Gradient Descent(319/499): loss=0.3399912347838997, w0=-0.3146640000007886, w1=0.008990067429455515\n",
      "Gradient Descent(320/499): loss=0.3399908335955923, w0=-0.31466400000078876, w1=0.00902245723885944\n",
      "Gradient Descent(321/499): loss=0.3399904395806575, w0=-0.3146640000007889, w1=0.009054514382935802\n",
      "Gradient Descent(322/499): loss=0.3399900526003643, w0=-0.31466400000078903, w1=0.009086242212971317\n",
      "Gradient Descent(323/499): loss=0.3399896725187407, w0=-0.31466400000078915, w1=0.00911764405064082\n",
      "Gradient Descent(324/499): loss=0.3399892992025172, w0=-0.31466400000078926, w1=0.009148723188139515\n",
      "Gradient Descent(325/499): loss=0.3399889325210725, w0=-0.31466400000078937, w1=0.009179482888318786\n",
      "Gradient Descent(326/499): loss=0.3399885723463794, w0=-0.3146640000007895, w1=0.009209926384825377\n",
      "Gradient Descent(327/499): loss=0.3399882185529517, w0=-0.3146640000007896, w1=0.009240056882243837\n",
      "Gradient Descent(328/499): loss=0.33998787101779293, w0=-0.3146640000007897, w1=0.009269877556242015\n",
      "Gradient Descent(329/499): loss=0.3399875296203456, w0=-0.3146640000007898, w1=0.009299391553719488\n",
      "Gradient Descent(330/499): loss=0.3399871942424413, w0=-0.31466400000078987, w1=0.009328601992958777\n",
      "Gradient Descent(331/499): loss=0.33998686476825346, w0=-0.3146640000007899, w1=0.0093575119637792\n",
      "Gradient Descent(332/499): loss=0.33998654108424786, w0=-0.31466400000079, w1=0.009386124527693208\n",
      "Gradient Descent(333/499): loss=0.33998622307913856, w0=-0.31466400000079003, w1=0.009414442718065139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(334/499): loss=0.33998591064383965, w0=-0.3146640000007901, w1=0.009442469540272162\n",
      "Gradient Descent(335/499): loss=0.3399856036714238, w0=-0.31466400000079014, w1=0.009470207971867366\n",
      "Gradient Descent(336/499): loss=0.33998530205707567, w0=-0.3146640000007902, w1=0.009497660962744838\n",
      "Gradient Descent(337/499): loss=0.33998500569805107, w0=-0.31466400000079026, w1=0.009524831435306618\n",
      "Gradient Descent(338/499): loss=0.33998471449363443, w0=-0.3146640000007903, w1=0.009551722284631423\n",
      "Gradient Descent(339/499): loss=0.33998442834509773, w0=-0.31466400000079037, w1=0.009578336378645021\n",
      "Gradient Descent(340/499): loss=0.33998414715566055, w0=-0.3146640000007904, w1=0.009604676558292157\n",
      "Gradient Descent(341/499): loss=0.33998387083045145, w0=-0.3146640000007905, w1=0.009630745637709917\n",
      "Gradient Descent(342/499): loss=0.3399835992764677, w0=-0.31466400000079053, w1=0.009656546404402448\n",
      "Gradient Descent(343/499): loss=0.3399833324025394, w0=-0.3146640000007906, w1=0.009682081619416923\n",
      "Gradient Descent(344/499): loss=0.3399830701192921, w0=-0.31466400000079064, w1=0.009707354017520646\n",
      "Gradient Descent(345/499): loss=0.3399828123391097, w0=-0.3146640000007907, w1=0.00973236630737924\n",
      "Gradient Descent(346/499): loss=0.3399825589761006, w0=-0.31466400000079076, w1=0.009757121171735796\n",
      "Gradient Descent(347/499): loss=0.3399823099460614, w0=-0.3146640000007908, w1=0.009781621267590933\n",
      "Gradient Descent(348/499): loss=0.33998206516644436, w0=-0.3146640000007908, w1=0.009805869226383627\n",
      "Gradient Descent(349/499): loss=0.33998182455632336, w0=-0.31466400000079087, w1=0.00982986765417281\n",
      "Gradient Descent(350/499): loss=0.33998158803636136, w0=-0.31466400000079087, w1=0.0098536191318196\n",
      "Gradient Descent(351/499): loss=0.3399813555287795, w0=-0.31466400000079087, w1=0.009877126215170113\n",
      "Gradient Descent(352/499): loss=0.3399811269573242, w0=-0.31466400000079087, w1=0.00990039143523878\n",
      "Gradient Descent(353/499): loss=0.3399809022472373, w0=-0.31466400000079087, w1=0.009923417298392107\n",
      "Gradient Descent(354/499): loss=0.3399806813252265, w0=-0.3146640000007909, w1=0.009946206286532772\n",
      "Gradient Descent(355/499): loss=0.33998046411943594, w0=-0.3146640000007909, w1=0.009968760857284072\n",
      "Gradient Descent(356/499): loss=0.3399802505594173, w0=-0.3146640000007909, w1=0.009991083444174573\n",
      "Gradient Descent(357/499): loss=0.3399800405761003, w0=-0.3146640000007909, w1=0.01001317645682297\n",
      "Gradient Descent(358/499): loss=0.33997983410176846, w0=-0.3146640000007909, w1=0.01003504228112305\n",
      "Gradient Descent(359/499): loss=0.3399796310700279, w0=-0.3146640000007909, w1=0.010056683279428713\n",
      "Gradient Descent(360/499): loss=0.33997943141578396, w0=-0.3146640000007909, w1=0.010078101790739046\n",
      "Gradient Descent(361/499): loss=0.33997923507521466, w0=-0.3146640000007909, w1=0.010099300130883289\n",
      "Gradient Descent(362/499): loss=0.33997904198574413, w0=-0.3146640000007909, w1=0.010120280592705772\n",
      "Gradient Descent(363/499): loss=0.3399788520860186, w0=-0.3146640000007909, w1=0.010141045446250677\n",
      "Gradient Descent(364/499): loss=0.3399786653158823, w0=-0.3146640000007909, w1=0.010161596938946615\n",
      "Gradient Descent(365/499): loss=0.3399784816163526, w0=-0.3146640000007909, w1=0.010181937295790996\n",
      "Gradient Descent(366/499): loss=0.33997830092959824, w0=-0.3146640000007909, w1=0.010202068719534075\n",
      "Gradient Descent(367/499): loss=0.3399781231989143, w0=-0.3146640000007909, w1=0.01022199339086273\n",
      "Gradient Descent(368/499): loss=0.33997794836870204, w0=-0.3146640000007909, w1=0.010241713468583836\n",
      "Gradient Descent(369/499): loss=0.33997777638444493, w0=-0.3146640000007909, w1=0.010261231089807277\n",
      "Gradient Descent(370/499): loss=0.3399776071926878, w0=-0.3146640000007909, w1=0.010280548370128496\n",
      "Gradient Descent(371/499): loss=0.33997744074101677, w0=-0.3146640000007909, w1=0.01029966740381059\n",
      "Gradient Descent(372/499): loss=0.33997727697803687, w0=-0.3146640000007909, w1=0.010318590263965897\n",
      "Gradient Descent(373/499): loss=0.3399771158533528, w0=-0.3146640000007909, w1=0.010337319002737034\n",
      "Gradient Descent(374/499): loss=0.339976957317549, w0=-0.3146640000007909, w1=0.010355855651477374\n",
      "Gradient Descent(375/499): loss=0.33997680132217045, w0=-0.3146640000007909, w1=0.010374202220930915\n",
      "Gradient Descent(376/499): loss=0.33997664781970216, w0=-0.3146640000007909, w1=0.010392360701411515\n",
      "Gradient Descent(377/499): loss=0.3399764967635529, w0=-0.3146640000007909, w1=0.010410333062981496\n",
      "Gradient Descent(378/499): loss=0.3399763481080351, w0=-0.3146640000007909, w1=0.010428121255629502\n",
      "Gradient Descent(379/499): loss=0.3399762018083472, w0=-0.3146640000007909, w1=0.01044572720944772\n",
      "Gradient Descent(380/499): loss=0.33997605782055657, w0=-0.3146640000007909, w1=0.0104631528348083\n",
      "Gradient Descent(381/499): loss=0.3399759161015824, w0=-0.3146640000007909, w1=0.010480400022539067\n",
      "Gradient Descent(382/499): loss=0.33997577660917844, w0=-0.3146640000007909, w1=0.010497470644098405\n",
      "Gradient Descent(383/499): loss=0.33997563930191643, w0=-0.3146640000007909, w1=0.010514366551749359\n",
      "Gradient Descent(384/499): loss=0.3399755041391701, w0=-0.3146640000007909, w1=0.01053108957873292\n",
      "Gradient Descent(385/499): loss=0.3399753710810996, w0=-0.3146640000007909, w1=0.010547641539440464\n",
      "Gradient Descent(386/499): loss=0.3399752400886362, w0=-0.3146640000007909, w1=0.010564024229585309\n",
      "Gradient Descent(387/499): loss=0.33997511112346546, w0=-0.3146640000007909, w1=0.010580239426373402\n",
      "Gradient Descent(388/499): loss=0.33997498414801375, w0=-0.3146640000007909, w1=0.01059628888867312\n",
      "Gradient Descent(389/499): loss=0.339974859125434, w0=-0.31466400000079087, w1=0.010612174357184145\n",
      "Gradient Descent(390/499): loss=0.33997473601959016, w0=-0.31466400000079087, w1=0.010627897554605421\n",
      "Gradient Descent(391/499): loss=0.3399746147950442, w0=-0.3146640000007908, w1=0.01064346018580215\n",
      "Gradient Descent(392/499): loss=0.33997449541704194, w0=-0.3146640000007908, w1=0.010658863937971861\n",
      "Gradient Descent(393/499): loss=0.33997437785149814, w0=-0.31466400000079076, w1=0.010674110480809473\n",
      "Gradient Descent(394/499): loss=0.33997426206498693, w0=-0.31466400000079076, w1=0.010689201466671405\n",
      "Gradient Descent(395/499): loss=0.3399741480247255, w0=-0.3146640000007907, w1=0.010704138530738688\n",
      "Gradient Descent(396/499): loss=0.339974035698562, w0=-0.31466400000079064, w1=0.010718923291179063\n",
      "Gradient Descent(397/499): loss=0.3399739250549648, w0=-0.3146640000007906, w1=0.010733557349308092\n",
      "Gradient Descent(398/499): loss=0.339973816063009, w0=-0.3146640000007906, w1=0.010748042289749196\n",
      "Gradient Descent(399/499): loss=0.3399737086923642, w0=-0.31466400000079053, w1=0.01076237968059272\n",
      "Gradient Descent(400/499): loss=0.3399736029132838, w0=-0.31466400000079053, w1=0.010776571073553915\n",
      "Gradient Descent(401/499): loss=0.3399734986965932, w0=-0.3146640000007905, w1=0.010790618004129898\n",
      "Gradient Descent(402/499): loss=0.33997339601367826, w0=-0.3146640000007905, w1=0.010804521991755538\n",
      "Gradient Descent(403/499): loss=0.33997329483647437, w0=-0.3146640000007904, w1=0.01081828453995831\n",
      "Gradient Descent(404/499): loss=0.3399731951374555, w0=-0.31466400000079037, w1=0.010831907136512046\n",
      "Gradient Descent(405/499): loss=0.3399730968896254, w0=-0.3146640000007903, w1=0.010845391253589638\n",
      "Gradient Descent(406/499): loss=0.339973000066504, w0=-0.31466400000079026, w1=0.010858738347914653\n",
      "Gradient Descent(407/499): loss=0.33997290464212027, w0=-0.3146640000007902, w1=0.010871949860911884\n",
      "Gradient Descent(408/499): loss=0.3399728105910006, w0=-0.31466400000079014, w1=0.010885027218856784\n",
      "Gradient Descent(409/499): loss=0.3399727178881597, w0=-0.31466400000079014, w1=0.010897971833023842\n",
      "Gradient Descent(410/499): loss=0.3399726265090902, w0=-0.3146640000007901, w1=0.010910785099833838\n",
      "Gradient Descent(411/499): loss=0.33997253642975495, w0=-0.31466400000079003, w1=0.010923468401000053\n",
      "Gradient Descent(412/499): loss=0.3399724476265765, w0=-0.31466400000079, w1=0.010936023103673306\n",
      "Gradient Descent(413/499): loss=0.3399723600764279, w0=-0.3146640000007899, w1=0.01094845056058595\n",
      "Gradient Descent(414/499): loss=0.3399722737566253, w0=-0.31466400000078987, w1=0.010960752110194767\n",
      "Gradient Descent(415/499): loss=0.33997218864491785, w0=-0.3146640000007898, w1=0.010972929076822726\n",
      "Gradient Descent(416/499): loss=0.33997210471948003, w0=-0.31466400000078976, w1=0.010984982770799647\n",
      "Gradient Descent(417/499): loss=0.33997202195890325, w0=-0.3146640000007897, w1=0.010996914488601803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(418/499): loss=0.3399719403421877, w0=-0.3146640000007897, w1=0.011008725512990364\n",
      "Gradient Descent(419/499): loss=0.3399718598487338, w0=-0.31466400000078965, w1=0.011020417113148773\n",
      "Gradient Descent(420/499): loss=0.33997178045833576, w0=-0.3146640000007896, w1=0.011031990544818987\n",
      "Gradient Descent(421/499): loss=0.3399717021511722, w0=-0.31466400000078953, w1=0.011043447050436675\n",
      "Gradient Descent(422/499): loss=0.33997162490779986, w0=-0.3146640000007895, w1=0.01105478785926525\n",
      "Gradient Descent(423/499): loss=0.33997154870914603, w0=-0.3146640000007894, w1=0.011066014187528835\n",
      "Gradient Descent(424/499): loss=0.33997147353650115, w0=-0.31466400000078937, w1=0.01107712723854413\n",
      "Gradient Descent(425/499): loss=0.3399713993715112, w0=-0.3146640000007893, w1=0.011088128202851173\n",
      "Gradient Descent(426/499): loss=0.3399713261961724, w0=-0.31466400000078926, w1=0.011099018258343018\n",
      "Gradient Descent(427/499): loss=0.3399712539928227, w0=-0.3146640000007892, w1=0.01110979857039431\n",
      "Gradient Descent(428/499): loss=0.33997118274413574, w0=-0.31466400000078915, w1=0.011120470291988784\n",
      "Gradient Descent(429/499): loss=0.33997111243311495, w0=-0.3146640000007891, w1=0.011131034563845673\n",
      "Gradient Descent(430/499): loss=0.33997104304308545, w0=-0.31466400000078903, w1=0.01114149251454503\n",
      "Gradient Descent(431/499): loss=0.3399709745576909, w0=-0.314664000000789, w1=0.011151845260651964\n",
      "Gradient Descent(432/499): loss=0.33997090696088283, w0=-0.3146640000007889, w1=0.011162093906839813\n",
      "Gradient Descent(433/499): loss=0.3399708402369194, w0=-0.31466400000078887, w1=0.011172239546012245\n",
      "Gradient Descent(434/499): loss=0.33997077437035583, w0=-0.3146640000007888, w1=0.01118228325942427\n",
      "Gradient Descent(435/499): loss=0.3399707093460411, w0=-0.31466400000078876, w1=0.011192226116802208\n",
      "Gradient Descent(436/499): loss=0.33997064514911013, w0=-0.3146640000007887, w1=0.011202069176462586\n",
      "Gradient Descent(437/499): loss=0.33997058176497996, w0=-0.31466400000078865, w1=0.011211813485429978\n",
      "Gradient Descent(438/499): loss=0.3399705191793436, w0=-0.3146640000007886, w1=0.011221460079553793\n",
      "Gradient Descent(439/499): loss=0.3399704573781645, w0=-0.31466400000078854, w1=0.011231009983624021\n",
      "Gradient Descent(440/499): loss=0.33997039634767146, w0=-0.3146640000007885, w1=0.011240464211485932\n",
      "Gradient Descent(441/499): loss=0.33997033607435423, w0=-0.3146640000007884, w1=0.01124982376615372\n",
      "Gradient Descent(442/499): loss=0.3399702765449569, w0=-0.31466400000078837, w1=0.011259089639923144\n",
      "Gradient Descent(443/499): loss=0.3399702177464746, w0=-0.3146640000007883, w1=0.011268262814483123\n",
      "Gradient Descent(444/499): loss=0.3399701596661475, w0=-0.31466400000078826, w1=0.011277344261026301\n",
      "Gradient Descent(445/499): loss=0.33997010229145647, w0=-0.3146640000007882, w1=0.011286334940358604\n",
      "Gradient Descent(446/499): loss=0.33997004561011873, w0=-0.31466400000078815, w1=0.011295235803007786\n",
      "Gradient Descent(447/499): loss=0.3399699896100825, w0=-0.3146640000007881, w1=0.011304047789330965\n",
      "Gradient Descent(448/499): loss=0.33996993427952404, w0=-0.31466400000078804, w1=0.011312771829621154\n",
      "Gradient Descent(449/499): loss=0.3399698796068415, w0=-0.314664000000788, w1=0.011321408844212795\n",
      "Gradient Descent(450/499): loss=0.33996982558065114, w0=-0.3146640000007879, w1=0.011329959743586308\n",
      "Gradient Descent(451/499): loss=0.3399697721897844, w0=-0.31466400000078787, w1=0.011338425428471646\n",
      "Gradient Descent(452/499): loss=0.3399697194232814, w0=-0.3146640000007878, w1=0.01134680678995089\n",
      "Gradient Descent(453/499): loss=0.3399696672703896, w0=-0.31466400000078776, w1=0.011355104709559848\n",
      "Gradient Descent(454/499): loss=0.33996961572055706, w0=-0.3146640000007877, w1=0.011363320059388698\n",
      "Gradient Descent(455/499): loss=0.33996956476343165, w0=-0.31466400000078765, w1=0.011371453702181672\n",
      "Gradient Descent(456/499): loss=0.3399695143888539, w0=-0.3146640000007876, w1=0.01137950649143579\n",
      "Gradient Descent(457/499): loss=0.339969464586856, w0=-0.31466400000078754, w1=0.011387479271498625\n",
      "Gradient Descent(458/499): loss=0.33996941534765623, w0=-0.3146640000007875, w1=0.011395372877665148\n",
      "Gradient Descent(459/499): loss=0.3399693666616568, w0=-0.3146640000007874, w1=0.01140318813627363\n",
      "Gradient Descent(460/499): loss=0.33996931851943935, w0=-0.31466400000078737, w1=0.011410925864800602\n",
      "Gradient Descent(461/499): loss=0.33996927091176243, w0=-0.3146640000007873, w1=0.011418586871954912\n",
      "Gradient Descent(462/499): loss=0.3399692238295563, w0=-0.31466400000078726, w1=0.011426171957770853\n",
      "Gradient Descent(463/499): loss=0.3399691772639214, w0=-0.3146640000007872, w1=0.011433681913700373\n",
      "Gradient Descent(464/499): loss=0.3399691312061255, w0=-0.31466400000078715, w1=0.011441117522704403\n",
      "Gradient Descent(465/499): loss=0.33996908564759803, w0=-0.3146640000007871, w1=0.011448479559343269\n",
      "Gradient Descent(466/499): loss=0.33996904057992944, w0=-0.31466400000078704, w1=0.011455768789866216\n",
      "Gradient Descent(467/499): loss=0.3399689959948665, w0=-0.314664000000787, w1=0.011462985972300051\n",
      "Gradient Descent(468/499): loss=0.33996895188431014, w0=-0.3146640000007869, w1=0.011470131856536909\n",
      "Gradient Descent(469/499): loss=0.3399689082403126, w0=-0.31466400000078687, w1=0.011477207184421141\n",
      "Gradient Descent(470/499): loss=0.3399688650550736, w0=-0.3146640000007868, w1=0.011484212689835342\n",
      "Gradient Descent(471/499): loss=0.33996882232093883, w0=-0.31466400000078676, w1=0.01149114909878552\n",
      "Gradient Descent(472/499): loss=0.33996878003039577, w0=-0.3146640000007867, w1=0.011498017129485418\n",
      "Gradient Descent(473/499): loss=0.33996873817607176, w0=-0.31466400000078665, w1=0.011504817492439975\n",
      "Gradient Descent(474/499): loss=0.3399686967507316, w0=-0.3146640000007866, w1=0.011511550890527956\n",
      "Gradient Descent(475/499): loss=0.3399686557472744, w0=-0.31466400000078654, w1=0.011518218019083778\n",
      "Gradient Descent(476/499): loss=0.33996861515873134, w0=-0.3146640000007865, w1=0.011524819565978455\n",
      "Gradient Descent(477/499): loss=0.3399685749782616, w0=-0.3146640000007864, w1=0.011531356211699779\n",
      "Gradient Descent(478/499): loss=0.33996853519915343, w0=-0.31466400000078637, w1=0.011537828629431642\n",
      "Gradient Descent(479/499): loss=0.3399684958148179, w0=-0.31466400000078626, w1=0.011544237485132594\n",
      "Gradient Descent(480/499): loss=0.3399684568187886, w0=-0.3146640000007862, w1=0.011550583437613575\n",
      "Gradient Descent(481/499): loss=0.339968418204719, w0=-0.3146640000007861, w1=0.011556867138614858\n",
      "Gradient Descent(482/499): loss=0.33996837996637996, w0=-0.31466400000078604, w1=0.011563089232882217\n",
      "Gradient Descent(483/499): loss=0.33996834209765725, w0=-0.3146640000007859, w1=0.011569250358242317\n",
      "Gradient Descent(484/499): loss=0.33996830459255023, w0=-0.31466400000078587, w1=0.011575351145677303\n",
      "Gradient Descent(485/499): loss=0.3399682674451689, w0=-0.3146640000007858, w1=0.011581392219398662\n",
      "Gradient Descent(486/499): loss=0.33996823064973186, w0=-0.3146640000007857, w1=0.011587374196920306\n",
      "Gradient Descent(487/499): loss=0.33996819420056434, w0=-0.31466400000078565, w1=0.011593297689130894\n",
      "Gradient Descent(488/499): loss=0.33996815809209624, w0=-0.3146640000007856, w1=0.011599163300365416\n",
      "Gradient Descent(489/499): loss=0.3399681223188602, w0=-0.31466400000078554, w1=0.011604971628476048\n",
      "Gradient Descent(490/499): loss=0.3399680868754897, w0=-0.3146640000007854, w1=0.011610723264902253\n",
      "Gradient Descent(491/499): loss=0.33996805175671674, w0=-0.31466400000078537, w1=0.011616418794740171\n",
      "Gradient Descent(492/499): loss=0.3399680169573702, w0=-0.31466400000078526, w1=0.011622058796811269\n",
      "Gradient Descent(493/499): loss=0.33996798247237414, w0=-0.3146640000007852, w1=0.011627643843730311\n",
      "Gradient Descent(494/499): loss=0.3399679482967459, w0=-0.31466400000078515, w1=0.011633174501972578\n",
      "Gradient Descent(495/499): loss=0.33996791442559393, w0=-0.3146640000007851, w1=0.011638651331940414\n",
      "Gradient Descent(496/499): loss=0.3399678808541173, w0=-0.314664000000785, w1=0.01164407488802907\n",
      "Gradient Descent(497/499): loss=0.33996784757760223, w0=-0.3146640000007849, w1=0.011649445718691847\n",
      "Gradient Descent(498/499): loss=0.3399678145914208, w0=-0.3146640000007848, w1=0.01165476436650458\n",
      "Gradient Descent(499/499): loss=0.3399677818910316, w0=-0.3146640000007847, w1=0.011660031368229426\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAEKCAYAAAD6q1UVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIQd/M7MS+i+Q43vLgfrGYgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9af46f5eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w, loss = func_GD (y=y_train, tx=tx_train, test_set=tx_test, max_iters=500,gamma=0.1, initial_w=initial_w);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 31)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.shape\n",
    "tx_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74.495599999999996"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_train = predict_labels(w, tx_train)\n",
    "right_train = np.sum(y_pred_train == y_train)/len(y_train)*100\n",
    "right_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Ridge regression    -------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbUAAAEKCAYAAACVNst9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9ab74ec940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "func_ridge_regression (y=y_train, tx=tx_train, test_set=tx_test, lambda_=lambda_);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## * SGD -------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def func_SGD (y, tx, test_set, max_iters, gamma, initial_w, batch_size):\n",
    "    name = 'Stochastic_Gradient_descent'\n",
    "    \n",
    "    w,loss = least_squares_SGD(y, tx, initial_w, max_iters,\\\n",
    "                               gamma, batch_size)\n",
    "    \n",
    "    y_pred = predict_labels(w, test_set)\n",
    "    create_csv_submission(ids_test, y_pred, name)\n",
    "    \n",
    "    plt.plot(w, 'go')\n",
    "    plt.title('SGD: weights with loss:  '+str(loss),fontsize=15, fontweight='bold');\n",
    "    plt.show()\n",
    "    \n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEKCAYAAAA8QgPpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9aae345208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w, loss = func_SGD (y=y_train, tx=tx_train, test_set=tx_test, max_iters=10,gamma=gamma, initial_w=initial_w,\\\n",
    "                   batch_size=5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "def cross_validation_logistic_regression(y,tx, k_fold, max_iters, single_gamma):\n",
    "    seed=1;\n",
    "    # get k'th subgroup in test, others in train\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    accuracy_train = np.zeros(k_fold)\n",
    "    accuracy_test = np.zeros(k_fold)\n",
    "    \n",
    "    \n",
    "    for k in range(k_fold):\n",
    "        #print('----- FOLD', k, '-----')\n",
    "        k_index = k_indices[k]\n",
    "        test_y = y[k_index]\n",
    "        test_tx = tx[k_index,:]\n",
    "\n",
    "        mask = np.ones(len(y), dtype=bool) # set all elements to True\n",
    "        mask[k_index] = False              # set test elements to False\n",
    "        train_tx = tx[mask,:]              # select only True elements (ie train elements)\n",
    "        train_y = y[mask]\n",
    "        weights,loss = logistic_regression(train_y, train_tx, initial_w, max_iters, single_gamma)\n",
    "        # Compute the predictions\n",
    "        y_pred_train = predict_labels(weights, train_tx)\n",
    "        y_pred_test = predict_labels(weights, test_tx)\n",
    "        predictions=True;\n",
    "        accuracy_train[k] = np.sum(y_pred_train == train_y)/len(train_y)\n",
    "        accuracy_test[k] = np.sum(y_pred_test == test_y)/len(test_y)\n",
    "        \n",
    "    # Compute accuracy of the predictions\n",
    "    \n",
    "    return np.mean(accuracy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- GAMMA 1e-06 ---\n",
      "0.712148\n",
      "--- GAMMA 1.2e-05 ---\n",
      "0.713708\n",
      "--- GAMMA 2.3e-05 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zampieri/Documents/EPFL/ML/LMO_ML/project1/lib/costs.py:36: RuntimeWarning: overflow encountered in exp\n",
      "  log = np.log(1+np.exp(dot_prod))\n",
      "/home/zampieri/Documents/EPFL/ML/LMO_ML/project1/lib/costs.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  result = np.exp(z)/(1+np.exp(z))\n",
      "/home/zampieri/Documents/EPFL/ML/LMO_ML/project1/lib/costs.py:66: RuntimeWarning: invalid value encountered in true_divide\n",
      "  result = np.exp(z)/(1+np.exp(z))\n",
      "/home/zampieri/Documents/EPFL/ML/LMO_ML/project1/lib/proj1_helpers.py:30: RuntimeWarning: invalid value encountered in less_equal\n",
      "  y_pred[np.where(y_pred <= 0)] = -1\n",
      "/home/zampieri/Documents/EPFL/ML/LMO_ML/project1/lib/proj1_helpers.py:31: RuntimeWarning: invalid value encountered in greater\n",
      "  y_pred[np.where(y_pred > 0)] = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "--- GAMMA 3.4e-05 ---\n",
      "0.0\n",
      "--- GAMMA 4.5e-05 ---\n",
      "0.0\n",
      "--- GAMMA 5.6e-05 ---\n",
      "0.0\n",
      "--- GAMMA 6.7e-05 ---\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-109c49dbf7b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msingle_gamma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgammas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'--- GAMMA'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msingle_gamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'---'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0maccuracy_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcross_validation_logistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_fold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msingle_gamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-a42028a20336>\u001b[0m in \u001b[0;36mcross_validation_logistic_regression\u001b[0;34m(y, tx, k_fold, max_iters, single_gamma)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mtrain_tx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m              \u001b[0;31m# select only True elements (ie train elements)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mtrain_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_tx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msingle_gamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;31m# Compute the predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0my_pred_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_tx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/EPFL/ML/LMO_ML/project1/lib/implementations.py\u001b[0m in \u001b[0;36mlogistic_regression\u001b[0;34m(y, tx, initial_w, max_iters, gamma)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;31m# Calculate actual loss and gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcosts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_logreg_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcosts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'logreg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/EPFL/ML/LMO_ML/project1/lib/costs.py\u001b[0m in \u001b[0;36mcompute_logreg_loss\u001b[0;34m(y, tx, w)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;34m\"\"\"Compute the cost by negative log likelihood.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mdot_prod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mlog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdot_prod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gammas=np.linspace(0.000001,0.0001,10);\n",
    "k_fold=4;\n",
    "max_iters=100;\n",
    "accuracy_test=np.zeros(len(gammas))\n",
    "for i, single_gamma in enumerate(gammas):\n",
    "        print('--- GAMMA', single_gamma, '---')\n",
    "        accuracy_test[i]=cross_validation_logistic_regression(y_train,tx_train, k_fold, max_iters, single_gamma)\n",
    "        print(accuracy_test[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *LOGISTIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def func_logistic (y, tx, test_set, max_iters, gamma, initial_w):\n",
    "    name = 'Logistic regression'\n",
    "    \n",
    "    #w,loss = logistic_regression(y, tx, initial_w, max_iters, gamma)\n",
    "    w,loss = logistic_regression_mat (y, tx, initial_w, max_iters, gamma)\n",
    "    \n",
    "    y_pred = predict_labels(w, test_set)\n",
    "    create_csv_submission(ids_test, y_pred, name)\n",
    "    \n",
    "    plt.plot(w, 'go')\n",
    "    plt.title('logistic reg: weights with loss:  '+str(loss),fontsize=15, fontweight='bold');\n",
    "    plt.show()\n",
    "    \n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent logistic (1/49): loss=[ 173286.7951405]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zampieri/Documents/EPFL/ML/LMO_ML/project1/lib/implementations.py:185: RuntimeWarning: overflow encountered in exp\n",
      "  log_like=log_like+(np.log(1+np.exp(tx[i,:].T.dot(initial_w)))-y[i,:].dot(tx[i,:].dot(initial_w)));\n",
      "/home/zampieri/Documents/EPFL/ML/LMO_ML/project1/lib/implementations.py:223: RuntimeWarning: overflow encountered in exp\n",
      "  sigma[i,:]= np.exp(v[i,:])/((1+np.exp(v[i,:])));\n",
      "/home/zampieri/Documents/EPFL/ML/LMO_ML/project1/lib/implementations.py:223: RuntimeWarning: invalid value encountered in true_divide\n",
      "  sigma[i,:]= np.exp(v[i,:])/((1+np.exp(v[i,:])));\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent logistic (2/49): loss=[ inf]\n",
      "Gradient Descent logistic (3/49): loss=[ nan]\n",
      "Gradient Descent logistic (4/49): loss=[ nan]\n",
      "Gradient Descent logistic (5/49): loss=[ nan]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-b8969dfd2e6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_logistic\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m                         \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-73-cec0c042ddca>\u001b[0m in \u001b[0;36mfunc_logistic\u001b[0;34m(y, tx, test_set, max_iters, gamma, initial_w)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#w,loss = logistic_regression(y, tx, initial_w, max_iters, gamma)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogistic_regression_mat\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/EPFL/ML/LMO_ML/project1/lib/implementations.py\u001b[0m in \u001b[0;36mlogistic_regression_mat\u001b[0;34m(y, tx, initial_w, max_iters, gamma)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m     \u001b[0;34m[\u001b[0m\u001b[0mw_opt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlog_like\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mlogistic_gradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog_like\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mw_opt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlog_like\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/EPFL/ML/LMO_ML/project1/lib/implementations.py\u001b[0m in \u001b[0;36mlogistic_gradient_descent\u001b[0;34m(y, tx, initial_w, max_iters, gamma)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0msigma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m                 \u001b[0msigma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "w, loss = func_logistic (y=y_train, tx=tx_train, test_set=tx_test, max_iters=50,\\\n",
    "                         gamma=0.0001, initial_w=initial_w);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70.532399999999996"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_train = predict_labels(w, tx_train)\n",
    "right_train = np.sum(y_pred_train == y_train)/len(y_train)*100\n",
    "right_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
