{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Project 1\n",
    "\n",
    "    We begin by loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from preprocessing_functions import *\n",
    "%matplotlib inline \n",
    "import numpy as np   # generic stuff\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#### REMOVE THIS LINE BEFORE SUBMISSION\n",
    "import pandas as pd\n",
    "#######################################################################\n",
    "\n",
    "from lib.proj1_helpers import * #the helper provided for the project\n",
    "from lib.costs import *\n",
    "\n",
    "# choose which implementations you would like\n",
    "from lib.implementations import *\n",
    "#from implementations import * #our implementations of the functions done by us\n",
    "\n",
    "\n",
    "import datetime\n",
    "from helpers import * #helpers of exo 2\n",
    "# Useful starting lines\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_FOLDER = '../data/' # get rid of the ..\n",
    "\n",
    "y_train, tx_train, ids_train = load_csv_data(DATA_FOLDER+'train.csv',sub_sample=False)\n",
    "\n",
    "y_test, tx_test, ids_test = load_csv_data(DATA_FOLDER+'test.csv',sub_sample=False)\n",
    "AAA = 0;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note that here we are only considering a sub_sample as the \"True\" value indicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 30)\n",
      "(568238, 30)\n",
      "(250000,)\n"
     ]
    }
   ],
   "source": [
    "print(tx_train.shape)\n",
    "print(tx_test.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>138.470</td>\n",
       "      <td>51.655</td>\n",
       "      <td>97.827</td>\n",
       "      <td>27.980</td>\n",
       "      <td>0.91</td>\n",
       "      <td>124.711</td>\n",
       "      <td>2.666</td>\n",
       "      <td>3.064</td>\n",
       "      <td>41.928</td>\n",
       "      <td>197.760</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.277</td>\n",
       "      <td>258.733</td>\n",
       "      <td>2.0</td>\n",
       "      <td>67.435</td>\n",
       "      <td>2.150</td>\n",
       "      <td>0.444</td>\n",
       "      <td>46.062</td>\n",
       "      <td>1.24</td>\n",
       "      <td>-2.475</td>\n",
       "      <td>113.497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>160.937</td>\n",
       "      <td>68.768</td>\n",
       "      <td>103.235</td>\n",
       "      <td>48.146</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.473</td>\n",
       "      <td>2.078</td>\n",
       "      <td>125.157</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.916</td>\n",
       "      <td>164.546</td>\n",
       "      <td>1.0</td>\n",
       "      <td>46.226</td>\n",
       "      <td>0.725</td>\n",
       "      <td>1.158</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>46.226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-999.000</td>\n",
       "      <td>162.172</td>\n",
       "      <td>125.953</td>\n",
       "      <td>35.635</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.148</td>\n",
       "      <td>9.336</td>\n",
       "      <td>197.814</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.186</td>\n",
       "      <td>260.414</td>\n",
       "      <td>1.0</td>\n",
       "      <td>44.251</td>\n",
       "      <td>2.053</td>\n",
       "      <td>-2.028</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>44.251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>143.905</td>\n",
       "      <td>81.417</td>\n",
       "      <td>80.943</td>\n",
       "      <td>0.414</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.310</td>\n",
       "      <td>0.414</td>\n",
       "      <td>75.968</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060</td>\n",
       "      <td>86.062</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>175.864</td>\n",
       "      <td>16.915</td>\n",
       "      <td>134.805</td>\n",
       "      <td>16.405</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.891</td>\n",
       "      <td>16.405</td>\n",
       "      <td>57.983</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.871</td>\n",
       "      <td>53.131</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0        1        2       3       4        5        6      7       8   \\\n",
       "0  138.470   51.655   97.827  27.980    0.91  124.711    2.666  3.064  41.928   \n",
       "1  160.937   68.768  103.235  48.146 -999.00 -999.000 -999.000  3.473   2.078   \n",
       "2 -999.000  162.172  125.953  35.635 -999.00 -999.000 -999.000  3.148   9.336   \n",
       "3  143.905   81.417   80.943   0.414 -999.00 -999.000 -999.000  3.310   0.414   \n",
       "4  175.864   16.915  134.805  16.405 -999.00 -999.000 -999.000  3.891  16.405   \n",
       "\n",
       "        9    ...        20       21   22       23       24       25       26  \\\n",
       "0  197.760   ...    -0.277  258.733  2.0   67.435    2.150    0.444   46.062   \n",
       "1  125.157   ...    -1.916  164.546  1.0   46.226    0.725    1.158 -999.000   \n",
       "2  197.814   ...    -2.186  260.414  1.0   44.251    2.053   -2.028 -999.000   \n",
       "3   75.968   ...     0.060   86.062  0.0 -999.000 -999.000 -999.000 -999.000   \n",
       "4   57.983   ...    -0.871   53.131  0.0 -999.000 -999.000 -999.000 -999.000   \n",
       "\n",
       "       27       28       29  \n",
       "0    1.24   -2.475  113.497  \n",
       "1 -999.00 -999.000   46.226  \n",
       "2 -999.00 -999.000   44.251  \n",
       "3 -999.00 -999.000    0.000  \n",
       "4 -999.00 -999.000    0.000  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# illegal: \n",
    "pd.DataFrame(tx_train).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for methods: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OUT_FOLDER = 'output/'\n",
    "\n",
    "def func_least_squares (y, tx, test_set, fct='mse'):\n",
    "    #name = 'least_squares'\n",
    "    w,loss = least_squares(y,tx,fct)\n",
    "    #y_pred = predict_labels(w, test_set)\n",
    "    #create_csv_submission(ids_test, y_pred, OUT_FOLDER+name)\n",
    "    \n",
    "    plt.plot(w, 'go')\n",
    "    plt.title('least squares weights for loss:  '+str(loss),fontsize=15, fontweight='bold');\n",
    "    plt.show()\n",
    "    \n",
    "    return w, loss\n",
    "\n",
    "def func_GD (y, tx, test_set, max_iters, gamma, initial_w):\n",
    "    name = 'Gradient_descent'\n",
    "    w,loss = least_squares_GD(y, tx, initial_w, max_iters, gamma,fct='mse');\n",
    "    y_pred = predict_labels(w, test_set)\n",
    "    create_csv_submission(ids_test, y_pred, OUT_FOLDER+name)\n",
    "    \n",
    "    plt.plot(w, 'go')\n",
    "    plt.title('GD: weights with loss:  '+str(loss),fontsize=15, fontweight='bold');\n",
    "    plt.show()\n",
    "    \n",
    "    return w, loss\n",
    "\n",
    "########################  RIDGE REGRESSION #######################################################\n",
    "def func_ridge_regression (y, tx, test_set, lambda_):\n",
    "    name = 'Ridge_regression'\n",
    "    \n",
    "    w,loss = ridge_regression(y, tx, lambda_=lambda_, fct='mse');\n",
    "    \n",
    "    y_pred = predict_labels(w, test_set)\n",
    "    create_csv_submission(ids_test, y_pred, OUT_FOLDER+name)\n",
    "    \n",
    "    plt.plot(w, 'go')\n",
    "    plt.title('Ridge regression: weights ;loss:  '+str(loss),fontsize=15, fontweight='bold');\n",
    "    plt.show()\n",
    "    \n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LEAST-SQUARES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#todo: cross_validation su degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "def cross_validation_least_square(y, tx, k_fold, degs):            \n",
    "    seed=1;\n",
    "\n",
    "    # get k'th subgroup in test, others in train\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    accuracy_train = np.zeros([k_fold, len(degs)])\n",
    "    accuracy_test = np.zeros([k_fold, len(degs)])\n",
    "               \n",
    "    for k in range(k_fold):\n",
    "        print('----- FOLD', k, '-----')\n",
    "        k_index = k_indices[k]\n",
    "        test_y = y[k_index]\n",
    "        test_tx = tx[k_index,:]\n",
    "        mask = np.ones(len(y), dtype=bool) # set all elements to True\n",
    "        mask[k_index] = False              # set test elements to False\n",
    "        train_tx = tx[mask,:]              # select only True elements (ie train elements)\n",
    "        train_y = y[mask]\n",
    "\n",
    "        len_init_data = 0\n",
    "\n",
    "        for i, single_deg in enumerate(degs):\n",
    "            \n",
    "            print('++++ Deg =', single_deg)\n",
    "            if i==0:\n",
    "                train_tx, test_tx, len_init_data = prepare_data(train_tx, test_tx, single_deg);\n",
    "            else:\n",
    "                train_tx = add_powers(train_tx, single_deg, 0, len_init_data, features='x', current_max_deg=single_deg-1)\n",
    "                test_tx = add_powers(test_tx, single_deg, 0, len_init_data, features='x', current_max_deg=single_deg-1)\n",
    "            weights,loss = least_squares(train_y,train_tx,fct='mse');\n",
    "\n",
    "            # Compute the predictions\n",
    "            y_pred_train = predict_labels(weights, train_tx)\n",
    "            y_pred_test = predict_labels(weights, test_tx);\n",
    "            accuracy_train[k, i] = np.sum(y_pred_train == train_y)/len(train_y)\n",
    "            accuracy_test[k, i] = np.sum(y_pred_test == test_y)/len(test_y)\n",
    "\n",
    "    accuracies_test=np.mean(accuracy_test, axis=0); # mean on the k's\n",
    "    accuracies_train=np.mean(accuracy_train, axis=0);\n",
    "\n",
    "    max_index, acc_max = max(enumerate(accuracies_test), key=operator.itemgetter(1))\n",
    "    deg_best = degs[max_index]\n",
    "    \n",
    "    return deg_best, acc_max;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Starting Jet  0 *****\n",
      "----- FOLD 0 -----\n",
      "++++ Deg = 2\n",
      "++++ Deg = 3\n",
      "++++ Deg = 4\n",
      "++++ Deg = 5\n",
      "++++ Deg = 6\n",
      "++++ Deg = 7\n",
      "++++ Deg = 8\n",
      "++++ Deg = 9\n",
      "++++ Deg = 10\n",
      "++++ Deg = 11\n",
      "++++ Deg = 12\n",
      "++++ Deg = 13\n",
      "----- FOLD 1 -----\n",
      "++++ Deg = 2\n",
      "++++ Deg = 3\n",
      "++++ Deg = 4\n",
      "++++ Deg = 5\n",
      "++++ Deg = 6\n",
      "++++ Deg = 7\n",
      "++++ Deg = 8\n",
      "++++ Deg = 9\n",
      "++++ Deg = 10\n",
      "++++ Deg = 11\n",
      "++++ Deg = 12\n",
      "++++ Deg = 13\n",
      "----- FOLD 2 -----\n",
      "++++ Deg = 2\n",
      "++++ Deg = 3\n",
      "++++ Deg = 4\n",
      "++++ Deg = 5\n",
      "++++ Deg = 6\n",
      "++++ Deg = 7\n",
      "++++ Deg = 8\n",
      "++++ Deg = 9\n",
      "++++ Deg = 10\n",
      "++++ Deg = 11\n",
      "++++ Deg = 12\n",
      "++++ Deg = 13\n",
      "----- FOLD 3 -----\n",
      "++++ Deg = 2\n",
      "++++ Deg = 3\n",
      "++++ Deg = 4\n",
      "++++ Deg = 5\n",
      "++++ Deg = 6\n",
      "++++ Deg = 7\n",
      "++++ Deg = 8\n",
      "++++ Deg = 9\n",
      "++++ Deg = 10\n",
      "++++ Deg = 11\n",
      "++++ Deg = 12\n",
      "++++ Deg = 13\n",
      "----- FOLD 4 -----\n",
      "++++ Deg = 2\n",
      "++++ Deg = 3\n",
      "++++ Deg = 4\n",
      "++++ Deg = 5\n",
      "++++ Deg = 6\n",
      "++++ Deg = 7\n",
      "++++ Deg = 8\n",
      "++++ Deg = 9\n",
      "++++ Deg = 10\n",
      "++++ Deg = 11\n",
      "++++ Deg = 12\n",
      "++++ Deg = 13\n",
      "12\n",
      "12\n",
      "[ 0.63061756  0.61141027  0.57356621  0.54820338  0.54161746  0.50752677\n",
      "  0.57925133  0.57876088  0.52059854  0.57772996  0.52022821  0.49412471]\n",
      "Best degree =  2\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbIAAAEKCAYAAAB36tAEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUmj/H7LeeSFpiYmaAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fea57557668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Jet  0 ***** Accuracy jet 83.4135698057\n",
      "**** Starting Jet  1 *****\n",
      "----- FOLD 0 -----\n",
      "++++ Deg = 2\n",
      "++++ Deg = 3\n",
      "++++ Deg = 4\n",
      "++++ Deg = 5\n",
      "++++ Deg = 6\n",
      "++++ Deg = 7\n",
      "++++ Deg = 8\n",
      "++++ Deg = 9\n",
      "++++ Deg = 10\n",
      "++++ Deg = 11\n",
      "++++ Deg = 12\n",
      "++++ Deg = 13\n",
      "----- FOLD 1 -----\n",
      "++++ Deg = 2\n",
      "++++ Deg = 3\n",
      "++++ Deg = 4\n",
      "++++ Deg = 5\n",
      "++++ Deg = 6\n",
      "++++ Deg = 7\n",
      "++++ Deg = 8\n",
      "++++ Deg = 9\n",
      "++++ Deg = 10\n",
      "++++ Deg = 11\n",
      "++++ Deg = 12\n",
      "++++ Deg = 13\n",
      "----- FOLD 2 -----\n",
      "++++ Deg = 2\n",
      "++++ Deg = 3\n",
      "++++ Deg = 4\n",
      "++++ Deg = 5\n",
      "++++ Deg = 6\n",
      "++++ Deg = 7\n",
      "++++ Deg = 8\n",
      "++++ Deg = 9\n",
      "++++ Deg = 10\n",
      "++++ Deg = 11\n",
      "++++ Deg = 12\n",
      "++++ Deg = 13\n",
      "----- FOLD 3 -----\n",
      "++++ Deg = 2\n",
      "++++ Deg = 3\n",
      "++++ Deg = 4\n",
      "++++ Deg = 5\n",
      "++++ Deg = 6\n",
      "++++ Deg = 7\n",
      "++++ Deg = 8\n",
      "++++ Deg = 9\n",
      "++++ Deg = 10\n",
      "++++ Deg = 11\n",
      "++++ Deg = 12\n",
      "++++ Deg = 13\n",
      "----- FOLD 4 -----\n",
      "++++ Deg = 2\n",
      "++++ Deg = 3\n",
      "++++ Deg = 4\n",
      "++++ Deg = 5\n",
      "++++ Deg = 6\n",
      "++++ Deg = 7\n",
      "++++ Deg = 8\n",
      "++++ Deg = 9\n",
      "++++ Deg = 10\n",
      "++++ Deg = 11\n",
      "++++ Deg = 12\n",
      "++++ Deg = 13\n",
      "12\n",
      "12\n",
      "[ 0.54350013  0.54740779  0.54806551  0.55459118  0.55474594  0.55686097\n",
      "  0.55203766  0.5567707   0.55433325  0.57225948  0.52580604  0.53687129]\n",
      "Best degree =  11\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbMAAAEKCAYAAACYKLs6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFMxERCTzFMxERCTzFMxERCTz/gcGwm3fLej4vgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fea0d5b6400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Jet  1 ***** Accuracy jet 80.7992881461\n",
      "**** Starting Jet  2 *****\n",
      "----- FOLD 0 -----\n",
      "++++ Deg = 2\n",
      "++++ Deg = 3\n",
      "++++ Deg = 4\n",
      "++++ Deg = 5\n",
      "++++ Deg = 6\n",
      "++++ Deg = 7\n",
      "++++ Deg = 8\n",
      "++++ Deg = 9\n",
      "++++ Deg = 10\n",
      "++++ Deg = 11\n",
      "++++ Deg = 12\n",
      "++++ Deg = 13\n",
      "----- FOLD 1 -----\n",
      "++++ Deg = 2\n",
      "++++ Deg = 3\n",
      "++++ Deg = 4\n",
      "++++ Deg = 5\n",
      "++++ Deg = 6\n",
      "++++ Deg = 7\n",
      "++++ Deg = 8\n",
      "++++ Deg = 9\n",
      "++++ Deg = 10\n",
      "++++ Deg = 11\n",
      "++++ Deg = 12\n",
      "++++ Deg = 13\n",
      "----- FOLD 2 -----\n",
      "++++ Deg = 2\n",
      "++++ Deg = 3\n",
      "++++ Deg = 4\n",
      "++++ Deg = 5\n",
      "++++ Deg = 6\n",
      "++++ Deg = 7\n",
      "++++ Deg = 8\n",
      "++++ Deg = 9\n",
      "++++ Deg = 10\n",
      "++++ Deg = 11\n",
      "++++ Deg = 12\n",
      "++++ Deg = 13\n",
      "----- FOLD 3 -----\n",
      "++++ Deg = 2\n",
      "++++ Deg = 3\n",
      "++++ Deg = 4\n",
      "++++ Deg = 5\n",
      "++++ Deg = 6\n",
      "++++ Deg = 7\n",
      "++++ Deg = 8\n",
      "++++ Deg = 9\n",
      "++++ Deg = 10\n",
      "++++ Deg = 11\n",
      "++++ Deg = 12\n",
      "++++ Deg = 13\n",
      "----- FOLD 4 -----\n",
      "++++ Deg = 2\n",
      "++++ Deg = 3\n",
      "++++ Deg = 4\n",
      "++++ Deg = 5\n",
      "++++ Deg = 6\n",
      "++++ Deg = 7\n",
      "++++ Deg = 8\n",
      "++++ Deg = 9\n",
      "++++ Deg = 10\n",
      "++++ Deg = 11\n",
      "++++ Deg = 12\n",
      "++++ Deg = 13\n",
      "12\n",
      "12\n",
      "[ 0.52015881  0.51406452  0.51136476  0.51396526  0.51390571  0.51660546\n",
      "  0.51408437  0.51644665  0.51868983  0.50989578  0.51134491  0.49725062]\n",
      "Best degree =  2\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAawAAAEKCAYAAABOjWFfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XucXVV99/HPbyYJZIhMkkm8ETKDNVrReJ0ifaxtahQh\nikFftkUHCBUbb+0DPlW8TCsXTR+1rUZagaZKE3Se4qUoYLGIkdR6AR1EHC5qomZCJEpIIAQSyO33\n/LHWyew52eecfS4z5+w53/frNa85Z5119l577X32b++11t7b3B0REZFW19HsAoiIiGShgCUiIrmg\ngCUiIrmggCUiIrmggCUiIrmggCUiIrmQOWCZ2QYzczPbPIHlyVKO2WZ2cfxb0syySDZmtjZuOzVd\nQ2Fmm+P3N2TIW9g2zqhlXmWm+0Ez+6WZ7YtlWdvI6afM79xCneV9OzezuWZ2uZndb2ZPmNlPzey9\nZtaZ4bsvN7OrzOxeM9sV/243s78o/r6ZHWVmHzazX8T1dH+cb0+tZUrs99L+zkjkm29m/2RmPzKz\nA4k8C1Lm3Wdma2I595rZb8zsm2Z2SkreaWb2v83sDjPbY2aPmNmdZvbXiTwLzexLcXqPxmXfEn93\nz0iZ5vPM7HozeyhO8/tmtjwlX6nldjObXWJ9/UVRvgVFn3/IzG4xs8cSec5Km1Yqd8/0B2wAHNic\n9TsT8Qf0xXI4cHEzy6K/zOtsbWGd1fj9zfH7GzLkLWwbaxtY/tckptvw6ZeY57mJeS1p9jqsYzlm\nAj9JqT8H/jXD968s8V0Hrkjk6wC+USLfCNBVS5kS+720vzMS+V5YIs+CoukdC9xfIu8hYFnRMl1X\nIu93EvlOLlPGbUB3Iu9zgV0l8p5V4reU9jc7ZV3NAbZXWP6HK8233J+aBFuYmR3d7DI0gruf6+7m\n7tbsstTohYnXfxSX5dx6J2pmM8xsqv8GLwAWx9cfAOYDX4nv32pmv1/h+weBzwAvArqANwAH4mdv\nM7OnxNevA14VX38aeBKwIr5/HvDuOst0SWEbTvx9NfH5w8AngDcSgkwpS4GnxdfXEQLYm+N7Ixyo\nFLwzLhfAPwMLgVlAP6FOCh4E/gr4HUIwfgHwi/jZU4FXJPJ+Is5zH/BK4ATgV/Gzy8ysK6XMf5yy\n7A+n5FsFzAP2pC558G/AW4CLy+QprYojpQ2knGEBLwWuB3YQKuHnwN8C0xN5ng1cS6iYR4EngE3A\nx4FjEvmOAf6RUNl7CRvBCGHlzGT8UWfxX8mjUMKKuYUQ/Z8Afg38F+OPkI6J83mEsAH8A7CyePqU\nOPJNqx/gdOCbhCOqJ4DHgNuBtxWVb21imv3A92P+C+LnsxP1si8uxzXAoqLp/ClwG7Az1t8Wwo/i\n5WXq5s2Jeb8gpr0qkfbMmPb6RNrJRd//flyve4EfAH9WavlS5r0pfu9bcTspzGNtIt/mmLYBWAbc\nGevyNuAlMc+SMtvGuXXUz4YK0+wBPhXLWFg3XwaeV+JodS3w13Heh0g5Uq2wnWWdX8VlzZgndd1V\nsd8Yid9/BOiMaScllu2yCt+flZJ2Q+L7vx/TVifSTkzk3RnT7qmlTIn1f3EVy7w2Ma3iM4zXJT57\na0ybnki7IZH35zHtuzXU+z8mpvnamDafcADgwI2JvBcm8r4hZZstuW9N5H1RnPbdwOdLLX+J7Tvz\nGVY1FVBYcckd8qvjjybtB52s+FNL5HHgC4l8V5TJN48aAhbQS/gxpn3nnxP5hlI+v794+lQXsD5a\nprzvKLGB70y8voBwpHhXiWnsJAYt4PcJO8C0fO8ps14XJPK9PaZ9KJF2dkz7+/j+MeLBCHBpmeV7\nT9ryJdKWppT314nXaQFrB+HoOvmdLYQf/JIyZTm3jvrZUGaa3cDPSnz+GDGYFv34dxblyxywss4v\ny7JmrY+0dVfFPuMoxnaQP06kz07M639qmO7Nie8vjGn/kkh7TiJvob4PEQ56qypTYv3vJBxE7gb+\nm0TTXUr51iamVRywuoBfxs++Svh9Jw8a/yrme1oi7YY4z0eB3xL2k08qMe9phBaBTfG7G4GZ8bPk\ngejqxHfOSKR/OGWb3Q7sj/+/lKzfmM+A78a8S8otf4nte9KaBD9N2Fl8j9C3NJOxU+/Xmtmp8fVP\nCcHtqcAMwlHiv8bP/iTRKfoH8f+XCKe+cwhHPpcCT7j7WsIpbEHyNH1DiTL2A4Wmtd8jbLC9hI3k\nOwBm9mzgTTHPHcDxhGaEgxnqoJyvxvL3EOqpF/hR/OwdJb5zD/AMYC7wH4Sg9VzCgcGpcVkWAw8Q\n6ufD8Xv/i7Dh7CY0DRwNPBN4K+GIMpW7byUEBAg7scK0PCUN4FZ3329mJwAfjGmfjuWdA/x7TLvU\nzOaUmi9wSSzvIcJR51zCzqGcuYRgOofwo4Cwrl7q7ht8fJPjusS2sZba62dJLGvBCYlpvht4Vkz/\nGCGgvCEuUxfhKLfYHEK9dQPPIQSarLLOL8uy1lQfVZrL2MCuRxLpyddPrmaCZvZyxpq4vunuW+Lr\nZJnfZWazzOwcQn1DWNY5dZRpDmHfNQv4Q+A/zezNKfnKcvc9hP3cXcDyON8hwkH13xF+SxC264LX\nxnkeE8v2duBGMxvXxG5mw4TAcgdhnd4LLHX3vTHLvET2apZ9HiEQziM0ed4a95kFKwjb07+X2Q83\nRhVHNRtInEEQfjiljmgLfx9LHGn9HSFwPZ6S7+SY72uMHWn/X+AsEqf3MU9f4nsXZyh3fyL/tYQf\n/SkkjlCAcxJ5zk6kX5JIr+UMawFwNbCVsCEll/nxEkdkv1dU/u9VqOPfxnxvZOxIci3wl8AfAUdn\nqKOr43d/TvhhPwTcCvwG+DHhh1pYbxfF76ysUC4HTitevvi+k7Ez8/WJcvxO4rtpZ1jbgI6Ydloi\n75tSjgrXFi1jPfVzcWK6fYn078e0vcBRifRbYvoBxo5uC9+/K+Pv7YjtLOv8sixrPfVRxT7j6Yll\n+HYivTOR/tMqpvcCQnN9YR9xfOKzYxPbSam/+dWWCXgXoTWgcECWbH3YXKKcaxN5is+wZhGaYYvL\ndpDQHPvkmO9lRZ8tJZwFJptDX1U07eGU6d5NHHTB+DO5SxPfW5pIvzKR/hHgxYRA2Us4AB/3+yIc\nNP2GEPSeXmn5S2zfk3KGNT9Dnrnx/98TOjefTQhexQpnQO8hHCk9HXg/8DngbjP7QalhlJW4+zCh\nM3AvoR/mE8BNwG/N7K0x29MSX/l14vX9VcyqeDhsByEAnw0cRzhCSUqrBzjy6LZSPRfq+FrgKsLG\nvQL4J0IQ3WZmr6kwjW/H/4uAlxN+GN8jBK3nEXZkRxXlrWb9F5tHOOOE8fW9tcL0fuHuh+LrxxPp\npeoyqZ76KaVwxLrd3Z9IpBeWo5Mj6+CuGudVzfyyLOtE1EexHYSACCGgFDwp8Xp7lgmZ2QuB9YTW\nit8Ar3T3+wqfu/sjhLOQLxFGwe0i/P6+G7PsITTrVVUmd/+0u693953u/pC7X0o4sAPoNbMsv4Ok\ntxJaXSAcxB9DGOW3l9DS8Nn42Y7Ed+6MZXiYsZYpCAH8MHfvJxxcPpdQVwAnxnmOWy6yLfvfuPuP\n3P0xdx9lfKvQ78X/fwE8hdCy8uS4npLb/IlmtpAGqSdgPZh4vdqPHEVihKNwgD+J/+8itDkbYVTL\nOO7+U3d/PuFI+3TCGc5BQuW8q5Ct2oK6+98QfuwvI0T2WwlHopeZ2TTGB6bjEq+fnjK55I4iOYqv\ntyjfIsY2qM8R+iqM0MxXrqyPFyUV6nkHoYO4uI5nxO8dcvfzCMu5hFD3PyUEn9Xl5gn8T+J1oUn3\n+/Gvk9AsCeEs8daickEYvFJcrg53HyoxvwfjtGD8wcLxKXmTDiReV7Ud1Fk/pRTqYL6ZJYNm4dqT\nQ4Sz1aTi9dvw+WVZ1gmqj3FiUL0nvn1G4hqnZyWy3VFpOmb2IsaC1VbgD9393pT5bXH3P3X32e4+\nm3AWWdimvuPuB6spU5kRnMlt71CJPKX8buL1Onff4+63EYbZw1hz50ZCn1Xx/JL2Fie4+353v4dw\nAFKwKP7/caK8yeXNuuxpyz0r/l8Zv3sHYd9dcBOhS6ch6glYP2dsOORbzew0Mzs6XkB3ppn9gLGd\n+Iz4fz/wWGz/fFfR9DCzC83s9THfTcAXGfuBF45kdia+8rtmNp0yzGyxmf0tYaXcTRhRVfiRzCQc\nXdzK2Ap4t5kdZ2YnEoZfFkueBbw6zuNcjtzZzki83gvsM7NXEUa5VeO/4v8e4BNmNs/MZprZS83s\nKsKZKGb2x2b2fwgB93bCkWbhSLDsUaC7/4zQJwZjw2i/RwhYEJrfAIZ9rD38Zsbq7CNm9vw4TLvP\nzN5N6CQuNb+DhGYRgCVmdko8g27Ehl0IEM80s5mFxHrqp4xvxP9HAxeZ2bHxAsw/jOnf8dBn0SiZ\n5pdlWbPWh9V50Tfw/+L/JwHvNbN5xG226PPUC8RjsPom4ah9MyFYbUybUbxo9VlxP7SI0DdUOLq/\nrIYyPd/MvmFmy8zsSWY2x8w+RGgpAtjo7jvivDvib3Me48/458T0wnDxbYnPVphZl5m9FHh+THsY\nDv9GvhzTXmhmrzCzbsIZTcG347zfa2ZvMrPe+Bt8FuP3r7+M09xOqEuApWa21Mz6CH1iEPathf3N\nO8zsX8zspFifvYTBHgXfowaxDucxFugAZsU66q44gSrajzdwZB/NMo7sm0n+9fn4PpLk36bE6yVF\n80j7Oy0x37SRUtNKlHtJmWnemsiXVsZtKWWcAdyXSN8d/+9J1g+hyesXRdM7xNgIIU9r804pfzeh\n87TUMlyc0iZc/HdNhvX75UT+LTGtq2j9frToO39XZp6byy0f6aMEk/X9b4m8m2PahhLr9dxE+k0p\nZXlmPfVD6T6s2YQj4bRp7iHRH5lIX5vx95Ys75Jq5pdlWbPWR9q6q+aP6i7STVvPa8uUs3jdby2R\n559qKROlLwZ2wu8iuU/qq1DOwu90IekXzxb+BhPTPI7xv4lS5fxqmen9CpibyFvuwuFk//0FZaa5\njUT/Yco6T66z4j68zWWmu6HUNA9/v4oNbwNFO6KYfjKhs/BBQnPZFuBGwpHAjJhnDuFo5yFCG+nH\ngPMSBS38IM8l7HB+Hae1gxDJi6/rOZlwvc+exDRKBawFhCGvd8UN5fG4Ev8FeGoi3zGE9uFH4nxX\nk3IdVsz7YsIZwh5CMBlIqx9C/8+3Y75NhP6swysz606BsKP6hziNQr0MEzpFn5HYENcRjpJ3x3lu\nJPQfpg6BLZrH+YllTe6wkh25Rwzljcv+3aJ5fo7xV+ynLl/87i/iOvlvwuipwrw+UWFHtiSRN7nT\nejZhEMIjic+fWU/9UCJgxc/mES7qHCXsxB4k9A89vyhf4fs1B6ys88uyrFnro9S6q+aP0DpwOWFH\n9wThgPO9xGugKqznw/Mv8Zdc96vjMuwhNKd9n8ROuNoyEc7ALozb5jbGhnZ/FTipaHp9Fcp5cSLv\ncwjXUW4jNHM/StifvS2lnCcQ+od2xHLeQ2i270jkOYvQZLqNMJjpMcL+7uPA/JRpPo9w7ezDsa5u\nBZYX5fmd+P07GLvGdpRw55GnV1jfyXXW0IBlcSKSIjb1/Vt8+8c+0UM224iZzSIE/u+4+yEzm0E4\nkCn0ly139+ubVkARaTnFI9dEJstswpHr42a2ndBvUhjE8l+E4bsiIodN9fuYSet6hNAs8gDhYsVD\nhIuq3wO8znXqLyJF1CQoIiK5oDMsERHJhdz3Yc2bN8/7+vqaXQwRkVy5/fbbH3T3Wq9BbIrcB6y+\nvj6Gh4ebXQwRkVwxs9Fml6FaahIUEZFcUMASEZFcUMASEZFcUMASEZFcUMASEZFcUMASaYKhkSH6\nVvfRcUkHfav7GBop9egwESnI/bB2kbwZGhli5Q0r2bM/PCprdNcoK28IzzodWDzQzKKJtDSdYYlM\nssH1g4eDVcGe/XsYXD/YpBKJ5IMClsgk27JrS1XpIhIoYIlMsoXdC6tKF5FAAUtkkq1auoqu6V3j\n0rqmd7Fq6aomlUgkHxoWsMys08zuMLOvxfcnmNltZrbRzL4QnyiLmR0V32+Kn/clpvGBmP4zM3t1\no8om0koGFg+w5vQ19Hb3Yhi93b2sOX2NBlyIVNDIUYLnA/cCx8b3HwM+6e7XmNmVwHnAFfH/Q+7+\nTDM7M+b7MzM7ETgTeC7wdOCbZvYsdz/YwDKKtISBxQMKUCJVasgZlpktAF4DfCa+N+AVwJdjlnXA\nGfH18vie+PnSmH85cI27P+HuvwI2ASc1onwiIpJ/jWoSXA1cSHjMOUAP8LC7H4jvtwLHxdfHAfcB\nxM93xfyH01O+M46ZrTSzYTMb3r59e4MWQUREWlndAcvMXgs84O63J5NTsnqFz8p9Z3yi+xp373f3\n/vnzc/X8MRERqVEj+rBeBrzOzJYBRxP6sFYDs81sWjyLWgDcH/NvBY4HtprZNKAb2JlIL0h+R0RE\n2lzdZ1ju/gF3X+DufYRBE99y9wHgFuCNMdsK4Lr4+vr4nvj5t9zdY/qZcRThCcAi4Af1lk9ERKaG\nibyX4PuAa8zsI8AdwGdj+meBz5nZJsKZ1ZkA7n63mX0RuAc4ALxLIwRFRKTAwslNfvX39/vw8HCz\niyEikitmdru79ze7HNXQnS5ERCQXFLBERCQXFLBERCQXFLCk4fQ0XRGZCHrisDSUnqYrIhNFZ1jS\nUHqarohMFAUsaSg9TVdEJooCljSUnqYrIhNFAUsaSk/TFZGJooAlDaWn6YrIRNGtmURE2pBuzSQi\nIjJBFLBERCQXFLBERCQXFLBERCQXFLBERCQXFLBERCQXFLBERCQXFLBERCQXFLBERCQXFLBERCQX\nFLBERCQXFLBERCQXFLBERHJkaGSIvtV9dFzSQd/qPoZGhppdpEkzrdkFEBGRbIZGhlh5w0r27N8D\nwOiuUVbesBKgLR7hozMsEZGcGFw/eDhYFezZv4fB9YNNKtHkUsASEcmJLbu2VJU+1ShgiUjutGs/\nzsLuhVWlTzUKWCKSK4V+nNFdozh+uB+nHYLWqqWr6JreNS6ta3oXq5aualKJJpcClojkSjv34wws\nHmDN6Wvo7e7FMHq7e1lz+pq2GHABGiUo0lRDI0MMrh9ky64tLOxeyKqlq9pm51Ordu/HGVg80Lbb\niM6wRJqknZu26tHu/TjtTAFLpEnauWmrHu3ej9POFLBEmqTdm7Zq1e79OO1MfVgiTbKweyGju0ZT\n06W8du7HaWc6wxJpEjVtiVSn7oBlZseb2S1mdq+Z3W1m58f0uWZ2s5ltjP/nxHQzs8vMbJOZ/cTM\nXpyY1oqYf6OZrai3bCKtTE1bItUxd69vAmZPA57m7j8ysycBtwNnAOcCO939o2b2fmCOu7/PzJYB\nfwUsA14KfMrdX2pmc4FhoB/wOJ2XuPtD5ebf39/vw8PDdS2DiEi7MbPb3b2/2eWoRt1nWO6+zd1/\nFF/vBu4FjgOWA+titnWEIEZMv9qDW4HZMei9GrjZ3XfGIHUzcGq95RMRkamhoX1YZtYHvAi4DXiK\nu2+DENSAJ8dsxwH3Jb62NaaVSk+bz0ozGzaz4e3btzdyEUREpEU1LGCZ2SzgP4AL3P2RcllT0rxM\n+pGJ7mvcvd/d++fPn199YUVEJHcaErDMbDohWA25+7Ux+bexqa/Qz/VATN8KHJ/4+gLg/jLpIiIi\nDRklaMBngXvd/ROJj64HCiP9VgDXJdLPiaMFTwZ2xSbDm4BTzGxOHFF4SkwTERFpyIXDLwPOBkbM\n7Mcx7YPAR4Evmtl5wBbgT+JnNxJGCG4C9gB/DuDuO83sw8APY75L3X1nA8onIiJTQN3D2ptNw9pF\nRKrXlsPaRUREJoMCloiI5IICloiI5IICloiI5IICloiI5IIClkiLGRoZom91Hx2XdNC3uo+hkaFm\nF0mkJegBjiItZGhkiJU3rGTP/j0AjO4aZeUNKwH02BFpezrDEmkhg+sHDwergj379zC4frBJJRJp\nHQpYU4iakvJvdNdoavqWXVsmuSQirUcBq0XUG2wKTUmju0Zx/HBTUqXpKMi1jqGRISz1oQWwsHvh\nJJdGpPUoYLWAWoNNUi1NSY2YrzTO4PpBPP2JOjy671EdVDSQDtTySQGrBTSi36JUk1G5piT1l7SW\ncutqx94dOqhoEB2o5ZcCVguoJdgUK9VkVK4pqRHzldqkHeFnbfbTQUV9dKCWXwpYLaCWYFNs1dJV\ndE3vGpfWNb2LVUtXTeh8GyHPzTO1lL3UEf6yRcuOWIel6KCidjpQyy8FrBZQS7ApNrB4gDWnr6G3\nuxfD6O3uZc3pa8peu9OI+dYrz80ztZa91BH+jRtvPGId9szsSZ2GBmHUrlUO1KR6eh5WixgaGWJw\n/SBbdm1hYfdCVi1dNSkXijZrvgV9q/tSh3L3dvey+YLNk1aOWtRadrskfSSgYRy66NC4tOILiSEc\nVFQ6GJHSVKdBHp+HpYAlTdVxSUfqyLi0nXerqaXsQyNDnH3t2anf6+3uZdXSVUccQABNPaiYipp9\noNYK8hiwdGsmaaqF3QtTz1Ly0DxTS9lLDV03jGWLlqXelmnN6Wta/mwzbwYWD7RdgJoK1IclTdUK\n/Wi1qqXspTr2HefGjTdq9NoUkeeBRK1MAUuaqpbBIq2ilrKXOvvqtE7dlmmKyPNAolanPiyRSZTW\n4V9gWMm+LTUJ5kc1g3Ga2ZeWxz4snWGJTKLCWVmndR7xWVqwykvzaF5MRlNd1uu8dCZWPQUskUk2\nsHiAQ155BGTPzJ7cNI/mwWQFiKzXeWW944b6w8YoYIk0QZZRkLNmzFKwaqBabxBdbbDIOhgny5mY\nzsLGU8ASaYK0nVqxqTzYohlnDdXekqnWYJF1ME6pg5a5M+cerpsVX1mhkaMJClgCtGazQyuWqVGS\nO7VS8nAtWi2addZQqj47rCN1G6vnJrkDiwfYfMFmDl10iM0XbE49U047aJneMZ3d+3YfrpuDfjB1\n+lP5YKYcBSxpyWaHVixToxV2ap9/w+dzey1aLZp1t/RSZ7UH/WDqNjbRN8lNOxM79qhj2XdwX8Xv\nTtWDmUoUsKRlHreQPKOa6KaQVjp7y/O1aLVo1t3Si+s5baRmchur9oys1jIlz8R27t1Z8TtT+WCm\nEgUsaYnHLRSfUU1kU0grnr0ld1yF+wm2QjCdCM28W3qynkuN1CxsY9WekTVCuQvL2+FgphIFLJm0\nHUi5s5q0s7yJKlOpM8rzv35+U866CvVilxgdl3Rw1rVnVRVMW+lsMYtWuR1Xpe2+2jOyRihVN+te\nv65sf1i7UMCSSdmBpJ3VnHXtWcz7+DyGRoYynTk1qkyl5rVj745JP+tK1gukXzxcbqfYimeLlbRK\nE2iW7T7LGdnortGS9V3twUSr1E2r0q2ZcmKib+Ey0dMvdbsaCDuJmdNmsmPvjiM+67RODvmhhpap\nXFmKTfRtkbKWpdQjS/L8PLFWkHW7HxoZYsVXVpRsqk57nlarP3crj7dmUsDKgbQN3zDe3v92Ln/N\n5VVNZ3D9IKO7Rum0Tg76wcPPYMryA6rn+6WeHVXQM7OHvQf2TsqPu9z9/IpV81yuWoJ+pXopKBWA\n8vw8sbzIur0Ur6NWP5jIY8BSk2AdJqvvIK3PxXGuHL4y8zyLm54KR4pZm5Dq/X6lvqede3dOSFNI\nsn9o2qXTsEuMwfWDrHjBisPXQKX1TWQtd3I+5Zo865m+YYzuGk3dxiZzAEMr9JU1owxZ+1eL11Er\nDGaaahSwajSZfQflnqGUtcO33I8uS8dxvd+vdGeHhd0LM11sWY1yQXbdnetYtmgZXdO7yjbzZO0z\nK1U/O/buKLtdZLnjReEMKm0bK/X9R/c92tBtsdl9ZUMjQ8z7+LyqB6Q0QjUBJlmmZo6GnKoUsFKk\nHZXXehV8lmlVUm4Dz/pjqpRvoj8vdCb3zOw54rOJGiFWKciuuX1Nyc+rPcMrt/zlAnrxHS8KZ3u9\n3b2pdVU8rVL1WilQVmuyr9VLnknN+/g83nLdW1L7OCfjesFqA0yhTK0yGnIqabk+LDM7FfgU0Al8\nxt0/Wi5/LX1YQyNDvO2Gt/HY/sdqL6hInTqtkyV9S9i0c1Nqv1fW/q1mMozPveFznP/18w8HlGOm\nH8PR045ODTBTydGdR/P4wcfL5pnRMYMDfiDT3flr0TOzh0+d9qmaWiPy2IfVUgHLzDqBnwOvArYC\nPwTe5O73lPpOtQFraGSIc649h0OoQ1paT3KgSTWjGZvlmOnHsO/gPvYf2t/sorStGZ0zuGr5VVUH\nrTwGrFZrEjwJ2OTuv3T3fcA1wPJGzmBw/aCClbSsZBPXskXLMKzJJSrvsf2PKVg12b6D+9rm7u2t\nFrCOA+5LvN8a08Yxs5VmNmxmw9u3b69qBhqhI61uy64tvPM/38mVw1e2fJOgtIZ22a+1WsBKO5w8\n4hfr7mvcvd/d++fPn1/VDDRCR1rd3JlzFaykKu2yX2u1gLUVOD7xfgFwfyNnsGrpKjpabrFFgsKo\nMgUryWpG54y2GXnYanvuHwKLzOwEM5sBnAlc38gZDCwe4Oo3XM0x049p5GRFajKjY8YRF0tnecSE\nCIRRgrUMuMirlholCGBmy4DVhGHtV7l72UOHem/NlLzdUJrkbVSy3ppoou/Ll9Tqt39phErDu+u9\nDVE1t2qC2uq2mlsolRsd2DOzhwcvfDDTPEttr8sWLWPdnesyLW/aLbOqVbiN2I0bb2zIqMdO6+To\naUcfviylwzoqDhuf3jGdY486lp17d5b8TVZz67Hi3/iyRcv44t1frGoof7Nvn5XHUYItF7Cq1ah7\nCTbqnmzV3vCy3uDWDveSqzS8uxHBObke5s6cy+59u1Of/Frr/Q2rObAYGhnirGvPSp1O1vVaLggb\nNulNjr3dvWzZtaXkfAsBIqt6l2Ei7lNZ7WUIzT6ozGPAarUmwaZp1G1UqrkjQCNud9MOt38pd/ui\nRt05IHlbqAcvfJCrll+VeveJWnZyQyNDPLrv0cxlH1g8kHqXC8i+Xsvd5aMZ/WOFA7I0vd29VV9Y\nW+8yTMT/90ZjAAAOnUlEQVQdMqoZqac7XtRGAStq1G1UqrnhZSNud9MOt38pd/uitEc6NOLmqIUA\n5hc5Bz50AL/Ix93fMOt8CgclxU1FPTN7yga/T532qbrWayOGOXdN7yoZOKs1d+bcskG7GQdYjR4K\nnnUZ9Iyr2k1rdgFaRWHjqbfvaWH3wtRmgbSNuRF3c25UuVtJqWbSSstU3AxWOGMFGlof1cyn1JnO\nrBmzypap3vVaajssKG5SK7wv7rsBUpu4q+3TSmtiLb6tUDX9iGkKz04r16Sb1OgguWzRsrKXI7TS\ns7DySn1YDVZNH1a9AyYmc3DHZKnnoXeTNQClmvk0q4+xXB9W1/QuVrxgBTduvDHTtpO2nZUbqFSs\n1KCI4voqns+j+x7NPIiheBup1CfZ6OBRqr5nzZjFY/sea8nfZx77sBSwJkA1TzGtdefc6k8zrVU9\nQWeygkMjRvxNRod7vQ/srDTts689u2JfUrmzsUrrpdLozWqeRt2sJ2o3e2BFOQpYTdCKAasatf6Q\n8vgDyaKeoFOqTjqtk3WvX9ewHVS1I/5a6cCimu2tUt53/uc7uWL4ipLzKgTHUmdjWbbVoZGhcXeC\nL2iFg7Nk/ZQK3K08WjePAUuDLpqs1ocWTtWnmdYz6rHUaMKDfrChz4ZKm0+ppwInB4w08knKtahm\nVGqWvJe/5vKSgzIKwWhg8UBdA4MGFg/w4IUP8vk3fH7S67DUwJq0h0mWMpVG67YCnWHlVNaj/Lz1\nc9V7RjI0MsSKr6xIvaankWefyea24gEMrXD0n6aaM8OsebOeEU+V7XDFC1Zkvui62u1gsusoj2dY\nClg5lWXH3mrNUVnl6WLqPDXNVlMvWfO2+vI3usk9ywXOhlW93Tbjt5rHgKUmwSap93qhLE1NjbjO\nq1HXNVWj1mbSgsm8mDpPTbPV1EvWvK18HWA9F+aXWn+VglXPzJ6atttG/FbbgQJWE2T9IVUKFpV2\n7PXuTBtxJ45mmMydaJ7uNFJNvWTN20p9dMXqCQKl1l/hovVSdu/bXdPvI08HPs2kgNUEWX5IrXDb\nprwe9U3mTrSVzzCKVVMv1eat54x4otQTBEqt15UvWVnyNmFQ+9N/83Tg00zqw2qCLP0DjegbqLdd\nvB1urNsIeRtQ0C4m6sL8Sk94qOX3oT6sbHSG1QRZjqYaddumes40qj3qa0Z/Vyto1TOMdlfv2W+p\n9VpIL9zbslgtZ0Wt3LTaSnQvwSZYtXRV6tFU8odUzT0Jy8lyD76k4lvazOicccQtbdJ+8JN1Hz+R\nrCb6PptZfsfVlle/lfJ0htUEWY6mmtE3UtxvtmPvDtydnpk9FY/68trfVa12PYvMq4k8+9VZ0eRT\nH1YLm+y+kTzcx6+Z8npdm0iaPPZhKWDJYRNxH7+emT3MmjFrSgxIaJWLZDXIQxohjwFLTYJyWKPv\n4ze9Yzq79+3O3XVcpTT7Wpm0e9jlvU5FqqGAJYfVe5PS4vb8Y4869oiH6OW5X6uZ18qUenIx5LtO\nRaqhgCWH1duJXNzBvXPvztR8eb16v5kXCZd6cnFBXutUpBoa1i7jNHJobaOG5reKiR4mXU6lgJTX\nOhWphgKWTJhGX6fSCpp1rUyp4A/5r1ORrNQkKBNG16k0TqmHU/bM7FGdStvQsHaRnNBwdmmkPA5r\nV8ASEWlDeQxYahIUEZFcUMASEZFcUMASEZFcUMASEZFcUMASEZFcUMASEZFcUMASEclID/BsLt2a\nSUQkg+IHeBYe7QLoAu5JojMsEZEM0u6Yr0e7TC4FLBGRDJr9AE9RwBIRyaSZD/CUoK6AZWZ/b2Y/\nNbOfmNlXzGx24rMPmNkmM/uZmb06kX5qTNtkZu9PpJ9gZreZ2UYz+4KZzainbCIijdTMB3hKUO8Z\n1s3A89z9+cDPgQ8AmNmJwJnAc4FTgcvNrNPMOoFPA6cBJwJvinkBPgZ80t0XAQ8B59VZNhGRhtHj\ncpqvrlGC7v6NxNtbgTfG18uBa9z9CeBXZrYJOCl+tsndfwlgZtcAy83sXuAVwJtjnnXAxcAV9ZRP\nRKSRmvUATwka2Yf1FuDr8fVxwH2Jz7bGtFLpPcDD7n6gKD2Vma00s2EzG96+fXuDii8iIq2s4hmW\nmX0TeGrKR4Pufl3MMwgcAApX0VlKfic9QHqZ/KncfQ2wBsLzsEoWXkREpoyKAcvdX1nuczNbAbwW\nWOpjT4PcChyfyLYAuD++Tkt/EJhtZtPiWVYyv4iISN2jBE8F3ge8zt2TV9RdD5xpZkeZ2QnAIuAH\nwA+BRXFE4AzCwIzrY6C7hbE+sBXAdfWUTSaPbldTP9WhSGX13prpn4GjgJvNDOBWd3+7u99tZl8E\n7iE0Fb7L3Q8CmNlfAjcBncBV7n53nNb7gGvM7CPAHcBn6yybTALdrqZ+qkORbGysFS+f+vv7fXh4\nuNnFaFt9q/sY3TV6RHpvdy+bL9g8+QXKIdWhNIOZ3e7u/c0uRzV0pwupi25XUz/VoUg2ClhSF92u\npn6qw+ZTH2I+KGBJXXS7mvqpDpur0Ic4umsUxw/3ISpotR4FLKmLbldTP9Vhc+mxIfmhQRci0tY6\nLunAU+5TYBiHLjrUhBJNDg26EBHJGfUh5ocCloi0NfUh5ocCloi0NfUh5of6sERE2pD6sERERCaI\nApaIiOSCApaIiOSCApaIiOSCApaIiOSCApaIiOSCApaIiOSCApaIiOSCApaIiOSCApaIiOSCApaI\niOSCApaIiOSCApaIiOSCApaIiOSCApaIiOSCApaIiOSCApaIiOSCApaIiOSCApaIiOSCApbIFDM0\nMkTf6j46Lumgb3UfQyNDzS6SSENMa3YBRKRxhkaGWHnDSvbs3wPA6K5RVt6wEoCBxQPNLJpI3XSG\nJTKFDK4fPBysCvbs38Pg+sEmlUikcRSwRKaQLbu2VJUukicKWCJTyMLuhVWli+SJApbIFLJq6Sq6\npneNS+ua3sWqpauaVCKRxlHAEplCBhYPsOb0NfR292IYvd29rDl9jQZcyJRg7t7sMtSlv7/fh4eH\nm10MEZFcMbPb3b2/2eWoRkPOsMzsPWbmZjYvvjczu8zMNpnZT8zsxYm8K8xsY/xbkUh/iZmNxO9c\nZmbWiLKJiMjUUHfAMrPjgVcByWFIpwGL4t9K4IqYdy5wEfBS4CTgIjObE79zRcxb+N6p9ZZNRESm\njkacYX0SuBBIti0uB6724FZgtpk9DXg1cLO773T3h4CbgVPjZ8e6+/c9tFFeDZzRgLKJiMgUUVfA\nMrPXAb929zuLPjoOuC/xfmtMK5e+NSW91HxXmtmwmQ1v3769jiUQEZG8qHhrJjP7JvDUlI8GgQ8C\np6R9LSXNa0hP5e5rgDUQBl2UyiciIlNHxYDl7q9MSzezxcAJwJ1xfMQC4EdmdhLhDOn4RPYFwP0x\nfUlR+oaYviAlv4iICFBHk6C7j7j7k929z937CEHnxe7+G+B64Jw4WvBkYJe7bwNuAk4xszlxsMUp\nwE3xs91mdnIcHXgOcF2dyyYiIlPIRN2t/UZgGbAJ2AP8OYC77zSzDwM/jPkudfed8fU7gLXATODr\n8U9ERATQhcMiIm2pbS8cFhERmWgKWCIikgsKWCItTo+8FwkmatCFiDSAHnkvMkZnWCItTI+8Fxmj\ngCXSwvTIe5ExClgiLUyPvBcZo4Al0sL0yHuRMQpYIi1Mj7wXGaM7XYiItCHd6UJERGSCKGCJiEgu\nKGCJiEguKGCJiEguKGCJiEgu5H6UoJltB0Zr/Po84MEGFievVA+qgwLVQ/vUQa+7z292IaqR+4BV\nDzMbztuwzomgelAdFKgeVAetTE2CIiKSCwpYIiKSC+0esNY0uwAtQvWgOihQPagOWlZb92GJiEh+\ntPsZloiI5IQCloiI5EJbBiwzO9XMfmZmm8zs/c0uz0Qys6vM7AEzuyuRNtfMbjazjfH/nJhuZnZZ\nrJefmNmLm1fyxjGz483sFjO718zuNrPzY3q71cPRZvYDM7sz1sMlMf0EM7st1sMXzGxGTD8qvt8U\nP+9rZvkbycw6zewOM/tafN92dZBHbRewzKwT+DRwGnAi8CYzO7G5pZpQa4FTi9LeD6x390XA+vge\nQp0sin8rgSsmqYwT7QDw1+7+HOBk4F1xnbdbPTwBvMLdXwC8EDjVzE4GPgZ8MtbDQ8B5Mf95wEPu\n/kzgkzHfVHE+cG/ifTvWQe60XcACTgI2ufsv3X0fcA2wvMllmjDu/m1gZ1HycmBdfL0OOCORfrUH\ntwKzzexpk1PSiePu29z9R/H1bsKO6jjarx7c3R+Nb6fHPwdeAXw5phfXQ6F+vgwsNTObpOJOGDNb\nALwG+Ex8b7RZHeRVOwas44D7Eu+3xrR28hR33wZhZw48OaZP+bqJTTovAm6jDeshNoX9GHgAuBn4\nBfCwux+IWZLLerge4ue7gJ7JLfGEWA1cCByK73tovzrIpXYMWGlHRxrbH0zpujGzWcB/ABe4+yPl\nsqakTYl6cPeD7v5CYAGhteE5adni/ylXD2b2WuABd789mZySdcrWQZ61Y8DaChyfeL8AuL9JZWmW\n3xaauOL/B2L6lK0bM5tOCFZD7n5tTG67eihw94eBDYQ+vdlmNi1+lFzWw/UQP+/myOblvHkZ8Doz\n20zoDngF4Yyrneogt9oxYP0QWBRHBc0AzgSub3KZJtv1wIr4egVwXSL9nDhK7mRgV6HJLM9in8Nn\ngXvd/ROJj9qtHuab2ez4eibwSkJ/3i3AG2O24noo1M8bgW95zu804O4fcPcF7t5H+O1/y90HaKM6\nyDV3b7s/YBnwc0L7/WCzyzPBy/rvwDZgP+Fo8TxCG/x6YGP8PzfmNcIIyl8AI0B/s8vfoDr4A0Iz\nzk+AH8e/ZW1YD88H7oj1cBfwoZj+DOAHwCbgS8BRMf3o+H5T/PwZzV6GBtfHEuBr7VwHefvTrZlE\nRCQX2rFJUEREckgBS0REckEBS0REckEBS0REckEBS0REckEBS0REckEBS0REcuH/A+3MiiGSQr0Z\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fea1672f748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Jet  2 ***** Accuracy jet 80.5831794994\n",
      "**** Starting Jet  3 *****\n",
      "----- FOLD 0 -----\n",
      "++++ Deg = 2\n",
      "++++ Deg = 3\n",
      "++++ Deg = 4\n",
      "++++ Deg = 5\n",
      "++++ Deg = 6\n",
      "++++ Deg = 7\n",
      "++++ Deg = 8\n",
      "++++ Deg = 9\n",
      "++++ Deg = 10\n",
      "++++ Deg = 11\n",
      "++++ Deg = 12\n",
      "++++ Deg = 13\n",
      "----- FOLD 1 -----\n",
      "++++ Deg = 2\n",
      "++++ Deg = 3\n",
      "++++ Deg = 4\n",
      "++++ Deg = 5\n",
      "++++ Deg = 6\n",
      "++++ Deg = 7\n",
      "++++ Deg = 8\n",
      "++++ Deg = 9\n",
      "++++ Deg = 10\n",
      "++++ Deg = 11\n",
      "++++ Deg = 12\n",
      "++++ Deg = 13\n",
      "----- FOLD 2 -----\n",
      "++++ Deg = 2\n",
      "++++ Deg = 3\n",
      "++++ Deg = 4\n",
      "++++ Deg = 5\n",
      "++++ Deg = 6\n",
      "++++ Deg = 7\n",
      "++++ Deg = 8\n",
      "++++ Deg = 9\n",
      "++++ Deg = 10\n",
      "++++ Deg = 11\n",
      "++++ Deg = 12\n",
      "++++ Deg = 13\n",
      "----- FOLD 3 -----\n",
      "++++ Deg = 2\n",
      "++++ Deg = 3\n",
      "++++ Deg = 4\n",
      "++++ Deg = 5\n",
      "++++ Deg = 6\n",
      "++++ Deg = 7\n",
      "++++ Deg = 8\n",
      "++++ Deg = 9\n",
      "++++ Deg = 10\n",
      "++++ Deg = 11\n",
      "++++ Deg = 12\n",
      "++++ Deg = 13\n",
      "----- FOLD 4 -----\n",
      "++++ Deg = 2\n",
      "++++ Deg = 3\n",
      "++++ Deg = 4\n",
      "++++ Deg = 5\n",
      "++++ Deg = 6\n",
      "++++ Deg = 7\n",
      "++++ Deg = 8\n",
      "++++ Deg = 9\n",
      "++++ Deg = 10\n",
      "++++ Deg = 11\n",
      "++++ Deg = 12\n",
      "++++ Deg = 13\n",
      "12\n",
      "12\n",
      "[ 0.51326715  0.50672383  0.50857401  0.50898014  0.50866426  0.50812274\n",
      "  0.51028881  0.51168773  0.51123646  0.49905235  0.55126354  0.51380866]\n",
      "Best degree =  12\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbgAAAEKCAYAAABgyEDNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fea1a916b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Jet  3 ***** Accuracy jet 84.8853997473\n",
      ">>>>>>>> Accuracy TOTAL  82.1628\n"
     ]
    }
   ],
   "source": [
    "OUT_FOLDER = 'output/'\n",
    "name = 'least_squares.csv'\n",
    "\n",
    "degs=range(2,14)\n",
    "k_fold=5\n",
    "\n",
    "mask_jets_train = split_jets_mask(tx_train)\n",
    "mask_jets_test = split_jets_mask(tx_test)\n",
    "len_mask = len(mask_jets_train)\n",
    "y_preds_train = np.zeros(len(y_train))\n",
    "y_preds_test = np.zeros(tx_test.shape[0])\n",
    "\n",
    "\n",
    "for mask_jet_id in range(len_mask):\n",
    "    print('**** Starting Jet ', mask_jet_id, '*****')\n",
    "    tx_single_jet_train = tx_train[mask_jets_train[mask_jet_id]]\n",
    "    tx_single_jet_test = tx_test[mask_jets_test[mask_jet_id]]\n",
    "    y_single_jet_train = y_train[mask_jets_train[mask_jet_id]]\n",
    "    \n",
    "    best_deg, best_acc = cross_validation_least_square(y_single_jet_train, tx_single_jet_train, k_fold, degs)\n",
    "    print('Best degree = ', best_deg)\n",
    "    \n",
    "    tx_single_jet_train, tx_single_jet_test,len_init_data = prepare_data(tx_single_jet_train,tx_single_jet_test, best_deg)\n",
    "    w, loss = func_least_squares(y=y_single_jet_train, tx=tx_single_jet_train, test_set=tx_single_jet_test, fct='mse')\n",
    "    \n",
    "    y_pred_train = predict_labels(w, tx_single_jet_train)\n",
    "    y_pred_test = predict_labels(w, tx_single_jet_test)\n",
    "    y_preds_train[mask_jets_train[mask_jet_id]] = y_pred_train\n",
    "    y_preds_test[mask_jets_test[mask_jet_id]] = y_pred_test\n",
    "    \n",
    "    right_train = np.sum(y_single_jet_train == y_pred_train)/len(y_single_jet_train)*100\n",
    "    \n",
    "    print('**** Jet ', mask_jet_id, '***** Accuracy jet', right_train)\n",
    "    \n",
    "\n",
    "create_csv_submission(ids_test, y_preds_test, OUT_FOLDER+name)\n",
    "right_train_total = np.sum(y_train == y_preds_train)/len(y_train)*100\n",
    "print('>>>>>>>> Accuracy TOTAL ',right_train_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_preds_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5b7597dc3927>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_preds_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_preds_test\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'y_preds_test' is not defined"
     ]
    }
   ],
   "source": [
    "len(y_preds_test[y_preds_test==-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "246651"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_preds_test[y_preds_test==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_preds_test[y_preds_test==-1])+len(y_preds_test[y_preds_test==1])==tx_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1., -1.,  1.,  1., -1.,  1.,  1., -1., -1., -1., -1.,  1.,  1.,\n",
       "       -1., -1.,  1., -1.,  1., -1., -1., -1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1., -1., -1., -1., -1., -1., -1., -1.,  1.,  1.,\n",
       "       -1., -1., -1.,  1.,  1.,  1.,  1., -1.,  1., -1.,  1.,  1., -1.,\n",
       "        1., -1., -1., -1.,  1., -1.,  1., -1., -1., -1.,  1., -1., -1.,\n",
       "       -1.,  1.,  1.,  1., -1.,  1., -1.,  1.,  1.,  1., -1.,  1.,  1.,\n",
       "        1.,  1., -1., -1., -1.,  1., -1., -1., -1.,  1., -1.,  1.,  1.,\n",
       "       -1., -1., -1., -1., -1., -1.,  1., -1.,  1., -1., -1., -1., -1.,\n",
       "       -1., -1., -1., -1., -1.,  1., -1.,  1., -1., -1.,  1., -1., -1.,\n",
       "       -1.,  1., -1., -1.,  1., -1., -1., -1.,  1., -1., -1., -1., -1.,\n",
       "       -1., -1.,  1.,  1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "       -1.,  1.,  1., -1., -1., -1.,  1.,  1., -1., -1., -1., -1., -1.,\n",
       "       -1., -1., -1., -1., -1.,  1., -1.,  1.,  1.,  1., -1., -1., -1.,\n",
       "       -1., -1., -1., -1., -1., -1.,  1.,  1., -1.,  1., -1., -1.,  1.,\n",
       "       -1., -1., -1., -1., -1., -1., -1., -1., -1.,  1.,  1.,  1., -1.,\n",
       "       -1.,  1.,  1., -1.,  1.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds_test[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cross validation su gamma e degree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def cross_validation_GD(y,tx, k_fold, max_iters, gammas):\n",
    "    \n",
    "    accuracies_test=np.zeros(len(gammas))\n",
    "    accuracies_train=np.zeros(len(gammas))\n",
    "    acc_max=0;\n",
    "    gamma_best=0;\n",
    "    \n",
    "    for i, single_gamma in enumerate(gammas):\n",
    "        \n",
    "        seed=1;\n",
    "\n",
    "        # get k'th subgroup in test, others in train\n",
    "        k_indices = build_k_indices(y, k_fold, seed)\n",
    "        accuracy_train = np.zeros(k_fold)\n",
    "        accuracy_test = np.zeros(k_fold)\n",
    "\n",
    "\n",
    "        for k in range(k_fold):\n",
    "            #print('----- FOLD', k, '-----')\n",
    "            k_index = k_indices[k]\n",
    "            test_y = y[k_index]\n",
    "            test_tx = tx[k_index,:]\n",
    "            mask = np.ones(len(y), dtype=bool) # set all elements to True\n",
    "            mask[k_index] = False              # set test elements to False\n",
    "            train_tx = tx[mask,:]              # select only True elements (ie train elements)\n",
    "            train_y = y[mask]\n",
    "            initial_w = np.zeros(train_tx.shape[1]);\n",
    "            weights,loss = logistic_regression(train_y, train_tx, initial_w, max_iters, single_gamma)\n",
    "            # Compute the predictions\n",
    "            y_pred_train = predict_labels(weights, train_tx)\n",
    "            y_pred_test = predict_labels(weights, test_tx);\n",
    "            accuracy_train[k] = np.sum(y_pred_train == train_y)/len(train_y)\n",
    "            accuracy_test[k] = np.sum(y_pred_test == test_y)/len(test_y)\n",
    "            \n",
    "        accuracies_test[i]=np.mean(accuracy_test);\n",
    "        accuracies_train[i]=np.mean(accuracy_train);\n",
    "        \n",
    "        print('GAMMA:', single_gamma, '---','ACCURANCY TEST:',accuracies_test[i], '---','ACCURANCY TRAIN:',accuracies_train[i])\n",
    "        if (accuracies_test[i]>acc_max):\n",
    "                gamma_best=gammas[i];\n",
    "                acc_max=accuracies_test[i];\n",
    "    \n",
    "    return [gamma_best,acc_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Starting Jet  0 *****\n",
      "Gradient Descent(0/499): loss=0.49999999999999784, w0=4.318881558209673e-06, w1=-0.00360114905465818\n",
      "Gradient Descent(1/499): loss=0.47984092459190764, w0=5.3194282256529834e-05, w1=-0.006710262173664215\n",
      "Gradient Descent(2/499): loss=0.4645713617677788, w0=0.00011887164466863113, w1=-0.009420834375421301\n",
      "Gradient Descent(3/499): loss=0.452604307467966, w0=0.00018513724341014546, w1=-0.011805594078871936\n",
      "Gradient Descent(4/499): loss=0.44292196291615404, w0=0.000242991769469354, w1=-0.013921787263083287\n",
      "Gradient Descent(5/499): loss=0.4348546694989052, w0=0.00028790284728711046, w1=-0.01581494114974068\n",
      "Gradient Descent(6/499): loss=0.4279533363483794, w0=0.0003180631955061308, w1=-0.0175215886442407\n",
      "Gradient Descent(7/499): loss=0.42191219070426056, w0=0.00033328939320437337, w1=-0.019071272564084807\n",
      "Gradient Descent(8/499): loss=0.41652026564905853, w0=0.00033432870998928264, w1=-0.020488043214210185\n",
      "Gradient Descent(9/499): loss=0.41163013287140016, w0=0.00032242578969605984, w1=-0.02179159388389409\n",
      "Gradient Descent(10/499): loss=0.40713738373289426, w0=0.00029905467696869317, w1=-0.02299813339317375\n",
      "Gradient Descent(11/499): loss=0.40296699353656096, w0=0.0002657558883035946, w1=-0.02412106461903506\n",
      "Gradient Descent(12/499): loss=0.3990641768564748, w0=0.00022404003938682012, w1=-0.02517151766331015\n",
      "Gradient Descent(13/499): loss=0.3953882106996055, w0=0.00017533345427780635, w1=-0.02615877256180632\n",
      "Gradient Descent(14/499): loss=0.39190823639830996, w0=0.00012095006513908331, w1=-0.02709059696669872\n",
      "Gradient Descent(15/499): loss=0.38860038942768416, w0=6.207958747630985e-05, w1=-0.027973517627508257\n",
      "Gradient Descent(16/499): loss=0.38544582506699665, w0=-2.1441377742422493e-07, w1=-0.028813039814778462\n",
      "Gradient Descent(17/499): loss=0.38242935121141464, w0=-6.499062802956367e-05, w1=-0.029613825460918668\n",
      "Gradient Descent(18/499): loss=0.3795384745147702, w0=-0.0001314228444457462, w1=-0.030379838328406187\n",
      "Gradient Descent(19/499): loss=0.37676272922116466, w0=-0.00019879115634205316, w1=-0.031114462685385473\n",
      "Gradient Descent(20/499): loss=0.3740932003079464, w0=-0.00026647252988278795, w1=-0.03182060058955996\n",
      "Gradient Descent(21/499): loss=0.37152218094025813, w0=-0.00033393135021320987, w1=-0.032500751828007136\n",
      "Gradient Descent(22/499): loss=0.3690429233507083, w0=-0.00040071030722896176, w1=-0.033157079746469385\n",
      "Gradient Descent(23/499): loss=0.3666494551694276, w0=-0.0004664218232019075, w1=-0.033791465565833706\n",
      "Gradient Descent(24/499): loss=0.3643364419781439, w0=-0.0005307401231302073, w1=-0.03440555328239556\n",
      "Gradient Descent(25/499): loss=0.36209908280821745, w0=-0.0005933939851676169, w1=-0.035000786850541234\n",
      "Gradient Descent(26/499): loss=0.3599330293582301, w0=-0.00065416016940691, w1=-0.03557844102842575\n",
      "Gradient Descent(27/499): loss=0.3578343224834033, w0=-0.0007128574999796127, w1=-0.0361396470116824\n",
      "Gradient Descent(28/499): loss=0.3557993414181879, w0=-0.0007693415622736544, w1=-0.036685413773993654\n",
      "Gradient Descent(29/499): loss=0.3538247625120923, w0=-0.0008234999704141325, w1=-0.037216645866360296\n",
      "Gradient Descent(30/499): loss=0.35190752517451485, w0=-0.0008752481576247434, w1=-0.03773415829127094\n",
      "Gradient Descent(31/499): loss=0.35004480336386573, w0=-0.0009245256421388724, w1=-0.038238688957539575\n",
      "Gradient Descent(32/499): loss=0.34823398140575135, w0=-0.0009712927229479899, w1=-0.0387309091314789\n",
      "Gradient Descent(33/499): loss=0.3464726332431386, w0=-0.0010155275622004663, w1=-0.03921143222644105\n",
      "Gradient Descent(34/499): loss=0.34475850444831135, w0=-0.0010572236140666083, w1=-0.03968082121248685\n",
      "Gradient Descent(35/499): loss=0.3430894964896054, w0=-0.00109638736308792, w1=-0.04013959487854986\n",
      "Gradient Descent(36/499): loss=0.34146365286428326, w0=-0.0011330363382531664, w1=-0.04058823313893288\n",
      "Gradient Descent(37/499): loss=0.33987914679564685, w0=-0.0011671973721801854, w1=-0.0410271815426851\n",
      "Gradient Descent(38/499): loss=0.3383342702566391, w0=-0.0011989050777642268, w1=-0.04145685511703735\n",
      "Gradient Descent(39/499): loss=0.3368274241301841, w0=-0.0012282005174438513, w1=-0.04187764165354774\n",
      "Gradient Descent(40/499): loss=0.33535710935280727, w0=-0.0012551300428165146, w1=-0.042289904527054646\n",
      "Gradient Descent(41/499): loss=0.3339219189158373, w0=-0.001279744284702902, w1=-0.04269398512223553\n",
      "Gradient Descent(42/499): loss=0.33252053061998127, w0=-0.001302097275915164, w1=-0.04309020492994461\n",
      "Gradient Descent(43/499): loss=0.3311517004958944, w0=-0.0013222456909377587, w1=-0.0434788673650738\n",
      "Gradient Descent(44/499): loss=0.3298142568167149, w0=-0.001340248188492087, w1=-0.04386025934905895\n",
      "Gradient Descent(45/499): loss=0.32850709463922445, w0=-0.001356164844540571, w1=-0.04423465269301679\n",
      "Gradient Descent(46/499): loss=0.32722917081898983, w0=-0.0013700566647060145, w1=-0.04460230531158681\n",
      "Gradient Descent(47/499): loss=0.32597949945196203, w0=-0.0013819851663518941, w1=-0.04496346229264871\n",
      "Gradient Descent(48/499): loss=0.32475714770091757, w0=-0.0013920120217020702, w1=-0.045318356844016326\n",
      "Gradient Descent(49/499): loss=0.32356123197008335, w0=-0.00140019875438724, w1=-0.04566721113482548\n",
      "Gradient Descent(50/499): loss=0.3223909143954705, w0=-0.0014066064827023407, w1=-0.04601023704651822\n",
      "Gradient Descent(51/499): loss=0.3212453996220214, w0=-0.0014112957036553563, w1=-0.04634763684598059\n",
      "Gradient Descent(52/499): loss=0.32012393184174937, w0=-0.0014143261125939776, w1=-0.0466796037914346\n",
      "Gradient Descent(53/499): loss=0.31902579206971005, w0=-0.0014157564538218226, w1=-0.04700632268005092\n",
      "Gradient Descent(54/499): loss=0.31795029563697896, w0=-0.0014156443981691268, w1=-0.047327970344882\n",
      "Gradient Descent(55/499): loss=0.3168967898818458, w0=-0.0014140464439717996, w1=-0.04764471610757038\n",
      "Gradient Descent(56/499): loss=0.31586465202223396, w0=-0.001411017838344628, w1=-0.04795672219232689\n",
      "Gradient Descent(57/499): loss=0.31485328719396516, w0=-0.0014066125160155463, w1=-0.0482641441058664\n",
      "Gradient Descent(58/499): loss=0.31386212664089297, w0=-0.0014008830533240365, w1=-0.04856713098731024\n",
      "Gradient Descent(59/499): loss=0.3128906260442187, w0=-0.0013938806352829817, w1=-0.04886582593149222\n",
      "Gradient Descent(60/499): loss=0.3119382639794307, w0=-0.001385655033864242, w1=-0.049160366288622345\n",
      "Gradient Descent(61/499): loss=0.3110045404903401, w0=-0.001376254595897958, w1=-0.04945088394285401\n",
      "Gradient Descent(62/499): loss=0.310088975770609, w0=-0.0013657262391777193, w1=-0.04973750557195443\n",
      "Gradient Descent(63/499): loss=0.3091911089440016, w0=-0.0013541154555415011, w1=-0.050020352889984666\n",
      "Gradient Descent(64/499): loss=0.30831049693534657, w0=-0.0013414663198545395, w1=-0.05029954287464593\n",
      "Gradient Descent(65/499): loss=0.3074467134248826, w0=-0.001327821503957614, w1=-0.050575187980736194\n",
      "Gradient Descent(66/499): loss=0.3065993478792849, w0=-0.0013132222947647937, w1=-0.0508473963409798\n",
      "Gradient Descent(67/499): loss=0.30576800465323384, w0=-0.0012977086158005543, w1=-0.05111627195533738\n",
      "Gradient Descent(68/499): loss=0.3049523021559035, w0=-0.0012813190515590656, w1=-0.051381914869770416\n",
      "Gradient Descent(69/499): loss=0.30415187207721905, w0=-0.0012640908741499222, w1=-0.05164442134532061\n",
      "Gradient Descent(70/499): loss=0.3033663586691582, w0=-0.0012460600717660144, w1=-0.05190388401826568\n",
      "Gradient Descent(71/499): loss=0.3025954180777627, w0=-0.0012272613785718305, w1=-0.052160392052028506\n",
      "Gradient Descent(72/499): loss=0.3018387177218892, w0=-0.0012077283056653118, w1=-0.052414031281443035\n",
      "Gradient Descent(73/499): loss=0.30109593571504445, w0=-0.0011874931728143735, w1=-0.05266488434991688\n",
      "Gradient Descent(74/499): loss=0.30036676032696047, w0=-0.0011665871407112016, w1=-0.05291303083997513\n",
      "Gradient Descent(75/499): loss=0.29965088948182794, w0=-0.001145040243524167, w1=-0.05315854739762189\n",
      "Gradient Descent(76/499): loss=0.2989480302903659, w0=-0.0011228814215592917, w1=-0.05340150785091388\n",
      "Gradient Descent(77/499): loss=0.29825789861312546, w0=-0.001100138553871237, w1=-0.053641983323103805\n",
      "Gradient Descent(78/499): loss=0.29758021865264406, w0=-0.0010768384906882451, w1=-0.05388004234067859\n",
      "Gradient Descent(79/499): loss=0.2969147225722525, w0=-0.0010530070855368074, w1=-0.05411575093658951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(80/499): loss=0.2962611501395158, w0=-0.001028669226970411, w1=-0.054349172748945754\n",
      "Gradient Descent(81/499): loss=0.29561924839245224, w0=-0.001003848869822909, w1=-0.054580369115421\n",
      "Gradient Descent(82/499): loss=0.2949887713268207, w0=-0.000978569065921109, w1=-0.05480939916360275\n",
      "Gradient Descent(83/499): loss=0.29436947960290394, w0=-0.0009528519942034092, w1=-0.0550363198974965\n",
      "Gradient Descent(84/499): loss=0.29376114027033845, w0=-0.000926718990201887, w1=-0.05526118628038136\n",
      "Gradient Descent(85/499): loss=0.2931635265096622, w0=-0.0009001905748544322, w1=-0.05548405131419941\n",
      "Gradient Descent(86/499): loss=0.2925764173893466, w0=-0.0008732864826214343, w1=-0.05570496611564868\n",
      "Gradient Descent(87/499): loss=0.29199959763718736, w0=-0.0008460256888883737, w1=-0.05592397998913814\n",
      "Gradient Descent(88/499): loss=0.2914328574250079, w0=-0.0008184264366415564, w1=-0.05614114049675281\n",
      "Gradient Descent(89/499): loss=0.29087599216571736, w0=-0.0007905062624092724, w1=-0.05635649352536787\n",
      "Gradient Descent(90/499): loss=0.2903288023218375, w0=-0.0007622820214649892, w1=-0.05657008335104199\n",
      "Gradient Descent(91/499): loss=0.28979109322468, w0=-0.0007337699122928708, w1=-0.056781952700812675\n",
      "Gradient Descent(92/499): loss=0.28926267490342555, w0=-0.0007049855003190451, w1=-0.056992142812009004\n",
      "Gradient Descent(93/499): loss=0.2887433619234035, w0=-0.0006759437409146823, w1=-0.057200693489190886\n",
      "Gradient Descent(94/499): loss=0.28823297323293895, w0=-0.0006466590016791697, w1=-0.057407643158817995\n",
      "Gradient Descent(95/499): loss=0.28773133201816814, w0=-0.0006171450840135227, w1=-0.05761302892174593\n",
      "Gradient Descent(96/499): loss=0.28723826556528065, w0=-0.0005874152439956953, w1=-0.057816886603642155\n",
      "Gradient Descent(97/499): loss=0.2867536051296824, w0=-0.0005574822125707155, w1=-0.05801925080340953\n",
      "Gradient Descent(98/499): loss=0.28627718581161155, w0=-0.0005273582150695738, w1=-0.058220154939700924\n",
      "Gradient Descent(99/499): loss=0.28580884643778115, w0=-0.0004970549900715961, w1=-0.05841963129560421\n",
      "Gradient Descent(100/499): loss=0.2853484294486437, w0=-0.0004665838076256501, w1=-0.058617711061573236\n",
      "Gradient Descent(101/499): loss=0.2848957807909164, w0=-0.0004359554868459999, w1=-0.05881442437667683\n",
      "Gradient Descent(102/499): loss=0.2844507498150214, w0=-0.0004051804128989418, w1=-0.05900980036823442\n",
      "Gradient Descent(103/499): loss=0.2840131891771285, w0=-0.00037426855339657144, w1=-0.05920386718990383\n",
      "Gradient Descent(104/499): loss=0.28358295474550804, w0=-0.00034322947421413824, w1=-0.05939665205828388\n",
      "Gradient Descent(105/499): loss=0.283159905510923, w0=-0.0003120723547474691, w1=-0.05958818128809156\n",
      "Gradient Descent(106/499): loss=0.282743903500813, w0=-0.000280806002626893, w1=-0.05977848032597097\n",
      "Gradient Descent(107/499): loss=0.28233481369703406, w0=-0.0002494388679039914, w1=-0.059967573782988684\n",
      "Gradient Descent(108/499): loss=0.2819325039569434, w0=-0.00021797905672733326, w1=-0.060155485465868\n",
      "Gradient Descent(109/499): loss=0.2815368449376283, w0=-0.00018643434452314913, w1=-0.06034223840701204\n",
      "Gradient Descent(110/499): loss=0.28114771002309275, w0=-0.00015481218869665985, w1=-0.06052785489336392\n",
      "Gradient Descent(111/499): loss=0.28076497525423283, w0=-0.00012311974086949505, w1=-0.06071235649414975\n",
      "Gradient Descent(112/499): loss=0.28038851926144054, w0=-9.136385866834984e-05, w1=-0.06089576408754883\n",
      "Gradient Descent(113/499): loss=0.28001822319968767, w0=-5.955111707970429e-05, w1=-0.06107809788633303\n",
      "Gradient Descent(114/499): loss=0.27965397068595205, w0=-2.768781938510503e-05, w1=-0.061259377462516196\n",
      "Gradient Descent(115/499): loss=0.27929564773885895, w0=4.219992308842438e-06, w1=-0.061439621771052166\n",
      "Gradient Descent(116/499): loss=0.27894314272041787, w0=3.6166526931965765e-05, w1=-0.061618849172619\n",
      "Gradient Descent(117/499): loss=0.27859634627974506, w0=6.814623469005897e-05, w1=-0.061797077455525\n",
      "Gradient Descent(118/499): loss=0.27825515129866696, w0=0.00010015379766937721, w1=-0.06197432385677101\n",
      "Gradient Descent(119/499): loss=0.27791945283910907, w0=0.00013218412079337095, w1=-0.06215060508230199\n",
      "Gradient Descent(120/499): loss=0.2775891480921812, w0=0.00016423232311930453, w1=-0.0623259373264794\n",
      "Gradient Descent(121/499): loss=0.27726413632887426, w0=0.00019629372946276222, w1=-0.06250033629080502\n",
      "Gradient Descent(122/499): loss=0.2769443188522911, w0=0.00022836386233840417, w1=-0.06267381720192522\n",
      "Gradient Descent(123/499): loss=0.2766295989513388, w0=0.0002604384342056925, w1=-0.06284639482894382\n",
      "Gradient Descent(124/499): loss=0.27631988185581224, w0=0.00029251334000864916, w1=-0.06301808350007045\n",
      "Gradient Descent(125/499): loss=0.2760150746928074, w0=0.0003245846499990613, w1=-0.06318889711863032\n",
      "Gradient Descent(126/499): loss=0.27571508644440346, w0=0.00035664860283287763, w1=-0.06335884917846024\n",
      "Gradient Descent(127/499): loss=0.27541982790655706, w0=0.0003887015989298825, w1=-0.06352795277871466\n",
      "Gradient Descent(128/499): loss=0.2751292116491579, w0=0.0004207401940870478, w1=-0.06369622063810493\n",
      "Gradient Descent(129/499): loss=0.27484315197719394, w0=0.00045276109333628503, w1=-0.06386366510859354\n",
      "Gradient Descent(130/499): loss=0.2745615648929831, w0=0.00048476114503763544, w1=-0.06403029818856473\n",
      "Gradient Descent(131/499): loss=0.2742843680594239, w0=0.0005167373351992223, w1=-0.0641961315354917\n",
      "Gradient Descent(132/499): loss=0.2740114807642287, w0=0.0005486867820156048, w1=-0.06436117647811997\n",
      "Gradient Descent(133/499): loss=0.27374282388509635, w0=0.0005806067306164405, w1=-0.06452544402818586\n",
      "Gradient Descent(134/499): loss=0.2734783198557925, w0=0.0006124945480176548, w1=-0.06468894489168785\n",
      "Gradient Descent(135/499): loss=0.27321789263309854, w0=0.0006443477182675796, w1=-0.06485168947972844\n",
      "Gradient Descent(136/499): loss=0.272961467664602, w0=0.000676163837780789, w1=-0.06501368791894306\n",
      "Gradient Descent(137/499): loss=0.2727089718572936, w0=0.0007079406108526183, w1=-0.06517495006153204\n",
      "Gradient Descent(138/499): loss=0.2724603335469439, w0=0.0007396758453475905, w1=-0.06533548549491111\n",
      "Gradient Descent(139/499): loss=0.272215482468233, w0=0.000771367448555225, w1=-0.06549530355099524\n",
      "Gradient Descent(140/499): loss=0.27197434972560547, w0=0.0008030134232069317, w1=-0.06565441331512989\n",
      "Gradient Descent(141/499): loss=0.2717368677648284, w0=0.0008346118636479149, w1=-0.06581282363468356\n",
      "Gradient Descent(142/499): loss=0.27150297034522697, w0=0.0008661609521582372, w1=-0.06597054312731462\n",
      "Gradient Descent(143/499): loss=0.27127259251257596, w0=0.0008976589554173976, w1=-0.06612758018892513\n",
      "Gradient Descent(144/499): loss=0.27104567057262896, w0=0.000929104221106987, w1=-0.06628394300131368\n",
      "Gradient Descent(145/499): loss=0.270822142065261, w0=0.0009604951746461781, w1=-0.06643963953953913\n",
      "Gradient Descent(146/499): loss=0.27060194573920804, w0=0.0009918303160549985, w1=-0.06659467757900613\n",
      "Gradient Descent(147/499): loss=0.2703850215273859, w0=0.0010231082169405254, w1=-0.06674906470228349\n",
      "Gradient Descent(148/499): loss=0.2701713105227687, w0=0.0010543275176013072, w1=-0.0669028083056657\n",
      "Gradient Descent(149/499): loss=0.2699607549548134, w0=0.001085486924245501, w1=-0.06705591560548727\n",
      "Gradient Descent(150/499): loss=0.26975329816641186, w0=0.0011165852063183752, w1=-0.0672083936441999\n",
      "Gradient Descent(151/499): loss=0.26954888459135706, w0=0.0011476211939349882, w1=-0.06736024929622132\n",
      "Gradient Descent(152/499): loss=0.2693474597323092, w0=0.0011785937754140079, w1=-0.06751148927356476\n",
      "Gradient Descent(153/499): loss=0.2691489701392457, w0=0.001209501894908789, w1=-0.06766212013125755\n",
      "Gradient Descent(154/499): loss=0.26895336338838494, w0=0.0012403445501319688, w1=-0.067812148272557\n",
      "Gradient Descent(155/499): loss=0.2687605880615681, w0=0.0012711207901699771, w1=-0.06796157995397131\n",
      "Gradient Descent(156/499): loss=0.2685705937260894, w0=0.001301829713383999, w1=-0.0681104212900933\n",
      "Gradient Descent(157/499): loss=0.26838333091496, w0=0.0013324704653940476, w1=-0.0682586782582538\n",
      "Gradient Descent(158/499): loss=0.26819875110759844, w0=0.0013630422371429403, w1=-0.06840635670300199\n",
      "Gradient Descent(159/499): loss=0.26801680671093153, w0=0.001393544263037084, w1=-0.06855346234041935\n",
      "Gradient Descent(160/499): loss=0.26783745104090034, w0=0.0014239758191610934, w1=-0.0687000007622734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(161/499): loss=0.26766063830435716, w0=0.001454336221563384, w1=-0.06884597744001768\n",
      "Gradient Descent(162/499): loss=0.26748632358134644, w0=0.0014846248246099777, w1=-0.06899139772864381\n",
      "Gradient Descent(163/499): loss=0.26731446280775817, w0=0.0015148410194038757, w1=-0.06913626687039127\n",
      "Gradient Descent(164/499): loss=0.2671450127583464, w0=0.0015449842322674445, w1=-0.06928058999832049\n",
      "Gradient Descent(165/499): loss=0.26697793103010325, w0=0.0015750539232853632, w1=-0.06942437213975439\n",
      "Gradient Descent(166/499): loss=0.26681317602597954, w0=0.0016050495849057663, w1=-0.06956761821959362\n",
      "Gradient Descent(167/499): loss=0.26665070693894444, w0=0.0016349707405973188, w1=-0.06971033306351018\n",
      "Gradient Descent(168/499): loss=0.2664904837363764, w0=0.0016648169435600298, w1=-0.0698525214010242\n",
      "Gradient Descent(169/499): loss=0.26633246714477676, w0=0.0016945877754877088, w1=-0.06999418786846837\n",
      "Gradient Descent(170/499): loss=0.2661766186347989, w0=0.0017242828453800402, w1=-0.07013533701184427\n",
      "Gradient Descent(171/499): loss=0.26602290040658655, w0=0.001753901788402334, w1=-0.07027597328957484\n",
      "Gradient Descent(172/499): loss=0.2658712753754129, w0=0.0017834442647910805, w1=-0.07041610107515694\n",
      "Gradient Descent(173/499): loss=0.26572170715761445, w0=0.0018129099588035107, w1=-0.07055572465971784\n",
      "Gradient Descent(174/499): loss=0.265574160056813, w0=0.0018422985777094359, w1=-0.07069484825447925\n",
      "Gradient Descent(175/499): loss=0.2654285990504186, w0=0.0018716098508236992, w1=-0.07083347599313262\n",
      "Gradient Descent(176/499): loss=0.265284989776408, w0=0.0019008435285776432, w1=-0.0709716119341289\n",
      "Gradient Descent(177/499): loss=0.2651432985203729, w0=0.0019299993816280555, w1=-0.07110926006288613\n",
      "Gradient Descent(178/499): loss=0.2650034922028298, w0=0.0019590772000021104, w1=-0.07124642429391806\n",
      "Gradient Descent(179/499): loss=0.2648655383667895, w0=0.0019880767922768917, w1=-0.07138310847288669\n",
      "Gradient Descent(180/499): loss=0.2647294051655758, w0=0.0020169979847921233, w1=-0.07151931637858175\n",
      "Gradient Descent(181/499): loss=0.2645950613508936, w0=0.002045840620894799, w1=-0.07165505172482982\n",
      "Gradient Descent(182/499): loss=0.2644624762611348, w0=0.0020746045602144454, w1=-0.07179031816233584\n",
      "Gradient Descent(183/499): loss=0.2643316198099231, w0=0.002103289677967807, w1=-0.07192511928045957\n",
      "Gradient Descent(184/499): loss=0.2642024624748872, w0=0.0021318958642917805, w1=-0.07205945860892939\n",
      "Gradient Descent(185/499): loss=0.2640749752866614, w0=0.0021604230236034853, w1=-0.072193339619496\n",
      "Gradient Descent(186/499): loss=0.26394912981810664, w0=0.002188871073986385, w1=-0.07232676572752819\n",
      "Gradient Descent(187/499): loss=0.2638248981737481, w0=0.002217239946601424, w1=-0.07245974029355277\n",
      "Gradient Descent(188/499): loss=0.26370225297942396, w0=0.0022455295851221916, w1=-0.07259226662474105\n",
      "Gradient Descent(189/499): loss=0.2635811673721433, w0=0.002273739945193143, w1=-0.07272434797634367\n",
      "Gradient Descent(190/499): loss=0.2634616149901446, w0=0.0023018709939099688, w1=-0.0728559875530758\n",
      "Gradient Descent(191/499): loss=0.2633435699631549, w0=0.002329922709321225, w1=-0.07298718851045466\n",
      "Gradient Descent(192/499): loss=0.2632270069028427, w0=0.002357895079950372, w1=-0.07311795395609101\n",
      "Gradient Descent(193/499): loss=0.2631119008934612, w0=0.0023857881043374083, w1=-0.07324828695093648\n",
      "Gradient Descent(194/499): loss=0.26299822748267865, w0=0.0024136017905993182, w1=-0.07337819051048838\n",
      "Gradient Descent(195/499): loss=0.2628859626725917, w0=0.0024413361560085708, w1=-0.07350766760595352\n",
      "Gradient Descent(196/499): loss=0.26277508291091634, w0=0.002468991226588957, w1=-0.07363672116537263\n",
      "Gradient Descent(197/499): loss=0.26266556508235533, w0=0.002496567036728062, w1=-0.07376535407470691\n",
      "Gradient Descent(198/499): loss=0.26255738650013705, w0=0.0025240636288057037, w1=-0.07389356917888801\n",
      "Gradient Descent(199/499): loss=0.2624505248977219, w0=0.002551481052837705, w1=-0.0740213692828329\n",
      "Gradient Descent(200/499): loss=0.2623449584206735, w0=0.0025788193661343685, w1=-0.07414875715242497\n",
      "Gradient Descent(201/499): loss=0.262240665618692, w0=0.002606078632973074, w1=-0.07427573551546247\n",
      "Gradient Descent(202/499): loss=0.26213762543780406, w0=0.002633258924284423, w1=-0.0744023070625757\n",
      "Gradient Descent(203/499): loss=0.2620358172127087, w0=0.0026603603173513857, w1=-0.07452847444811392\n",
      "Gradient Descent(204/499): loss=0.2619352206592733, w0=0.0026873828955209216, w1=-0.07465424029100334\n",
      "Gradient Descent(205/499): loss=0.2618358158671802, w0=0.0027143267479275775, w1=-0.07477960717557704\n",
      "Gradient Descent(206/499): loss=0.2617375832927169, w0=0.002741191969228568, w1=-0.0749045776523779\n",
      "Gradient Descent(207/499): loss=0.26164050375170916, w0=0.002767978659349883, w1=-0.07502915423893575\n",
      "Gradient Descent(208/499): loss=0.26154455841259505, w0=0.0027946869232429654, w1=-0.07515333942051933\n",
      "Gradient Descent(209/499): loss=0.2614497287896338, w0=0.002821316870651543, w1=-0.07527713565086436\n",
      "Gradient Descent(210/499): loss=0.26135599673625004, w0=0.002847868615888187, w1=-0.07540054535287834\n",
      "Gradient Descent(211/499): loss=0.26126334443850885, w0=0.002874342277620214, w1=-0.075523570919323\n",
      "Gradient Descent(212/499): loss=0.2611717544087197, w0=0.002900737978664545, w1=-0.07564621471347535\n",
      "Gradient Descent(213/499): loss=0.2610812094791666, w0=0.002927055845791157, w1=-0.07576847906976786\n",
      "Gradient Descent(214/499): loss=0.2609916927959604, w0=0.0029532960095347803, w1=-0.07589036629440876\n",
      "Gradient Descent(215/499): loss=0.2609031878130149, w0=0.0029794586040144994, w1=-0.07601187866598313\n",
      "Gradient Descent(216/499): loss=0.2608156782861383, w0=0.0030055437667609406, w1=-0.07613301843603536\n",
      "Gradient Descent(217/499): loss=0.26072914826724375, w0=0.0030315516385507313, w1=-0.0762537878296338\n",
      "Gradient Descent(218/499): loss=0.2606435820986712, w0=0.0030574823632479363, w1=-0.0763741890459182\n",
      "Gradient Descent(219/499): loss=0.2605589644076236, w0=0.003083336087652183, w1=-0.07649422425863048\n",
      "Gradient Descent(220/499): loss=0.2604752801007095, w0=0.003109112961353202, w1=-0.07661389561662961\n",
      "Gradient Descent(221/499): loss=0.26039251435859584, w0=0.003134813136591522, w1=-0.07673320524439098\n",
      "Gradient Descent(222/499): loss=0.26031065263076225, w0=0.00316043676812506, w1=-0.0768521552424909\n",
      "Gradient Descent(223/499): loss=0.26022968063036206, w0=0.0031859840131013713, w1=-0.0769707476880768\n",
      "Gradient Descent(224/499): loss=0.2601495843291816, w0=0.0032114550309353227, w1=-0.0770889846353236\n",
      "Gradient Descent(225/499): loss=0.26007034995269884, w0=0.0032368499831919636, w1=-0.07720686811587672\n",
      "Gradient Descent(226/499): loss=0.25999196397523905, w0=0.0032621690334743826, w1=-0.07732440013928223\n",
      "Gradient Descent(227/499): loss=0.2599144131152261, w0=0.0032874123473163445, w1=-0.07744158269340459\n",
      "Gradient Descent(228/499): loss=0.2598376843305255, w0=0.003312580092079509, w1=-0.07755841774483248\n",
      "Gradient Descent(229/499): loss=0.2597617648138796, w0=0.003337672436855038, w1=-0.07767490723927296\n",
      "Gradient Descent(230/499): loss=0.25968664198843133, w0=0.0033626895523694166, w1=-0.07779105310193463\n",
      "Gradient Descent(231/499): loss=0.2596123035033368, w0=0.003387631610894305, w1=-0.0779068572378999\n",
      "Gradient Descent(232/499): loss=0.25953873722946186, w0=0.003412498786160261, w1=-0.07802232153248705\n",
      "Gradient Descent(233/499): loss=0.25946593125516487, w0=0.0034372912532741654, w1=-0.07813744785160204\n",
      "Gradient Descent(234/499): loss=0.25939387388215995, w0=0.0034620091886402023, w1=-0.0782522380420809\n",
      "Gradient Descent(235/499): loss=0.2593225536214629, w0=0.0034866527698842417, w1=-0.07836669393202261\n",
      "Gradient Descent(236/499): loss=0.25925195918941424, w0=0.0035112221757814827, w1=-0.07848081733111308\n",
      "Gradient Descent(237/499): loss=0.2591820795037821, w0=0.003535717586187224, w1=-0.07859461003094027\n",
      "Gradient Descent(238/499): loss=0.25911290367993967, w0=0.0035601391819706277, w1=-0.07870807380530115\n",
      "Gradient Descent(239/499): loss=0.25904442102711767, w0=0.0035844871449513516, w1=-0.07882121041050025\n",
      "Gradient Descent(240/499): loss=0.2589766210447298, w0=0.003608761657838932, w1=-0.07893402158564061\n",
      "Gradient Descent(241/499): loss=0.25890949341877006, w0=0.003632962904174797, w1=-0.07904650905290704\n",
      "Gradient Descent(242/499): loss=0.2588430280182805, w0=0.0036570910682768043, w1=-0.079158674517842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(243/499): loss=0.2587772148918869, w0=0.0036811463351861936, w1=-0.07927051966961458\n",
      "Gradient Descent(244/499): loss=0.258712044264403, w0=0.0037051288906168552, w1=-0.07938204618128243\n",
      "Gradient Descent(245/499): loss=0.2586475065334996, w0=0.003729038920906813, w1=-0.07949325571004723\n",
      "Gradient Descent(246/499): loss=0.25858359226644057, w0=0.003752876612971831, w1=-0.07960414989750372\n",
      "Gradient Descent(247/499): loss=0.25852029219687933, w0=0.0037766421542610533, w1=-0.07971473036988251\n",
      "Gradient Descent(248/499): loss=0.2584575972217208, w0=0.003800335732714591, w1=-0.07982499873828697\n",
      "Gradient Descent(249/499): loss=0.25839549839804254, w0=0.003823957536722971, w1=-0.0799349565989243\n",
      "Gradient Descent(250/499): loss=0.25833398694007503, w0=0.0038475077550883716, w1=-0.08004460553333101\n",
      "Gradient Descent(251/499): loss=0.25827305421624375, w0=0.0038709865769875683, w1=-0.080153947108593\n",
      "Gradient Descent(252/499): loss=0.2582126917462645, w0=0.0038943941919365116, w1=-0.0802629828775604\n",
      "Gradient Descent(253/499): loss=0.2581528911982986, w0=0.003917730789756474, w1=-0.08037171437905731\n",
      "Gradient Descent(254/499): loss=0.258093644386162, w0=0.003940996560541695, w1=-0.08048014313808668\n",
      "Gradient Descent(255/499): loss=0.2580349432665878, w0=0.003964191694628461, w1=-0.08058827066603043\n",
      "Gradient Descent(256/499): loss=0.2579767799365432, w0=0.003987316382565562, w1=-0.08069609846084491\n",
      "Gradient Descent(257/499): loss=0.2579191466305977, w0=0.004010370815086054, w1=-0.08080362800725199\n",
      "Gradient Descent(258/499): loss=0.2578620357183425, w0=0.004033355183080293, w1=-0.08091086077692579\n",
      "Gradient Descent(259/499): loss=0.25780543970185965, w0=0.004056269677570156, w1=-0.08101779822867522\n",
      "Gradient Descent(260/499): loss=0.2577493512132411, w0=0.00407911448968443, w1=-0.0811244418086225\n",
      "Gradient Descent(261/499): loss=0.2576937630121543, w0=0.004101889810635293, w1=-0.08123079295037772\n",
      "Gradient Descent(262/499): loss=0.2576386679834567, w0=0.004124595831695852, w1=-0.08133685307520963\n",
      "Gradient Descent(263/499): loss=0.2575840591348546, w0=0.004147232744178689, w1=-0.08144262359221273\n",
      "Gradient Descent(264/499): loss=0.25752992959460863, w0=0.004169800739415374, w1=-0.08154810589847075\n",
      "Gradient Descent(265/499): loss=0.2574762726092833, w0=0.0041923000087369, w1=-0.08165330137921673\n",
      "Gradient Descent(266/499): loss=0.25742308154153937, w0=0.004214730743454999, w1=-0.08175821140798964\n",
      "Gradient Descent(267/499): loss=0.25737034986796975, w0=0.004237093134844305, w1=-0.08186283734678786\n",
      "Gradient Descent(268/499): loss=0.25731807117697686, w0=0.004259387374125322, w1=-0.08196718054621933\n",
      "Gradient Descent(269/499): loss=0.2572662391666906, w0=0.004281613652448165, w1=-0.08207124234564883\n",
      "Gradient Descent(270/499): loss=0.25721484764292685, w0=0.004303772160877041, w1=-0.08217502407334207\n",
      "Gradient Descent(271/499): loss=0.2571638905171861, w0=0.0043258630903754295, w1=-0.08227852704660703\n",
      "Gradient Descent(272/499): loss=0.2571133618046888, w0=0.0043478866317919465, w1=-0.08238175257193242\n",
      "Gradient Descent(273/499): loss=0.25706325562245047, w0=0.004369842975846847, w1=-0.08248470194512346\n",
      "Gradient Descent(274/499): loss=0.25701356618739296, w0=0.0043917323131191476, w1=-0.0825873764514349\n",
      "Gradient Descent(275/499): loss=0.2569642878144925, w0=0.004413554834034333, w1=-0.08268977736570157\n",
      "Gradient Descent(276/499): loss=0.25691541491496195, w0=0.004435310728852636, w1=-0.08279190595246644\n",
      "Gradient Descent(277/499): loss=0.2568669419944717, w0=0.0044570001876578415, w1=-0.0828937634661061\n",
      "Gradient Descent(278/499): loss=0.25681886365139983, w0=0.00447862340034662, w1=-0.082995351150954\n",
      "Gradient Descent(279/499): loss=0.2567711745751201, w0=0.004500180556618338, w1=-0.08309667024142137\n",
      "Gradient Descent(280/499): loss=0.2567238695443205, w0=0.00452167184596535, w1=-0.08319772196211586\n",
      "Gradient Descent(281/499): loss=0.2566769434253551, w0=0.004543097457663732, w1=-0.08329850752795806\n",
      "Gradient Descent(282/499): loss=0.2566303911706277, w0=0.004564457580764442, w1=-0.0833990281442958\n",
      "Gradient Descent(283/499): loss=0.2565842078170052, w0=0.004585752404084898, w1=-0.0834992850070165\n",
      "Gradient Descent(284/499): loss=0.256538388484263, w0=0.004606982116200938, w1=-0.0835992793026575\n",
      "Gradient Descent(285/499): loss=0.25649292837355964, w0=0.004628146905439158, w1=-0.08369901220851429\n",
      "Gradient Descent(286/499): loss=0.25644782276594036, w0=0.004649246959869606, w1=-0.08379848489274709\n",
      "Gradient Descent(287/499): loss=0.2564030670208699, w0=0.004670282467298812, w1=-0.08389769851448534\n",
      "Gradient Descent(288/499): loss=0.25635865657479295, w0=0.004691253615263151, w1=-0.08399665422393049\n",
      "Gradient Descent(289/499): loss=0.2563145869397226, w0=0.004712160591022508, w1=-0.0840953531624571\n",
      "Gradient Descent(290/499): loss=0.25627085370185493, w0=0.004733003581554244, w1=-0.08419379646271206\n",
      "Gradient Descent(291/499): loss=0.2562274525202123, w0=0.0047537827735474455, w1=-0.08429198524871229\n",
      "Gradient Descent(292/499): loss=0.256184379125309, w0=0.004774498353397437, w1=-0.0843899206359407\n",
      "Gradient Descent(293/499): loss=0.2561416293178468, w0=0.004795150507200557, w1=-0.08448760373144068\n",
      "Gradient Descent(294/499): loss=0.25609919896743116, w0=0.0048157394207491815, w1=-0.08458503563390893\n",
      "Gradient Descent(295/499): loss=0.2560570840113152, w0=0.004836265279526974, w1=-0.08468221743378684\n",
      "Gradient Descent(296/499): loss=0.25601528045316585, w0=0.0048567282687043654, w1=-0.0847791502133504\n",
      "Gradient Descent(297/499): loss=0.25597378436185453, w0=0.004877128573134246, w1=-0.08487583504679862\n",
      "Gradient Descent(298/499): loss=0.2559325918702702, w0=0.004897466377347863, w1=-0.08497227300034059\n",
      "Gradient Descent(299/499): loss=0.2558916991741553, w0=0.004917741865550902, w1=-0.08506846513228118\n",
      "Gradient Descent(300/499): loss=0.2558511025309642, w0=0.004937955221619772, w1=-0.08516441249310538\n",
      "Gradient Descent(301/499): loss=0.255810798258743, w0=0.004958106629098048, w1=-0.08526011612556134\n",
      "Gradient Descent(302/499): loss=0.2557707827350312, w0=0.0049781962711930965, w1=-0.08535557706474209\n",
      "Gradient Descent(303/499): loss=0.255731052395783, w0=0.004998224330772851, w1=-0.0854507963381661\n",
      "Gradient Descent(304/499): loss=0.2556916037343109, w0=0.00501819099036275, w1=-0.08554577496585657\n",
      "Gradient Descent(305/499): loss=0.25565243330024845, w0=0.005038096432142813, w1=-0.08564051396041951\n",
      "Gradient Descent(306/499): loss=0.25561353769853223, w0=0.005057940837944862, w1=-0.08573501432712069\n",
      "Gradient Descent(307/499): loss=0.25557491358840423, w0=0.005077724389249873, w1=-0.08582927706396146\n",
      "Gradient Descent(308/499): loss=0.25553655768243255, w0=0.005097447267185452, w1=-0.0859233031617534\n",
      "Gradient Descent(309/499): loss=0.2554984667455509, w0=0.005117109652523437, w1=-0.08601709360419195\n",
      "Gradient Descent(310/499): loss=0.25546063759411636, w0=0.005136711725677608, w1=-0.08611064936792896\n",
      "Gradient Descent(311/499): loss=0.2554230670949846, w0=0.005156253666701515, w1=-0.08620397142264413\n",
      "Gradient Descent(312/499): loss=0.2553857521646031, w0=0.005175735655286404, w1=-0.08629706073111554\n",
      "Gradient Descent(313/499): loss=0.2553486897681221, w0=0.005195157870759245, w1=-0.08638991824928904\n",
      "Gradient Descent(314/499): loss=0.25531187691852, w0=0.005214520492080854, w1=-0.08648254492634687\n",
      "Gradient Descent(315/499): loss=0.2552753106757492, w0=0.0052338236978441074, w1=-0.08657494170477507\n",
      "Gradient Descent(316/499): loss=0.25523898814589424, w0=0.0052530676662722415, w1=-0.08666710952043018\n",
      "Gradient Descent(317/499): loss=0.25520290648034816, w0=0.005272252575217233, w1=-0.08675904930260485\n",
      "Gradient Descent(318/499): loss=0.255167062875004, w0=0.00529137860215826, w1=-0.0868507619740927\n",
      "Gradient Descent(319/499): loss=0.25513145456946107, w0=0.005310445924200237, w1=-0.08694224845125213\n",
      "Gradient Descent(320/499): loss=0.2550960788462467, w0=0.005329454718072418, w1=-0.08703350964406945\n",
      "Gradient Descent(321/499): loss=0.2550609330300523, w0=0.005348405160127076, w1=-0.08712454645622099\n",
      "Gradient Descent(322/499): loss=0.25502601448698436, w0=0.005367297426338235, w1=-0.08721535978513453\n",
      "Gradient Descent(323/499): loss=0.2549913206238288, w0=0.005386131692300473, w1=-0.08730595052204981\n",
      "Gradient Descent(324/499): loss=0.25495684888733017, w0=0.005404908133227781, w1=-0.08739631955207827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(325/499): loss=0.2549225967634836, w0=0.005423626923952479, w1=-0.08748646775426205\n",
      "Gradient Descent(326/499): loss=0.2548885617768401, w0=0.005442288238924178, w1=-0.08757639600163215\n",
      "Gradient Descent(327/499): loss=0.25485474148982584, w0=0.005460892252208809, w1=-0.08766610516126591\n",
      "Gradient Descent(328/499): loss=0.2548211335020727, w0=0.005479439137487681, w1=-0.08775559609434369\n",
      "Gradient Descent(329/499): loss=0.2547877354497633, w0=0.0054979290680566015, w1=-0.08784486965620485\n",
      "Gradient Descent(330/499): loss=0.2547545450049863, w0=0.005516362216825031, w1=-0.087933926696403\n",
      "Gradient Descent(331/499): loss=0.2547215598751056, w0=0.005534738756315286, w1=-0.08802276805876062\n",
      "Gradient Descent(332/499): loss=0.2546887778021403, w0=0.005553058858661779, w1=-0.0881113945814229\n",
      "Gradient Descent(333/499): loss=0.2546561965621562, w0=0.005571322695610295, w1=-0.08819980709691094\n",
      "Gradient Descent(334/499): loss=0.2546238139646697, w0=0.005589530438517309, w1=-0.08828800643217428\n",
      "Gradient Descent(335/499): loss=0.2545916278520619, w0=0.005607682258349339, w1=-0.08837599340864286\n",
      "Gradient Descent(336/499): loss=0.2545596360990038, w0=0.005625778325682324, w1=-0.08846376884227816\n",
      "Gradient Descent(337/499): loss=0.25452783661189304, w0=0.005643818810701047, w1=-0.08855133354362395\n",
      "Gradient Descent(338/499): loss=0.25449622732829985, w0=0.005661803883198578, w1=-0.08863868831785617\n",
      "Gradient Descent(339/499): loss=0.25446480621642453, w0=0.005679733712575747, w1=-0.08872583396483243\n",
      "Gradient Descent(340/499): loss=0.254433571274565, w0=0.005697608467840653, w1=-0.08881277127914075\n",
      "Gradient Descent(341/499): loss=0.25440252053059254, w0=0.005715428317608193, w1=-0.08889950105014782\n",
      "Gradient Descent(342/499): loss=0.2543716520414401, w0=0.005733193430099614, w1=-0.08898602406204659\n",
      "Gradient Descent(343/499): loss=0.25434096389259797, w0=0.005750903973142098, w1=-0.08907234109390336\n",
      "Gradient Descent(344/499): loss=0.25431045419761966, w0=0.005768560114168364, w1=-0.08915845291970431\n",
      "Gradient Descent(345/499): loss=0.2542801210976373, w0=0.005786162020216292, w1=-0.08924436030840147\n",
      "Gradient Descent(346/499): loss=0.25424996276088463, w0=0.005803709857928578, w1=-0.08933006402395809\n",
      "Gradient Descent(347/499): loss=0.2542199773822309, w0=0.0058212037935523936, w1=-0.08941556482539359\n",
      "Gradient Descent(348/499): loss=0.25419016318272236, w0=0.00583864399293908, w1=-0.08950086346682791\n",
      "Gradient Descent(349/499): loss=0.25416051840913123, w0=0.0058560306215438545, w1=-0.08958596069752538\n",
      "Gradient Descent(350/499): loss=0.25413104133351544, w0=0.005873363844425535, w1=-0.08967085726193803\n",
      "Gradient Descent(351/499): loss=0.25410173025278404, w0=0.005890643826246284, w1=-0.0897555538997485\n",
      "Gradient Descent(352/499): loss=0.2540725834882731, w0=0.005907870731271371, w1=-0.08984005134591236\n",
      "Gradient Descent(353/499): loss=0.25404359938532683, w0=0.005925044723368945, w1=-0.08992435033070001\n",
      "Gradient Descent(354/499): loss=0.2540147763128885, w0=0.00594216596600983, w1=-0.09000845157973807\n",
      "Gradient Descent(355/499): loss=0.2539861126630984, w0=0.0059592346222673365, w1=-0.09009235581405033\n",
      "Gradient Descent(356/499): loss=0.25395760685089835, w0=0.005976250854817074, w1=-0.0901760637500982\n",
      "Gradient Descent(357/499): loss=0.25392925731364513, w0=0.005993214825936796, w1=-0.09025957609982078\n",
      "Gradient Descent(358/499): loss=0.2539010625107291, w0=0.006010126697506247, w1=-0.09034289357067436\n",
      "Gradient Descent(359/499): loss=0.25387302092320174, w0=0.0060269866310070285, w1=-0.09042601686567163\n",
      "Gradient Descent(360/499): loss=0.25384513105340867, w0=0.006043794787522472, w1=-0.09050894668342031\n",
      "Gradient Descent(361/499): loss=0.2538173914246302, w0=0.006060551327737529, w1=-0.0905916837181615\n",
      "Gradient Descent(362/499): loss=0.25378980058072803, w0=0.006077256411938675, w1=-0.09067422865980743\n",
      "Gradient Descent(363/499): loss=0.2537623570857987, w0=0.006093910200013817, w1=-0.09075658219397897\n",
      "Gradient Descent(364/499): loss=0.2537350595238333, w0=0.006110512851452226, w1=-0.09083874500204264\n",
      "Gradient Descent(365/499): loss=0.2537079064983839, w0=0.006127064525344461, w1=-0.09092071776114721\n",
      "Gradient Descent(366/499): loss=0.2536808966322348, w0=0.0061435653803823265, w1=-0.09100250114425995\n",
      "Gradient Descent(367/499): loss=0.25365402856708164, w0=0.006160015574858821, w1=-0.09108409582020244\n",
      "Gradient Descent(368/499): loss=0.2536273009632146, w0=0.006176415266668109, w1=-0.09116550245368601\n",
      "Gradient Descent(369/499): loss=0.2536007124992088, w0=0.006192764613305497, w1=-0.09124672170534685\n",
      "Gradient Descent(370/499): loss=0.25357426187161936, w0=0.006209063771867419, w1=-0.09132775423178065\n",
      "Gradient Descent(371/499): loss=0.2535479477946828, w0=0.006225312899051436, w1=-0.09140860068557691\n",
      "Gradient Descent(372/499): loss=0.2535217690000232, w0=0.00624151215115624, w1=-0.09148926171535293\n",
      "Gradient Descent(373/499): loss=0.2534957242363646, w0=0.006257661684081668, w1=-0.0915697379657874\n",
      "Gradient Descent(374/499): loss=0.253469812269247, w0=0.006273761653328733, w1=-0.09165003007765363\n",
      "Gradient Descent(375/499): loss=0.25344403188075, w0=0.00628981221399965, w1=-0.09173013868785238\n",
      "Gradient Descent(376/499): loss=0.253418381869219, w0=0.006305813520797882, w1=-0.09181006442944449\n",
      "Gradient Descent(377/499): loss=0.25339286104899816, w0=0.00632176572802819, w1=-0.09188980793168303\n",
      "Gradient Descent(378/499): loss=0.25336746825016704, w0=0.006337668989596693, w1=-0.09196936982004515\n",
      "Gradient Descent(379/499): loss=0.25334220231828286, w0=0.006353523459010937, w1=-0.09204875071626366\n",
      "Gradient Descent(380/499): loss=0.25331706211412675, w0=0.006369329289379966, w1=-0.09212795123835815\n",
      "Gradient Descent(381/499): loss=0.2532920465134554, w0=0.0063850866334144146, w1=-0.09220697200066597\n",
      "Gradient Descent(382/499): loss=0.2532671544067559, w0=0.0064007956434265914, w1=-0.09228581361387267\n",
      "Gradient Descent(383/499): loss=0.25324238469900673, w0=0.006416456471330586, w1=-0.09236447668504237\n",
      "Gradient Descent(384/499): loss=0.2532177363094419, w0=0.0064320692686423726, w1=-0.0924429618176476\n",
      "Gradient Descent(385/499): loss=0.25319320817131885, w0=0.006447634186479929, w1=-0.09252126961159898\n",
      "Gradient Descent(386/499): loss=0.2531687992316927, w0=0.00646315137556336, w1=-0.09259940066327448\n",
      "Gradient Descent(387/499): loss=0.2531445084511923, w0=0.006478620986215026, w1=-0.0926773555655485\n",
      "Gradient Descent(388/499): loss=0.2531203348038005, w0=0.006494043168359685, w1=-0.09275513490782059\n",
      "Gradient Descent(389/499): loss=0.253096277276641, w0=0.0065094180715246355, w1=-0.09283273927604387\n",
      "Gradient Descent(390/499): loss=0.2530723348697655, w0=0.0065247458448398745, w1=-0.09291016925275318\n",
      "Gradient Descent(391/499): loss=0.25304850659594635, w0=0.006540026637038256, w1=-0.09298742541709297\n",
      "Gradient Descent(392/499): loss=0.25302479148047385, w0=0.006555260596455661, w1=-0.09306450834484485\n",
      "Gradient Descent(393/499): loss=0.25300118856095494, w0=0.006570447871031172, w1=-0.0931414186084549\n",
      "Gradient Descent(394/499): loss=0.2529776968871176, w0=0.006585588608307256, w1=-0.09321815677706073\n",
      "Gradient Descent(395/499): loss=0.2529543155206176, w0=0.006600682955429957, w1=-0.09329472341651823\n",
      "Gradient Descent(396/499): loss=0.25293104353484863, w0=0.006615731059149091, w1=-0.09337111908942805\n",
      "Gradient Descent(397/499): loss=0.2529078800147565, w0=0.006630733065818452, w1=-0.09344734435516186\n",
      "Gradient Descent(398/499): loss=0.25288482405665635, w0=0.006645689121396023, w1=-0.09352339976988829\n",
      "Gradient Descent(399/499): loss=0.2528618747680528, w0=0.006660599371444197, w1=-0.09359928588659866\n",
      "Gradient Descent(400/499): loss=0.2528390312674636, w0=0.00667546396113, w1=-0.09367500325513248\n",
      "Gradient Descent(401/499): loss=0.25281629268424677, w0=0.006690283035225323, w1=-0.09375055242220257\n",
      "Gradient Descent(402/499): loss=0.2527936581584299, w0=0.006705056738107165, w1=-0.09382593393142011\n",
      "Gradient Descent(403/499): loss=0.25277112684054326, w0=0.006719785213757878, w1=-0.0939011483233193\n",
      "Gradient Descent(404/499): loss=0.2527486978914557, w0=0.006734468605765422, w1=-0.09397619613538187\n",
      "Gradient Descent(405/499): loss=0.25272637048221325, w0=0.0067491070573236205, w1=-0.09405107790206128\n",
      "Gradient Descent(406/499): loss=0.2527041437938805, w0=0.006763700711232435, w1=-0.09412579415480676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(407/499): loss=0.2526820170173852, w0=0.006778249709898235, w1=-0.09420034542208705\n",
      "Gradient Descent(408/499): loss=0.2526599893533654, w0=0.006792754195334078, w1=-0.09427473222941393\n",
      "Gradient Descent(409/499): loss=0.25263806001201927, w0=0.006807214309159999, w1=-0.09434895509936554\n",
      "Gradient Descent(410/499): loss=0.25261622821295693, w0=0.006821630192603304, w1=-0.09442301455160947\n",
      "Gradient Descent(411/499): loss=0.25259449318505656, w0=0.0068360019864988734, w1=-0.09449691110292557\n",
      "Gradient Descent(412/499): loss=0.25257285416632125, w0=0.0068503298312894674, w1=-0.09457064526722869\n",
      "Gradient Descent(413/499): loss=0.25255131040373896, w0=0.00686461386702604, w1=-0.09464421755559098\n",
      "Gradient Descent(414/499): loss=0.2525298611531459, w0=0.006878854233368062, w1=-0.09471762847626418\n",
      "Gradient Descent(415/499): loss=0.25250850567909056, w0=0.006893051069583849, w1=-0.09479087853470158\n",
      "Gradient Descent(416/499): loss=0.25248724325470145, w0=0.006907204514550893, w1=-0.09486396823357983\n",
      "Gradient Descent(417/499): loss=0.2524660731615568, w0=0.006921314706756207, w1=-0.09493689807282048\n",
      "Gradient Descent(418/499): loss=0.2524449946895562, w0=0.00693538178429667, w1=-0.09500966854961138\n",
      "Gradient Descent(419/499): loss=0.2524240071367949, w0=0.006949405884879381, w1=-0.09508228015842782\n",
      "Gradient Descent(420/499): loss=0.25240310980944003, w0=0.006963387145822022, w1=-0.09515473339105354\n",
      "Gradient Descent(421/499): loss=0.2523823020216091, w0=0.006977325704053221, w1=-0.09522702873660148\n",
      "Gradient Descent(422/499): loss=0.2523615830952507, w0=0.006991221696112928, w1=-0.09529916668153433\n",
      "Gradient Descent(423/499): loss=0.25234095236002696, w0=0.007005075258152798, w1=-0.09537114770968494\n",
      "Gradient Descent(424/499): loss=0.25232040915319853, w0=0.00701888652593657, w1=-0.0954429723022765\n",
      "Gradient Descent(425/499): loss=0.2522999528195106, w0=0.007032655634840463, w1=-0.09551464093794251\n",
      "Gradient Descent(426/499): loss=0.2522795827110825, w0=0.007046382719853578, w1=-0.09558615409274668\n",
      "Gradient Descent(427/499): loss=0.2522592981872974, w0=0.0070600679155782995, w1=-0.09565751224020243\n",
      "Gradient Descent(428/499): loss=0.252239098614695, w0=0.007073711356230709, w1=-0.09572871585129243\n",
      "Gradient Descent(429/499): loss=0.25221898336686627, w0=0.007087313175641, w1=-0.09579976539448783\n",
      "Gradient Descent(430/499): loss=0.2521989518243488, w0=0.007100873507253905, w1=-0.09587066133576734\n",
      "Gradient Descent(431/499): loss=0.2521790033745255, w0=0.007114392484129126, w1=-0.09594140413863614\n",
      "Gradient Descent(432/499): loss=0.2521591374115233, w0=0.007127870238941769, w1=-0.09601199426414461\n",
      "Gradient Descent(433/499): loss=0.25213935333611553, w0=0.007141306903982785, w1=-0.09608243217090691\n",
      "Gradient Descent(434/499): loss=0.2521196505556242, w0=0.007154702611159424, w1=-0.0961527183151193\n",
      "Gradient Descent(435/499): loss=0.25210002848382485, w0=0.007168057491995683, w1=-0.09622285315057846\n",
      "Gradient Descent(436/499): loss=0.25208048654085335, w0=0.0071813716776327734, w1=-0.09629283712869943\n",
      "Gradient Descent(437/499): loss=0.25206102415311293, w0=0.007194645298829582, w1=-0.09636267069853358\n",
      "Gradient Descent(438/499): loss=0.2520416407531848, w0=0.007207878485963147, w1=-0.09643235430678626\n",
      "Gradient Descent(439/499): loss=0.2520223357797381, w0=0.007221071369029136, w1=-0.09650188839783443\n",
      "Gradient Descent(440/499): loss=0.2520031086774429, w0=0.0072342240776423275, w1=-0.09657127341374401\n",
      "Gradient Descent(441/499): loss=0.25198395889688463, w0=0.007247336741037107, w1=-0.09664050979428714\n",
      "Gradient Descent(442/499): loss=0.2519648858944787, w0=0.0072604094880679565, w1=-0.09670959797695922\n",
      "Gradient Descent(443/499): loss=0.2519458891323877, w0=0.007273442447209962, w1=-0.09677853839699589\n",
      "Gradient Descent(444/499): loss=0.2519269680784404, w0=0.007286435746559319, w1=-0.0968473314873898\n",
      "Gradient Descent(445/499): loss=0.2519081222060501, w0=0.007299389513833842, w1=-0.0969159776789072\n",
      "Gradient Descent(446/499): loss=0.2518893509941366, w0=0.007312303876373491, w1=-0.0969844774001044\n",
      "Gradient Descent(447/499): loss=0.25187065392704844, w0=0.007325178961140891, w1=-0.09705283107734414\n",
      "Gradient Descent(448/499): loss=0.25185203049448657, w0=0.007338014894721862, w1=-0.09712103913481175\n",
      "Gradient Descent(449/499): loss=0.2518334801914285, w0=0.007350811803325957, w1=-0.09718910199453117\n",
      "Gradient Descent(450/499): loss=0.25181500251805566, w0=0.007363569812787001, w1=-0.09725702007638082\n",
      "Gradient Descent(451/499): loss=0.2517965969796798, w0=0.00737628904856364, w1=-0.09732479379810938\n",
      "Gradient Descent(452/499): loss=0.25177826308667256, w0=0.007388969635739892, w1=-0.09739242357535136\n",
      "Gradient Descent(453/499): loss=0.2517600003543939, w0=0.007401611699025701, w1=-0.0974599098216426\n",
      "Gradient Descent(454/499): loss=0.25174180830312476, w0=0.007414215362757506, w1=-0.09752725294843556\n",
      "Gradient Descent(455/499): loss=0.25172368645799775, w0=0.007426780750898805, w1=-0.09759445336511452\n",
      "Gradient Descent(456/499): loss=0.25170563434893145, w0=0.007439307987040727, w1=-0.09766151147901064\n",
      "Gradient Descent(457/499): loss=0.2516876515105638, w0=0.007451797194402614, w1=-0.09772842769541687\n",
      "Gradient Descent(458/499): loss=0.2516697374821882, w0=0.007464248495832603, w1=-0.09779520241760274\n",
      "Gradient Descent(459/499): loss=0.25165189180768965, w0=0.007476662013808213, w1=-0.09786183604682905\n",
      "Gradient Descent(460/499): loss=0.25163411403548275, w0=0.007489037870436939, w1=-0.09792832898236233\n",
      "Gradient Descent(461/499): loss=0.2516164037184494, w0=0.007501376187456854, w1=-0.09799468162148928\n",
      "Gradient Descent(462/499): loss=0.2515987604138791, w0=0.007513677086237209, w1=-0.09806089435953105\n",
      "Gradient Descent(463/499): loss=0.25158118368340976, w0=0.007525940687779041, w1=-0.0981269675898574\n",
      "Gradient Descent(464/499): loss=0.25156367309296784, w0=0.007538167112715789, w1=-0.09819290170390065\n",
      "Gradient Descent(465/499): loss=0.25154622821271244, w0=0.007550356481313911, w1=-0.09825869709116965\n",
      "Gradient Descent(466/499): loss=0.2515288486169778, w0=0.007562508913473506, w1=-0.09832435413926353\n",
      "Gradient Descent(467/499): loss=0.2515115338842175, w0=0.007574624528728942, w1=-0.09838987323388539\n",
      "Gradient Descent(468/499): loss=0.2514942835969507, w0=0.007586703446249488, w1=-0.09845525475885575\n",
      "Gradient Descent(469/499): loss=0.2514770973417069, w0=0.007598745784839951, w1=-0.09852049909612609\n",
      "Gradient Descent(470/499): loss=0.2514599747089742, w0=0.007610751662941319, w1=-0.09858560662579208\n",
      "Gradient Descent(471/499): loss=0.25144291529314633, w0=0.007622721198631402, w1=-0.09865057772610675\n",
      "Gradient Descent(472/499): loss=0.25142591869247144, w0=0.007634654509625488, w1=-0.09871541277349363\n",
      "Gradient Descent(473/499): loss=0.2514089845090022, w0=0.007646551713276993, w1=-0.09878011214255968\n",
      "Gradient Descent(474/499): loss=0.2513921123485453, w0=0.007658412926578122, w1=-0.0988446762061081\n",
      "Gradient Descent(475/499): loss=0.25137530182061274, w0=0.007670238266160535, w1=-0.09890910533515111\n",
      "Gradient Descent(476/499): loss=0.2513585525383749, w0=0.007682027848296007, w1=-0.0989733998989226\n",
      "Gradient Descent(477/499): loss=0.25134186411861137, w0=0.0076937817888971085, w1=-0.09903756026489059\n",
      "Gradient Descent(478/499): loss=0.25132523618166563, w0=0.007705500203517874, w1=-0.09910158679876967\n",
      "Gradient Descent(479/499): loss=0.25130866835139876, w0=0.007717183207354487, w1=-0.09916547986453329\n",
      "Gradient Descent(480/499): loss=0.25129216025514456, w0=0.00772883091524596, w1=-0.099229239824426\n",
      "Gradient Descent(481/499): loss=0.2512757115236649, w0=0.007740443441674825, w1=-0.09929286703897547\n",
      "Gradient Descent(482/499): loss=0.2512593217911062, w0=0.007752020900767826, w1=-0.09935636186700458\n",
      "Gradient Descent(483/499): loss=0.25124299069495665, w0=0.00776356340629661, w1=-0.09941972466564322\n",
      "Gradient Descent(484/499): loss=0.25122671787600315, w0=0.007775071071678434, w1=-0.09948295579034011\n",
      "Gradient Descent(485/499): loss=0.25121050297829045, w0=0.007786544009976859, w1=-0.0995460555948745\n",
      "Gradient Descent(486/499): loss=0.2511943456490799, w0=0.007797982333902466, w1=-0.09960902443136775\n",
      "Gradient Descent(487/499): loss=0.2511782455388088, w0=0.00780938615581356, w1=-0.0996718626502948\n",
      "Gradient Descent(488/499): loss=0.25116220230105113, w0=0.007820755587716886, w1=-0.0997345706004956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(489/499): loss=0.25114621559247796, w0=0.00783209074126835, w1=-0.09979714862918636\n",
      "Gradient Descent(490/499): loss=0.2511302850728192, w0=0.007843391727773737, w1=-0.09985959708197077\n",
      "Gradient Descent(491/499): loss=0.25111441040482596, w0=0.007854658658189434, w1=-0.09992191630285115\n",
      "Gradient Descent(492/499): loss=0.25109859125423295, w0=0.007865891643123162, w1=-0.09998410663423936\n",
      "Gradient Descent(493/499): loss=0.2510828272897213, w0=0.007877090792834707, w1=-0.10004616841696783\n",
      "Gradient Descent(494/499): loss=0.25106711818288335, w0=0.007888256217236652, w1=-0.1001081019903003\n",
      "Gradient Descent(495/499): loss=0.2510514636081866, w0=0.007899388025895116, w1=-0.10016990769194263\n",
      "Gradient Descent(496/499): loss=0.2510358632429381, w0=0.007910486328030495, w1=-0.10023158585805339\n",
      "Gradient Descent(497/499): loss=0.2510203167672506, w0=0.007921551232518208, w1=-0.10029313682325443\n",
      "Gradient Descent(498/499): loss=0.2510048238640088, w0=0.007932582847889443, w1=-0.10035456092064136\n",
      "Gradient Descent(499/499): loss=0.25098938421883465, w0=0.007943581282331908, w1=-0.10041585848179395\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEKCAYAAAAb7IIBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3X28HVV97/HPLycnmBAJ5ARCeDgJ1tiWGkE9RSu0UhN6\nIUoDubbFBgxqmwu1Ftp6EV/xCrTmVWpbL7S9qGmliSTV2ooFbKyFCPWhVgmP4UEMak4EAiQBQ4BI\nQvK7f6yZnDk7M3vP3jP77H32fN+v13md2bPXnjVrnn6z1qyZMXdHRESqZ0KnZ0BERDpDAUBEpKIU\nAEREKkoBQESkohQAREQqSgFARKSiujYAmNmQmX3WzDab2U/N7Fkze8jMPmNmCxLpTjczT/ztNbMd\nZna3mf2lmQ12shyNmNkd0XxvbvH3cblXNUh3uJldGf2d3kpeZUqbbzM7J57HlPSr4t+0mF+h5dxN\nzOw0M/uame2K/r5mZqfl/O0fmtm/m9mWaL96zMxuNrPXp6T1On+H16R9bTSdZ83sRTP7tpktSpnm\ncdG6fNLMXjKz75nZZWbWV5Nuopn9QbQfP2tmz5nZRjO73MwmtzjN3zKz26N0e8zsBTO7x8w+ZGYT\na9LOM7PPm9lwYjndZGan1Fm2v1OzjI5LfPc6M7vOzO43s2fM7Plo+ENmdkjNdOJj36OJaT2akWeu\nsmdy9677Ay4D9gGe8XdvIu3pddI5sBNY0Oky1SnrHdF8bm7x93E5VzVINyeR9souKPdB8w2sisen\npM/8biyWc7f8AW8D9qRs53uAX83x+59m7Cd7gF/OWEdpf4cn0v1CtJ+lpTs/ke5o4LGMdJ+pyfv/\n1cn7n1qc5qfqTPOTNfvK8xnpXgJOTlmuRwDbatIel/j+8jp5f6VmWpempHk0Jc/cZc/667oaQHTW\n8OeE2skzwLuB6cArgJ8H/ghIjYbA6uh3xwIfBl4GDgO+2K01AXc/3d3N3ed0el7GUlRmc/cLOz0v\n48x1QD/wLHBy9PdsNO66HL9/AvgDYBYwQNhniH5/ecZvfjWxvuK/nyS+/wRhP9sDLABOAH4UfffX\nZjYlGr6csG8CLIl+8+no83trajHnR/+fIwSY44HN0bh3mtkrW5jm7cBCYCZwKPC7ie9+OzF8bvQ9\nhGU6lZFlM6kmbWwFMAN4MeU7CAflLwKnAlOAtxKCJsCZZvaLibSPAFcAZwCPZ0wPmit7xlx1wVlN\nTVS7l5Eo9vYc6U9PpF9V893HEt9dkxi/Kh7fYNofjtLtB46Ixv0uI2dMk6Nxf5jIZ1Y0bgLwAeAe\nwkbxPOEsdEFNHneQcmYaTfMx4AXgS8AvJfK4MuUsbRWwFPh+lNd64FVRmgtJP0tw4PQoze9Fy/65\nKM8fAF8AfqFNy2fUOiPs3Gnzd0ftOgNeA3w1Wq6bSJxl1pnXrOV8KrCOcBB9KVp+H4vnPUpzKPBX\n0TLZDfwE2Aj8faKMDdOklbvJfeONid9flxh/XWL8GxpMY2rN54HEbx+p+W7UNpIxvSMZqa2vS4y/\nLPH7xTX79tOJdK/LKNOOaNw3E+PWJNIONDvNjPmP89meGPeBxO8XRONenRj3NzXTeH20DB6omcdk\nDWBqSt5/k0j7roz520x2DaBQ2d27rAZgZkcDJ0UfH3H3fys4yb9ODJ/Zwu+/Ef034E3R8C9F//uB\noWj4LdH/R919azS8Ksr/ZGAy4QDxVuA/zOw362VqZu8lnFUdSzhbOIdw9lDPWVGec6O83gb8Y4Pf\nxPn9FqHKfRLwyijPVwG/Qah1ZSmyfIr4BvBrhOX6auCzZnZisxMxs7OB/yQsu8MJZ3dzgeWE9dQf\nJf1LQs3zVYSa6DTgtcD7GDlTzJOmqDckhr+fMXxQW36Suz9fM+oVieGss81/tnBtbZuZ/bOZJbeJ\nkxm5lthonka13adIzvvfRf/nmdmJUXv6qdG4u919RwvTPMDMpprZ7xJaFyA0D8W+SGh9ADjXzA4F\n3pn4/j8S0zHCvjMB+H1Cq8NBUpY75Fv29bRU9qSuCgBAspnmkXjAzN6cchGqYfXG3Z9mpJrVShPQ\nnYQ2Uxg5sL2FEF2T4+L/X4/m95eBC6JxywkH1VmEs1AD/q+ZpS77aPwV0cfnCBv9TBLLI8NRhBrA\ndODWaNybzOw4d19FqJbHrvKRqvwdQLwsfxjN5xTCgf8PgOE6eba0fNJ4aAJbnfgcz9/pKcm/Tahu\nL4s+G7C4znweJNpx/xroI9SY3kpYdmuiJKcRqtXxMMA/E5oDjgBOAf6EUGvIm6aoGYnh5zKGj2py\nmlcmhj9TJ9+J0f93Av9tZj/bwjzdH/0/0sx+28ymEmqesYHE8IcJNarDgAeBHxPa5u9g9LpuZprx\nxWoHdgEro9F/6+4fidO4+xPALxNq4L9H2D7+jFCru8Tdb0lMcilh+/5ctC/lYmZzGWlK+j7wzby/\nTWiq7Gm6LQAkeeMkudhBE3a/MD7A1J0B95eA70Yff8nMphOaH9YRqnxvMbPZjLTDxQe4sxKTWUHY\n2LYSmqsAjgF+lnTHMRKs/sXd/ysKZB+rN6/Ad939s+7+LKHJKHZ8g9/ByEH+WOCjhOsuhxMujN2Z\n9aMCy6eoD0dngGsS4/KUM+k1hAMKhOX89WjZfSSR5tei//HyOTX6/h3AC+5+hbvvaiJNMrBd2OT8\n1pPcjnPvN2b2EeB3oo9r3H1tTZIVhGanqYRldVM0/jDCAbrZefpzQtMgwFrCfvG/Eun2Job/N/DH\nKdM9HkjW9pqZZpbfN7MVB2bc7BjgXwn7YtKhwOujGgFmNg24Osrzgznyiac/SGjCnBL99rfcfX/e\n3ycULnu3BYAtieHXxAPu/t/RwfqqZiZmZkcRNlaofyZbT9zMcQrh7NYIVcAHCJH/LYm08QHuyBzT\nnZ4xflZiOFktfKzB9DYlhn+aGD6kNmGK64BbCE0gFxOqw98Ghs1sqN4PaW35FBWXtdlyJiXPXH+c\nGE4u53g9fpDQnn8M4cLbDcCDZvZdG+kOmSdNUdsTw4clhl+ZGN6WZ0Jm9n+AP40+/gvw3to07v4R\nd7/b3V9w92HCthGLL1rmnid330C4SPwNwrp7inAWHk/jx9G8HcnICc99wGzCuvgP4GeAG+Mulnmn\nmSjTA9Gx5DBgESMtBB8ys5nR8GWEpkCiMk8BzibUFi8kHPQhXO+aCXwOOMrMTmb0fn1ibeeT6PMd\nhBr584TrnPfSgmbLnqarAoC7P8lItebnzWx+wUlekhj+9xanER/gphHacyEcHL9NqNrGvRUec/cf\nRcPJneJkr+lBAUxw929l5PdEYjgZDBqd4SbbHtPOAjPPDN39RXf/dUJ5FhCafrYSDmZ/1iDfVpZP\n5qw0+D6e35ej/0Vqicl1dFzG8PYon++5++sIB5+zCSci+wgHwffnTVOCuxPDr8kYvqfRRMzso4Sm\nKQhnjue5+96aNGnHBk8ZvpfQCSDXPLn7N9z9V9x9srsfDVzDSDD+z+j/zxCuIUG4sLzF3bczch3s\nFYw0K+ad5uiCuO9y95sJPYMgHNzjZtKfSyS93t13u/uXCQdYCNfXINSMIDRF3hP9nZ347VcZWc5E\nteE7onx2Ame4+zcooJWy106gq/4I7XvxVeytwP8kLOiphAtD8XenRelPT4xbRTgDnQV8iFAFckLb\n3fGJPFbFv8kxP68kHFydsEO/SNg4381IDxgH/jHxm7cm5ul2woXKSYRmn8sJ7YVx2jtI9E4hBOXh\naNwzhAupR0bTiad5ZeL3B/UqYXSvn9OjcYclxn0e6E+kfyeh6jiXcGHpKELTjpO456Ks5VNnvj+R\nGD+vJn3qOkubTsZ81i5nY6SHxXOENt/DCdch4mm+J0p7GaFr4GC0Hk9kpJ/4NXnTNDO/dcrxvcS2\ncVL090w07uFEutMTeV2YGH9FYvxnCCcjafm8n9Cl8BTCAXc2oWkx/u2nE2m/Go17CZhPaC76YTRu\nBzAlSjeDsG0eE21nbyGc8MXr4Ogo3exEPvcSTn4GEvk44eDZzDSnEi7WnkY4S59C6BL6E0a20zjt\nPyTyuSia7jsY6e30rSjdlYl0WX+rEmX6UWKZvLHOOj4kKtcMQquIR8szHtffTNnrbk9jeXBvYiP/\nPzkWbFoASPv7CTA/z8GkzvxsSEzv69G4uTX5XFTzm3+sM093ZB2YonHvTfnN1sTwFfUOKKQEgGj8\nIynTndhgQ766Tcsnbb7flZL/x+qts7TpZMxj2nJexEjwqv37L0Z2tDsy0jhwVt40zcxvnXLkuhGM\n7ABQd79KpEu7GSm5LSZPqPLeCPbqjDQvE9rBk+W8sU7+DwKTmpkmIbjXK/u1ibS/SAhmWWkzux0z\nurtyshvolQ3yvzJj/037O73Z5Zn111VNQDF3/1NCpP4CoUlkL+HM8nuE6PwOwg6aZh/hjOge4C+A\n17r7+oKzlKym/Vc0j5sY3YxQ2759PqEp5R5C+9zz0fyvJPQMyuTu1xMugD1B6FN+C6Mv7jzbdAmC\npYSeO7trxt9GWNY/YuSehYcIF4Q/QmOtLJ80XwCuJRxg2s7dbyIcUL9KOIDtJfTjv5rQ/ztuFllF\naH9+gnCgfYbQxHWeu3+liTRlzPPXonm+nXC/xgvR8Nvc/fay8iFsc39BOAN/hrBsthBqBW909wPt\ny+7+IOHi9y2E5bgb+A5wjrsnL9Q/S7i4+jgjy+jLhDuQ/6km/98mHDQfIuw/ewk1tusIgS6++Jl3\nmrsJNYD7CSeF8XHidsIB99JEee4kBNAvE7bhfYQz6q8Dv1FTpk5qZnmmsiiSSBeJ7oc41t3vij5P\nJVTX4/sHTnL3+7N+LyKSx8TGSaQDXg18w8xeIET5mYxcFPu0Dv4iUoaubAISfkzoc/0c4eD/U0LT\nyvsY3RVPRKRlagISEako1QBERCqqq68BzJgxw+fMmdPp2RARGTfuuuuu7e6e52kE3R0A5syZw4YN\nGzo9GyIi44aZ5X7sjZqAREQqSgFARKSiFABERCpKAUBEpKIUAEREKqrSAWDtxrXMuWYOE66awJxr\n5rB2Y+0LkUREeldlA8DajWtZdssyhncO4zjDO4dZdssyBYExoMAr0h0qGwCWr1/Oi3tfHDXuxb0v\nsnx93Sc1S0EKvCLdo7IBYMvOLU2Nl3Io8Ip0j8oGgMFpg02Nl3Io8Ip0j8oGgBXzVzClf8qocVP6\np7Bi/ooOzVE1KPCKdI9SAoCZnWlmj5jZo2Z2ecr3P2dm3zazl8zsg2XkWdSSeUtYefZKZk+bjWHM\nnjablWevZMm8JZ2etZ6mwCvSPQq/D8DM+oDvA2cAjxHeOfsud38okeYoYDZwDvCsu/9lnmkPDQ25\nHgbXe9ZuXMvy9cvZsnMLg9MGWTF/hQKvSEnM7C53H8qTtoyngZ4CPOruP4wy/zywiPAyZwDc/Wng\naTN7ewn5NU0HnO6yZN4SLX+RLlBGADiW8ArD2GPAm1qdmJktA5YBDA4WbxeOux3GPU/iboeADkIi\nUmlddxHY3Ve6+5C7Dx15ZK53GtSlboci3Uc3A3aHMmoAjwPHJz4fF43rCup2KNJdVCvvHmXUAO4E\n5prZCWY2CTgPuLmE6ZZC3Q5Fuotq5d2jcABw95eB3we+CjwMfMHdHzSzi8zsIgAzO9rMHgP+CPiI\nmT1mZocVzTsPdTsUaY9Wm3FUK+8epbwT2N3XAetqxn0qMfwkoWlozMVVyuXrlzO8c5g+6xt1tqEq\np0jzijTjDE4bZHjnwa+tVa187HXdReB2WDJvyYGawD7fB6CHkIkUUKQZR7Xy7lGJAABqd5Txqxt7\nzBRpxtFd+N2jMgFA7Y6d040HsPGiWx+fndVcM33y9Fy/j2vlg9MG2bJzC8vXL+94maqoMgFAvYE6\no1sPYONFt9ZcV8xfQf+E/oPG79qzK9e61XbRHSoTAHq13bHbz66LHMC6vWxjoVtrrkvmLeGwQw7u\nyLdn355c67ZbA1vVVCYAjMd2x0YHwPFwFtXqAWw8lG0sdHPN9Zndz6SOzxOcujWwVU1lAgCEILD5\n0s3sv2I/my/d3PUH/0YHwPFwFtXqAWw8lG0slFVzbUdtqkhw6ubAViWVCgDjSZ4D4Hg4i2r1ADYe\nyjYWyqi5tqs2VSQ49WqT7HijANCl8hwAx8NZVKsHsG4v21henyhac21XbapIcBqPTbK9qJQ7gaV8\nee6WXDF/xai7MaE7z6Jaef5/N5dtvD3MrB21qdp3bNyw+Iamy673QnSeagBdKk8VuZfPorq5bOPt\n+kTZtSldoO8dhV8J2U5VfyWk3mTWnSZcNQHn4P3GMPZfsb8Dc1RfbY0FwslEqwF1zjVzUmuns6fN\nZvOlm1uaP23n5RnrV0JKm6iK3J3G28PMkg9ELOMgW2aT0nhrTus1agISadJ47MFSVhfotRvXMsHS\nDxutBMBWm9N0k2A5FABEmtTN1yfaKT5bj5+om9RqAGylNqFrEOVRABhndObTHcbTTYXNqLd9pZ2t\nA/RZX8sBsJUL1OPtInw3UwDIoVsOujrzkXZqtH1lnZXv9/0tB8BWmtN0k2B5FAAa6NRBNy3o6Myn\nPbolwHdao+2rHTfntdKc1u03CY4nCgANdOKgmxV00nqeQL4zHx3k0o1FgB8vy77RmXWzZ+t5y91s\nc1qe+Rgvy7zTFAAa6ER1Myvo9FlfavpGZz5qOsqWN8C3ekAZq2VfxgGv0Zl1M2fr7Sx3o/nQ9p6f\nAkADnahuZgWXfb6vpe6H7azFdPpMq17+eeYtT4AvckAZixpk2vxdcOMF2FXW1DrJe/d5o7P1tRvX\nsvRLS1PLfclXLsksQzPbUdp8xNM4/8bzS1nmnd62x0LPBYCyV1raTtE/oZ/n9zyPXWVM/JOJ2FXG\njI/PYMbHZ7SUbzzP8fTS7jIFDpzpNNv9sF21mE6fadXLP++85QnwRQ7iRd6HkHc7Tpu/eBtqZp2U\n+eTRtK6iADt272jLey2S08jSzPaeNk/n33g+Mz4+o6cCQU89CqLsW96T043vopw+eTq79uxiz749\ndX+TN9+0eS4yvTRZt+73WR/7fX/DO0Pj8g/vHKbP+tjn+5g9bTbP73meHbt3HJS+2UcC1HsUQO2y\nh/AiksFpg3XzB1LLPDB5gKmTph7Ia+Hchay+b3Xm8h+YPJCaB6Q/+qG2LFnzWDsftWVuZjvOejRF\nUqN1UjvfC+cuZN2mdU3fOZy1rdWbl7yPlli7cS2XfOWSA8tzYPIA1551LUvmLWkp33pmfHxG3fV+\n0dBFXPf261K/b/XRFmU9EqOZR0H0VABo5hklWQe1hXMX8oUHv5C6kdXLI03aATbvASKp0QbXSN4g\nU1vWZn5bK16myWWbdmAf3jmMYQcdwKZOmsoFr7ug7sG5DHHe8Xw2K1m+V09/NV/70ddGlWUCE9hP\n4+cDxfNRL7D2WR+rz10NMCooPrP7mYYBAGDN4jUABx1Ef/MXfrPhcs67DeYJRrWBM8+zldZuXMt7\n/vU97N2/d1SaSX2TuH7R9Zx/4/l185zSP4WlJy09KKjBwY/I+NaWb/HJDZ9sWIa0J6BmBe9GeU/p\nn8ILe184aJ5bOekb8wBgZmcC1wJ9wN+7+9U131v0/ULgReBCd7+70XSbDQB5H9LV6kFt9rTZuQ/+\nZYsPoLVBq9GBNlZ79pQl3tFPHTz1QF5lSU67leUvxaUF22aknSQkNXOCFE8PaLhd1tOoTFnBuSjD\nDjpwF12+tVp5wN6YBgAz6wO+D5wBPAbcCbzL3R9KpFkIfIAQAN4EXOvub2o07XbVAJrdSJPKXsHt\nyLv2rLY2eHSSYUyfPL3QDi+dlxUIWjm5inu3tVIDaySu4Xxqw6c6tt8W0coTZpsJAGVcBD4FeNTd\nf+jue4DPA4tq0iwCPuvBfwOHm9msEvIeJW8/5SIXPzu5EeXNO04X71CN7iMYS47r4N8DduzekXqh\ntvZCctaD45L2+b627Vc7du/gkxs+OS4P/tD+m9vKCADHAj9OfH4sGtdsGgDMbJmZbTCzDdu2bWtq\nRvL2YmjnQjUss79+PfFvDEudZlH17iMQaUVWT6hkF828LQz7vfveo9BphrX9CbNd1w3U3Ve6+5C7\nDx155JFN/z5PP+W0mkIZBiYPMDhtsKWq7H7fj1/h7L9iP2sWrxkVxMo6e0m7j6ATBiYPdMV8SHGN\natN5T7a6/eRk/gnzx3Qe4+tl7X7IYBkB4HHg+MTn46JxzaYZM8maQln6J/Sza8+ulptZ4i6OcHAQ\nK3M+J0+czMDkgQPBZf4J80upYeQ1pX8K1551LSvPXtn1O30ecRPH7GmzuXjo4o6XKU+TSyN91nfg\n4myjbWNw2mDdexZWzF9B/4T+utOY1DeJZW9c1jBdJxjGxUMXc9u7b2P1uavH5MRlYPIANyy+oeVe\nf80oIwDcCcw1sxPMbBJwHnBzTZqbgXdb8GZgp7tvLSHvlsUHWb/CWbN4zUEr1jDmnzD/wI6Qps/6\nDhxIDzvksNR7A/IeXHft2ZV5g0m9Gku9pqM0O3bvYPfLu7lh8Q1svnQzt737Nm5YfMOBIFNmMOiz\nPi4euji1SW7JvCWpO9SU/imsWbxmVC2o1YPq7GmzWbN4TeY6jA+W8fQHJg8cCI5pw8m08bT3fXQf\nfoWz+dLNXPf260pvyphgExoG64HJA6xZvAa/wtn30X2p23Oa/gn9HNp/6EHTWn3uarZftn1UjTRt\nGU7pn8LCuQvr3sS1ZN4S/uGcf8hcBwOTB7h+0fVc9/br6qZLmjhh9IsM42USB+FGZY/Xafx/Ut+k\n1HSzp80edSCubWIemDxw0PLLyi+PeJvaftn2MXu8eFndQBcC1xC6gV7v7ivM7CIAd/9U1A30b4Ez\nCd1A3+PuDbv3lPFO4Kz+/rVdJdNuhKnXN7q2j26e/s/x7ybYBJ7f8/xB39Xr8tXoZqm0nhcTbELm\nASkrr7R8gLo3JhW5AS/PzS/1pp9n3ur1Hy97R8vqYRb3fmp0E1va72q7MOe5WShrPRa50Shtmlk9\ny1p9P3AZ5WjU5bnVZdrKvOZdN2Vuh5W9EaxWvS5pjQ5Q9bqKpgWQIl1Lk9YsXtP0xlCv++uWnVty\nv8A87x25WTtdOzf+Vucta9kMTB5g+2XbG+abd76SJxi13XXTtrW83SXLOpC2S957bzoh627ebl+m\nRSkARBodlOttCM1u2K3eXFarlbv/6tU+su5wTd4bkPWIizIeo5HUrkd11NPOA1S9dZ68szdPMByL\n5d8Ozdx9P9YabW/tPhPvlLG+D6BrNeqhUO/7Zp8CmtYFtV67Z1Y7ZStPLazX0yLr/a21bbc7du84\n6BpG2U+t7MS7Fdr5NNesVyQCBw7+9Z5vn7zYv/2y7Vy/6Ppx957hVt7oNVbqdQvv9IMMu0VPB4BG\nO3m975vZsONeEBfceAHAgQus1551beo04l4wWWofRdzoqZB5urUmL1ivPHsl6zaty1VbGd45XNpO\n0Yl3K7TzAFXkBCPNeHzPcBlPEC2q3j5Su0yBUh4Z3SuPip7YOMn4tWL+irrXAOodBOINuNkLlPGZ\nRJ5pZF1AiwNTnmnX5pPV5LXf949q8oiDVR5pebZicNpg3fK2Q9712Iqs8iS/7xX1mkviXl2dmq88\n+0ha2jR5366XN89u19PXACB/L6BWFWkDbdRG2cq06z36efW5q1t6qmne8jTSiWsA7VSkk8F40sx6\nG+t29Wb2kbIeGd3N1z1A1wCAg5tl1ixew8sffflAn+2yNsqsM4bhncMNq4aNqs+tNJlkNQft832j\n2jizXnSTpYxmmm5oLihT7Q2FyXsExnO5ajXz2syxbldvZh9ptA3nbRrsRFNmu/RkDWAszzQbnVW0\n40UueV7usfRLS1MvACffUZD20o929+uW8SdvT6pOnBmXVQNoplVANYAuN5a9TRpdgC2Sb6sXMJfM\nW5J5A1j85MXhncOsvm81K+avGHXRsZt7dUhn5O1J1e0X+bPSrlm8pqlWgV7aR3oyAIzlhpjnuUKt\n5lukySTPBci04NRrzTRSXN4DXju73GZpZnstum0nm5Vrn6k1XveRnmwCamcVrZW7TjtRNcx7Y1o3\n3LFZq1dv0BnPij6yY7yvv/FUtso3AbWritboIlc3VQ1rz3ayHqg2ll0V8/Sd1g063SnPPQq9XHvs\nxE2MY6EnA0C7NsRGG0G37QDJnTbryZtjFZzyHth7dUergl6uufVSz5+knmwCapdufvBVHp3cQfM2\nj433ZVxV46mJpBXd1LzbSOWbgNqlExe5ytTJRw3kPYMa78u4qnq95tZNzbtlUgBoQq9uBGMh74Fd\ny3h86tUmkli3Ne+WpaefBVS2dj5XptelPZcp7cCuZTw+deI5T2Otk888ahddA5Ax08sXCauu168B\njCd6IYyIjDkF+O6gACAiUlHqBSQiIg0pAIiIVFQlA0CvvM5NRKSIynUD7aXXuYmIFFGoBmBm083s\nVjPbFP0/IiPd9Wb2tJk9UCS/MvT6HYsiInkVbQK6HFjv7nOB9dHnNKuAMwvmlVu9Jp5ev2NRRCSv\nogFgEbA6Gl4NnJOWyN2/DjxTMK9cGj11Us+aEREJigaAme6+NRp+EphZcHqY2TIz22BmG7Zt29b0\n7xs18ehZMyIiQcOLwGZ2G3B0ylejGs3d3c2s8F1l7r4SWAnhRrBmf9+oiUfPmhERCRoGAHdfkPWd\nmT1lZrPcfauZzQKeLnXuWpDnoVS9+FAnEZFmFW0CuhlYGg0vBW4qOL3C1MQjIpJP0QBwNXCGmW0C\nFkSfMbNjzGxdnMjMPgd8G/hZM3vMzN5XMN9MvfrcbhGRsulhcF1CT1IUkTI08zC4yt0J3I10d7KI\ndEIlnwXUbXR3soh0ggJAF9DdySLSCQoAXUB3J4tIJygAdAF1XRWRTlAA6ALquioinaBuoCIiPUTv\nBBYRkYYUAEREKkoBQESkohQAREQqSgFARKSiFABERCpKAUBEpKIUAEREKkoBQESkohQAREQqSgFA\nRKSiFABERCpKAUBEpKIUAEREKkoBQESkohQAREQqSgFARKSiCgUAM5tuZrea2abo/xEpaY43s9vN\n7CEze9C+zl8AAAAJ3klEQVTMLimSp4iIlKNoDeByYL27zwXWR59rvQz8sbufCLwZeL+ZnVgwXxER\nKahoAFgErI6GVwPn1CZw963ufnc0vAt4GDi2YL4iIlJQ0QAw0923RsNPAjPrJTazOcDrge/USbPM\nzDaY2YZt27YVnD0REckysVECM7sNODrlq+XJD+7uZuZ1pjMV+CJwqbs/l5XO3VcCKwGGhoYypyci\nIsU0DADuviDrOzN7ysxmuftWM5sFPJ2Rrp9w8F/r7je2PLciIlKaok1ANwNLo+GlwE21CczMgM8A\nD7v7JwrmJyIiJSkaAK4GzjCzTcCC6DNmdoyZrYvSnApcALzNzO6N/hYWzFdERApq2ARUj7vvAOan\njH8CWBgNfxOwIvmIiEj5dCewiEhFKQCIiFSUAoCISEUpAIiIVJQCgIhIRSkAiIhUlAKAiEhFKQCI\niFSUAoCISEUpAIiIVJQCgIhIRSkAiIhUlAKAiEhFKQCIiFSUAoCISEUpAIiIVJQCgIhIRSkAiIhU\nlAKAiEhFKQCIiFSUAoCISEUpAIiIVJQCgIhIRSkAiIhUVKEAYGbTzexWM9sU/T8iJc0rzOy7Znaf\nmT1oZlcVyVNERMpRtAZwObDe3ecC66PPtV4C3ubuJwEnA2ea2ZsL5isiIgUVDQCLgNXR8GrgnNoE\nHjwffeyP/rxgviIiUlDRADDT3bdGw08CM9MSmVmfmd0LPA3c6u7fyZqgmS0zsw1mtmHbtm0FZ09E\nRLJMbJTAzG4Djk75annyg7u7maWe2bv7PuBkMzsc+JKZvdbdH8hIuxJYCTA0NKSagohImzQMAO6+\nIOs7M3vKzGa5+1Yzm0U4w683rZ+Y2e3AmUBqABARkbFRtAnoZmBpNLwUuKk2gZkdGZ35Y2aTgTOA\n7xXMV0RECioaAK4GzjCzTcCC6DNmdoyZrYvSzAJuN7P7gTsJ1wC+XDBfEREpqGETUD3uvgOYnzL+\nCWBhNHw/8Poi+YiISPl0J7CISEUpAIiIVJQCgIhIRSkAiIhUlAKAiEhFKQCIiFSUAoCISEUpAIiI\nVJQCgIhIRSkAiIhUlAKAiEhFKQCIiFSUAoCISEUpAIiIVJQCgIhIRSkAiIhUlAKAiEhFKQCIiFSU\nAoCISEUpAIiIVJQCgIhIRSkAiIhUlAKAiEhFFQoAZjbdzG41s03R/yPqpO0zs3vM7MtF8hQRkXIU\nrQFcDqx397nA+uhzlkuAhwvmJyIiJSkaABYBq6Ph1cA5aYnM7Djg7cDfF8xPRERKUjQAzHT3rdHw\nk8DMjHTXAJcB+wvmJyIiJZnYKIGZ3QYcnfLV8uQHd3cz85TfvwN42t3vMrPTc+S3DFgGMDg42Ci5\niIi0qGEAcPcFWd+Z2VNmNsvdt5rZLODplGSnAr9uZguBVwCHmdkadz8/I7+VwEqAoaGhgwKKiIiU\no2gT0M3A0mh4KXBTbQJ3/7C7H+fuc4DzgK9lHfxFRGTsFA0AVwNnmNkmYEH0GTM7xszWFZ05ERFp\nn4ZNQPW4+w5gfsr4J4CFKePvAO4okqeIiJRDdwKLiFSUAoCISEUpAIiIVJQCgIhIRSkAiIhUlAKA\niEhFKQCIiFSUAoCISEUpAIiIVJQCgIhIRSkAiIhUlAKAiEhFKQCIiFSUAoCISEUpAIiIVJQCgIhI\nRSkAiIhUlAKAiEhFKQCIiFSUAoCISEUpAIiIdIm1G9cy55o5TLhqAnOumcPajWvbmt/Etk5dRERy\nWbtxLctuWcaLe18EYHjnMMtuWQbAknlL2pKnagAiIl1g+frlBw7+sRf3vsjy9cvblqcCgIhIF9iy\nc0tT48tQKACY2XQzu9XMNkX/j8hIt9nMNprZvWa2oUieIiK9aHDaYFPjy1C0BnA5sN7d5wLro89Z\nftXdT3b3oYJ5ioj0nBXzVzClf8qocVP6p7Bi/oq25Vk0ACwCVkfDq4FzCk5PRKSSlsxbwsqzVzJ7\n2mwMY/a02aw8e2XbLgADmLu3/mOzn7j74dGwAc/Gn2vS/QjYCewDPu3uK+tMcxmwDGBwcPCNw8PD\nLc+fiEjVmNldeVtaGnYDNbPbgKNTvhp1adrd3cyyoslp7v64mR0F3Gpm33P3r6cljILDSoChoaHW\no5OIiNTVMAC4+4Ks78zsKTOb5e5bzWwW8HTGNB6P/j9tZl8CTgFSA4CIiIyNotcAbgaWRsNLgZtq\nE5jZoWb2yngY+DXggYL5iohIQUUDwNXAGWa2CVgQfcbMjjGzdVGamcA3zew+4LvAv7n7vxfMV0RE\nCir0KAh33wHMTxn/BLAwGv4hcFKRfEREpHyFegG1m5ltA1rtBjQD2F7i7IwXVS03qOxVLHtVyw3Z\nZZ/t7kfmmUBXB4AizGxDFW86q2q5QWWvYtmrWm4op+x6FpCISEUpAIiIVFQvB4DMu417XFXLDSp7\nFVW13FBC2Xv2GoCIiNTXyzUAERGpQwFARKSiei4AmNmZZvaImT1qZvXeT9AT0l62k/dFPeONmV1v\nZk+b2QOJcZllNbMPR9vBI2b2Pzoz18VllPtKM3s8Wu/3mtnCxHe9Uu7jzex2M3vIzB40s0ui8VVY\n51llL3e9u3vP/AF9wA+AVwGTgPuAEzs9X20u82ZgRs24jwOXR8OXA3/e6fksqay/ArwBeKBRWYET\no/V/CHBCtF30dboMJZb7SuCDKWl7qdyzgDdEw68Evh+VrwrrPKvspa73XqsBnAI86u4/dPc9wOcJ\nL62pmp58UY+HR4g/UzM6q6yLgM+7+0vu/iPgUcL2Me5klDtLL5V7q7vfHQ3vAh4GjqUa6zyr7Fla\nKnuvBYBjgR8nPj9G/YXWCxy4zczuil6mAzDT3bdGw08SHsjXq7LKWoVt4QNmdn/URBQ3g/Rkuc1s\nDvB64DtUbJ3XlB1KXO+9FgCq6DR3Pxk4C3i/mf1K8ksP9cNK9PWtUlmBTxKaOk8GtgJ/1dnZaR8z\nmwp8EbjU3Z9Lftfr6zyl7KWu914LAI8Dxyc+HxeN61meeNkOEL9s56noBT3Ue1FPj8gqa09vC+7+\nlLvvc/f9wN8xUt3vqXKbWT/hALjW3W+MRldinaeVvez13msB4E5grpmdYGaTgPMIL63pSXVettPw\nRT09JKusNwPnmdkhZnYCMJfwPoqeEB8AI+cy8pKlnil39J7xzwAPu/snEl/1/DrPKnvp673TV7vb\ncPV8IeGK+Q+A5Z2enzaX9VWEK//3AQ/G5QUGgPXAJuA2YHqn57Wk8n6OUO3dS2jjfF+9shLeW/0D\n4BHgrE7Pf8nlvgHYCNwf7fyzerDcpxGad+4H7o3+FlZknWeVvdT1rkdBiIhUVK81AYmISE4KACIi\nFaUAICJSUQoAIiIVpQAgIlJRCgAiIhWlACAiUlH/H9QCpT8r7NgOAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x162db3978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Jet  0 *****accuracy jet 82.7429863982\n",
      "**** Starting Jet  1 *****\n",
      "Gradient Descent(0/499): loss=0.5000000000000115, w0=0.0003949255898895035, w1=-0.002870062449644063\n",
      "Gradient Descent(1/499): loss=0.47667491168944576, w0=0.0007525025631038378, w1=-0.00529430216734861\n",
      "Gradient Descent(2/499): loss=0.4625156526930503, w0=0.0011113512996971308, w1=-0.0073585912706762065\n",
      "Gradient Descent(3/499): loss=0.45270662777388837, w0=0.0014771618484380774, w1=-0.009134318467178285\n",
      "Gradient Descent(4/499): loss=0.4453720773883114, w0=0.0018450176280977561, w1=-0.01067885581539123\n",
      "Gradient Descent(5/499): loss=0.4395917517588707, w0=0.0022081716923546334, w1=-0.012037578947658736\n",
      "Gradient Descent(6/499): loss=0.43484594437245283, w0=0.0025610690795439947, w1=-0.013246136661279103\n",
      "Gradient Descent(7/499): loss=0.4308183134987158, w0=0.00290004165194421, w1=-0.014332495593800617\n",
      "Gradient Descent(8/499): loss=0.4273076078542932, w0=0.003223150156044803, w1=-0.015318631519798316\n",
      "Gradient Descent(9/499): loss=0.42418160652248305, w0=0.0035297846444699746, w1=-0.016221873096624394\n",
      "Gradient Descent(10/499): loss=0.4213510149551212, w0=0.0038202586198048654, w1=-0.017055948149144566\n",
      "Gradient Descent(11/499): loss=0.4187539791339081, w0=0.004095473461877447, w1=-0.017831790379503124\n",
      "Gradient Descent(12/499): loss=0.4163466128506488, w0=0.0043566661544392465, w1=-0.018558158862387722\n",
      "Gradient Descent(13/499): loss=0.4140970561986739, w0=0.004605230661001137, w1=-0.019242113488576313\n",
      "Gradient Descent(14/499): loss=0.41198166348552645, w0=0.004842597474947171, w1=-0.019889380401511992\n",
      "Gradient Descent(15/499): loss=0.40998250539791303, w0=0.005070156443924448, w1=-0.02050463367515581\n",
      "Gradient Descent(16/499): loss=0.4080857002768844, w0=0.005289210544924672, w1=-0.021091713231591543\n",
      "Gradient Descent(17/499): loss=0.4062802794772331, w0=0.005500951117006728, w1=-0.021653794147336623\n",
      "Gradient Descent(18/499): loss=0.404557403552017, w0=0.005706447531112911, w1=-0.02219351879878983\n",
      "Gradient Descent(19/499): loss=0.40290981303637574, w0=0.005906646244606637, w1=-0.02271310050213943\n",
      "Gradient Descent(20/499): loss=0.4013314386370222, w0=0.0061023756811122406, w1=-0.023214405200024702\n",
      "Gradient Descent(21/499): loss=0.3998171212723979, w0=0.006294354476436815, w1=-0.023699016167444582\n",
      "Gradient Descent(22/499): loss=0.3983624087549095, w0=0.006483201426384864, w1=-0.024168285522421864\n",
      "Gradient Descent(23/499): loss=0.3969634065300265, w0=0.0066694460381538714, w1=-0.02462337543384887\n",
      "Gradient Descent(24/499): loss=0.395616666913458, w0=0.006853538984392976, w1=-0.02506529124550168\n",
      "Gradient Descent(25/499): loss=0.3943191059886558, w0=0.007035862034299261, w1=-0.02549490822596258\n",
      "Gradient Descent(26/499): loss=0.3930679405427854, w0=0.007216737223903064, w1=-0.025912993267834923\n",
      "Gradient Descent(27/499): loss=0.39186063963598383, w0=0.007396435153345462, w1=-0.026320222565438336\n",
      "Gradient Descent(28/499): loss=0.3906948869421623, w0=0.007575182380916677, w1=-0.02671719607525766\n",
      "Gradient Descent(29/499): loss=0.3895685510835719, w0=0.007753167935184859, w1=-0.027104449390739832\n",
      "Gradient Descent(30/499): loss=0.38847966194830985, w0=0.007930548997199418, w1=-0.027482463529863662\n",
      "Gradient Descent(31/499): loss=0.38742639152612435, w0=0.008107455821249535, w1=-0.02785167303073119\n",
      "Gradient Descent(32/499): loss=0.3864070381891235, w0=0.008283995969755775, w1=-0.028212472670108443\n",
      "Gradient Descent(33/499): loss=0.38542001362573797, w0=0.008460257938900028, w1=-0.02856522305701124\n",
      "Gradient Descent(34/499): loss=0.38446383184024246, w0=0.008636314248862232, w1=-0.028910255304039546\n",
      "Gradient Descent(35/499): loss=0.38353709977849626, w0=0.008812224067609283, w1=-0.02924787494014753\n",
      "Gradient Descent(36/499): loss=0.3826385092490271, w0=0.00898803543113459, w1=-0.02957836519756731\n",
      "Gradient Descent(37/499): loss=0.38176682988829713, w0=0.0091637871165759, w1=-0.029901989780907034\n",
      "Gradient Descent(38/499): loss=0.3809209029778662, w0=0.00933951021819194, w1=-0.030218995206657835\n",
      "Gradient Descent(39/499): loss=0.38009963596493057, w0=0.009515229470028093, w1=-0.03052961278542327\n",
      "Gradient Descent(40/499): loss=0.3793019975704074, w0=0.009690964353402392, w1=-0.030834060306320073\n",
      "Gradient Descent(41/499): loss=0.37852701339333855, w0=0.009866730022170344, w1=-0.031132543472562824\n",
      "Gradient Descent(42/499): loss=0.3777737619390172, w0=0.010042538074103465, w1=-0.031425257128747185\n",
      "Gradient Descent(43/499): loss=0.37704137101245017, w0=0.01021839719263207, w1=-0.031712386313402924\n",
      "Gradient Descent(44/499): loss=0.37632901442969524, w0=0.010394313679628049, w1=-0.03199410716469619\n",
      "Gradient Descent(45/499): loss=0.3756359090080843, w0=0.010570291896797658, w1=-0.03227058770248096\n",
      "Gradient Descent(46/499): loss=0.3749613118029518, w0=0.010746334630572708, w1=-0.03254198850604181\n",
      "Gradient Descent(47/499): loss=0.3743045175637176, w0=0.010922443393084224, w1=-0.03280846330368168\n",
      "Gradient Descent(48/499): loss=0.3736648563863204, w0=0.011098618669831268, w1=-0.033070159487667325\n",
      "Gradient Descent(49/499): loss=0.37304169154233535, w0=0.011274860122976812, w1=-0.03332721856585261\n",
      "Gradient Descent(50/499): loss=0.37243441746781575, w0=0.011451166757773926, w1=-0.033579776559476776\n",
      "Gradient Descent(51/499): loss=0.37184245789711323, w0=0.011627537058414458, w1=-0.03382796435511567\n",
      "Gradient Descent(52/499): loss=0.3712652641287611, w0=0.011803969098568097, w1=-0.03407190801749659\n",
      "Gradient Descent(53/499): loss=0.3707023134120316, w0=0.011980460631015155, w1=-0.03431172906882852\n",
      "Gradient Descent(54/499): loss=0.3701531074440607, w0=0.012157009160048027, w1=-0.034547544739413155\n",
      "Gradient Descent(55/499): loss=0.36961717096852836, w0=0.012333611999703637, w1=-0.03477946819356029\n",
      "Gradient Descent(56/499): loss=0.36909405046780913, w0=0.012510266320374738, w1=-0.03500760873420822\n",
      "Gradient Descent(57/499): loss=0.3685833129413172, w0=0.012686969185916401, w1=-0.03523207198912762\n",
      "Gradient Descent(58/499): loss=0.36808454476346575, w0=0.012863717583002778, w1=-0.035452960081147934\n",
      "Gradient Descent(59/499): loss=0.3675973506152711, w0=0.013040508444186983, w1=-0.03567037178447621\n",
      "Gradient Descent(60/499): loss=0.3671213524841724, w0=0.01321733866586452, w1=-0.035884402668867166\n",
      "Gradient Descent(61/499): loss=0.36665618872710837, w0=0.013394205122130104, w1=-0.03609514523314139\n",
      "Gradient Descent(62/499): loss=0.3662015131923202, w0=0.013571104675342276, w1=-0.03630268902932768\n",
      "Gradient Descent(63/499): loss=0.3657569943957218, w0=0.013748034184064239, w1=-0.036507120778518906\n",
      "Gradient Descent(64/499): loss=0.36532231474802535, w0=0.013924990508928052, w1=-0.03670852447937347\n",
      "Gradient Descent(65/499): loss=0.3648971698291053, w0=0.01410197051686864, w1=-0.03690698151006103\n",
      "Gradient Descent(66/499): loss=0.36448126770636813, w0=0.014278971084090765, w1=-0.037102570724338674\n",
      "Gradient Descent(67/499): loss=0.3640743282941426, w0=0.014455989098063119, w1=-0.03729536854234827\n",
      "Gradient Descent(68/499): loss=0.36367608275132957, w0=0.014633021458776764, w1=-0.037485449036644594\n",
      "Gradient Descent(69/499): loss=0.36328627291476157, w0=0.014810065079458277, w1=-0.03767288401389544\n",
      "Gradient Descent(70/499): loss=0.36290465076590933, w0=0.014987116886889374, w1=-0.037857743092636446\n",
      "Gradient Descent(71/499): loss=0.36253097792874667, w0=0.015164173821453183, w1=-0.038040093777413804\n",
      "Gradient Descent(72/499): loss=0.3621650251967419, w0=0.015341232837001468, w1=-0.03822000152960562\n",
      "Gradient Descent(73/499): loss=0.36180657208709355, w0=0.01551829090061603, w1=-0.03839752983517698\n",
      "Gradient Descent(74/499): loss=0.36145540642045454, w0=0.015695344992320375, w1=-0.03857274026959269\n",
      "Gradient Descent(75/499): loss=0.36111132392452433, w0=0.01587239210478387, w1=-0.03874569256008551\n",
      "Gradient Descent(76/499): loss=0.3607741278599884, w0=0.0160494292430495, w1=-0.038916444645455205\n",
      "Gradient Descent(77/499): loss=0.3604436286674012, w0=0.01622645342430732, w1=-0.03908505273355421\n",
      "Gradient Descent(78/499): loss=0.3601196436336976, w0=0.016403461677728666, w1=-0.03925157135659937\n",
      "Gradient Descent(79/499): loss=0.35980199657711354, w0=0.016580451044370538, w1=-0.039416053424434494\n",
      "Gradient Descent(80/499): loss=0.359490517549373, w0=0.0167574185771552, w1=-0.03957855027585656\n",
      "Gradient Descent(81/499): loss=0.35918504255408223, w0=0.01693436134092663, w1=-0.03973911172810719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(82/499): loss=0.358885413280339, w0=0.017111276412582914, w1=-0.03989778612462224\n",
      "Gradient Descent(83/499): loss=0.35859147685063086, w0=0.017288160881281777, w1=-0.0400546203811237\n",
      "Gradient Descent(84/499): loss=0.358303085582159, w0=0.017465011848714962, w1=-0.04020966003013162\n",
      "Gradient Descent(85/499): loss=0.3580200967607815, w0=0.017641826429446325, w1=-0.04036294926396709\n",
      "Gradient Descent(86/499): loss=0.35774237242682094, w0=0.017818601751307832, w1=-0.04051453097631221\n",
      "Gradient Descent(87/499): loss=0.3574697791720318, w0=0.017995334955847226, w1=-0.040664446802387996\n",
      "Gradient Descent(88/499): loss=0.3572021879470687, w0=0.018172023198821108, w1=-0.04081273715780707\n",
      "Gradient Descent(89/499): loss=0.3569394738788371, w0=0.018348663650727053, w1=-0.04095944127615411\n",
      "Gradient Descent(90/499): loss=0.3566815160971518, w0=0.018525253497368586, w1=-0.04110459724534383\n",
      "Gradient Descent(91/499): loss=0.35642819757015964, w0=0.018701789940447022, w1=-0.04124824204280295\n",
      "Gradient Descent(92/499): loss=0.35617940494802375, w0=0.018878270198174514, w1=-0.041390411569520474\n",
      "Gradient Descent(93/499): loss=0.3559350284143923, w0=0.019054691505902874, w1=-0.04153114068300743\n",
      "Gradient Descent(94/499): loss=0.35569496154520885, w0=0.019231051116763206, w1=-0.04167046322920578\n",
      "Gradient Descent(95/499): loss=0.35545910117444784, w0=0.01940734630231169, w1=-0.04180841207338355\n",
      "Gradient Descent(96/499): loss=0.355227347266383, w0=0.019583574353177247, w1=-0.04194501913005193\n",
      "Gradient Descent(97/499): loss=0.35499960279402587, w0=0.019759732579707204, w1=-0.04208031539193795\n",
      "Gradient Descent(98/499): loss=0.35477577362338525, w0=0.019935818312607448, w1=-0.04221433095804524\n",
      "Gradient Descent(99/499): loss=0.35455576840322983, w0=0.020111828903573884, w1=-0.04234709506083361\n",
      "Gradient Descent(100/499): loss=0.3543394984600483, w0=0.020287761725912425, w1=-0.04247863609254698\n",
      "Gradient Descent(101/499): loss=0.35412687769792317, w0=0.020463614175144976, w1=-0.042608981630718086\n",
      "Gradient Descent(102/499): loss=0.35391782250304993, w0=0.020639383669599252, w1=-0.042738158462877075\n",
      "Gradient Descent(103/499): loss=0.3537122516526514, w0=0.02081506765098054, w1=-0.04286619261049\n",
      "Gradient Descent(104/499): loss=0.3535100862280515, w0=0.02099066358492375, w1=-0.04299310935215248\n",
      "Gradient Descent(105/499): loss=0.35331124953168297, w0=0.021166168961524436, w1=-0.04311893324606249\n",
      "Gradient Descent(106/499): loss=0.3531156670078244, w0=0.021341581295847546, w1=-0.04324368815179563\n",
      "Gradient Descent(107/499): loss=0.35292326616686664, w0=0.021516898128413082, w1=-0.043367397251405246\n",
      "Gradient Descent(108/499): loss=0.3527339765129257, w0=0.021692117025657813, w1=-0.04349008306986896\n",
      "Gradient Descent(109/499): loss=0.3525477294746252, w0=0.021867235580372558, w1=-0.04361176749490246\n",
      "Gradient Descent(110/499): loss=0.35236445833888697, w0=0.022042251412114598, w1=-0.043732471796160695\n",
      "Gradient Descent(111/499): loss=0.35218409818757324, w0=0.02221716216759496, w1=-0.04385221664384576\n",
      "Gradient Descent(112/499): loss=0.35200658583683564, w0=0.022391965521040426, w1=-0.043971022126740394\n",
      "Gradient Descent(113/499): loss=0.3518318597790334, w0=0.022566659174530296, w1=-0.044088907769685014\n",
      "Gradient Descent(114/499): loss=0.3516598601270903, w0=0.02274124085830794, w1=-0.044205892550515935\n",
      "Gradient Descent(115/499): loss=0.35149052856116864, w0=0.022915708331067375, w1=-0.04432199491648164\n",
      "Gradient Descent(116/499): loss=0.351323808277547, w0=0.023090059380215067, w1=-0.04443723280015347\n",
      "Gradient Descent(117/499): loss=0.3511596439395886, w0=0.023264291822107364, w1=-0.044551623634846584\n",
      "Gradient Descent(118/499): loss=0.3509979816307013, w0=0.023438403502263904, w1=-0.04466518436956647\n",
      "Gradient Descent(119/499): loss=0.3508387688091902, w0=0.02361239229555748, w1=-0.044777931483495875\n",
      "Gradient Descent(120/499): loss=0.35068195426491244, w0=0.023786256106380847, w1=-0.04488988100003646\n",
      "Gradient Descent(121/499): loss=0.35052748807764555, w0=0.023959992868791, w1=-0.0450010485004191\n",
      "Gradient Descent(122/499): loss=0.3503753215770893, w0=0.02413360054663154, w1=-0.04511144913689617\n",
      "Gradient Descent(123/499): loss=0.35022540730442386, w0=0.024307077133633663, w1=-0.045221097645529035\n",
      "Gradient Descent(124/499): loss=0.3500776989753488, w0=0.02448042065349644, w1=-0.045330008358583036\n",
      "Gradient Descent(125/499): loss=0.3499321514445349, w0=0.02465362915994701, w1=-0.04543819521654249\n",
      "Gradient Descent(126/499): loss=0.3497887206714241, w0=0.024826700736781344, w1=-0.0455456717797573\n",
      "Gradient Descent(127/499): loss=0.34964736368731486, w0=0.02499963349788623, w1=-0.045652451239732696\n",
      "Gradient Descent(128/499): loss=0.3495080385636714, w0=0.025172425587243145, w1=-0.045758546430073155\n",
      "Gradient Descent(129/499): loss=0.349370704381607, w0=0.025345075178914715, w1=-0.04586396983709131\n",
      "Gradient Descent(130/499): loss=0.34923532120248146, w0=0.02551758047701439, w1=-0.04596873361009214\n",
      "Gradient Descent(131/499): loss=0.3491018500395696, w0=0.025689939715660033, w1=-0.04607284957134256\n",
      "Gradient Descent(132/499): loss=0.34897025283074684, w0=0.025862151158912054, w1=-0.046176329225736217\n",
      "Gradient Descent(133/499): loss=0.34884049241215315, w0=0.02603421310069679, w1=-0.04627918377016281\n",
      "Gradient Descent(134/499): loss=0.3487125324927869, w0=0.02620612386471575, w1=-0.04638142410259126\n",
      "Gradient Descent(135/499): loss=0.3485863376299919, w0=0.026377881804341353, w1=-0.046483060830875444\n",
      "Gradient Descent(136/499): loss=0.34846187320579824, w0=0.026549485302499825, w1=-0.04658410428129118\n",
      "Gradient Descent(137/499): loss=0.3483391054040792, w0=0.026720932771541847, w1=-0.046684564506812756\n",
      "Gradient Descent(138/499): loss=0.34821800118849094, w0=0.02689222265310155, w1=-0.046784451295137086\n",
      "Gradient Descent(139/499): loss=0.3480985282811615, w0=0.02706335341794447, w1=-0.04688377417646328\n",
      "Gradient Descent(140/499): loss=0.3479806551420977, w0=0.027234323565805043, w1=-0.0469825424310352\n",
      "Gradient Descent(141/499): loss=0.3478643509492797, w0=0.027405131625214164, w1=-0.047080765096454424\n",
      "Gradient Descent(142/499): loss=0.34774958557941604, w0=0.02757577615331741, w1=-0.04717845097477055\n",
      "Gradient Descent(143/499): loss=0.34763632958933055, w0=0.027746255735684407, w1=-0.04727560863935594\n",
      "Gradient Descent(144/499): loss=0.34752455419795714, w0=0.027916568986109908, w1=-0.047372246441571356\n",
      "Gradient Descent(145/499): loss=0.3474142312689176, w0=0.02808671454640705, w1=-0.047468372517229185\n",
      "Gradient Descent(146/499): loss=0.3473053332936571, w0=0.02825669108619328, w1=-0.0475639947928603\n",
      "Gradient Descent(147/499): loss=0.3471978333751181, w0=0.028426497302669426, w1=-0.047659120991790774\n",
      "Gradient Descent(148/499): loss=0.3470917052119311, w0=0.02859613192039239, w1=-0.04775375864003429\n",
      "Gradient Descent(149/499): loss=0.34698692308309964, w0=0.028765593691041855, w1=-0.047847915072005864\n",
      "Gradient Descent(150/499): loss=0.34688346183316254, w0=0.02893488139318145, w1=-0.04794159743606254\n",
      "Gradient Descent(151/499): loss=0.34678129685781584, w0=0.02910399383201482, w1=-0.04803481269987633\n",
      "Gradient Descent(152/499): loss=0.34668040408997375, w0=0.029272929839136937, w1=-0.04812756765564452\n",
      "Gradient Descent(153/499): loss=0.34658075998625537, w0=0.02944168827228105, w1=-0.04821986892514254\n",
      "Gradient Descent(154/499): loss=0.34648234151387847, w0=0.029610268015061666, w1=-0.04831172296462404\n",
      "Gradient Descent(155/499): loss=0.3463851261379484, w0=0.029778667976713885, w1=-0.04840313606957312\n",
      "Gradient Descent(156/499): loss=0.34628909180912326, w0=0.029946887091829417, w1=-0.04849411437931302\n",
      "Gradient Descent(157/499): loss=0.3461942169516448, w0=0.03011492432008964, w1=-0.04858466388147602\n",
      "Gradient Descent(158/499): loss=0.34610048045172204, w0=0.030282778645995975, w1=-0.048674790416338445\n",
      "Gradient Descent(159/499): loss=0.3460078616462513, w0=0.0304504490785979, w1=-0.04876449968102535\n",
      "Gradient Descent(160/499): loss=0.3459163403118649, w0=0.030617934651218856, w1=-0.048853797233588535\n",
      "Gradient Descent(161/499): loss=0.3458258966542941, w0=0.030785234421180363, w1=-0.04894268849696207\n",
      "Gradient Descent(162/499): loss=0.3457365112980353, w0=0.030952347469524556, w1=-0.0490311787627989\n",
      "Gradient Descent(163/499): loss=0.3456481652763115, w0=0.03111927290073542, w1=-0.049119273195192395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(164/499): loss=0.3455608400213145, w0=0.03128600984245897, w1=-0.04920697683428617\n",
      "Gradient Descent(165/499): loss=0.34547451735472223, w0=0.03145255744522256, w1=-0.0492942945997758\n",
      "Gradient Descent(166/499): loss=0.34538917947847964, w0=0.0316189148821536, w1=-0.04938123129430569\n",
      "Gradient Descent(167/499): loss=0.34530480896583426, w0=0.0317850813486978, w1=-0.04946779160676424\n",
      "Gradient Descent(168/499): loss=0.3452213887526181, w0=0.03195105606233724, w1=-0.049553980115480616\n",
      "Gradient Descent(169/499): loss=0.3451389021287692, w0=0.03211683826230836, w1=-0.04963980129132599\n",
      "Gradient Descent(170/499): loss=0.3450573327300819, w0=0.03228242720932012, w1=-0.049725259500722256\n",
      "Gradient Descent(171/499): loss=0.3449766645301803, w0=0.032447822185272424, w1=-0.04981035900856107\n",
      "Gradient Descent(172/499): loss=0.34489688183270734, w0=0.03261302249297504, w1=-0.04989510398103592\n",
      "Gradient Descent(173/499): loss=0.3448179692637231, w0=0.03277802745586714, w1=-0.04997949848839\n",
      "Gradient Descent(174/499): loss=0.3447399117643031, w0=0.03294283641773754, w1=-0.05006354650758229\n",
      "Gradient Descent(175/499): loss=0.344662694583334, w0=0.033107448742445936, w1=-0.05014725192487458\n",
      "Gradient Descent(176/499): loss=0.34458630327049594, w0=0.033271863813645046, w1=-0.05023061853834165\n",
      "Gradient Descent(177/499): loss=0.3445107236694315, w0=0.03343608103450397, w1=-0.05031365006030712\n",
      "Gradient Descent(178/499): loss=0.3444359419110873, w0=0.03360009982743278, w1=-0.05039635011970715\n",
      "Gradient Descent(179/499): loss=0.34436194440723283, w0=0.0337639196338085, w1=-0.050478722264384226\n",
      "Gradient Descent(180/499): loss=0.3442887178441418, w0=0.03392753991370249, w1=-0.05056076996331317\n",
      "Gradient Descent(181/499): loss=0.34421624917643634, w0=0.03409096014560944, w1=-0.05064249660876151\n",
      "Gradient Descent(182/499): loss=0.3441445256210892, w0=0.03425417982617801, w1=-0.050723905518386056\n",
      "Gradient Descent(183/499): loss=0.3440735346515748, w0=0.03441719846994318, w1=-0.05080499993726786\n",
      "Gradient Descent(184/499): loss=0.3440032639921703, w0=0.03458001560906041, w1=-0.05088578303988719\n",
      "Gradient Descent(185/499): loss=0.34393370161239845, w0=0.03474263079304172, w1=-0.050966257932040564\n",
      "Gradient Descent(186/499): loss=0.34386483572160875, w0=0.034905043588493646, w1=-0.05104642765270149\n",
      "Gradient Descent(187/499): loss=0.34379665476369403, w0=0.035067253578857295, w1=-0.05112629517582664\n",
      "Gradient Descent(188/499): loss=0.34372914741193666, w0=0.035229260364150396, w1=-0.05120586341210918\n",
      "Gradient Descent(189/499): loss=0.34366230256398345, w0=0.03539106356071155, w1=-0.051285135210680774\n",
      "Gradient Descent(190/499): loss=0.3435961093369407, w0=0.03555266280094658, w1=-0.051364113360763936\n",
      "Gradient Descent(191/499): loss=0.34353055706259217, w0=0.03571405773307717, w1=-0.05144280059327612\n",
      "Gradient Descent(192/499): loss=0.3434656352827312, w0=0.035875248020891724, w1=-0.05152119958238711\n",
      "Gradient Descent(193/499): loss=0.3434013337446065, w0=0.03603623334349855, w1=-0.051599312947031045\n",
      "Gradient Descent(194/499): loss=0.3433376423964776, w0=0.03619701339508137, w1=-0.051677143252374574\n",
      "Gradient Descent(195/499): loss=0.343274551383277, w0=0.03635758788465721, w1=-0.05175469301124233\n",
      "Gradient Descent(196/499): loss=0.34321205104237607, w0=0.036517956535836706, w1=-0.051831964685501115\n",
      "Gradient Descent(197/499): loss=0.3431501318994517, w0=0.036678119086586775, w1=-0.05190896068740401\n",
      "Gradient Descent(198/499): loss=0.34308878466445125, w0=0.03683807528899581, w1=-0.05198568338089563\n",
      "Gradient Descent(199/499): loss=0.3430280002276528, w0=0.03699782490904129, w1=-0.0520621350828797\n",
      "Gradient Descent(200/499): loss=0.3429677696558181, w0=0.0371573677263599, w1=-0.05213831806445007\n",
      "Gradient Descent(201/499): loss=0.34290808418843594, w0=0.037316703534020126, w1=-0.05221423455208633\n",
      "Gradient Descent(202/499): loss=0.34284893523405324, w0=0.037475832138297416, w1=-0.05228988672881501\n",
      "Gradient Descent(203/499): loss=0.3427903143666915, w0=0.037634753358451796, w1=-0.05236527673533747\n",
      "Gradient Descent(204/499): loss=0.34273221332234804, w0=0.037793467026508064, w1=-0.05244040667112544\n",
      "Gradient Descent(205/499): loss=0.34267462399557497, w0=0.03795197298703851, w1=-0.052515278595485264\n",
      "Gradient Descent(206/499): loss=0.34261753843614096, w0=0.03811027109694818, w1=-0.05258989452859161\n",
      "Gradient Descent(207/499): loss=0.34256094884576666, w0=0.03826836122526269, w1=-0.05266425645249182\n",
      "Gradient Descent(208/499): loss=0.3425048475749372, w0=0.03842624325291856, w1=-0.05273836631208153\n",
      "Gradient Descent(209/499): loss=0.3424492271197864, w0=0.03858391707255612, w1=-0.052812226016052594\n",
      "Gradient Descent(210/499): loss=0.34239408011905154, w0=0.03874138258831497, w1=-0.052885837437814075\n",
      "Gradient Descent(211/499): loss=0.34233939935109925, w0=0.03889863971563193, w1=-0.05295920241638711\n",
      "Gradient Descent(212/499): loss=0.3422851777310164, w0=0.039055688381041564, w1=-0.053032322757274424\n",
      "Gradient Descent(213/499): loss=0.34223140830776905, w0=0.039212528521979234, w1=-0.05310520023330529\n",
      "Gradient Descent(214/499): loss=0.3421780842614234, w0=0.03936916008658664, w1=-0.053177836585456634\n",
      "Gradient Descent(215/499): loss=0.3421251989004306, w0=0.03952558303351989, w1=-0.05325023352365098\n",
      "Gradient Descent(216/499): loss=0.3420727456589715, w0=0.03968179733176009, w1=-0.05332239272753199\n",
      "Gradient Descent(217/499): loss=0.3420207180943614, w0=0.039837802960426376, w1=-0.05339431584721821\n",
      "Gradient Descent(218/499): loss=0.34196910988451207, w0=0.03999359990859146, w1=-0.05346600450403566\n",
      "Gradient Descent(219/499): loss=0.3419179148254511, w0=0.040149188175099625, w1=-0.05353746029122997\n",
      "Gradient Descent(220/499): loss=0.3418671268288949, w0=0.04030456776838719, w1=-0.0536086847746586\n",
      "Gradient Descent(221/499): loss=0.34181673991987677, w0=0.04045973870630536, w1=-0.053679679493463715\n",
      "Gradient Descent(222/499): loss=0.34176674823442665, w0=0.04061470101594558, w1=-0.05375044596072643\n",
      "Gradient Descent(223/499): loss=0.34171714601730174, w0=0.040769454733467186, w1=-0.05382098566410277\n",
      "Gradient Descent(224/499): loss=0.3416679276197677, w0=0.04092399990392753, w1=-0.053891300066442056\n",
      "Gradient Descent(225/499): loss=0.34161908749742775, w0=0.041078336581114455, w1=-0.05396139060638816\n",
      "Gradient Descent(226/499): loss=0.3415706202080999, w0=0.041232464827381084, w1=-0.05403125869896412\n",
      "Gradient Descent(227/499): loss=0.3415225204097401, w0=0.041386384713482986, w1=-0.05410090573614067\n",
      "Gradient Descent(228/499): loss=0.34147478285840993, w0=0.04154009631841764, w1=-0.05417033308738907\n",
      "Gradient Descent(229/499): loss=0.3414274024062911, w0=0.04169359972926619, w1=-0.05423954210021882\n",
      "Gradient Descent(230/499): loss=0.34138037399973875, w0=0.04184689504103747, w1=-0.05430853410070056\n",
      "Gradient Descent(231/499): loss=0.34133369267738156, w0=0.04199998235651431, w1=-0.05437731039397468\n",
      "Gradient Descent(232/499): loss=0.3412873535682584, w0=0.042152861786102044, w1=-0.05444587226474605\n",
      "Gradient Descent(233/499): loss=0.3412413518899995, w0=0.04230553344767927, w1=-0.054514220977765274\n",
      "Gradient Descent(234/499): loss=0.34119568294704306, w0=0.04245799746645074, w1=-0.05458235777829681\n",
      "Gradient Descent(235/499): loss=0.34115034212889195, w0=0.04261025397480252, w1=-0.05465028389257442\n",
      "Gradient Descent(236/499): loss=0.3411053249084082, w0=0.04276230311215916, w1=-0.05471800052824427\n",
      "Gradient Descent(237/499): loss=0.3410606268401427, w0=0.042914145024843156, w1=-0.05478550887479606\n",
      "Gradient Descent(238/499): loss=0.34101624355870125, w0=0.04306577986593638, w1=-0.05485281010398248\n",
      "Gradient Descent(239/499): loss=0.34097217077714537, w0=0.04321720779514366, w1=-0.05491990537022744\n",
      "Gradient Descent(240/499): loss=0.3409284042854273, w0=0.04336842897865843, w1=-0.05498679581102328\n",
      "Gradient Descent(241/499): loss=0.340884939948858, w0=0.043519443589030385, w1=-0.05505348254731736\n",
      "Gradient Descent(242/499): loss=0.3408417737066077, w0=0.04367025180503514, w1=-0.05511996668388829\n",
      "Gradient Descent(243/499): loss=0.34079890157023807, w0=0.04382085381154596, w1=-0.05518624930971215\n",
      "Gradient Descent(244/499): loss=0.3407563196222657, w0=0.04397124979940737, w1=-0.05525233149831893\n",
      "Gradient Descent(245/499): loss=0.34071402401475614, w0=0.04412143996531079, w1=-0.055318214308139504\n",
      "Gradient Descent(246/499): loss=0.3406720109679467, w0=0.04427142451167202, w1=-0.05538389878284342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(247/499): loss=0.34063027676889845, w0=0.04442120364651071, w1=-0.055449385951667755\n",
      "Gradient Descent(248/499): loss=0.3405888177701782, w0=0.04457077758333167, w1=-0.05551467682973727\n",
      "Gradient Descent(249/499): loss=0.3405476303885653, w0=0.044720146541008, w1=-0.05557977241837616\n",
      "Gradient Descent(250/499): loss=0.34050671110378744, w0=0.04486931074366614, w1=-0.055644673705411624\n",
      "Gradient Descent(251/499): loss=0.34046605645728245, w0=0.04501827042057267, w1=-0.05570938166546945\n",
      "Gradient Descent(252/499): loss=0.34042566305098676, w0=0.04516702580602294, w1=-0.055773897260261905\n",
      "Gradient Descent(253/499): loss=0.34038552754614676, w0=0.04531557713923143, w1=-0.05583822143886809\n",
      "Gradient Descent(254/499): loss=0.34034564666215783, w0=0.04546392466422386, w1=-0.05590235513800703\n",
      "Gradient Descent(255/499): loss=0.3403060171754255, w0=0.04561206862973109, w1=-0.055966299282303685\n",
      "Gradient Descent(256/499): loss=0.34026663591825096, w0=0.04576000928908464, w1=-0.05603005478454801\n",
      "Gradient Descent(257/499): loss=0.34022749977774003, w0=0.04590774690011393, w1=-0.056093622545947436\n",
      "Gradient Descent(258/499): loss=0.34018860569473414, w0=0.04605528172504516, w1=-0.05615700345637273\n",
      "Gradient Descent(259/499): loss=0.3401499506627635, w0=0.04620261403040186, w1=-0.05622019839459763\n",
      "Gradient Descent(260/499): loss=0.3401115317270224, w0=0.046349744086907, w1=-0.05628320822853231\n",
      "Gradient Descent(261/499): loss=0.34007334598336536, w0=0.04649667216938677, w1=-0.05634603381545087\n",
      "Gradient Descent(262/499): loss=0.3400353905773238, w0=0.046643398556675854, w1=-0.05640867600221303\n",
      "Gradient Descent(263/499): loss=0.3399976627031426, w0=0.04678992353152428, w1=-0.05647113562548021\n",
      "Gradient Descent(264/499): loss=0.3399601596028379, w0=0.04693624738050584, w1=-0.05653341351192608\n",
      "Gradient Descent(265/499): loss=0.33992287856527303, w0=0.04708237039392798, w1=-0.05659551047844188\n",
      "Gradient Descent(266/499): loss=0.33988581692525305, w0=0.04722829286574315, w1=-0.056657427332336446\n",
      "Gradient Descent(267/499): loss=0.3398489720626397, w0=0.047374015093461695, w1=-0.0567191648715313\n",
      "Gradient Descent(268/499): loss=0.3398123414014824, w0=0.04751953737806615, w1=-0.0567807238847508\n",
      "Gradient Descent(269/499): loss=0.3397759224091679, w0=0.04766486002392695, w1=-0.05684210515170754\n",
      "Gradient Descent(270/499): loss=0.3397397125955878, w0=0.04780998333871956, w1=-0.056903309443283145\n",
      "Gradient Descent(271/499): loss=0.33970370951232187, w0=0.04795490763334303, w1=-0.05696433752170455\n",
      "Gradient Descent(272/499): loss=0.33966791075183966, w0=0.04809963322183984, w1=-0.05702519014071591\n",
      "Gradient Descent(273/499): loss=0.3396323139467162, w0=0.04824416042131716, w1=-0.05708586804574625\n",
      "Gradient Descent(274/499): loss=0.3395969167688663, w0=0.048388489551869425, w1=-0.05714637197407303\n",
      "Gradient Descent(275/499): loss=0.3395617169287911, w0=0.04853262093650216, w1=-0.057206702654981616\n",
      "Gradient Descent(276/499): loss=0.33952671217484365, w0=0.048676554901057205, w1=-0.05726686080992088\n",
      "Gradient Descent(277/499): loss=0.3394919002925061, w0=0.04882029177413911, w1=-0.057326847152655035\n",
      "Gradient Descent(278/499): loss=0.3394572791036843, w0=0.048963831887042834, w1=-0.057386662389411654\n",
      "Gradient Descent(279/499): loss=0.33942284646601306, w0=0.04910717557368269, w1=-0.05744630721902623\n",
      "Gradient Descent(280/499): loss=0.33938860027218076, w0=0.049250323170522456, w1=-0.057505782333083136\n",
      "Gradient Descent(281/499): loss=0.33935453844926206, w0=0.04939327501650673, w1=-0.05756508841605323\n",
      "Gradient Descent(282/499): loss=0.3393206589580674, w0=0.04953603145299345, w1=-0.057624226145428115\n",
      "Gradient Descent(283/499): loss=0.33928695979250495, w0=0.04967859282368759, w1=-0.05768319619185121\n",
      "Gradient Descent(284/499): loss=0.3392534389789552, w0=0.049820959474575974, w1=-0.05774199921924564\n",
      "Gradient Descent(285/499): loss=0.33922009457565766, w0=0.049963131753863244, w1=-0.05780063588493912\n",
      "Gradient Descent(286/499): loss=0.3391869246721111, w0=0.05010511001190894, w1=-0.05785910683978584\n",
      "Gradient Descent(287/499): loss=0.33915392738848454, w0=0.05024689460116566, w1=-0.05791741272828547\n",
      "Gradient Descent(288/499): loss=0.33912110087504077, w0=0.05038848587611831, w1=-0.05797555418869934\n",
      "Gradient Descent(289/499): loss=0.3390884433115719, w0=0.05052988419322441, w1=-0.05803353185316391\n",
      "Gradient Descent(290/499): loss=0.3390559529068442, w0=0.05067108991085548, w1=-0.05809134634780155\n",
      "Gradient Descent(291/499): loss=0.3390236278980573, w0=0.050812103389239374, w1=-0.05814899829282872\n",
      "Gradient Descent(292/499): loss=0.33899146655031054, w0=0.05095292499040375, w1=-0.05820648830266168\n",
      "Gradient Descent(293/499): loss=0.3389594671560828, w0=0.051093555078120405, w1=-0.05826381698601966\n",
      "Gradient Descent(294/499): loss=0.3389276280347222, w0=0.05123399401785071, w1=-0.05832098494602569\n",
      "Gradient Descent(295/499): loss=0.3388959475319441, w0=0.051374242176691946, w1=-0.05837799278030511\n",
      "Gradient Descent(296/499): loss=0.33886442401934097, w0=0.05151429992332462, w1=-0.05843484108108178\n",
      "Gradient Descent(297/499): loss=0.3388330558939023, w0=0.051654167627960684, w1=-0.05849153043527208\n",
      "Gradient Descent(298/499): loss=0.3388018415775424, w0=0.05179384566229277, w1=-0.0585480614245768\n",
      "Gradient Descent(299/499): loss=0.33877077951663903, w0=0.0519333343994442, w1=-0.05860443462557089\n",
      "Gradient Descent(300/499): loss=0.33873986818157986, w0=0.05207263421392002, w1=-0.058660650609791185\n",
      "Gradient Descent(301/499): loss=0.3387091060663191, w0=0.05221174548155884, w1=-0.05871670994382216\n",
      "Gradient Descent(302/499): loss=0.33867849168794306, w0=0.05235066857948555, w1=-0.058772613189379735\n",
      "Gradient Descent(303/499): loss=0.33864802358624263, w0=0.05248940388606494, w1=-0.058828360903393204\n",
      "Gradient Descent(304/499): loss=0.33861770032329647, w0=0.05262795178085608, w1=-0.058883953638085296\n",
      "Gradient Descent(305/499): loss=0.3385875204830603, w0=0.0527663126445676, w1=-0.05893939194105051\n",
      "Gradient Descent(306/499): loss=0.33855748267096575, w0=0.05290448685901376, w1=-0.05899467635533164\n",
      "Gradient Descent(307/499): loss=0.33852758551352696, w0=0.053042474807071305, w1=-0.059049807419494664\n",
      "Gradient Descent(308/499): loss=0.33849782765795433, w0=0.053180276872637114, w1=-0.05910478566770191\n",
      "Gradient Descent(309/499): loss=0.3384682077717758, w0=0.05331789344058666, w1=-0.05915961162978368\n",
      "Gradient Descent(310/499): loss=0.33843872454246715, w0=0.05345532489673321, w1=-0.05921428583130825\n",
      "Gradient Descent(311/499): loss=0.3384093766770873, w0=0.05359257162778774, w1=-0.05926880879365038\n",
      "Gradient Descent(312/499): loss=0.3383801629019224, w0=0.05372963402131965, w1=-0.0593231810340583\n",
      "Gradient Descent(313/499): loss=0.3383510819621359, w0=0.0538665124657182, w1=-0.05937740306571926\n",
      "Gradient Descent(314/499): loss=0.3383221326214271, w0=0.0540032073501546, w1=-0.05943147539782364\n",
      "Gradient Descent(315/499): loss=0.33829331366169413, w0=0.05413971906454488, w1=-0.05948539853562772\n",
      "Gradient Descent(316/499): loss=0.33826462388270495, w0=0.054276047999513424, w1=-0.05953917298051506\n",
      "Gradient Descent(317/499): loss=0.33823606210177526, w0=0.05441219454635718, w1=-0.0595927992300566\n",
      "Gradient Descent(318/499): loss=0.33820762715345076, w0=0.05454815909701054, w1=-0.059646277778069454\n",
      "Gradient Descent(319/499): loss=0.33817931788919775, w0=0.054683942044010884, w1=-0.05969960911467446\n",
      "Gradient Descent(320/499): loss=0.33815113317709866, w0=0.0548195437804648, w1=-0.05975279372635252\n",
      "Gradient Descent(321/499): loss=0.33812307190155383, w0=0.05495496470001489, w1=-0.05980583209599977\n",
      "Gradient Descent(322/499): loss=0.3380951329629877, w0=0.05509020519680725, w1=-0.05985872470298155\n",
      "Gradient Descent(323/499): loss=0.33806731527756434, w0=0.055225265665459544, w1=-0.05991147202318528\n",
      "Gradient Descent(324/499): loss=0.33803961777690433, w0=0.05536014650102972, w1=-0.05996407452907222\n",
      "Gradient Descent(325/499): loss=0.33801203940780955, w0=0.05549484809898525, w1=-0.06001653268972816\n",
      "Gradient Descent(326/499): loss=0.3379845791319923, w0=0.05562937085517306, w1=-0.06006884697091305\n",
      "Gradient Descent(327/499): loss=0.33795723592581206, w0=0.05576371516578996, w1=-0.06012101783510961\n",
      "Gradient Descent(328/499): loss=0.3379300087800122, w0=0.05589788142735365, w1=-0.060173045741571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(329/499): loss=0.33790289669946855, w0=0.05603187003667436, w1=-0.0602249311463674\n",
      "Gradient Descent(330/499): loss=0.3378758987029362, w0=0.056165681390826935, w1=-0.060276674502431744\n",
      "Gradient Descent(331/499): loss=0.33784901382280635, w0=0.05629931588712354, w1=-0.06032827625960448\n",
      "Gradient Descent(332/499): loss=0.33782224110486536, w0=0.05643277392308685, w1=-0.06037973686467743\n",
      "Gradient Descent(333/499): loss=0.33779557960805817, w0=0.05656605589642381, w1=-0.060431056761436786\n",
      "Gradient Descent(334/499): loss=0.3377690284042581, w0=0.05669916220499987, w1=-0.06048223639070521\n",
      "Gradient Descent(335/499): loss=0.33774258657803946, w0=0.056832093246813736, w1=-0.060533276190383144\n",
      "Gradient Descent(336/499): loss=0.337716253226455, w0=0.056964849419972645, w1=-0.060584176595489224\n",
      "Gradient Descent(337/499): loss=0.3376900274588182, w0=0.057097431122668074, w1=-0.06063493803819997\n",
      "Gradient Descent(338/499): loss=0.33766390839648885, w0=0.057229838753152004, w1=-0.06068556094788865\n",
      "Gradient Descent(339/499): loss=0.3376378951726636, w0=0.057362072709713594, w1=-0.060736045751163384\n",
      "Gradient Descent(340/499): loss=0.33761198693216976, w0=0.05749413339065636, w1=-0.06078639287190452\n",
      "Gradient Descent(341/499): loss=0.3375861828312637, w0=0.057626021194275776, w1=-0.06083660273130126\n",
      "Gradient Descent(342/499): loss=0.33756048203743283, w0=0.05775773651883739, w1=-0.06088667574788758\n",
      "Gradient Descent(343/499): loss=0.33753488372920104, w0=0.05788927976255529, w1=-0.06093661233757746\n",
      "Gradient Descent(344/499): loss=0.3375093870959388, w0=0.05802065132357109, w1=-0.060986412913699446\n",
      "Gradient Descent(345/499): loss=0.3374839913376757, w0=0.0581518515999333, w1=-0.06103607788703051\n",
      "Gradient Descent(346/499): loss=0.33745869566491793, w0=0.05828288098957711, w1=-0.06108560766582933\n",
      "Gradient Descent(347/499): loss=0.3374334992984675, w0=0.05841373989030463, w1=-0.06113500265586884\n",
      "Gradient Descent(348/499): loss=0.33740840146924655, w0=0.05854442869976548, w1=-0.06118426326046827\n",
      "Gradient Descent(349/499): loss=0.33738340141812434, w0=0.05867494781543784, w1=-0.06123338988052449\n",
      "Gradient Descent(350/499): loss=0.33735849839574755, w0=0.05880529763460984, w1=-0.06128238291454285\n",
      "Gradient Descent(351/499): loss=0.33733369166237326, w0=0.058935478554361376, w1=-0.06133124275866733\n",
      "Gradient Descent(352/499): loss=0.33730898048770663, w0=0.059065490971546265, w1=-0.06137996980671027\n",
      "Gradient Descent(353/499): loss=0.33728436415073965, w0=0.05919533528277482, w1=-0.0614285644501814\n",
      "Gradient Descent(354/499): loss=0.33725984193959435, w0=0.05932501188439675, w1=-0.061477027078316454\n",
      "Gradient Descent(355/499): loss=0.33723541315136907, w0=0.05945452117248445, w1=-0.06152535807810519\n",
      "Gradient Descent(356/499): loss=0.3372110770919869, w0=0.05958386354281664, w1=-0.06157355783431891\n",
      "Gradient Descent(357/499): loss=0.33718683307604663, w0=0.05971303939086233, w1=-0.06162162672953747\n",
      "Gradient Descent(358/499): loss=0.3371626804266782, w0=0.05984204911176516, w1=-0.06166956514417584\n",
      "Gradient Descent(359/499): loss=0.33713861847539917, w0=0.05997089310032805, w1=-0.06171737345651013\n",
      "Gradient Descent(360/499): loss=0.33711464656197476, w0=0.060099571750998185, w1=-0.06176505204270315\n",
      "Gradient Descent(361/499): loss=0.3370907640342802, w0=0.06022808545785236, w1=-0.06181260127682956\n",
      "Gradient Descent(362/499): loss=0.3370669702481656, w0=0.06035643461458259, w1=-0.061860021530900526\n",
      "Gradient Descent(363/499): loss=0.3370432645673246, w0=0.06048461961448206, w1=-0.061907313174887936\n",
      "Gradient Descent(364/499): loss=0.337019646363163, w0=0.06061264085043141, w1=-0.06195447657674821\n",
      "Gradient Descent(365/499): loss=0.3369961150146722, w0=0.06074049871488526, w1=-0.06200151210244568\n",
      "Gradient Descent(366/499): loss=0.33697266990830443, w0=0.060868193599859094, w1=-0.06204842011597552\n",
      "Gradient Descent(367/499): loss=0.33694931043784876, w0=0.06099572589691642, w1=-0.062095200979386374\n",
      "Gradient Descent(368/499): loss=0.33692603600431154, w0=0.06112309599715617, w1=-0.06214185505280246\n",
      "Gradient Descent(369/499): loss=0.3369028460157983, w0=0.06125030429120049, w1=-0.062188382694445384\n",
      "Gradient Descent(370/499): loss=0.3368797398873972, w0=0.06137735116918267, w1=-0.06223478426065555\n",
      "Gradient Descent(371/499): loss=0.33685671704106657, w0=0.06150423702073546, w1=-0.06228106010591317\n",
      "Gradient Descent(372/499): loss=0.3368337769055208, w0=0.061630962234979596, w1=-0.06232721058285899\n",
      "Gradient Descent(373/499): loss=0.33681091891612314, w0=0.06175752720051263, w1=-0.06237323604231454\n",
      "Gradient Descent(374/499): loss=0.33678814251477657, w0=0.06188393230539795, w1=-0.06241913683330216\n",
      "Gradient Descent(375/499): loss=0.33676544714981904, w0=0.06201017793715416, w1=-0.06246491330306463\n",
      "Gradient Descent(376/499): loss=0.3367428322759194, w0=0.06213626448274459, w1=-0.06251056579708447\n",
      "Gradient Descent(377/499): loss=0.33672029735397646, w0=0.062262192328567154, w1=-0.0625560946591029\n",
      "Gradient Descent(378/499): loss=0.3366978418510176, w0=0.06238796186044437, w1=-0.06260150023113854\n",
      "Gradient Descent(379/499): loss=0.3366754652401028, w0=0.06251357346361368, w1=-0.06264678285350574\n",
      "Gradient Descent(380/499): loss=0.33665316700022696, w0=0.06263902752271798, w1=-0.06269194286483261\n",
      "Gradient Descent(381/499): loss=0.33663094661622606, w0=0.06276432442179636, w1=-0.06273698060207882\n",
      "Gradient Descent(382/499): loss=0.33660880357868483, w0=0.06288946454427508, w1=-0.06278189640055298\n",
      "Gradient Descent(383/499): loss=0.3365867373838454, w0=0.06301444827295875, w1=-0.0628266905939299\n",
      "Gradient Descent(384/499): loss=0.33656474753351795, w0=0.06313927599002184, w1=-0.06287136351426738\n",
      "Gradient Descent(385/499): loss=0.3365428335349936, w0=0.06326394807700018, w1=-0.0629159154920229\n",
      "Gradient Descent(386/499): loss=0.3365209949009579, w0=0.06338846491478291, w1=-0.06296034685606994\n",
      "Gradient Descent(387/499): loss=0.3364992311494065, w0=0.06351282688360445, w1=-0.06300465793371404\n",
      "Gradient Descent(388/499): loss=0.33647754180356293, w0=0.06363703436303678, w1=-0.06304884905070866\n",
      "Gradient Descent(389/499): loss=0.3364559263917959, w0=0.06376108773198189, w1=-0.06309292053127072\n",
      "Gradient Descent(390/499): loss=0.33643438444754076, w0=0.06388498736866441, w1=-0.06313687269809594\n",
      "Gradient Descent(391/499): loss=0.33641291550922003, w0=0.06400873365062443, w1=-0.06318070587237391\n",
      "Gradient Descent(392/499): loss=0.3363915191201674, w0=0.06413232695471054, w1=-0.06322442037380295\n",
      "Gradient Descent(393/499): loss=0.33637019482855124, w0=0.06425576765707301, w1=-0.06326801652060468\n",
      "Gradient Descent(394/499): loss=0.3363489421873006, w0=0.06437905613315723, w1=-0.06331149462953842\n",
      "Gradient Descent(395/499): loss=0.336327760754033, w0=0.06450219275769715, w1=-0.06335485501591535\n",
      "Gradient Descent(396/499): loss=0.336306650090982, w0=0.06462517790470919, w1=-0.06339809799361244\n",
      "Gradient Descent(397/499): loss=0.33628560976492716, w0=0.06474801194748597, w1=-0.06344122387508615\n",
      "Gradient Descent(398/499): loss=0.3362646393471261, w0=0.06487069525859054, w1=-0.06348423297138597\n",
      "Gradient Descent(399/499): loss=0.33624373841324473, w0=0.06499322820985051, w1=-0.06352712559216771\n",
      "Gradient Descent(400/499): loss=0.33622290654329257, w0=0.06511561117235252, w1=-0.06356990204570655\n",
      "Gradient Descent(401/499): loss=0.3362021433215564, w0=0.06523784451643679, w1=-0.06361256263891003\n",
      "Gradient Descent(402/499): loss=0.3361814483365369, w0=0.06535992861169179, w1=-0.06365510767733068\n",
      "Gradient Descent(403/499): loss=0.33616082118088497, w0=0.06548186382694922, w1=-0.06369753746517855\n",
      "Gradient Descent(404/499): loss=0.3361402614513401, w0=0.06560365053027892, w1=-0.06373985230533352\n",
      "Gradient Descent(405/499): loss=0.3361197687486696, w0=0.06572528908898413, w1=-0.0637820524993575\n",
      "Gradient Descent(406/499): loss=0.3360993426776093, w0=0.06584677986959676, w1=-0.0638241383475063\n",
      "Gradient Descent(407/499): loss=0.3360789828468047, w0=0.06596812323787289, w1=-0.06386611014874147\n",
      "Gradient Descent(408/499): loss=0.3360586888687528, w0=0.06608931955878831, w1=-0.06390796820074188\n",
      "Gradient Descent(409/499): loss=0.33603846035974694, w0=0.06621036919653434, w1=-0.06394971279991517\n",
      "Gradient Descent(410/499): loss=0.33601829693982, w0=0.06633127251451362, w1=-0.06399134424140902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(411/499): loss=0.3359981982326906, w0=0.0664520298753362, w1=-0.06403286281912224\n",
      "Gradient Descent(412/499): loss=0.33597816386570917, w0=0.06657264164081562, w1=-0.06407426882571571\n",
      "Gradient Descent(413/499): loss=0.3359581934698057, w0=0.06669310817196519, w1=-0.06411556255262318\n",
      "Gradient Descent(414/499): loss=0.33593828667943787, w0=0.0668134298289944, w1=-0.06415674429006192\n",
      "Gradient Descent(415/499): loss=0.33591844313254016, w0=0.06693360697130538, w1=-0.06419781432704312\n",
      "Gradient Descent(416/499): loss=0.33589866247047406, w0=0.0670536399574896, w1=-0.0642387729513823\n",
      "Gradient Descent(417/499): loss=0.3358789443379792, w0=0.06717352914532458, w1=-0.06427962044970946\n",
      "Gradient Descent(418/499): loss=0.33585928838312534, w0=0.06729327489177078, w1=-0.06432035710747908\n",
      "Gradient Descent(419/499): loss=0.3358396942572646, w0=0.06741287755296858, w1=-0.06436098320898007\n",
      "Gradient Descent(420/499): loss=0.3358201616149853, w0=0.06753233748423534, w1=-0.06440149903734549\n",
      "Gradient Descent(421/499): loss=0.33580069011406655, w0=0.06765165504006267, w1=-0.06444190487456218\n",
      "Gradient Descent(422/499): loss=0.3357812794154331, w0=0.06777083057411368, w1=-0.06448220100148026\n",
      "Gradient Descent(423/499): loss=0.3357619291831113, w0=0.06788986443922042, w1=-0.06452238769782245\n",
      "Gradient Descent(424/499): loss=0.3357426390841858, w0=0.06800875698738143, w1=-0.0645624652421933\n",
      "Gradient Descent(425/499): loss=0.33572340878875745, w0=0.06812750856975934, w1=-0.06460243391208832\n",
      "Gradient Descent(426/499): loss=0.3357042379699005, w0=0.06824611953667857, w1=-0.06464229398390291\n",
      "Gradient Descent(427/499): loss=0.33568512630362274, w0=0.06836459023762319, w1=-0.06468204573294123\n",
      "Gradient Descent(428/499): loss=0.3356660734688243, w0=0.06848292102123482, w1=-0.06472168943342493\n",
      "Gradient Descent(429/499): loss=0.3356470791472583, w0=0.06860111223531067, w1=-0.06476122535850178\n",
      "Gradient Descent(430/499): loss=0.33562814302349187, w0=0.06871916422680158, w1=-0.0648006537802541\n",
      "Gradient Descent(431/499): loss=0.33560926478486786, w0=0.06883707734181028, w1=-0.06483997496970723\n",
      "Gradient Descent(432/499): loss=0.3355904441214669, w0=0.06895485192558967, w1=-0.06487918919683772\n",
      "Gradient Descent(433/499): loss=0.3355716807260707, w0=0.06907248832254113, w1=-0.06491829673058153\n",
      "Gradient Descent(434/499): loss=0.33555297429412567, w0=0.06918998687621306, w1=-0.06495729783884209\n",
      "Gradient Descent(435/499): loss=0.335534324523707, w0=0.06930734792929932, w1=-0.06499619278849822\n",
      "Gradient Descent(436/499): loss=0.33551573111548383, w0=0.069424571823638, w1=-0.06503498184541195\n",
      "Gradient Descent(437/499): loss=0.3354971937726845, w0=0.06954165890020995, w1=-0.06507366527443634\n",
      "Gradient Descent(438/499): loss=0.3354787122010624, w0=0.06965860949913774, w1=-0.06511224333942302\n",
      "Gradient Descent(439/499): loss=0.3354602861088626, w0=0.06977542395968436, w1=-0.06515071630322977\n",
      "Gradient Descent(440/499): loss=0.33544191520679006, w0=0.06989210262025226, w1=-0.06518908442772797\n",
      "Gradient Descent(441/499): loss=0.3354235992079761, w0=0.07000864581838233, w1=-0.06522734797380994\n",
      "Gradient Descent(442/499): loss=0.3354053378279469, w0=0.07012505389075298, w1=-0.06526550720139616\n",
      "Gradient Descent(443/499): loss=0.33538713078459315, w0=0.07024132717317931, w1=-0.06530356236944249\n",
      "Gradient Descent(444/499): loss=0.3353689777981382, w0=0.07035746600061232, w1=-0.06534151373594714\n",
      "Gradient Descent(445/499): loss=0.33535087859110935, w0=0.07047347070713823, w1=-0.06537936155795775\n",
      "Gradient Descent(446/499): loss=0.33533283288830695, w0=0.0705893416259778, w1=-0.06541710609157819\n",
      "Gradient Descent(447/499): loss=0.33531484041677617, w0=0.0707050790894858, w1=-0.06545474759197545\n",
      "Gradient Descent(448/499): loss=0.33529690090577746, w0=0.0708206834291505, w1=-0.06549228631338627\n",
      "Gradient Descent(449/499): loss=0.335279014086759, w0=0.07093615497559322, w1=-0.06552972250912384\n",
      "Gradient Descent(450/499): loss=0.33526117969332914, w0=0.07105149405856791, w1=-0.0655670564315843\n",
      "Gradient Descent(451/499): loss=0.3352433974612288, w0=0.0711667010069609, w1=-0.06560428833225325\n",
      "Gradient Descent(452/499): loss=0.33522566712830504, w0=0.07128177614879062, w1=-0.06564141846171212\n",
      "Gradient Descent(453/499): loss=0.3352079884344837, w0=0.07139671981120736, w1=-0.0656784470696445\n",
      "Gradient Descent(454/499): loss=0.33519036112174516, w0=0.07151153232049316, w1=-0.06571537440484235\n",
      "Gradient Descent(455/499): loss=0.335172784934097, w0=0.07162621400206173, w1=-0.06575220071521215\n",
      "Gradient Descent(456/499): loss=0.3351552596175512, w0=0.0717407651804584, w1=-0.06578892624778104\n",
      "Gradient Descent(457/499): loss=0.3351377849200968, w0=0.07185518617936011, w1=-0.06582555124870279\n",
      "Gradient Descent(458/499): loss=0.3351203605916777, w0=0.07196947732157558, w1=-0.06586207596326372\n",
      "Gradient Descent(459/499): loss=0.33510298638416813, w0=0.07208363892904533, w1=-0.06589850063588863\n",
      "Gradient Descent(460/499): loss=0.33508566205134904, w0=0.07219767132284194, w1=-0.06593482551014652\n",
      "Gradient Descent(461/499): loss=0.3350683873488851, w0=0.0723115748231702, w1=-0.0659710508287564\n",
      "Gradient Descent(462/499): loss=0.3350511620343022, w0=0.07242534974936747, w1=-0.06600717683359289\n",
      "Gradient Descent(463/499): loss=0.33503398586696503, w0=0.07253899641990391, w1=-0.06604320376569184\n",
      "Gradient Descent(464/499): loss=0.33501685860805436, w0=0.07265251515238293, w1=-0.06607913186525587\n",
      "Gradient Descent(465/499): loss=0.3349997800205469, w0=0.07276590626354154, w1=-0.0661149613716598\n",
      "Gradient Descent(466/499): loss=0.33498274986919274, w0=0.07287917006925088, w1=-0.06615069252345611\n",
      "Gradient Descent(467/499): loss=0.3349657679204948, w0=0.07299230688451663, w1=-0.06618632555838021\n",
      "Gradient Descent(468/499): loss=0.33494883394268865, w0=0.07310531702347967, w1=-0.06622186071335576\n",
      "Gradient Descent(469/499): loss=0.33493194770572127, w0=0.07321820079941653, w1=-0.06625729822449987\n",
      "Gradient Descent(470/499): loss=0.3349151089812321, w0=0.07333095852474017, w1=-0.0662926383271283\n",
      "Gradient Descent(471/499): loss=0.334898317542533, w0=0.07344359051100052, w1=-0.06632788125576047\n",
      "Gradient Descent(472/499): loss=0.3348815731645886, w0=0.07355609706888526, w1=-0.0663630272441246\n",
      "Gradient Descent(473/499): loss=0.33486487562399797, w0=0.07366847850822053, w1=-0.06639807652516262\n",
      "Gradient Descent(474/499): loss=0.33484822469897524, w0=0.07378073513797175, w1=-0.06643302933103513\n",
      "Gradient Descent(475/499): loss=0.3348316201693312, w0=0.07389286726624439, w1=-0.0664678858931263\n",
      "Gradient Descent(476/499): loss=0.33481506181645615, w0=0.07400487520028487, w1=-0.0665026464420486\n",
      "Gradient Descent(477/499): loss=0.3347985494233005, w0=0.07411675924648142, w1=-0.06653731120764769\n",
      "Gradient Descent(478/499): loss=0.3347820827743585, w0=0.07422851971036504, w1=-0.06657188041900701\n",
      "Gradient Descent(479/499): loss=0.33476566165565025, w0=0.0743401568966104, w1=-0.06660635430445251\n",
      "Gradient Descent(480/499): loss=0.3347492858547049, w0=0.0744516711090369, w1=-0.06664073309155726\n",
      "Gradient Descent(481/499): loss=0.33473295516054397, w0=0.07456306265060965, w1=-0.06667501700714597\n",
      "Gradient Descent(482/499): loss=0.33471666936366434, w0=0.0746743318234405, w1=-0.06670920627729954\n",
      "Gradient Descent(483/499): loss=0.3347004282560225, w0=0.07478547892878923, w1=-0.06674330112735949\n",
      "Gradient Descent(484/499): loss=0.3346842316310188, w0=0.07489650426706453, w1=-0.0667773017819324\n",
      "Gradient Descent(485/499): loss=0.3346680792834804, w0=0.07500740813782523, w1=-0.06681120846489426\n",
      "Gradient Descent(486/499): loss=0.33465197100964744, w0=0.07511819083978147, w1=-0.06684502139939476\n",
      "Gradient Descent(487/499): loss=0.3346359066071567, w0=0.07522885267079582, w1=-0.06687874080786162\n",
      "Gradient Descent(488/499): loss=0.3346198858750268, w0=0.07533939392788463, w1=-0.06691236691200476\n",
      "Gradient Descent(489/499): loss=0.33460390861364314, w0=0.07544981490721919, w1=-0.06694589993282055\n",
      "Gradient Descent(490/499): loss=0.3345879746247441, w0=0.07556011590412703, w1=-0.06697934009059586\n",
      "Gradient Descent(491/499): loss=0.3345720837114051, w0=0.07567029721309326, w1=-0.06701268760491225\n",
      "Gradient Descent(492/499): loss=0.3345562356780269, w0=0.07578035912776185, w1=-0.06704594269464993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(493/499): loss=0.33454043033031855, w0=0.07589030194093702, w1=-0.06707910557799185\n",
      "Gradient Descent(494/499): loss=0.3345246674752867, w0=0.0760001259445846, w1=-0.06711217647242759\n",
      "Gradient Descent(495/499): loss=0.33450894692121946, w0=0.07610983142983345, w1=-0.06714515559475738\n",
      "Gradient Descent(496/499): loss=0.33449326847767535, w0=0.07621941868697683, w1=-0.06717804316109591\n",
      "Gradient Descent(497/499): loss=0.33447763195546815, w0=0.07632888800547392, w1=-0.0672108393868762\n",
      "Gradient Descent(498/499): loss=0.3344620371666551, w0=0.07643823967395123, w1=-0.06724354448685345\n",
      "Gradient Descent(499/499): loss=0.3344464839245245, w0=0.0765474739802041, w1=-0.06727615867510871\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEKCAYAAAAb7IIBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11436d588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Jet  1 *****accuracy jet 76.1670793356\n",
      "**** Starting Jet  2 *****\n",
      "Gradient Descent(0/499): loss=0.5000000000000043, w0=0.00029614216829728485, w1=-0.0028070218212909188\n",
      "Gradient Descent(1/499): loss=0.4197604536606605, w0=0.0006274347081664108, w1=-0.004812881859546138\n",
      "Gradient Descent(2/499): loss=0.396578588811233, w0=0.0010305208109886257, w1=-0.006335252443573691\n",
      "Gradient Descent(3/499): loss=0.3869249310657019, w0=0.0014822084712019987, w1=-0.007548178482821083\n",
      "Gradient Descent(4/499): loss=0.38154677066029263, w0=0.0019551357713601717, w1=-0.008553232447355966\n",
      "Gradient Descent(5/499): loss=0.37785545804383813, w0=0.0024297122143383297, w1=-0.009412718230184169\n",
      "Gradient Descent(6/499): loss=0.3749745424229546, w0=0.002894240094219203, w1=-0.010166421664398954\n",
      "Gradient Descent(7/499): loss=0.3725601169243452, w0=0.003342689481336082, w1=-0.010840634321985242\n",
      "Gradient Descent(8/499): loss=0.370455684768107, w0=0.0037726222927022637, w1=-0.011453271117201699\n",
      "Gradient Descent(9/499): loss=0.36857836846157144, w0=0.004183714510816486, w1=-0.01201688934647451\n",
      "Gradient Descent(10/499): loss=0.36687804268608326, w0=0.004576802830253557, w1=-0.012540526292263564\n",
      "Gradient Descent(11/499): loss=0.3653212540878828, w0=0.004953299652813467, w1=-0.013030847716918028\n",
      "Gradient Descent(12/499): loss=0.3638841466025154, w0=0.005314846411259397, w1=-0.01349288306223041\n",
      "Gradient Descent(13/499): loss=0.36254893789140874, w0=0.0056631153089945424, w1=-0.013930506885728573\n",
      "Gradient Descent(14/499): loss=0.36130194923831793, w0=0.005999701527549921, w1=-0.014346761125627468\n",
      "Gradient Descent(15/499): loss=0.3601323987122321, w0=0.006326069698477053, w1=-0.014744075462433476\n",
      "Gradient Descent(16/499): loss=0.35903161273251993, w0=0.0066435323481052995, w1=-0.015124421112522381\n",
      "Gradient Descent(17/499): loss=0.35799248808845235, w0=0.006953246693747574, w1=-0.015489420260243119\n",
      "Gradient Descent(18/499): loss=0.35700911368254, w0=0.007256221516539566, w1=-0.015840425342850406\n",
      "Gradient Descent(19/499): loss=0.356076498657968, w0=0.007553329122609666, w1=-0.016178577456698214\n",
      "Gradient Descent(20/499): loss=0.3551903735447888, w0=0.007845319424092511, w1=-0.016504850041169954\n",
      "Gradient Descent(21/499): loss=0.3543470426290409, w0=0.008132834413036015, w1=-0.01682008200514878\n",
      "Gradient Descent(22/499): loss=0.353543272867879, w0=0.00841642206325514, w1=-0.017125003163829592\n",
      "Gradient Descent(23/499): loss=0.3527762092471925, w0=0.008696549160503107, w1=-0.017420253994185154\n",
      "Gradient Descent(24/499): loss=0.3520433095077846, w0=0.008973612842369756, w1=-0.01770640113794309\n",
      "Gradient Descent(25/499): loss=0.35134229321869664, w0=0.009247950795614548, w1=-0.017983949683697015\n",
      "Gradient Descent(26/499): loss=0.3506711015909298, w0=0.009519850153507705, w1=-0.01825335298309917\n",
      "Gradient Descent(27/499): loss=0.35002786541343806, w0=0.009789555186333575, w1=-0.018515020560480266\n",
      "Gradient Descent(28/499): loss=0.3494108791922526, w0=0.010057273901607168, w1=-0.01876932453502951\n",
      "Gradient Descent(29/499): loss=0.3488185800728867, w0=0.010323183677574543, w1=-0.019016604872867562\n",
      "Gradient Descent(30/499): loss=0.34824953048616225, w0=0.010587436051039855, w1=-0.019257173711561597\n",
      "Gradient Descent(31/499): loss=0.34770240371942496, w0=0.010850160772785727, w1=-0.01949131894409825\n",
      "Gradient Descent(32/499): loss=0.3471759718071375, w0=0.011111469233511428, w1=-0.019719307207680586\n",
      "Gradient Descent(33/499): loss=0.3466690952768142, w0=0.01137145735194927, w1=-0.01994138639118943\n",
      "Gradient Descent(34/499): loss=0.346180714392063, w0=0.011630208005613266, w1=-0.02015778775108301\n",
      "Gradient Descent(35/499): loss=0.34570984161395324, w0=0.011887793074039754, w1=-0.020368727706990098\n",
      "Gradient Descent(36/499): loss=0.3452555550620664, w0=0.012144275154682556, w1=-0.02057440937389626\n",
      "Gradient Descent(37/499): loss=0.3448169928024249, w0=0.012399709002943095, w1=-0.020775023876617175\n",
      "Gradient Descent(38/499): loss=0.3443933478247179, w0=0.012654142740164017, w1=-0.02097075148344893\n",
      "Gradient Descent(39/499): loss=0.3439838635984751, w0=0.012907618866749918, w1=-0.021161762588926283\n",
      "Gradient Descent(40/499): loss=0.3435878301190676, w0=0.013160175111825012, w1=-0.02134821857008785\n",
      "Gradient Descent(41/499): loss=0.3432045803710531, w0=0.013411845145904948, w1=-0.021530272536226008\n",
      "Gradient Descent(42/499): loss=0.34283348714952255, w0=0.01366265917885445, w1=-0.02170806998854857\n",
      "Gradient Descent(43/499): loss=0.3424739601905474, w0=0.013912644461832688, w1=-0.02188174940331409\n",
      "Gradient Descent(44/499): loss=0.34212544357016234, w0=0.014161825708908764, w1=-0.022051442749680784\n",
      "Gradient Descent(45/499): loss=0.34178741333803614, w0=0.014410225451482957, w1=-0.022217275951619684\n",
      "Gradient Descent(46/499): loss=0.3414593753574009, w0=0.014657864336506574, w1=-0.022379369301699443\n",
      "Gradient Descent(47/499): loss=0.34114086332722904, w0=0.014904761377693473, w1=-0.022537837833285144\n",
      "Gradient Descent(48/499): loss=0.3408314369662483, w0=0.015150934167407265, w1=-0.02269279165665281\n",
      "Gradient Descent(49/499): loss=0.3405306803413593, w0=0.015396399055644322, w1=-0.022844336263662644\n",
      "Gradient Descent(50/499): loss=0.34023820032546004, w0=0.015641171301475354, w1=-0.022992572804923124\n",
      "Gradient Descent(51/499): loss=0.3399536251717294, w0=0.01588526520142434, w1=-0.0231375983427878\n",
      "Gradient Descent(52/499): loss=0.33967660319311255, w0=0.01612869419852523, w1=-0.023279506083035164\n",
      "Gradient Descent(53/499): loss=0.33940680153717667, w0=0.016371470975180054, w1=-0.023418385587671307\n",
      "Gradient Descent(54/499): loss=0.33914390504771863, w0=0.016613607532427542, w1=-0.023554322970951346\n",
      "Gradient Descent(55/499): loss=0.3388876152055089, w0=0.01685511525780165, w1=-0.023687401080426603\n",
      "Gradient Descent(56/499): loss=0.33863764914144023, w0=0.01709600498360111, w1=-0.023817699664581304\n",
      "Gradient Descent(57/499): loss=0.338393738716078, w0=0.017336287037092085, w1=-0.023945295528416843\n",
      "Gradient Descent(58/499): loss=0.33815562966025975, w0=0.017575971283916435, w1=-0.024070262678167458\n",
      "Gradient Descent(59/499): loss=0.3379230807719362, w0=0.017815067165770127, w1=-0.024192672456183093\n",
      "Gradient Descent(60/499): loss=0.3376958631649248, w0=0.018053583733242454, w1=-0.024312593666888854\n",
      "Gradient Descent(61/499): loss=0.33747375956567083, w0=0.018291529674561855, w1=-0.02443009269462253\n",
      "Gradient Descent(62/499): loss=0.33725656365447115, w0=0.018528913340873067, w1=-0.024545233614058956\n",
      "Gradient Descent(63/499): loss=0.3370440794479393, w0=0.01876574276856932, w1=-0.02465807829385015\n",
      "Gradient Descent(64/499): loss=0.3368361207197769, w0=0.01900202569911884, w1=-0.024768686494041334\n",
      "Gradient Descent(65/499): loss=0.3366325104571688, w0=0.019237769596754446, w1=-0.02487711595776319\n",
      "Gradient Descent(66/499): loss=0.33643308035034064, w0=0.019472981664336045, w1=-0.024983422497648726\n",
      "Gradient Descent(67/499): loss=0.33623767031302454, w0=0.019707668857646587, w1=-0.025087660077377907\n",
      "Gradient Descent(68/499): loss=0.33604612803175327, w0=0.019941837898340746, w1=-0.025189880888713486\n",
      "Gradient Descent(69/499): loss=0.33585830854207027, w0=0.02017549528573115, w1=-0.02529013542435665\n",
      "Gradient Descent(70/499): loss=0.3356740738298875, w0=0.02040864730756807, w1=-0.025388472546920345\n",
      "Gradient Descent(71/499): loss=0.33549329245635645, w0=0.02064130004994425, w1=-0.02548493955429111\n",
      "Gradient Descent(72/499): loss=0.3353158392047374, w0=0.02087345940643631, w1=-0.025579582241626014\n",
      "Gradient Descent(73/499): loss=0.3351415947478644, w0=0.021105131086577075, w1=-0.02567244496020997\n",
      "Gradient Descent(74/499): loss=0.3349704453349031, w0=0.02133632062373902, w1=-0.02576357067337955\n",
      "Gradient Descent(75/499): loss=0.3348022824961875, w0=0.02156703338249687, w1=-0.025853001009702285\n",
      "Gradient Descent(76/499): loss=0.33463700276501085, w0=0.021797274565527443, w1=-0.025940776313585073\n",
      "Gradient Descent(77/499): loss=0.3344745074153209, w0=0.022027049220096285, w1=-0.026026935693471517\n",
      "Gradient Descent(78/499): loss=0.33431470221433957, w0=0.022256362244173534, w1=-0.02611151706777547\n",
      "Gradient Descent(79/499): loss=0.33415749718919646, w0=0.022485218392215414, w1=-0.02619455720868687\n",
      "Gradient Descent(80/499): loss=0.33400280640672336, w0=0.022713622280642724, w1=-0.026276091783975605\n",
      "Gradient Descent(81/499): loss=0.3338505477656143, w0=0.022941578393043373, w1=-0.02635615539690995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(82/499): loss=0.33370064280020706, w0=0.02316909108512244, w1=-0.026434781624397576\n",
      "Gradient Descent(83/499): loss=0.3335530164951932, w0=0.023396164589420124, w1=-0.026512003053449463\n",
      "Gradient Descent(84/499): loss=0.33340759711060125, w0=0.023622803019815414, w1=-0.02658785131605993\n",
      "Gradient Descent(85/499): loss=0.3332643160164491, w0=0.023849010375831058, w1=-0.026662357122589617\n",
      "Gradient Descent(86/499): loss=0.3331231075364914, w0=0.024074790546753586, w1=-0.026735550293732244\n",
      "Gradient Descent(87/499): loss=0.33298390880053, w0=0.024300147315580528, w1=-0.026807459791140617\n",
      "Gradient Descent(88/499): loss=0.33284665960478355, w0=0.02452508436280562, w1=-0.026878113746782263\n",
      "Gradient Descent(89/499): loss=0.33271130227985063, w0=0.024749605270051628, w1=-0.026947539491090546\n",
      "Gradient Descent(90/499): loss=0.3325777815658218, w0=0.024973713523559413, w1=-0.02701576357997283\n",
      "Gradient Descent(91/499): loss=0.33244604449412873, w0=0.025197412517541075, w1=-0.027082811820733327\n",
      "Gradient Descent(92/499): loss=0.33231604027574346, w0=0.025420705557404177, w1=-0.02714870929696473\n",
      "Gradient Descent(93/499): loss=0.332187720195361, w0=0.02564359586285351, w1=-0.027213480392459227\n",
      "Gradient Descent(94/499): loss=0.3320610375112256, w0=0.025866086570876256, w1=-0.027277148814186553\n",
      "Gradient Descent(95/499): loss=0.33193594736027665, w0=0.026088180738615963, w1=-0.027339737614383765\n",
      "Gradient Descent(96/499): loss=0.3318124066683145, w0=0.026309881346140294, w1=-0.027401269211798796\n",
      "Gradient Descent(97/499): loss=0.33169037406490043, w0=0.0265311912991072, w1=-0.02746176541212734\n",
      "Gradient Descent(98/499): loss=0.331569809802725, w0=0.026752113431333803, w1=-0.027521247427680348\n",
      "Gradient Descent(99/499): loss=0.3314506756811928, w0=0.026972650507272003, w1=-0.02757973589631729\n",
      "Gradient Descent(100/499): loss=0.33133293497398886, w0=0.027192805224394587, w1=-0.027637250899678247\n",
      "Gradient Descent(101/499): loss=0.33121655236040226, w0=0.027412580215495405, w1=-0.027693811980746216\n",
      "Gradient Descent(102/499): loss=0.33110149386020027, w0=0.02763197805090691, w1=-0.027749438160769073\n",
      "Gradient Descent(103/499): loss=0.3309877267718553, w0=0.02785100124063828, w1=-0.02780414795556922\n",
      "Gradient Descent(104/499): loss=0.33087521961393873, w0=0.028069652236437113, w1=-0.027857959391267277\n",
      "Gradient Descent(105/499): loss=0.3307639420695084, w0=0.02828793343377753, w1=-0.027910890019444887\n",
      "Gradient Descent(106/499): loss=0.3306538649333245, w0=0.028505847173777424, w1=-0.027962956931770304\n",
      "Gradient Descent(107/499): loss=0.3305449600617404, w0=0.028723395745047474, w1=-0.028014176774109254\n",
      "Gradient Descent(108/499): loss=0.3304372003251209, w0=0.028940581385474364, w1=-0.02806456576014237\n",
      "Gradient Descent(109/499): loss=0.3303305595626517, w0=0.02915740628394063, w1=-0.028114139684509442\n",
      "Gradient Descent(110/499): loss=0.33022501253941094, w0=0.029373872581983377, w1=-0.0281629139354997\n",
      "Gradient Descent(111/499): loss=0.33012053490557924, w0=0.029589982375394074, w1=-0.028210903507306394\n",
      "Gradient Descent(112/499): loss=0.33001710315767474, w0=0.029805737715761536, w1=-0.02825812301186304\n",
      "Gradient Descent(113/499): loss=0.32991469460170375, w0=0.030021140611960095, w1=-0.028304586690277873\n",
      "Gradient Descent(114/499): loss=0.32981328731812476, w0=0.030236193031584913, w1=-0.028350308423882215\n",
      "Gradient Descent(115/499): loss=0.3297128601285299, w0=0.030450896902336308, w1=-0.028395301744907817\n",
      "Gradient Descent(116/499): loss=0.3296133925639507, w0=0.03066525411335489, w1=-0.02843957984680739\n",
      "Gradient Descent(117/499): loss=0.329514864834706, w0=0.030879266516509265, w1=-0.028483155594232035\n",
      "Gradient Descent(118/499): loss=0.32941725780170555, w0=0.031092935927637937, w1=-0.028526041532678504\n",
      "Gradient Descent(119/499): loss=0.3293205529491384, w0=0.031306264127747084, w1=-0.028568249897818802\n",
      "Gradient Descent(120/499): loss=0.32922473235846905, w0=0.031519252864165716, w1=-0.028609792624523928\n",
      "Gradient Descent(121/499): loss=0.3291297786836753, w0=0.03173190385165972, w1=-0.02865068135559312\n",
      "Gradient Descent(122/499): loss=0.3290356751276643, w0=0.03194421877350631, w1=-0.028690927450199473\n",
      "Gradient Descent(123/499): loss=0.32894240541980146, w0=0.03215619928253015, w1=-0.028730541992062247\n",
      "Gradient Descent(124/499): loss=0.32884995379450144, w0=0.03236784700210264, w1=-0.02876953579735589\n",
      "Gradient Descent(125/499): loss=0.3287583049708191, w0=0.032579163527105603, w1=-0.028807919422365165\n",
      "Gradient Descent(126/499): loss=0.32866744413299576, w0=0.03279015042486061, w1=-0.028845703170895617\n",
      "Gradient Descent(127/499): loss=0.32857735691190715, w0=0.0330008092360252, w1=-0.028882897101448\n",
      "Gradient Descent(128/499): loss=0.32848802936737076, w0=0.03321114147545716, w1=-0.028919511034165107\n",
      "Gradient Descent(129/499): loss=0.32839944797126563, w0=0.033421148633047994, w1=-0.028955554557559\n",
      "Gradient Descent(130/499): loss=0.32831159959142686, w0=0.03363083217452666, w1=-0.028991037035026358\n",
      "Gradient Descent(131/499): loss=0.32822447147627304, w0=0.033840193542234664, w1=-0.029025967611159303\n",
      "Gradient Descent(132/499): loss=0.3281380512401314, w0=0.03404923415587347, w1=-0.029060355217858862\n",
      "Gradient Descent(133/499): loss=0.32805232684922625, w0=0.03425795541322531, w1=-0.029094208580257814\n",
      "Gradient Descent(134/499): loss=0.3279672866082956, w0=0.03446635869084822, w1=-0.02912753622245953\n",
      "Gradient Descent(135/499): loss=0.32788291914780665, w0=0.03467444534474637, w1=-0.029160346473099074\n",
      "Gradient Descent(136/499): loss=0.32779921341174084, w0=0.03488221671101638, w1=-0.029192647470732625\n",
      "Gradient Descent(137/499): loss=0.3277161586459177, w0=0.03508967410647066, w1=-0.02922444716906104\n",
      "Gradient Descent(138/499): loss=0.327633744386835, w0=0.035296818829238505, w1=-0.029255753341993162\n",
      "Gradient Descent(139/499): loss=0.32755196045099677, w0=0.03550365215934575, w1=-0.029286573588554254\n",
      "Gradient Descent(140/499): loss=0.32747079692470676, w0=0.03571017535927375, w1=-0.029316915337644765\n",
      "Gradient Descent(141/499): loss=0.3273902441543045, w0=0.03591638967449849, w1=-0.029346785852654395\n",
      "Gradient Descent(142/499): loss=0.32731029273682377, w0=0.0361222963340104, w1=-0.02937619223593629\n",
      "Gradient Descent(143/499): loss=0.32723093351105104, w0=0.03632789655081574, w1=-0.029405141433145984\n",
      "Gradient Descent(144/499): loss=0.32715215754896565, w0=0.036533191522420065, w1=-0.029433640237449564\n",
      "Gradient Descent(145/499): loss=0.3270739561475451, w0=0.03673818243129454, w1=-0.02946169529360534\n",
      "Gradient Descent(146/499): loss=0.3269963208209149, w0=0.036942870445325626, w1=-0.029489313101923185\n",
      "Gradient Descent(147/499): loss=0.3269192432928297, w0=0.037147256718248776, w1=-0.029516500022105516\n",
      "Gradient Descent(148/499): loss=0.3268427154894685, w0=0.03735134239006673, w1=-0.029543262276973816\n",
      "Gradient Descent(149/499): loss=0.3267667295325296, w0=0.03755512858745296, w1=-0.029569605956084325\n",
      "Gradient Descent(150/499): loss=0.3266912777326102, w0=0.03775861642414075, w1=-0.029595537019236597\n",
      "Gradient Descent(151/499): loss=0.32661635258286037, w0=0.03796180700129854, w1=-0.029621061299878272\n",
      "Gradient Descent(152/499): loss=0.3265419467528947, w0=0.03816470140789186, w1=-0.029646184508409484\n",
      "Gradient Descent(153/499): loss=0.3264680530829537, w0=0.03836730072103258, w1=-0.02967091223539007\n",
      "Gradient Descent(154/499): loss=0.3263946645782987, w0=0.038569606006315656, w1=-0.029695249954652708\n",
      "Gradient Descent(155/499): loss=0.3263217744038355, w0=0.03877161831814406, w1=-0.029719203026324987\n",
      "Gradient Descent(156/499): loss=0.32624937587895103, w0=0.03897333870004218, w1=-0.0297427766997633\n",
      "Gradient Descent(157/499): loss=0.32617746247255663, w0=0.03917476818495818, w1=-0.029765976116401353\n",
      "Gradient Descent(158/499): loss=0.32610602779832654, w0=0.03937590779555566, w1=-0.029788806312515997\n",
      "Gradient Descent(159/499): loss=0.3260350656101245, w0=0.039576758544495085, w1=-0.029811272221912984\n",
      "Gradient Descent(160/499): loss=0.3259645697976091, w0=0.03977732143470526, w1=-0.029833378678535184\n",
      "Gradient Descent(161/499): loss=0.32589453438201, w0=0.03997759745964529, w1=-0.02985513041899568\n",
      "Gradient Descent(162/499): loss=0.32582495351206764, w0=0.04017758760355731, w1=-0.029876532085038092\n",
      "Gradient Descent(163/499): loss=0.3257558214601286, w0=0.040377292841710384, w1=-0.029897588225926423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(164/499): loss=0.3256871326183909, w0=0.04057671414063583, w1=-0.029918303300766606\n",
      "Gradient Descent(165/499): loss=0.3256188814952921, w0=0.04077585245835431, w1=-0.029938681680761883\n",
      "Gradient Descent(166/499): loss=0.3255510627120327, w0=0.040974708744595, w1=-0.029958727651404053\n",
      "Gradient Descent(167/499): loss=0.32548367099923176, w0=0.04117328394100711, w1=-0.02997844541460262\n",
      "Gradient Descent(168/499): loss=0.32541670119370597, w0=0.04137157898136399, w1=-0.02999783909075368\n",
      "Gradient Descent(169/499): loss=0.32535014823536934, w0=0.04156959479176021, w1=-0.0300169127207505\n",
      "Gradient Descent(170/499): loss=0.3252840071642463, w0=0.04176733229080171, w1=-0.030035670267937485\n",
      "Gradient Descent(171/499): loss=0.3252182731175955, w0=0.04196479238978944, w1=-0.030054115620009342\n",
      "Gradient Descent(172/499): loss=0.3251529413271373, w0=0.04216197599289653, w1=-0.030072252590857086\n",
      "Gradient Descent(173/499): loss=0.32508800711638264, w0=0.042358883997339475, w1=-0.030090084922362505\n",
      "Gradient Descent(174/499): loss=0.3250234658980575, w0=0.04255551729354325, w1=-0.030107616286142683\n",
      "Gradient Descent(175/499): loss=0.32495931317161975, w0=0.04275187676530091, w1=-0.030124850285246065\n",
      "Gradient Descent(176/499): loss=0.3248955445208646, w0=0.042947963289927525, w1=-0.030141790455801558\n",
      "Gradient Descent(177/499): loss=0.3248321556116144, w0=0.04314377773840901, w1=-0.030158440268622088\n",
      "Gradient Descent(178/499): loss=0.3247691421894896, w0=0.04333932097554569, w1=-0.030174803130763962\n",
      "Gradient Descent(179/499): loss=0.32470650007775764, w0=0.043534593860091106, w1=-0.030190882387043413\n",
      "Gradient Descent(180/499): loss=0.32464422517525704, w0=0.04372959724488597, w1=-0.030206681321511573\n",
      "Gradient Descent(181/499): loss=0.3245823134543919, w0=0.04392433197698761, w1=-0.030222203158889144\n",
      "Gradient Descent(182/499): loss=0.32452076095919635, w0=0.044118798897795075, w1=-0.03023745106596197\n",
      "Gradient Descent(183/499): loss=0.3244595638034642, w0=0.04431299884316988, w1=-0.030252428152938673\n",
      "Gradient Descent(184/499): loss=0.32439871816894217, w0=0.04450693264355277, w1=-0.030267137474771486\n",
      "Gradient Descent(185/499): loss=0.3243382203035837, w0=0.044700601124076526, w1=-0.03028158203244139\n",
      "Gradient Descent(186/499): loss=0.3242780665198616, w0=0.044894005104674936, w1=-0.0302957647742086\n",
      "Gradient Descent(187/499): loss=0.32421825319313613, w0=0.04508714540018817, w1=-0.030309688596829414\n",
      "Gradient Descent(188/499): loss=0.3241587767600772, w0=0.04528002282046459, w1=-0.03032335634674047\n",
      "Gradient Descent(189/499): loss=0.32409963371713874, w0=0.04547263817045918, w1=-0.030336770821211317\n",
      "Gradient Descent(190/499): loss=0.3240408206190819, w0=0.045664992250328704, w1=-0.030349934769466253\n",
      "Gradient Descent(191/499): loss=0.3239823340775466, w0=0.045857085855523706, w1=-0.03036285089377635\n",
      "Gradient Descent(192/499): loss=0.3239241707596695, w0=0.04604891977687745, w1=-0.0303755218505225\n",
      "Gradient Descent(193/499): loss=0.3238663273867447, w0=0.046240494800691964, w1=-0.030387950251230388\n",
      "Gradient Descent(194/499): loss=0.3238088007329288, w0=0.04643181170882122, w1=-0.03040013866357815\n",
      "Gradient Descent(195/499): loss=0.3237515876239853, w0=0.04662287127875163, w1=-0.030412089612377544\n",
      "Gradient Descent(196/499): loss=0.3236946849360693, w0=0.046813674283679864, w1=-0.030423805580529404\n",
      "Gradient Descent(197/499): loss=0.32363808959454954, w0=0.04700422149258819, w1=-0.030435289009954115\n",
      "Gradient Descent(198/499): loss=0.32358179857286706, w0=0.04719451367031732, w1=-0.03044654230249783\n",
      "Gradient Descent(199/499): loss=0.3235258088914289, w0=0.04738455157763696, w1=-0.030457567820815136\n",
      "Gradient Descent(200/499): loss=0.3234701176165364, w0=0.047574335971314025, w1=-0.03046836788922885\n",
      "Gradient Descent(201/499): loss=0.32341472185934406, w0=0.04776386760417872, w1=-0.030478944794567583\n",
      "Gradient Descent(202/499): loss=0.3233596187748518, w0=0.04795314722518851, w1=-0.030489300786981723\n",
      "Gradient Descent(203/499): loss=0.32330480556092633, w0=0.04814217557948998, w1=-0.03049943808073848\n",
      "Gradient Descent(204/499): loss=0.3232502794573522, w0=0.048330953408478865, w1=-0.030509358854996522\n",
      "Gradient Descent(205/499): loss=0.32319603774491057, w0=0.04851948144985806, w1=-0.03051906525456087\n",
      "Gradient Descent(206/499): loss=0.3231420777444846, w0=0.048707760437693884, w1=-0.03052855939061854\n",
      "Gradient Descent(207/499): loss=0.32308839681619217, w0=0.04889579110247057, w1=-0.030537843341455515\n",
      "Gradient Descent(208/499): loss=0.3230349923585425, w0=0.049083574171143035, w1=-0.030546919153155574\n",
      "Gradient Descent(209/499): loss=0.32298186180761773, w0=0.04927111036718805, w1=-0.030555788840281477\n",
      "Gradient Descent(210/499): loss=0.3229290026362777, w0=0.04945840041065381, w1=-0.03056445438653901\n",
      "Gradient Descent(211/499): loss=0.3228764123533874, w0=0.04964544501820797, w1=-0.03057291774542437\n",
      "Gradient Descent(212/499): loss=0.3228240885030666, w0=0.04983224490318428, w1=-0.030581180840855357\n",
      "Gradient Descent(213/499): loss=0.3227720286639594, w0=0.0500188007756277, w1=-0.03058924556778684\n",
      "Gradient Descent(214/499): loss=0.3227202304485252, w0=0.05020511334233825, w1=-0.0305971137928109\n",
      "Gradient Descent(215/499): loss=0.32266869150234934, w0=0.05039118330691347, w1=-0.030604787354742116\n",
      "Gradient Descent(216/499): loss=0.32261740950347184, w0=0.05057701136978969, w1=-0.030612268065188384\n",
      "Gradient Descent(217/499): loss=0.32256638216173517, w0=0.05076259822828196, w1=-0.030619557709107674\n",
      "Gradient Descent(218/499): loss=0.3225156072181494, w0=0.05094794457662293, w1=-0.030626658045351116\n",
      "Gradient Descent(219/499): loss=0.32246508244427496, w0=0.05113305110600048, w1=-0.030633570807192794\n",
      "Gradient Descent(220/499): loss=0.32241480564162106, w0=0.051317918504594315, w1=-0.030640297702846604\n",
      "Gradient Descent(221/499): loss=0.3223647746410608, w0=0.051502547457611486, w1=-0.03064684041597054\n",
      "Gradient Descent(222/499): loss=0.32231498730226205, w0=0.05168693864732086, w1=-0.030653200606158764\n",
      "Gradient Descent(223/499): loss=0.32226544151313186, w0=0.05187109275308663, w1=-0.03065937990942174\n",
      "Gradient Descent(224/499): loss=0.3222161351892772, w0=0.05205501045140089, w1=-0.03066537993865485\n",
      "Gradient Descent(225/499): loss=0.32216706627347835, w0=0.05223869241591522, w1=-0.030671202284095707\n",
      "Gradient Descent(226/499): loss=0.3221182327351759, w0=0.05242213931747148, w1=-0.030676848513770526\n",
      "Gradient Descent(227/499): loss=0.32206963256997234, w0=0.05260535182413164, w1=-0.030682320173929866\n",
      "Gradient Descent(228/499): loss=0.32202126379914425, w0=0.05278833060120689, w1=-0.03068761878947396\n",
      "Gradient Descent(229/499): loss=0.3219731244691679, w0=0.05297107631128587, w1=-0.03069274586436798\n",
      "Gradient Descent(230/499): loss=0.32192521265125756, w0=0.05315358961426217, w1=-0.03069770288204749\n",
      "Gradient Descent(231/499): loss=0.32187752644091316, w0=0.05333587116736109, w1=-0.030702491305814317\n",
      "Gradient Descent(232/499): loss=0.32183006395748126, w0=0.053517921625165644, w1=-0.030707112579223133\n",
      "Gradient Descent(233/499): loss=0.3217828233437253, w0=0.053699741639641925, w1=-0.030711568126458994\n",
      "Gradient Descent(234/499): loss=0.32173580276540814, w0=0.05388133186016376, w1=-0.030715859352706043\n",
      "Gradient Descent(235/499): loss=0.32168900041088216, w0=0.054062692933536736, w1=-0.03071998764450766\n",
      "Gradient Descent(236/499): loss=0.321642414490692, w0=0.054243825504021605, w1=-0.030723954370118236\n",
      "Gradient Descent(237/499): loss=0.3215960432371842, w0=0.05442473021335707, w1=-0.03072776087984682\n",
      "Gradient Descent(238/499): loss=0.3215498849041292, w0=0.05460540770078203, w1=-0.030731408506392863\n",
      "Gradient Descent(239/499): loss=0.32150393776634895, w0=0.054785858603057194, w1=-0.030734898565174223\n",
      "Gradient Descent(240/499): loss=0.321458200119356, w0=0.05496608355448624, w1=-0.03073823235464769\n",
      "Gradient Descent(241/499): loss=0.321412670278999, w0=0.05514608318693638, w1=-0.03074141115662217\n",
      "Gradient Descent(242/499): loss=0.32136734658111843, w0=0.05532585812985845, w1=-0.030744436236564768\n",
      "Gradient Descent(243/499): loss=0.321322227381208, w0=0.055505409010306504, w1=-0.03074730884389992\n",
      "Gradient Descent(244/499): loss=0.32127731105408625, w0=0.055684736452956955, w1=-0.030750030212301768\n",
      "Gradient Descent(245/499): loss=0.32123259599357323, w0=0.055863841080127236, w1=-0.030752601559979956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(246/499): loss=0.3211880806121766, w0=0.05604272351179403, w1=-0.030755024089959\n",
      "Gradient Descent(247/499): loss=0.32114376334078315, w0=0.056221384365611075, w1=-0.030757298990351428\n",
      "Gradient Descent(248/499): loss=0.32109964262835855, w0=0.05639982425692656, w1=-0.030759427434624804\n",
      "Gradient Descent(249/499): loss=0.3210557169416527, w0=0.05657804379880011, w1=-0.03076141058186283\n",
      "Gradient Descent(250/499): loss=0.3210119847649122, w0=0.05675604360201943, w1=-0.030763249577020688\n",
      "Gradient Descent(251/499): loss=0.32096844459959895, w0=0.05693382427511653, w1=-0.030764945551174684\n",
      "Gradient Descent(252/499): loss=0.3209250949641149, w0=0.057111386424383584, w1=-0.030766499621766474\n",
      "Gradient Descent(253/499): loss=0.32088193439353296, w0=0.05728873065388853, w1=-0.030767912892841878\n",
      "Gradient Descent(254/499): loss=0.3208389614393331, w0=0.05746585756549021, w1=-0.030769186455284498\n",
      "Gradient Descent(255/499): loss=0.320796174669145, w0=0.057642767758853304, w1=-0.030770321387044247\n",
      "Gradient Descent(256/499): loss=0.3207535726664953, w0=0.057819461831462846, w1=-0.030771318753360907\n",
      "Gradient Descent(257/499): loss=0.32071115403056083, w0=0.05799594037863854, w1=-0.030772179606982867\n",
      "Gradient Descent(258/499): loss=0.3206689173759267, w0=0.05817220399354869, w1=-0.030772904988381117\n",
      "Gradient Descent(259/499): loss=0.32062686133234986, w0=0.05834825326722393, w1=-0.030773495925958672\n",
      "Gradient Descent(260/499): loss=0.3205849845445264, w0=0.0585240887885706, w1=-0.030773953436255493\n",
      "Gradient Descent(261/499): loss=0.32054328567186574, w0=0.05869971114438392, w1=-0.030774278524149035\n",
      "Gradient Descent(262/499): loss=0.3205017633882671, w0=0.0588751209193609, w1=-0.030774472183050537\n",
      "Gradient Descent(263/499): loss=0.32046041638190215, w0=0.059050318696112956, w1=-0.030774535395097134\n",
      "Gradient Descent(264/499): loss=0.3204192433550019, w0=0.05922530505517835, w1=-0.03077446913133993\n",
      "Gradient Descent(265/499): loss=0.32037824302364654, w0=0.059400080575034334, w1=-0.03077427435192809\n",
      "Gradient Descent(266/499): loss=0.3203374141175619, w0=0.05957464583210914, w1=-0.03077395200628908\n",
      "Gradient Descent(267/499): loss=0.3202967553799174, w0=0.05974900140079371, w1=-0.030773503033305134\n",
      "Gradient Descent(268/499): loss=0.32025626556713005, w0=0.059923147853453186, w1=-0.03077292836148604\n",
      "Gradient Descent(269/499): loss=0.32021594344867055, w0=0.06009708576043829, w1=-0.030772228909138336\n",
      "Gradient Descent(270/499): loss=0.32017578780687483, w0=0.06027081569009641, w1=-0.030771405584531012\n",
      "Gradient Descent(271/499): loss=0.32013579743675846, w0=0.060444338208782554, w1=-0.03077045928605777\n",
      "Gradient Descent(272/499): loss=0.3200959711458341, w0=0.0606176538808701, w1=-0.03076939090239598\n",
      "Gradient Descent(273/499): loss=0.3200563077539339, w0=0.060790763268761365, w1=-0.03076820131266234\n",
      "Gradient Descent(274/499): loss=0.32001680609303396, w0=0.060963666932898015, w1=-0.030766891386565385\n",
      "Gradient Descent(275/499): loss=0.3199774650070826, w0=0.0611363654317713, w1=-0.030765461984554886\n",
      "Gradient Descent(276/499): loss=0.3199382833518323, w0=0.06130885932193211, w1=-0.030763913957968205\n",
      "Gradient Descent(277/499): loss=0.3198992599946743, w0=0.061481149158000924, w1=-0.030762248149173707\n",
      "Gradient Descent(278/499): loss=0.3198603938144761, w0=0.06165323549267753, w1=-0.03076046539171127\n",
      "Gradient Descent(279/499): loss=0.3198216837014227, w0=0.06182511887675067, w1=-0.030758566510429977\n",
      "Gradient Descent(280/499): loss=0.31978312855686053, w0=0.0619967998591075, w1=-0.030756552321623064\n",
      "Gradient Descent(281/499): loss=0.31974472729314357, w0=0.06216827898674294, w1=-0.03075442363316015\n",
      "Gradient Descent(282/499): loss=0.31970647883348335, w0=0.06233955680476885, w1=-0.030752181244616865\n",
      "Gradient Descent(283/499): loss=0.31966838211180093, w0=0.06251063385642312, w1=-0.030749825947401905\n",
      "Gradient Descent(284/499): loss=0.3196304360725819, w0=0.06268151068307862, w1=-0.030747358524881557\n",
      "Gradient Descent(285/499): loss=0.319592639670734, w0=0.06285218782425199, w1=-0.030744779752501807\n",
      "Gradient Descent(286/499): loss=0.31955499187144687, w0=0.06302266581761237, w1=-0.030742090397908026\n",
      "Gradient Descent(287/499): loss=0.31951749165005483, w0=0.06319294519898998, w1=-0.030739291221062337\n",
      "Gradient Descent(288/499): loss=0.3194801379919022, w0=0.0633630265023846, w1=-0.03073638297435868\n",
      "Gradient Descent(289/499): loss=0.3194429298922101, w0=0.06353291025997393, w1=-0.030733366402735655\n",
      "Gradient Descent(290/499): loss=0.3194058663559468, w0=0.06370259700212186, w1=-0.03073024224378719\n",
      "Gradient Descent(291/499): loss=0.31936894639769897, w0=0.06387208725738665, w1=-0.030727011227871047\n",
      "Gradient Descent(292/499): loss=0.3193321690415468, w0=0.06404138155252893, w1=-0.030723674078215277\n",
      "Gradient Descent(293/499): loss=0.31929553332093974, w0=0.06421048041251977, w1=-0.030720231511022614\n",
      "Gradient Descent(294/499): loss=0.31925903827857505, w0=0.06437938436054842, w1=-0.030716684235572892\n",
      "Gradient Descent(295/499): loss=0.3192226829662791, w0=0.06454809391803022, w1=-0.0307130329543235\n",
      "Gradient Descent(296/499): loss=0.319186466444889, w0=0.06471660960461423, w1=-0.030709278363007953\n",
      "Gradient Descent(297/499): loss=0.3191503877841382, w0=0.06488493193819082, w1=-0.03070542115073258\n",
      "Gradient Descent(298/499): loss=0.3191144460625423, w0=0.06505306143489925, w1=-0.030701462000071407\n",
      "Gradient Descent(299/499): loss=0.3190786403672881, w0=0.06522099860913508, w1=-0.03069740158715926\n",
      "Gradient Descent(300/499): loss=0.3190429697941234, w0=0.06538874397355754, w1=-0.03069324058178313\n",
      "Gradient Descent(301/499): loss=0.3190074334472497, w0=0.06555629803909684, w1=-0.030688979647471804\n",
      "Gradient Descent(302/499): loss=0.31897203043921596, w0=0.06572366131496134, w1=-0.030684619441583894\n",
      "Gradient Descent(303/499): loss=0.31893675989081416, w0=0.06589083430864477, w1=-0.03068016061539418\n",
      "Gradient Descent(304/499): loss=0.31890162093097696, w0=0.06605781752593323, w1=-0.030675603814178394\n",
      "Gradient Descent(305/499): loss=0.3188666126966764, w0=0.06622461147091224, w1=-0.030670949677296427\n",
      "Gradient Descent(306/499): loss=0.3188317343328251, w0=0.06639121664597367, w1=-0.030666198838274025\n",
      "Gradient Descent(307/499): loss=0.3187969849921781, w0=0.06655763355182258, w1=-0.030661351924883\n",
      "Gradient Descent(308/499): loss=0.31876236383523726, w0=0.06672386268748409, w1=-0.030656409559219954\n",
      "Gradient Descent(309/499): loss=0.31872787003015646, w0=0.06688990455031012, w1=-0.030651372357783615\n",
      "Gradient Descent(310/499): loss=0.31869350275264796, w0=0.067055759635986, w1=-0.03064624093155074\n",
      "Gradient Descent(311/499): loss=0.3186592611858923, w0=0.06722142843853725, w1=-0.03064101588605068\n",
      "Gradient Descent(312/499): loss=0.3186251445204462, w0=0.06738691145033601, w1=-0.030635697821438587\n",
      "Gradient Descent(313/499): loss=0.31859115195415577, w0=0.06755220916210765, w1=-0.030630287332567323\n",
      "Gradient Descent(314/499): loss=0.3185572826920676, w0=0.06771732206293721, w1=-0.030624785009058093\n",
      "Gradient Descent(315/499): loss=0.3185235359463442, w0=0.06788225064027582, w1=-0.030619191435369803\n",
      "Gradient Descent(316/499): loss=0.318489910936178, w0=0.06804699537994707, w1=-0.03061350719086722\n",
      "Gradient Descent(317/499): loss=0.3184564068877088, w0=0.06821155676615329, w1=-0.030607732849887895\n",
      "Gradient Descent(318/499): loss=0.31842302303394177, w0=0.06837593528148186, w1=-0.030601868981807945\n",
      "Gradient Descent(319/499): loss=0.31838975861466623, w0=0.06854013140691143, w1=-0.030595916151106663\n",
      "Gradient Descent(320/499): loss=0.31835661287637673, w0=0.06870414562181801, w1=-0.030589874917430003\n",
      "Gradient Descent(321/499): loss=0.31832358507219405, w0=0.06886797840398123, w1=-0.03058374583565296\n",
      "Gradient Descent(322/499): loss=0.3182906744617881, w0=0.06903163022959032, w1=-0.03057752945594087\n",
      "Gradient Descent(323/499): loss=0.3182578803113023, w0=0.06919510157325016, w1=-0.030571226323809667\n",
      "Gradient Descent(324/499): loss=0.31822520189327874, w0=0.06935839290798734, w1=-0.03056483698018506\n",
      "Gradient Descent(325/499): loss=0.31819263848658375, w0=0.06952150470525606, w1=-0.030558361961460744\n",
      "Gradient Descent(326/499): loss=0.3181601893763361, w0=0.06968443743494405, w1=-0.03055180179955558\n",
      "Gradient Descent(327/499): loss=0.3181278538538348, w0=0.06984719156537848, w1=-0.030545157021969832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(328/499): loss=0.3180956312164891, w0=0.07000976756333176, w1=-0.030538428151840404\n",
      "Gradient Descent(329/499): loss=0.31806352076774835, w0=0.07017216589402735, w1=-0.030531615707995192\n",
      "Gradient Descent(330/499): loss=0.3180315218170344, w0=0.07033438702114554, w1=-0.030524720205006486\n",
      "Gradient Descent(331/499): loss=0.3179996336796731, w0=0.0704964314068292, w1=-0.03051774215324349\n",
      "Gradient Descent(332/499): loss=0.3179678556768289, w0=0.07065829951168939, w1=-0.03051068205892395\n",
      "Gradient Descent(333/499): loss=0.3179361871354382, w0=0.07081999179481113, w1=-0.03050354042416493\n",
      "Gradient Descent(334/499): loss=0.3179046273881461, w0=0.07098150871375893, w1=-0.030496317747032747\n",
      "Gradient Descent(335/499): loss=0.31787317577324126, w0=0.07114285072458242, w1=-0.030489014521592056\n",
      "Gradient Descent(336/499): loss=0.31784183163459445, w0=0.07130401828182191, w1=-0.03048163123795415\n",
      "Gradient Descent(337/499): loss=0.3178105943215958, w0=0.07146501183851393, w1=-0.03047416838232446\n",
      "Gradient Descent(338/499): loss=0.31777946318909434, w0=0.0716258318461967, w1=-0.030466626437049248\n",
      "Gradient Descent(339/499): loss=0.3177484375973378, w0=0.0717864787549156, w1=-0.03045900588066158\n",
      "Gradient Descent(340/499): loss=0.31771751691191297, w0=0.07194695301322863, w1=-0.030451307187926508\n",
      "Gradient Descent(341/499): loss=0.31768670050368775, w0=0.07210725506821179, w1=-0.03044353082988556\n",
      "Gradient Descent(342/499): loss=0.3176559877487536, w0=0.07226738536546445, w1=-0.03043567727390046\n",
      "Gradient Descent(343/499): loss=0.3176253780283679, w0=0.07242734434911476, w1=-0.03042774698369619\n",
      "Gradient Descent(344/499): loss=0.31759487072889886, w0=0.07258713246182487, w1=-0.030419740419403332\n",
      "Gradient Descent(345/499): loss=0.31756446524177007, w0=0.07274675014479634, w1=-0.030411658037599744\n",
      "Gradient Descent(346/499): loss=0.3175341609634052, w0=0.0729061978377753, w1=-0.030403500291351554\n",
      "Gradient Descent(347/499): loss=0.3175039572951756, w0=0.07306547597905777, w1=-0.03039526763025353\n",
      "Gradient Descent(348/499): loss=0.31747385364334635, w0=0.07322458500549484, w1=-0.03038696050046878\n",
      "Gradient Descent(349/499): loss=0.3174438494190239, w0=0.07338352535249784, w1=-0.03037857934476784\n",
      "Gradient Descent(350/499): loss=0.3174139440381053, w0=0.07354229745404357, w1=-0.030370124602567147\n",
      "Gradient Descent(351/499): loss=0.3173841369212264, w0=0.07370090174267938, w1=-0.030361596709966913\n",
      "Gradient Descent(352/499): loss=0.3173544274937126, w0=0.07385933864952826, w1=-0.030352996099788383\n",
      "Gradient Descent(353/499): loss=0.31732481518552824, w0=0.07401760860429404, w1=-0.030344323201610544\n",
      "Gradient Descent(354/499): loss=0.3172952994312287, w0=0.07417571203526636, w1=-0.030335578441806232\n",
      "Gradient Descent(355/499): loss=0.3172658796699118, w0=0.07433364936932575, w1=-0.030326762243577714\n",
      "Gradient Descent(356/499): loss=0.3172365553451704, w0=0.07449142103194865, w1=-0.03031787502699169\n",
      "Gradient Descent(357/499): loss=0.31720732590504497, w0=0.07464902744721243, w1=-0.030308917209013783\n",
      "Gradient Descent(358/499): loss=0.3171781908019779, w0=0.07480646903780029, w1=-0.030299889203542468\n",
      "Gradient Descent(359/499): loss=0.31714914949276773, w0=0.07496374622500633, w1=-0.030290791421442524\n",
      "Gradient Descent(360/499): loss=0.3171202014385237, w0=0.07512085942874039, w1=-0.03028162427057795\n",
      "Gradient Descent(361/499): loss=0.3170913461046214, w0=0.07527780906753301, w1=-0.030272388155844376\n",
      "Gradient Descent(362/499): loss=0.317062582960659, w0=0.0754345955585403, w1=-0.030263083479201015\n",
      "Gradient Descent(363/499): loss=0.3170339114804138, w0=0.07559121931754885, w1=-0.0302537106397021\n",
      "Gradient Descent(364/499): loss=0.31700533114179924, w0=0.07574768075898052, w1=-0.030244270033527875\n",
      "Gradient Descent(365/499): loss=0.3169768414268229, w0=0.07590398029589729, w1=-0.03023476205401511\n",
      "Gradient Descent(366/499): loss=0.31694844182154497, w0=0.07606011834000612, w1=-0.030225187091687155\n",
      "Gradient Descent(367/499): loss=0.3169201318160366, w0=0.07621609530166368, w1=-0.03021554553428357\n",
      "Gradient Descent(368/499): loss=0.31689191090433977, w0=0.07637191158988113, w1=-0.03020583776678929\n",
      "Gradient Descent(369/499): loss=0.31686377858442666, w0=0.07652756761232891, w1=-0.030196064171463387\n",
      "Gradient Descent(370/499): loss=0.31683573435816054, w0=0.07668306377534144, w1=-0.03018622512786738\n",
      "Gradient Descent(371/499): loss=0.3168077777312563, w0=0.07683840048392182, w1=-0.030176321012893154\n",
      "Gradient Descent(372/499): loss=0.3167799082132422, w0=0.07699357814174658, w1=-0.03016635220079046\n",
      "Gradient Descent(373/499): loss=0.3167521253174215, w0=0.07714859715117031, w1=-0.030156319063194027\n",
      "Gradient Descent(374/499): loss=0.3167244285608347, w0=0.07730345791323033, w1=-0.03014622196915026\n",
      "Gradient Descent(375/499): loss=0.31669681746422296, w0=0.07745816082765135, w1=-0.030136061285143574\n",
      "Gradient Descent(376/499): loss=0.31666929155199036, w0=0.07761270629285005, w1=-0.03012583737512233\n",
      "Gradient Descent(377/499): loss=0.31664185035216885, w0=0.07776709470593976, w1=-0.03011555060052443\n",
      "Gradient Descent(378/499): loss=0.31661449339638176, w0=0.07792132646273496, w1=-0.030105201320302498\n",
      "Gradient Descent(379/499): loss=0.3165872202198081, w0=0.07807540195775592, w1=-0.030094789890948753\n",
      "Gradient Descent(380/499): loss=0.316560030361149, w0=0.07822932158423321, w1=-0.030084316666519492\n",
      "Gradient Descent(381/499): loss=0.31653292336259137, w0=0.07838308573411226, w1=-0.030073781998659246\n",
      "Gradient Descent(382/499): loss=0.31650589876977564, w0=0.0785366947980579, w1=-0.030063186236624594\n",
      "Gradient Descent(383/499): loss=0.3164789561317608, w0=0.0786901491654588, w1=-0.030052529727307636\n",
      "Gradient Descent(384/499): loss=0.31645209500099175, w0=0.07884344922443201, w1=-0.030041812815259136\n",
      "Gradient Descent(385/499): loss=0.31642531493326653, w0=0.07899659536182742, w1=-0.030031035842711357\n",
      "Gradient Descent(386/499): loss=0.31639861548770354, w0=0.0791495879632322, w1=-0.030020199149600565\n",
      "Gradient Descent(387/499): loss=0.31637199622670975, w0=0.07930242741297525, w1=-0.030009303073589223\n",
      "Gradient Descent(388/499): loss=0.31634545671594905, w0=0.07945511409413164, w1=-0.02999834795008788\n",
      "Gradient Descent(389/499): loss=0.31631899652431117, w0=0.07960764838852698, w1=-0.02998733411227678\n",
      "Gradient Descent(390/499): loss=0.3162926152238798, w0=0.07976003067674182, w1=-0.02997626189112712\n",
      "Gradient Descent(391/499): loss=0.31626631238990377, w0=0.07991226133811606, w1=-0.02996513161542209\n",
      "Gradient Descent(392/499): loss=0.31624008760076505, w0=0.08006434075075326, w1=-0.02995394361177757\n",
      "Gradient Descent(393/499): loss=0.31621394043795065, w0=0.08021626929152502, w1=-0.029942698204662572\n",
      "Gradient Descent(394/499): loss=0.3161878704860216, w0=0.08036804733607528, w1=-0.0299313957164194\n",
      "Gradient Descent(395/499): loss=0.31616187733258494, w0=0.08051967525882467, w1=-0.029920036467283546\n",
      "Gradient Descent(396/499): loss=0.316135960568265, w0=0.08067115343297478, w1=-0.029908620775403308\n",
      "Gradient Descent(397/499): loss=0.3161101197866739, w0=0.08082248223051243, w1=-0.029897148956859153\n",
      "Gradient Descent(398/499): loss=0.3160843545843848, w0=0.08097366202221401, w1=-0.02988562132568282\n",
      "Gradient Descent(399/499): loss=0.31605866456090326, w0=0.08112469317764964, w1=-0.029874038193876167\n",
      "Gradient Descent(400/499): loss=0.3160330493186401, w0=0.0812755760651875, w1=-0.029862399871429784\n",
      "Gradient Descent(401/499): loss=0.31600750846288456, w0=0.08142631105199795, w1=-0.029850706666341334\n",
      "Gradient Descent(402/499): loss=0.31598204160177645, w0=0.08157689850405786, w1=-0.02983895888463368\n",
      "Gradient Descent(403/499): loss=0.31595664834628157, w0=0.08172733878615471, w1=-0.02982715683037276\n",
      "Gradient Descent(404/499): loss=0.3159313283101638, w0=0.08187763226189082, w1=-0.029815300805685233\n",
      "Gradient Descent(405/499): loss=0.31590608110995977, w0=0.0820277792936875, w1=-0.029803391110775897\n",
      "Gradient Descent(406/499): loss=0.3158809063649536, w0=0.08217778024278923, w1=-0.029791428043944874\n",
      "Gradient Descent(407/499): loss=0.3158558036971512, w0=0.08232763546926775, w1=-0.029779411901604592\n",
      "Gradient Descent(408/499): loss=0.3158307727312558, w0=0.08247734533202622, w1=-0.02976734297829653\n",
      "Gradient Descent(409/499): loss=0.3158058130946427, w0=0.08262691018880335, w1=-0.029755221566707752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(410/499): loss=0.3157809244173348, w0=0.08277633039617743, w1=-0.02974304795768725\n",
      "Gradient Descent(411/499): loss=0.31575610633197937, w0=0.0829256063095705, w1=-0.029730822440262046\n",
      "Gradient Descent(412/499): loss=0.3157313584738227, w0=0.08307473828325232, w1=-0.029718545301653118\n",
      "Gradient Descent(413/499): loss=0.31570668048068806, w0=0.08322372667034454, w1=-0.029706216827291115\n",
      "Gradient Descent(414/499): loss=0.3156820719929515, w0=0.08337257182282466, w1=-0.02969383730083187\n",
      "Gradient Descent(415/499): loss=0.31565753265351837, w0=0.08352127409153007, w1=-0.029681407004171732\n",
      "Gradient Descent(416/499): loss=0.31563306210780157, w0=0.08366983382616208, w1=-0.029668926217462692\n",
      "Gradient Descent(417/499): loss=0.31560866000369847, w0=0.08381825137528993, w1=-0.029656395219127333\n",
      "Gradient Descent(418/499): loss=0.3155843259915682, w0=0.08396652708635478, w1=-0.029643814285873583\n",
      "Gradient Descent(419/499): loss=0.31556005972421064, w0=0.08411466130567366, w1=-0.029631183692709298\n",
      "Gradient Descent(420/499): loss=0.31553586085684293, w0=0.08426265437844344, w1=-0.029618503712956663\n",
      "Gradient Descent(421/499): loss=0.31551172904708036, w0=0.08441050664874478, w1=-0.02960577461826641\n",
      "Gradient Descent(422/499): loss=0.31548766395491223, w0=0.0845582184595461, w1=-0.02959299667863187\n",
      "Gradient Descent(423/499): loss=0.31546366524268316, w0=0.08470579015270746, w1=-0.029580170162402844\n",
      "Gradient Descent(424/499): loss=0.3154397325750708, w0=0.08485322206898448, w1=-0.029567295336299326\n",
      "Gradient Descent(425/499): loss=0.31541586561906537, w0=0.08500051454803223, w1=-0.029554372465425034\n",
      "Gradient Descent(426/499): loss=0.3153920640439498, w0=0.08514766792840914, w1=-0.029541401813280806\n",
      "Gradient Descent(427/499): loss=0.31536832752127963, w0=0.08529468254758085, w1=-0.02952838364177781\n",
      "Gradient Descent(428/499): loss=0.3153446557248619, w0=0.08544155874192408, w1=-0.029515318211250616\n",
      "Gradient Descent(429/499): loss=0.3153210483307365, w0=0.08558829684673046, w1=-0.029502205780470105\n",
      "Gradient Descent(430/499): loss=0.31529750501715637, w0=0.08573489719621036, w1=-0.029489046606656213\n",
      "Gradient Descent(431/499): loss=0.3152740254645682, w0=0.08588136012349674, w1=-0.029475840945490554\n",
      "Gradient Descent(432/499): loss=0.3152506093555929, w0=0.08602768596064894, w1=-0.029462589051128866\n",
      "Gradient Descent(433/499): loss=0.3152272563750073, w0=0.08617387503865645, w1=-0.029449291176213324\n",
      "Gradient Descent(434/499): loss=0.3152039662097253, w0=0.08631992768744272, w1=-0.029435947571884714\n",
      "Gradient Descent(435/499): loss=0.3151807385487789, w0=0.08646584423586894, w1=-0.029422558487794452\n",
      "Gradient Descent(436/499): loss=0.3151575730833006, w0=0.08661162501173776, w1=-0.029409124172116472\n",
      "Gradient Descent(437/499): loss=0.3151344695065048, w0=0.08675727034179709, w1=-0.029395644871558983\n",
      "Gradient Descent(438/499): loss=0.31511142751367016, w0=0.08690278055174375, w1=-0.029382120831376073\n",
      "Gradient Descent(439/499): loss=0.31508844680212145, w0=0.08704815596622727, w1=-0.029368552295379203\n",
      "Gradient Descent(440/499): loss=0.3150655270712127, w0=0.08719339690885355, w1=-0.029354939505948547\n",
      "Gradient Descent(441/499): loss=0.31504266802230946, w0=0.08733850370218858, w1=-0.02934128270404422\n",
      "Gradient Descent(442/499): loss=0.31501986935877135, w0=0.0874834766677621, w1=-0.02932758212921737\n",
      "Gradient Descent(443/499): loss=0.31499713078593583, w0=0.08762831612607129, w1=-0.029313838019621157\n",
      "Gradient Descent(444/499): loss=0.3149744520111005, w0=0.08777302239658442, w1=-0.02930005061202158\n",
      "Gradient Descent(445/499): loss=0.3149518327435077, w0=0.08791759579774451, w1=-0.029286220141808223\n",
      "Gradient Descent(446/499): loss=0.31492927269432636, w0=0.08806203664697293, w1=-0.029272346843004848\n",
      "Gradient Descent(447/499): loss=0.31490677157663705, w0=0.08820634526067306, w1=-0.02925843094827988\n",
      "Gradient Descent(448/499): loss=0.3148843291054157, w0=0.08835052195423389, w1=-0.02924447268895679\n",
      "Gradient Descent(449/499): loss=0.31486194499751735, w0=0.0884945670420336, w1=-0.029230472295024346\n",
      "Gradient Descent(450/499): loss=0.31483961897166035, w0=0.08863848083744316, w1=-0.029216429995146745\n",
      "Gradient Descent(451/499): loss=0.3148173507484108, w0=0.08878226365282993, w1=-0.02920234601667367\n",
      "Gradient Descent(452/499): loss=0.31479514005016745, w0=0.08892591579956115, w1=-0.029188220585650196\n",
      "Gradient Descent(453/499): loss=0.3147729866011458, w0=0.08906943758800757, w1=-0.029174053926826614\n",
      "Gradient Descent(454/499): loss=0.31475089012736357, w0=0.08921282932754696, w1=-0.029159846263668132\n",
      "Gradient Descent(455/499): loss=0.31472885035662523, w0=0.08935609132656759, w1=-0.029145597818364486\n",
      "Gradient Descent(456/499): loss=0.3147068670185076, w0=0.08949922389247185, w1=-0.029131308811839444\n",
      "Gradient Descent(457/499): loss=0.314684939844345, w0=0.08964222733167965, w1=-0.0291169794637602\n",
      "Gradient Descent(458/499): loss=0.31466306856721415, w0=0.08978510194963195, w1=-0.02910260999254668\n",
      "Gradient Descent(459/499): loss=0.31464125292192113, w0=0.08992784805079426, w1=-0.02908820061538073\n",
      "Gradient Descent(460/499): loss=0.3146194926449859, w0=0.0900704659386601, w1=-0.029073751548215235\n",
      "Gradient Descent(461/499): loss=0.31459778747462896, w0=0.09021295591575441, w1=-0.029059263005783122\n",
      "Gradient Descent(462/499): loss=0.31457613715075694, w0=0.09035531828363709, w1=-0.029044735201606265\n",
      "Gradient Descent(463/499): loss=0.314554541414949, w0=0.09049755334290632, w1=-0.029030168348004308\n",
      "Gradient Descent(464/499): loss=0.31453300001044326, w0=0.09063966139320208, w1=-0.029015562656103402\n",
      "Gradient Descent(465/499): loss=0.3145115126821232, w0=0.0907816427332095, w1=-0.029000918335844828\n",
      "Gradient Descent(466/499): loss=0.31449007917650434, w0=0.09092349766066228, w1=-0.028986235595993556\n",
      "Gradient Descent(467/499): loss=0.31446869924172094, w0=0.09106522647234609, w1=-0.02897151464414669\n",
      "Gradient Descent(468/499): loss=0.31444737262751254, w0=0.09120682946410193, w1=-0.028956755686741862\n",
      "Gradient Descent(469/499): loss=0.3144260990852116, w0=0.09134830693082949, w1=-0.02894195892906549\n",
      "Gradient Descent(470/499): loss=0.31440487836773023, w0=0.09148965916649052, w1=-0.028927124575261014\n",
      "Gradient Descent(471/499): loss=0.3143837102295478, w0=0.09163088646411217, w1=-0.028912252828336985\n",
      "Gradient Descent(472/499): loss=0.31436259442669795, w0=0.0917719891157903, w1=-0.02889734389017512\n",
      "Gradient Descent(473/499): loss=0.31434153071675586, w0=0.09191296741269285, w1=-0.028882397961538245\n",
      "Gradient Descent(474/499): loss=0.31432051885882717, w0=0.09205382164506307, w1=-0.028867415242078188\n",
      "Gradient Descent(475/499): loss=0.3142995586135341, w0=0.09219455210222288, w1=-0.028852395930343555\n",
      "Gradient Descent(476/499): loss=0.3142786497430046, w0=0.09233515907257615, w1=-0.028837340223787467\n",
      "Gradient Descent(477/499): loss=0.3142577920108596, w0=0.09247564284361195, w1=-0.028822248318775192\n",
      "Gradient Descent(478/499): loss=0.3142369851822018, w0=0.09261600370190784, w1=-0.02880712041059172\n",
      "Gradient Descent(479/499): loss=0.3142162290236028, w0=0.0927562419331331, w1=-0.028791956693449243\n",
      "Gradient Descent(480/499): loss=0.31419552330309314, w0=0.09289635782205198, w1=-0.028776757360494593\n",
      "Gradient Descent(481/499): loss=0.314174867790149, w0=0.09303635165252695, w1=-0.028761522603816567\n",
      "Gradient Descent(482/499): loss=0.31415426225568227, w0=0.09317622370752189, w1=-0.02874625261445322\n",
      "Gradient Descent(483/499): loss=0.3141337064720278, w0=0.09331597426910532, w1=-0.028730947582399052\n",
      "Gradient Descent(484/499): loss=0.31411320021293376, w0=0.09345560361845356, w1=-0.028715607696612157\n",
      "Gradient Descent(485/499): loss=0.3140927432535493, w0=0.09359511203585401, w1=-0.028700233145021282\n",
      "Gradient Descent(486/499): loss=0.3140723353704141, w0=0.0937344998007082, w1=-0.028684824114532825\n",
      "Gradient Descent(487/499): loss=0.3140519763414477, w0=0.09387376719153503, w1=-0.02866938079103776\n",
      "Gradient Descent(488/499): loss=0.31403166594593795, w0=0.09401291448597394, w1=-0.028653903359418517\n",
      "Gradient Descent(489/499): loss=0.3140114039645314, w0=0.09415194196078802, w1=-0.028638392003555765\n",
      "Gradient Descent(490/499): loss=0.31399119017922195, w0=0.09429084989186713, w1=-0.028622846906335153\n",
      "Gradient Descent(491/499): loss=0.31397102437334057, w0=0.09442963855423105, w1=-0.028607268249653987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(492/499): loss=0.3139509063315449, w0=0.09456830822203258, w1=-0.02859165621442783\n",
      "Gradient Descent(493/499): loss=0.31393083583980946, w0=0.09470685916856064, w1=-0.028576010980597057\n",
      "Gradient Descent(494/499): loss=0.3139108126854149, w0=0.09484529166624338, w1=-0.028560332727133333\n",
      "Gradient Descent(495/499): loss=0.3138908366569377, w0=0.09498360598665122, w1=-0.028544621632046043\n",
      "Gradient Descent(496/499): loss=0.3138709075442415, w0=0.09512180240049994, w1=-0.028528877872388662\n",
      "Gradient Descent(497/499): loss=0.3138510251384657, w0=0.09525988117765376, w1=-0.028513101624265052\n",
      "Gradient Descent(498/499): loss=0.31383118923201647, w0=0.09539784258712834, w1=-0.028497293062835723\n",
      "Gradient Descent(499/499): loss=0.3138113996185569, w0=0.09553568689709385, w1=-0.028481452362324015\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x162d8b0b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Jet  2 *****accuracy jet 78.062287858\n",
      "**** Starting Jet  3 *****\n",
      "Gradient Descent(0/499): loss=0.5000000000000024, w0=0.00015222538345797363, w1=-0.0018823800415020493\n",
      "Gradient Descent(1/499): loss=0.47562766309063986, w0=0.00042173743260116945, w1=-0.003359809643059851\n",
      "Gradient Descent(2/499): loss=0.4629769235073622, w0=0.0007738823201639273, w1=-0.0045457983181821935\n",
      "Gradient Descent(3/499): loss=0.45504763138726523, w0=0.001171983621101816, w1=-0.005521207766064697\n",
      "Gradient Descent(4/499): loss=0.4493616451913824, w0=0.0015886801647374976, w1=-0.006342818424426589\n",
      "Gradient Descent(5/499): loss=0.44488141119465807, w0=0.0020062158744417356, w1=-0.0070504522144393044\n",
      "Gradient Descent(6/499): loss=0.441119372137999, w0=0.002414146028709097, w1=-0.007672197515214919\n",
      "Gradient Descent(7/499): loss=0.43782355889443997, w0=0.00280699367029315, w1=-0.008228057854178576\n",
      "Gradient Descent(8/499): loss=0.43485242395134505, w0=0.003182447713960593, w1=-0.008732449750081509\n",
      "Gradient Descent(9/499): loss=0.43212061903885485, w0=0.0035401131498135918, w1=-0.009195902271790018\n",
      "Gradient Descent(10/499): loss=0.4295736455489714, w0=0.0038806880822285315, w1=-0.009626214579353143\n",
      "Gradient Descent(11/499): loss=0.42717507686756007, w0=0.004205439925721231, w1=-0.010029248194596198\n",
      "Gradient Descent(12/499): loss=0.42489964056809193, w0=0.004515881755439634, w1=-0.010409473204046248\n",
      "Gradient Descent(13/499): loss=0.4227292315004987, w0=0.004813579059233292, w1=-0.010770348112057428\n",
      "Gradient Descent(14/499): loss=0.4206504948554167, w0=0.0051000398450499465, w1=-0.01111458658616269\n",
      "Gradient Descent(15/499): loss=0.418653303172574, w0=0.005376657091300283, w1=-0.011444346744068686\n",
      "Gradient Descent(16/499): loss=0.4167297685236756, w0=0.005644683382387831, w1=-0.011761366964571036\n",
      "Gradient Descent(17/499): loss=0.41487358770968175, w0=0.005905224767884777, w1=-0.012067064452800607\n",
      "Gradient Descent(18/499): loss=0.4130796007103302, w0=0.006159245600850167, w1=-0.012362607619260198\n",
      "Gradient Descent(19/499): loss=0.41134348860241976, w0=0.006407579180394128, w1=-0.012648969864895358\n",
      "Gradient Descent(20/499): loss=0.4096615641373351, w0=0.006650941009219464, w1=-0.012926970025837627\n",
      "Gradient Descent(21/499): loss=0.4080306246373084, w0=0.006889942752946411, w1=-0.013197303143951043\n",
      "Gradient Descent(22/499): loss=0.4064478472345098, w0=0.007125105801181779, w1=-0.013460564144313944\n",
      "Gradient Descent(23/499): loss=0.4049107131471561, w0=0.007356873842349681, w1=-0.013717266253880166\n",
      "Gradient Descent(24/499): loss=0.4034169520495482, w0=0.007585624181246221, w1=-0.013967855477533514\n",
      "Gradient Descent(25/499): loss=0.40196450048043014, w0=0.007811677719467613, w1=-0.01421272208559186\n",
      "Gradient Descent(26/499): loss=0.40055147016198867, w0=0.00803530762961817, w1=-0.014452209811511078\n",
      "Gradient Descent(27/499): loss=0.3991761233978507, w0=0.008256746814100779, w1=-0.01468662327697568\n",
      "Gradient Descent(28/499): loss=0.39783685359450177, w0=0.008476194267555139, w1=-0.014916234031253885\n",
      "Gradient Descent(29/499): loss=0.39653216954565185, w0=0.008693820471078239, w1=-0.015141285497275087\n",
      "Gradient Descent(30/499): loss=0.3952606825252029, w0=0.008909771944225106, w1=-0.015361997047796436\n",
      "Gradient Descent(31/499): loss=0.39402109551295783, w0=0.009124175072519239, w1=-0.01557856738395734\n",
      "Gradient Descent(32/499): loss=0.3928121940691848, w0=0.009337139316971764, w1=-0.01579117735038954\n",
      "Gradient Descent(33/499): loss=0.3916328385072477, w0=0.009548759899851319, w1=-0.015999992292289725\n",
      "Gradient Descent(34/499): loss=0.3904819571064495, w0=0.009759120048801456, w1=-0.016205164037952066\n",
      "Gradient Descent(35/499): loss=0.38935854017261506, w0=0.009968292869999259, w1=-0.016406832573406183\n",
      "Gradient Descent(36/499): loss=0.38826163480038955, w0=0.010176342910699455, w1=-0.016605127462724592\n",
      "Gradient Descent(37/499): loss=0.3871903402245612, w0=0.010383327462328128, w1=-0.016800169057320638\n",
      "Gradient Descent(38/499): loss=0.38614380367193873, w0=0.010589297647278825, w1=-0.01699206952947229\n",
      "Gradient Descent(39/499): loss=0.3851212166431516, w0=0.010794299325656006, w1=-0.01718093375887693\n",
      "Gradient Descent(40/499): loss=0.38412181156707104, w0=0.010998373852308084, w1=-0.01736686009589363\n",
      "Gradient Descent(41/499): loss=0.38314485878067084, w0=0.011201558709483672, w1=-0.01754994102098157\n",
      "Gradient Descent(42/499): loss=0.3821896637949548, w0=0.011403888036217696, w1=-0.017730263716482893\n",
      "Gradient Descent(43/499): loss=0.3812555648137012, w0=0.011605393072002194, w1=-0.01790791056416279\n",
      "Gradient Descent(44/499): loss=0.3803419304766563, w0=0.011806102529322331, w1=-0.018082959579682263\n",
      "Gradient Descent(45/499): loss=0.3794481578027544, w0=0.012006042907154384, w1=-0.018255484793342353\n",
      "Gradient Descent(46/499): loss=0.37857367031218714, w0=0.01220523875545299, w1=-0.018425556584925035\n",
      "Gradient Descent(47/499): loss=0.37771791630882895, w0=0.01240371289893381, w1=-0.018593241979204984\n",
      "Gradient Descent(48/499): loss=0.3768803673068021, w0=0.012601486627028495, w1=-0.018758604907669018\n",
      "Gradient Descent(49/499): loss=0.3760605165868707, w0=0.012798579855703337, w1=-0.018921706441117753\n",
      "Gradient Descent(50/499): loss=0.375257877870015, w0=0.012995011265850771, w1=-0.01908260499710542\n",
      "Gradient Descent(51/499): loss=0.3744719840969446, w0=0.013190798422149621, w1=-0.01924135652557333\n",
      "Gradient Descent(52/499): loss=0.3737023863035476, w0=0.01338595787561723, w1=-0.019398014675530092\n",
      "Gradient Descent(53/499): loss=0.37294865258334853, w0=0.01358050525252025, w1=-0.019552630945209815\n",
      "Gradient Descent(54/499): loss=0.37221036712898603, w0=0.013774455331851052, w1=-0.019705254817785244\n",
      "Gradient Descent(55/499): loss=0.3714871293455563, w0=0.013967822113196852, w1=-0.019855933884414097\n",
      "Gradient Descent(56/499): loss=0.3707785530293956, w0=0.014160618876514786, w1=-0.020004713956144806\n",
      "Gradient Descent(57/499): loss=0.3700842656065282, w0=0.014352858235067083, w1=-0.020151639165994835\n",
      "Gradient Descent(58/499): loss=0.3694039074255753, w0=0.014544552182556498, w1=-0.02029675206233403\n",
      "Gradient Descent(59/499): loss=0.36873713110043976, w0=0.014735712135325495, w1=-0.020440093694552203\n",
      "Gradient Descent(60/499): loss=0.3680836008985306, w0=0.014926348970336848, w1=-0.020581703691859604\n",
      "Gradient Descent(61/499): loss=0.36744299217070125, w0=0.015116473059532861, w1=-0.020721620335957862\n",
      "Gradient Descent(62/499): loss=0.36681499081944147, w0=0.015306094301071027, w1=-0.020859880628223828\n",
      "Gradient Descent(63/499): loss=0.36619929280218255, w0=0.01549522214785176, w1=-0.020996520351967543\n",
      "Gradient Descent(64/499): loss=0.3655956036668757, w0=0.015683865633686053, w1=-0.021131574130255768\n",
      "Gradient Descent(65/499): loss=0.3650036381172568, w0=0.01587203339739478, w1=-0.02126507547973252\n",
      "Gradient Descent(66/499): loss=0.3644231196054545, w0=0.016059733705085077, w1=-0.021397056860816426\n",
      "Gradient Descent(67/499): loss=0.3638537799498045, w0=0.016246974470810818, w1=-0.021527549724610105\n",
      "Gradient Descent(68/499): loss=0.3632953589759277, w0=0.016433763275792535, w1=-0.021656584556818115\n",
      "Gradient Descent(69/499): loss=0.362747604179298, w0=0.01662010738634574, w1=-0.021784190918936602\n",
      "Gradient Descent(70/499): loss=0.36221027040768716, w0=0.016806013770644876, w1=-0.021910397486948605\n",
      "Gradient Descent(71/499): loss=0.3616831195620061, w0=0.016991489114431973, w1=-0.02203523208773366\n",
      "Gradient Descent(72/499): loss=0.3611659203141979, w0=0.017176539835763956, w1=-0.022158721733378135\n",
      "Gradient Descent(73/499): loss=0.3606584478409431, w0=0.01736117209888014, w1=-0.02228089265355339\n",
      "Gradient Descent(74/499): loss=0.3601604835720512, w0=0.017545391827260823, w1=-0.022401770326111778\n",
      "Gradient Descent(75/499): loss=0.35967181495249967, w0=0.017729204715939195, w1=-0.022521379506035607\n",
      "Gradient Descent(76/499): loss=0.35919223521717014, w0=0.017912616243121432, w1=-0.022639744252860913\n",
      "Gradient Descent(77/499): loss=0.35872154317740884, w0=0.01809563168116358, w1=-0.02275688795668632\n",
      "Gradient Descent(78/499): loss=0.35825954301861, w0=0.01827825610694871, w1=-0.022872833362866857\n",
      "Gradient Descent(79/499): loss=0.3578060441080817, w0=0.018460494411703263, w1=-0.022987602595483516\n",
      "Gradient Descent(80/499): loss=0.3573608608125177, w0=0.01864235131028784, w1=-0.02310121717967116\n",
      "Gradient Descent(81/499): loss=0.35692381232444526, w0=0.018823831349994383, w1=-0.023213698062880095\n",
      "Gradient Descent(82/499): loss=0.3564947224970748, w0=0.019004938918878958, w1=-0.023325065635140196\n",
      "Gradient Descent(83/499): loss=0.3560734196870158, w0=0.019185678253656892, w1=-0.023435339748390634\n",
      "Gradient Descent(84/499): loss=0.35565973660436645, w0=0.019366053447184978, w1=-0.023544539734933027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(85/499): loss=0.35525351016972223, w0=0.0195460684555536, w1=-0.023652684425061205\n",
      "Gradient Descent(86/499): loss=0.3548545813776811, w0=0.019725727104810003, w1=-0.0237597921639165\n",
      "Gradient Descent(87/499): loss=0.3544627951664534, w0=0.01990503309733249, w1=-0.023865880827613734\n",
      "Gradient Descent(88/499): loss=0.3540780002932159, w0=0.020083990017874115, w1=-0.02397096783867957\n",
      "Gradient Descent(89/499): loss=0.35370004921487375, w0=0.020262601339293196, w1=-0.024075070180841845\n",
      "Gradient Descent(90/499): loss=0.35332879797391564, w0=0.020440870427986976, w1=-0.02417820441320561\n",
      "Gradient Descent(91/499): loss=0.3529641060890751, w0=0.020618800549043814, w1=-0.02428038668384908\n",
      "Gradient Descent(92/499): loss=0.3526058364505256, w0=0.020796394871128426, w1=-0.024381632742870318\n",
      "Gradient Descent(93/499): loss=0.35225385521935987, w0=0.02097365647111386, w1=-0.024481957954913407\n",
      "Gradient Descent(94/499): loss=0.35190803173111673, w0=0.02115058833847319, w1=-0.02458137731120081\n",
      "Gradient Descent(95/499): loss=0.3515682384031408, w0=0.0213271933794432, w1=-0.02467990544109693\n",
      "Gradient Descent(96/499): loss=0.3512343506455661, w0=0.021503474420971738, w1=-0.024777556623226213\n",
      "Gradient Descent(97/499): loss=0.3509062467757391, w0=0.021679434214459725, w1=-0.024874344796167614\n",
      "Gradient Descent(98/499): loss=0.35058380793589744, w0=0.02185507543930837, w1=-0.02497028356874587\n",
      "Gradient Descent(99/499): loss=0.35026691801394216, w0=0.022030400706281494, w1=-0.0250653862299388\n",
      "Gradient Descent(100/499): loss=0.3499554635671453, w0=0.022205412560692465, w1=-0.025159665758418628\n",
      "Gradient Descent(101/499): loss=0.3496493337486476, w0=0.02238011348542469, w1=-0.02525313483174425\n",
      "Gradient Descent(102/499): loss=0.3493484202366085, w0=0.02255450590379424, w1=-0.025345805835220385\n",
      "Gradient Descent(103/499): loss=0.34905261716588265, w0=0.022728592182262693, w1=-0.025437690870438626\n",
      "Gradient Descent(104/499): loss=0.34876182106209785, w0=0.02290237463300795, w1=-0.025528801763514516\n",
      "Gradient Descent(105/499): loss=0.34847593077802497, w0=0.023075855516360325, w1=-0.025619150073034024\n",
      "Gradient Descent(106/499): loss=0.348194847432132, w0=0.023249037043110883, w1=-0.025708747097722005\n",
      "Gradient Descent(107/499): loss=0.3479184743492206, w0=0.023421921376698684, w1=-0.025797603883844597\n",
      "Gradient Descent(108/499): loss=0.34764671700305316, w0=0.023594510635283186, w1=-0.025885731232356803\n",
      "Gradient Descent(109/499): loss=0.34737948296087945, w0=0.023766806893707835, w1=-0.025973139705805948\n",
      "Gradient Descent(110/499): loss=0.34711668182977906, w0=0.023938812185360498, w1=-0.026059839635001168\n",
      "Gradient Descent(111/499): loss=0.34685822520474185, w0=0.024110528503936166, w1=-0.02614584112545848\n",
      "Gradient Descent(112/499): loss=0.3466040266184104, w0=0.02428195780510703, w1=-0.02623115406363059\n",
      "Gradient Descent(113/499): loss=0.3463540014924135, w0=0.02445310200810484, w1=-0.02631578812293007\n",
      "Gradient Descent(114/499): loss=0.34610806709022557, w0=0.024623962997220163, w1=-0.02639975276955414\n",
      "Gradient Descent(115/499): loss=0.34586614247148684, w0=0.024794542623222917, w1=-0.026483057268118903\n",
      "Gradient Descent(116/499): loss=0.34562814844772577, w0=0.02496484270470838, w1=-0.026565710687110423\n",
      "Gradient Descent(117/499): loss=0.345394007539427, w0=0.025134865029372642, w1=-0.02664772190415982\n",
      "Gradient Descent(118/499): loss=0.3451636439343899, w0=0.025304611355221225, w1=-0.026729099611149058\n",
      "Gradient Descent(119/499): loss=0.34493698344732815, w0=0.02547408341171447, w1=-0.026809852319153945\n",
      "Gradient Descent(120/499): loss=0.3447139534806617, w0=0.025643282900853052, w1=-0.026889988363230435\n",
      "Gradient Descent(121/499): loss=0.3444944829864546, w0=0.025812211498206863, w1=-0.026969515907050136\n",
      "Gradient Descent(122/499): loss=0.34427850242945546, w0=0.025980870853890274, w1=-0.027048442947390616\n",
      "Gradient Descent(123/499): loss=0.34406594375119925, w0=0.02614926259348669, w1=-0.027126777318485857\n",
      "Gradient Descent(124/499): loss=0.34385674033513103, w0=0.02631738831892511, w1=-0.027204526696241984\n",
      "Gradient Descent(125/499): loss=0.3436508269727132, w0=0.02648524960931131, w1=-0.027281698602323153\n",
      "Gradient Descent(126/499): loss=0.3434481398304816, w0=0.02665284802171609, w1=-0.027358300408112306\n",
      "Gradient Descent(127/499): loss=0.3432486164180161, w0=0.026820185091922934, w1=-0.02743433933855124\n",
      "Gradient Descent(128/499): loss=0.34305219555679267, w0=0.026987262335137256, w1=-0.027509822475864316\n",
      "Gradient Descent(129/499): loss=0.34285881734988666, w0=0.027154081246659363, w1=-0.0275847567631699\n",
      "Gradient Descent(130/499): loss=0.3426684231524966, w0=0.027320643302523092, w1=-0.027659149007983493\n",
      "Gradient Descent(131/499): loss=0.3424809555432625, w0=0.027486949960102, w1=-0.027733005885616314\n",
      "Gradient Descent(132/499): loss=0.3422963582963491, w0=0.027653002658684894, w1=-0.027806333942472977\n",
      "Gradient Descent(133/499): loss=0.34211457635426923, w0=0.027818802820022342, w1=-0.027879139599251725\n",
      "Gradient Descent(134/499): loss=0.341935555801424, w0=0.027984351848845828, w1=-0.02795142915405057\n",
      "Gradient Descent(135/499): loss=0.3417592438383339, w0=0.028149651133360983, w1=-0.028023208785382534\n",
      "Gradient Descent(136/499): loss=0.34158558875653994, w0=0.028314702045716376, w1=-0.02809448455510307\n",
      "Gradient Descent(137/499): loss=0.3414145399141524, w0=0.02847950594244915, w1=-0.028165262411252646\n",
      "Gradient Descent(138/499): loss=0.3412460477120269, w0=0.02864406416490885, w1=-0.02823554819081727\n",
      "Gradient Descent(139/499): loss=0.34108006357054665, w0=0.02880837803966061, w1=-0.028305347622409764\n",
      "Gradient Descent(140/499): loss=0.34091653990699516, w0=0.028972448878868812, w1=-0.028374666328874355\n",
      "Gradient Descent(141/499): loss=0.34075543011349585, w0=0.029136277980662376, w1=-0.02844350982981714\n",
      "Gradient Descent(142/499): loss=0.34059668853550645, w0=0.029299866629482616, w1=-0.028511883544064844\n",
      "Gradient Descent(143/499): loss=0.34044027045084696, w0=0.029463216096414682, w1=-0.028579792792054203\n",
      "Gradient Descent(144/499): loss=0.34028613204924874, w0=0.029626327639503475, w1=-0.028647242798154247\n",
      "Gradient Descent(145/499): loss=0.340134230412405, w0=0.029789202504054913, w1=-0.028714238692923614\n",
      "Gradient Descent(146/499): loss=0.3399845234945121, w0=0.02995184192292334, w1=-0.028780785515305014\n",
      "Gradient Descent(147/499): loss=0.33983697010328373, w0=0.030114247116785852, w1=-0.028846888214758837\n",
      "Gradient Descent(148/499): loss=0.33969152988142626, w0=0.03027641929440432, w1=-0.028912551653337847\n",
      "Gradient Descent(149/499): loss=0.3395481632885602, w0=0.030438359652875675, w1=-0.02897778060770483\n",
      "Gradient Descent(150/499): loss=0.3394068315835773, w0=0.030600069377871256, w1=-0.02904257977109498\n",
      "Gradient Descent(151/499): loss=0.3392674968074181, w0=0.0307615496438657, w1=-0.029106953755224787\n",
      "Gradient Descent(152/499): loss=0.3391301217662597, w0=0.03092280161435604, w1=-0.02917090709214907\n",
      "Gradient Descent(153/499): loss=0.33899467001510275, w0=0.03108382644207152, w1=-0.029234444236067778\n",
      "Gradient Descent(154/499): loss=0.3388611058417441, w0=0.031244625269174625, w1=-0.02929756956508413\n",
      "Gradient Descent(155/499): loss=0.3387293942511274, w0=0.03140519922745388, w1=-0.02936028738291557\n",
      "Gradient Descent(156/499): loss=0.338599500950059, w0=0.03156554943850877, w1=-0.029422601920558997\n",
      "Gradient Descent(157/499): loss=0.3384713923322801, w0=0.031725677013927346, w1=-0.029484517337911686\n",
      "Gradient Descent(158/499): loss=0.33834503546388595, w0=0.031885583055456815, w1=-0.029546037725349222\n",
      "Gradient Descent(159/499): loss=0.3382203980690818, w0=0.03204526865516752, w1=-0.029607167105261754\n",
      "Gradient Descent(160/499): loss=0.3380974485162667, w0=0.032204734895610744, w1=-0.029667909433549848\n",
      "Gradient Descent(161/499): loss=0.33797615580443763, w0=0.032363982849970614, w1=-0.02972826860108115\n",
      "Gradient Descent(162/499): loss=0.33785648954990377, w0=0.03252301358221043, w1=-0.029788248435108994\n",
      "Gradient Descent(163/499): loss=0.33773841997330434, w0=0.03268182814721373, w1=-0.029847852700654168\n",
      "Gradient Descent(164/499): loss=0.33762191788692086, w0=0.03284042759092045, w1=-0.029907085101850847\n",
      "Gradient Descent(165/499): loss=0.3375069546822767, w0=0.0329988129504583, w1=-0.02996594928325782\n",
      "Gradient Descent(166/499): loss=0.33739350231801746, w0=0.03315698525426978, w1=-0.030024448831135996\n",
      "Gradient Descent(167/499): loss=0.3372815333080629, w0=0.03331494552223495, w1=-0.03008258727469321\n",
      "Gradient Descent(168/499): loss=0.33717102071002514, w0=0.03347269476579023, w1=-0.030140368087297243\n",
      "Gradient Descent(169/499): loss=0.33706193811388657, w0=0.033630233988043455, w1=-0.03019779468765804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(170/499): loss=0.3369542596309293, w0=0.033787564183885385, w1=-0.03025487044097997\n",
      "Gradient Descent(171/499): loss=0.336847959882911, w0=0.033944686340097806, w1=-0.03031159866008502\n",
      "Gradient Descent(172/499): loss=0.33674301399148177, w0=0.034101601435458484, w1=-0.03036798260650776\n",
      "Gradient Descent(173/499): loss=0.33663939756783373, w0=0.03425831044084307, w1=-0.030424025491562888\n",
      "Gradient Descent(174/499): loss=0.3365370867025805, w0=0.03441481431932416, w1=-0.03047973047738614\n",
      "Gradient Descent(175/499): loss=0.3364360579558575, w0=0.03457111402626763, w1=-0.030535100677949317\n",
      "Gradient Descent(176/499): loss=0.33633628834764184, w0=0.03472721050942641, w1=-0.03059013916005017\n",
      "Gradient Descent(177/499): loss=0.3362377553482816, w0=0.03488310470903182, w1=-0.030644848944277867\n",
      "Gradient Descent(178/499): loss=0.3361404368692349, w0=0.03503879755788254, w1=-0.030699233005954712\n",
      "Gradient Descent(179/499): loss=0.33604431125400824, w0=0.035194289981431495, w1=-0.0307532942760548\n",
      "Gradient Descent(180/499): loss=0.3359493572692933, w0=0.035349582897870505, w1=-0.030807035642100238\n",
      "Gradient Descent(181/499): loss=0.33585555409629575, w0=0.03550467721821307, w1=-0.030860459949035562\n",
      "Gradient Descent(182/499): loss=0.33576288132225174, w0=0.03565957384637522, w1=-0.030913570000080985\n",
      "Gradient Descent(183/499): loss=0.3356713189321281, w0=0.035814273679254535, w1=-0.030966368557565013\n",
      "Gradient Descent(184/499): loss=0.33558084730050025, w0=0.035968777606807535, w1=-0.031018858343737042\n",
      "Gradient Descent(185/499): loss=0.3354914471836066, w0=0.036123086512125376, w1=-0.031071042041560464\n",
      "Gradient Descent(186/499): loss=0.3354030997115722, w0=0.036277201271508006, w1=-0.031122922295486823\n",
      "Gradient Descent(187/499): loss=0.33531578638080006, w0=0.03643112275453689, w1=-0.031174501712211557\n",
      "Gradient Descent(188/499): loss=0.3352294890465243, w0=0.036584851824146225, w1=-0.031225782861411786\n",
      "Gradient Descent(189/499): loss=0.33514418991552297, w0=0.03673838933669292, w1=-0.03127676827646667\n",
      "Gradient Descent(190/499): loss=0.33505987153898653, w0=0.03689173614202519, w1=-0.03132746045516081\n",
      "Gradient Descent(191/499): loss=0.3349765168055369, w0=0.03704489308354999, w1=-0.03137786186037113\n",
      "Gradient Descent(192/499): loss=0.3348941089343958, w0=0.037197860998299186, w1=-0.03142797492073766\n",
      "Gradient Descent(193/499): loss=0.3348126314686979, w0=0.0373506407169947, w1=-0.03147780203131875\n",
      "Gradient Descent(194/499): loss=0.3347320682689445, w0=0.03750323306411246, w1=-0.03152734555423103\n",
      "Gradient Descent(195/499): loss=0.33465240350659814, w0=0.037655638857945414, w1=-0.03157660781927451\n",
      "Gradient Descent(196/499): loss=0.3345736216578083, w0=0.03780785891066544, w1=-0.03162559112454336\n",
      "Gradient Descent(197/499): loss=0.33449570749727364, w0=0.03795989402838437, w1=-0.031674297737022586\n",
      "Gradient Descent(198/499): loss=0.3344186460922304, w0=0.03811174501121408, w1=-0.03172272989317103\n",
      "Gradient Descent(199/499): loss=0.3343424227965686, w0=0.03826341265332565, w1=-0.0317708897994911\n",
      "Gradient Descent(200/499): loss=0.33426702324507085, w0=0.038414897743007693, w1=-0.03181877963308554\n",
      "Gradient Descent(201/499): loss=0.3341924333477725, w0=0.03856620106272387, w1=-0.03186640154220153\n",
      "Gradient Descent(202/499): loss=0.33411863928443924, w0=0.03871732338916957, w1=-0.03191375764676255\n",
      "Gradient Descent(203/499): loss=0.33404562749916056, w0=0.038868265493327836, w1=-0.03196085003888824\n",
      "Gradient Descent(204/499): loss=0.33397338469505566, w0=0.03901902814052454, w1=-0.03200768078340262\n",
      "Gradient Descent(205/499): loss=0.3339018978290898, w0=0.03916961209048284, w1=-0.03205425191833091\n",
      "Gradient Descent(206/499): loss=0.3338311541069987, w0=0.03932001809737693, w1=-0.03210056545538533\n",
      "Gradient Descent(207/499): loss=0.3337611409783181, w0=0.039470246909885084, w1=-0.03214662338044006\n",
      "Gradient Descent(208/499): loss=0.3336918461315167, w0=0.03962029927124208, w1=-0.03219242765399572\n",
      "Gradient Descent(209/499): loss=0.33362325748923033, w0=0.03977017591929097, w1=-0.03223798021163363\n",
      "Gradient Descent(210/499): loss=0.33355536320359397, w0=0.039919877586534175, w1=-0.03228328296446001\n",
      "Gradient Descent(211/499): loss=0.33348815165167134, w0=0.040069405000184063, w1=-0.03232833779954055\n",
      "Gradient Descent(212/499): loss=0.33342161143097837, w0=0.04021875888221284, w1=-0.03237314658032542\n",
      "Gradient Descent(213/499): loss=0.3333557313550989, w0=0.0403679399494019, w1=-0.03241771114706504\n",
      "Gradient Descent(214/499): loss=0.33329050044939146, w0=0.04051694891339066, w1=-0.0324620333172169\n",
      "Gradient Descent(215/499): loss=0.33322590794678386, w0=0.04066578648072472, w1=-0.0325061148858435\n",
      "Gradient Descent(216/499): loss=0.33316194328365434, w0=0.04081445335290362, w1=-0.03254995762600179\n",
      "Gradient Descent(217/499): loss=0.3330985960957984, w0=0.04096295022642795, w1=-0.032593563289124224\n",
      "Gradient Descent(218/499): loss=0.3330358562144762, w0=0.041111277792846054, w1=-0.03263693360539166\n",
      "Gradient Descent(219/499): loss=0.3329737136625436, w0=0.04125943673880012, w1=-0.032680070284098334\n",
      "Gradient Descent(220/499): loss=0.332912158650661, w0=0.04140742774607183, w1=-0.03272297501400906\n",
      "Gradient Descent(221/499): loss=0.33285118157358, w0=0.04155525149162754, w1=-0.03276564946370887\n",
      "Gradient Descent(222/499): loss=0.3327907730065065, w0=0.0417029086476629, w1=-0.032808095281945296\n",
      "Gradient Descent(223/499): loss=0.3327309237015373, w0=0.041850399881647106, w1=-0.03285031409796343\n",
      "Gradient Descent(224/499): loss=0.33267162458417, w0=0.04199772585636661, w1=-0.032892307521833954\n",
      "Gradient Descent(225/499): loss=0.3326128667498846, w0=0.04214488722996842, w1=-0.032934077144774386\n",
      "Gradient Descent(226/499): loss=0.3325546414607936, w0=0.04229188465600291, w1=-0.03297562453946354\n",
      "Gradient Descent(227/499): loss=0.33249694014236103, w0=0.042438718783466216, w1=-0.03301695126034952\n",
      "Gradient Descent(228/499): loss=0.33243975438018836, w0=0.04258539025684223, w1=-0.03305805884395137\n",
      "Gradient Descent(229/499): loss=0.3323830759168651, w0=0.042731899716144064, w1=-0.03309894880915443\n",
      "Gradient Descent(230/499): loss=0.3323268966488844, w0=0.04287824779695522, w1=-0.033139622657499694\n",
      "Gradient Descent(231/499): loss=0.33227120862361964, w0=0.04302443513047022, w1=-0.03318008187346722\n",
      "Gradient Descent(232/499): loss=0.33221600403636475, w0=0.04317046234353492, w1=-0.03322032792475376\n",
      "Gradient Descent(233/499): loss=0.3321612752274316, w0=0.04331633005868638, w1=-0.033260362262544724\n",
      "Gradient Descent(234/499): loss=0.3321070146793086, w0=0.0434620388941923, w1=-0.033300186321780674\n",
      "Gradient Descent(235/499): loss=0.33205321501387575, w0=0.04360758946409011, w1=-0.033339801521418404\n",
      "Gradient Descent(236/499): loss=0.3319998689896754, w0=0.04375298237822567, w1=-0.03337920926468678\n",
      "Gradient Descent(237/499): loss=0.3319469694992399, w0=0.04389821824229152, w1=-0.03341841093933743\n",
      "Gradient Descent(238/499): loss=0.3318945095664713, w0=0.04404329765786484, w1=-0.03345740791789045\n",
      "Gradient Descent(239/499): loss=0.33184248234407504, w0=0.04418822122244493, w1=-0.03349620155787519\n",
      "Gradient Descent(240/499): loss=0.33179088111104516, w0=0.0443329895294904, w1=-0.03353479320206624\n",
      "Gradient Descent(241/499): loss=0.33173969927020014, w0=0.044477603168455955, w1=-0.033573184178714806\n",
      "Gradient Descent(242/499): loss=0.33168893034576785, w0=0.04462206272482879, w1=-0.03361137580177545\n",
      "Gradient Descent(243/499): loss=0.3316385679810201, w0=0.04476636878016468, w1=-0.03364936937112843\n",
      "Gradient Descent(244/499): loss=0.3315886059359541, w0=0.044910521912123635, w1=-0.03368716617279764\n",
      "Gradient Descent(245/499): loss=0.33153903808501983, w0=0.0450545226945053, w1=-0.0337247674791643\n",
      "Gradient Descent(246/499): loss=0.33148985841489553, w0=0.04519837169728389, w1=-0.033762174549176516\n",
      "Gradient Descent(247/499): loss=0.3314410610223042, w0=0.045342069486642864, w1=-0.03379938862855473\n",
      "Gradient Descent(248/499): loss=0.33139264011187675, w0=0.0454856166250092, w1=-0.03383641094999323\n",
      "Gradient Descent(249/499): loss=0.3313445899940568, w0=0.04562901367108736, w1=-0.033873242733357754\n",
      "Gradient Descent(250/499): loss=0.3312969050830479, w0=0.04577226117989293, w1=-0.033909885185879333\n",
      "Gradient Descent(251/499): loss=0.33124957989480136, w0=0.04591535970278583, w1=-0.03394633950234435\n",
      "Gradient Descent(252/499): loss=0.3312026090450444, w0=0.04605830978750335, w1=-0.03398260686528106\n",
      "Gradient Descent(253/499): loss=0.3311559872473482, w0=0.046201111978192703, w1=-0.03401868844514249\n",
      "Gradient Descent(254/499): loss=0.33110970931123407, w0=0.04634376681544334, w1=-0.034054585400485914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(255/499): loss=0.3310637701403175, w0=0.04648627483631892, w1=-0.03409029887814893\n",
      "Gradient Descent(256/499): loss=0.3310181647304895, w0=0.046628636574388954, w1=-0.034125830013422176\n",
      "Gradient Descent(257/499): loss=0.330972888168134, w0=0.04677085255976015, w1=-0.0341611799302189\n",
      "Gradient Descent(258/499): loss=0.33092793562838085, w0=0.04691292331910741, w1=-0.03419634974124126\n",
      "Gradient Descent(259/499): loss=0.33088330237339336, w0=0.04705484937570455, w1=-0.03423134054814354\n",
      "Gradient Descent(260/499): loss=0.3308389837506905, w0=0.047196631249454675, w1=-0.034266153441692426\n",
      "Gradient Descent(261/499): loss=0.330794975191502, w0=0.0473382694569203, w1=-0.03430078950192416\n",
      "Gradient Descent(262/499): loss=0.3307512722091565, w0=0.04747976451135313, w1=-0.03433524979829891\n",
      "Gradient Descent(263/499): loss=0.3307078703975011, w0=0.04762111692272352, w1=-0.034369535389852275\n",
      "Gradient Descent(264/499): loss=0.3306647654293534, w0=0.0477623271977497, w1=-0.03440364732534396\n",
      "Gradient Descent(265/499): loss=0.3306219530549832, w0=0.04790339583992665, w1=-0.03443758664340381\n",
      "Gradient Descent(266/499): loss=0.330579429100625, w0=0.04804432334955471, w1=-0.034471354372675095\n",
      "Gradient Descent(267/499): loss=0.3305371894670198, w0=0.04818511022376788, w1=-0.034504951531955284\n",
      "Gradient Descent(268/499): loss=0.3304952301279853, w0=0.048325756956561876, w1=-0.0345383791303342\n",
      "Gradient Descent(269/499): loss=0.330453547129016, w0=0.04846626403882183, w1=-0.034571638167329735\n",
      "Gradient Descent(270/499): loss=0.33041213658590884, w0=0.04860663195834978, w1=-0.034604729633021046\n",
      "Gradient Descent(271/499): loss=0.3303709946834177, w0=0.04874686119989184, w1=-0.03463765450817948\n",
      "Gradient Descent(272/499): loss=0.33033011767393394, w0=0.04888695224516512, w1=-0.03467041376439705\n",
      "Gradient Descent(273/499): loss=0.3302895018761929, w0=0.04902690557288433, w1=-0.03470300836421267\n",
      "Gradient Descent(274/499): loss=0.3302491436740063, w0=0.049166721658788166, w1=-0.03473543926123617\n",
      "Gradient Descent(275/499): loss=0.33020903951501956, w0=0.049306400975665375, w1=-0.03476770740027009\n",
      "Gradient Descent(276/499): loss=0.3301691859094933, w0=0.049445943993380606, w1=-0.0347998137174293\n",
      "Gradient Descent(277/499): loss=0.33012957942910937, w0=0.04958535117889995, w1=-0.03483175914025859\n",
      "Gradient Descent(278/499): loss=0.3300902167058005, w0=0.049724622996316255, w1=-0.03486354458784813\n",
      "Gradient Descent(279/499): loss=0.3300510944306023, w0=0.04986375990687416, w1=-0.03489517097094693\n",
      "Gradient Descent(280/499): loss=0.33001220935252884, w0=0.050002762368994876, w1=-0.03492663919207439\n",
      "Gradient Descent(281/499): loss=0.3299735582774692, w0=0.05014163083830072, w1=-0.0349579501456298\n",
      "Gradient Descent(282/499): loss=0.3299351380671074, w0=0.050280365767639404, w1=-0.03498910471800006\n",
      "Gradient Descent(283/499): loss=0.3298969456378619, w0=0.05041896760710806, w1=-0.03502010378766546\n",
      "Gradient Descent(284/499): loss=0.32985897795984676, w0=0.05055743680407702, w1=-0.03505094822530367\n",
      "Gradient Descent(285/499): loss=0.3298212320558534, w0=0.050695773803213356, w1=-0.03508163889389199\n",
      "Gradient Descent(286/499): loss=0.32978370500035215, w0=0.05083397904650419, w1=-0.035112176648807794\n",
      "Gradient Descent(287/499): loss=0.3297463939185127, w0=0.050972052973279745, w1=-0.03514256233792728\n",
      "Gradient Descent(288/499): loss=0.32970929598524557, w0=0.05110999602023618, w1=-0.03517279680172258\n",
      "Gradient Descent(289/499): loss=0.3296724084242601, w0=0.051247808621458155, w1=-0.03520288087335722\n",
      "Gradient Descent(290/499): loss=0.3296357285071429, w0=0.05138549120844121, w1=-0.035232815378779904\n",
      "Gradient Descent(291/499): loss=0.3295992535524532, w0=0.05152304421011388, w1=-0.035262601136816826\n",
      "Gradient Descent(292/499): loss=0.3295629809248362, w0=0.051660468052859614, w1=-0.03529223895926237\n",
      "Gradient Descent(293/499): loss=0.32952690803415424, w0=0.05179776316053841, w1=-0.03532172965096831\n",
      "Gradient Descent(294/499): loss=0.32949103233463406, w0=0.05193492995450832, w1=-0.03535107400993156\n",
      "Gradient Descent(295/499): loss=0.3294553513240317, w0=0.052071968853646615, w1=-0.0353802728273804\n",
      "Gradient Descent(296/499): loss=0.3294198625428131, w0=0.05220888027437086, w1=-0.035409326887859346\n",
      "Gradient Descent(297/499): loss=0.329384563573351, w0=0.05234566463065968, w1=-0.03543823696931261\n",
      "Gradient Descent(298/499): loss=0.3293494520391375, w0=0.05248232233407333, w1=-0.03546700384316614\n",
      "Gradient Descent(299/499): loss=0.329314525604012, w0=0.05261885379377411, w1=-0.035495628274408386\n",
      "Gradient Descent(300/499): loss=0.3292797819714035, w0=0.05275525941654648, w1=-0.03552411102166971\n",
      "Gradient Descent(301/499): loss=0.32924521888358976, w0=0.05289153960681703, w1=-0.03555245283730051\n",
      "Gradient Descent(302/499): loss=0.3292108341209682, w0=0.05302769476667425, w1=-0.03558065446744809\n",
      "Gradient Descent(303/499): loss=0.32917662550134275, w0=0.053163725295888045, w1=-0.03560871665213226\n",
      "Gradient Descent(304/499): loss=0.3291425908792245, w0=0.053299631591929096, w1=-0.035636640125319786\n",
      "Gradient Descent(305/499): loss=0.32910872814514497, w0=0.05343541404998801, w1=-0.035664425614997546\n",
      "Gradient Descent(306/499): loss=0.32907503522498427, w0=0.05357107306299425, w1=-0.035692073843244604\n",
      "Gradient Descent(307/499): loss=0.32904151007931043, w0=0.05370660902163493, w1=-0.0357195855263031\n",
      "Gradient Descent(308/499): loss=0.3290081507027337, w0=0.05384202231437334, w1=-0.035746961374648015\n",
      "Gradient Descent(309/499): loss=0.3289749551232715, w0=0.05397731332746734, w1=-0.03577420209305582\n",
      "Gradient Descent(310/499): loss=0.32894192140172707, w0=0.05411248244498752, w1=-0.03580130838067207\n",
      "Gradient Descent(311/499): loss=0.3289090476310791, w0=0.05424753004883525, w1=-0.035828280931077906\n",
      "Gradient Descent(312/499): loss=0.3288763319358845, w0=0.05438245651876044, w1=-0.035855120432355544\n",
      "Gradient Descent(313/499): loss=0.3288437724716916, w0=0.0545172622323792, w1=-0.035881827567152705\n",
      "Gradient Descent(314/499): loss=0.32881136742446515, w0=0.05465194756519129, w1=-0.035908403012746073\n",
      "Gradient Descent(315/499): loss=0.32877911501002244, w0=0.05478651289059738, w1=-0.03593484744110375\n",
      "Gradient Descent(316/499): loss=0.32874701347348045, w0=0.054920958579916176, w1=-0.03596116151894674\n",
      "Gradient Descent(317/499): loss=0.32871506108871373, w0=0.05505528500240131, w1=-0.03598734590780951\n",
      "Gradient Descent(318/499): loss=0.3286832561578229, w0=0.055189492525258114, w1=-0.0360134012640996\n",
      "Gradient Descent(319/499): loss=0.32865159701061236, w0=0.05532358151366019, w1=-0.0360393282391563\n",
      "Gradient Descent(320/499): loss=0.3286200820040802, w0=0.05545755233076581, w1=-0.03606512747930849\n",
      "Gradient Descent(321/499): loss=0.32858870952191555, w0=0.0555914053377342, w1=-0.03609079962593152\n",
      "Gradient Descent(322/499): loss=0.3285574779740077, w0=0.055725140893741566, w1=-0.03611634531550331\n",
      "Gradient Descent(323/499): loss=0.3285263857959633, w0=0.05585875935599706, w1=-0.03614176517965954\n",
      "Gradient Descent(324/499): loss=0.32849543144863375, w0=0.0559922610797585, w1=-0.036167059845248005\n",
      "Gradient Descent(325/499): loss=0.32846461341765093, w0=0.056125646418347976, w1=-0.036192229934382224\n",
      "Gradient Descent(326/499): loss=0.32843393021297296, w0=0.0562589157231673, w1=-0.03621727606449416\n",
      "Gradient Descent(327/499): loss=0.32840338036843747, w0=0.056392069343713276, w1=-0.0362421988483862\n",
      "Gradient Descent(328/499): loss=0.32837296244132463, w0=0.05652510762759281, w1=-0.03626699889428235\n",
      "Gradient Descent(329/499): loss=0.3283426750119274, w0=0.05665803092053792, w1=-0.03629167680587868\n",
      "Gradient Descent(330/499): loss=0.3283125166831312, w0=0.05679083956642055, w1=-0.03631623318239299\n",
      "Gradient Descent(331/499): loss=0.32828248608000127, w0=0.05692353390726722, w1=-0.03634066861861381\n",
      "Gradient Descent(332/499): loss=0.3282525818493772, w0=0.057056114283273576, w1=-0.03636498370494859\n",
      "Gradient Descent(333/499): loss=0.32822280265947673, w0=0.05718858103281877, w1=-0.036389179027471255\n",
      "Gradient Descent(334/499): loss=0.32819314719950576, w0=0.05732093449247969, w1=-0.03641325516796905\n",
      "Gradient Descent(335/499): loss=0.3281636141792769, w0=0.05745317499704507, w1=-0.03643721270398867\n",
      "Gradient Descent(336/499): loss=0.32813420232883417, w0=0.057585302879529406, w1=-0.03646105220888178\n",
      "Gradient Descent(337/499): loss=0.32810491039808654, w0=0.05771731847118682, w1=-0.036484774251849816\n",
      "Gradient Descent(338/499): loss=0.3280757371564468, w0=0.05784922210152472, w1=-0.036508379397988185\n",
      "Gradient Descent(339/499): loss=0.3280466813924786, w0=0.057981014098317296, w1=-0.036531868208329786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(340/499): loss=0.3280177419135494, w0=0.058112694787619004, w1=-0.036555241239887966\n",
      "Gradient Descent(341/499): loss=0.32798891754549075, w0=0.05824426449377777, w1=-0.036578499045698806\n",
      "Gradient Descent(342/499): loss=0.3279602071322645, w0=0.05837572353944818, w1=-0.03660164217486286\n",
      "Gradient Descent(343/499): loss=0.32793160953563594, w0=0.05850707224560445, w1=-0.03662467117258622\n",
      "Gradient Descent(344/499): loss=0.32790312363485236, w0=0.058638310931553334, w1=-0.036647586580221124\n",
      "Gradient Descent(345/499): loss=0.3278747483263288, w0=0.05876943991494688, w1=-0.03667038893530585\n",
      "Gradient Descent(346/499): loss=0.32784648252333926, w0=0.058900459511795046, w1=-0.036693078771604166\n",
      "Gradient Descent(347/499): loss=0.3278183251557135, w0=0.059031370036478216, w1=-0.03671565661914417\n",
      "Gradient Descent(348/499): loss=0.32779027516954007, w0=0.05916217180175957, w1=-0.03673812300425659\n",
      "Gradient Descent(349/499): loss=0.32776233152687506, w0=0.059292865118797354, w1=-0.036760478449612576\n",
      "Gradient Descent(350/499): loss=0.32773449320545567, w0=0.05942345029715702, w1=-0.03678272347426093\n",
      "Gradient Descent(351/499): loss=0.32770675919842085, w0=0.059553927644823236, w1=-0.036804858593664884\n",
      "Gradient Descent(352/499): loss=0.327679128514035, w0=0.059684297468211814, w1=-0.03682688431973828\n",
      "Gradient Descent(353/499): loss=0.32765160017541906, w0=0.05981456007218147, w1=-0.03684880116088136\n",
      "Gradient Descent(354/499): loss=0.3276241732202846, w0=0.059944715760045515, w1=-0.03687060962201596\n",
      "Gradient Descent(355/499): loss=0.32759684670067574, w0=0.06007476483358341, w1=-0.03689231020462031\n",
      "Gradient Descent(356/499): loss=0.3275696196827131, w0=0.06020470759305223, w1=-0.036913903406763315\n",
      "Gradient Descent(357/499): loss=0.32754249124634416, w0=0.06033454433719795, w1=-0.036935389723138375\n",
      "Gradient Descent(358/499): loss=0.32751546048509816, w0=0.060464275363266734, w1=-0.03695676964509676\n",
      "Gradient Descent(359/499): loss=0.3274885265058459, w0=0.06059390096701602, w1=-0.03697804366068054\n",
      "Gradient Descent(360/499): loss=0.32746168842856316, w0=0.06072342144272554, w1=-0.03699921225465504\n",
      "Gradient Descent(361/499): loss=0.32743494538609924, w0=0.06085283708320823, w1=-0.0370202759085409\n",
      "Gradient Descent(362/499): loss=0.32740829652395026, w0=0.06098214817982101, w1=-0.037041235100645706\n",
      "Gradient Descent(363/499): loss=0.3273817410000359, w0=0.06111135502247551, w1=-0.03706209030609515\n",
      "Gradient Descent(364/499): loss=0.3273552779844811, w0=0.06124045789964864, w1=-0.03708284199686384\n",
      "Gradient Descent(365/499): loss=0.32732890665940106, w0=0.06136945709839311, w1=-0.03710349064180566\n",
      "Gradient Descent(366/499): loss=0.3273026262186914, w0=0.061498352904347804, w1=-0.037124036706683766\n",
      "Gradient Descent(367/499): loss=0.32727643586782107, w0=0.06162714560174808, w1=-0.03714448065420014\n",
      "Gradient Descent(368/499): loss=0.32725033482363053, w0=0.06175583547343598, w1=-0.03716482294402481\n",
      "Gradient Descent(369/499): loss=0.3272243223141319, w0=0.061884422800870295, w1=-0.03718506403282465\n",
      "Gradient Descent(370/499): loss=0.32719839757831487, w0=0.062012907864136615, w1=-0.037205204374291825\n",
      "Gradient Descent(371/499): loss=0.3271725598659551, w0=0.062141290941957215, w1=-0.03722524441917186\n",
      "Gradient Descent(372/499): loss=0.32714680843742616, w0=0.062269572311700876, w1=-0.03724518461529135\n",
      "Gradient Descent(373/499): loss=0.32712114256351577, w0=0.062397752249392605, w1=-0.03726502540758533\n",
      "Gradient Descent(374/499): loss=0.3270955615252443, w0=0.06252583102972327, w1=-0.037284767238124225\n",
      "Gradient Descent(375/499): loss=0.3270700646136884, w0=0.06265380892605917, w1=-0.03730441054614057\n",
      "Gradient Descent(376/499): loss=0.3270446511298062, w0=0.06278168621045142, w1=-0.037323955768055285\n",
      "Gradient Descent(377/499): loss=0.3270193203842666, w0=0.06290946315364541, w1=-0.037343403337503654\n",
      "Gradient Descent(378/499): loss=0.326994071697282, w0=0.06303714002509, w1=-0.037362753685361\n",
      "Gradient Descent(379/499): loss=0.32696890439844384, w0=0.06316471709294677, w1=-0.037382007239768\n",
      "Gradient Descent(380/499): loss=0.3269438178265606, w0=0.06329219462409909, w1=-0.037401164426155695\n",
      "Gradient Descent(381/499): loss=0.32691881132950035, w0=0.06341957288416115, w1=-0.03742022566727018\n",
      "Gradient Descent(382/499): loss=0.3268938842640348, w0=0.06354685213748694, w1=-0.037439191383197006\n",
      "Gradient Descent(383/499): loss=0.3268690359956871, w0=0.06367403264717902, w1=-0.037458061991385246\n",
      "Gradient Descent(384/499): loss=0.3268442658985816, w0=0.0638011146750974, w1=-0.037476837906671286\n",
      "Gradient Descent(385/499): loss=0.3268195733552977, w0=0.06392809848186813, w1=-0.03749551954130232\n",
      "Gradient Descent(386/499): loss=0.3267949577567257, w0=0.064054984326892, w1=-0.03751410730495953\n",
      "Gradient Descent(387/499): loss=0.3267704185019247, w0=0.06418177246835301, w1=-0.037532601604781\n",
      "Gradient Descent(388/499): loss=0.32674595499798437, w0=0.06430846316322689, w1=-0.03755100284538438\n",
      "Gradient Descent(389/499): loss=0.3267215666598887, w0=0.06443505666728942, w1=-0.03756931142888921\n",
      "Gradient Descent(390/499): loss=0.326697252910382, w0=0.0645615532351248, w1=-0.037587527754938996\n",
      "Gradient Descent(391/499): loss=0.3266730131798382, w0=0.0646879531201338, w1=-0.037605652220723054\n",
      "Gradient Descent(392/499): loss=0.3266488469061314, w0=0.06481425657454196, w1=-0.037623685220998036\n",
      "Gradient Descent(393/499): loss=0.32662475353450987, w0=0.06494046384940769, w1=-0.03764162714810922\n",
      "Gradient Descent(394/499): loss=0.3266007325174721, w0=0.06506657519463023, w1=-0.03765947839201157\n",
      "Gradient Descent(395/499): loss=0.32657678331464496, w0=0.06519259085895764, w1=-0.037677239340290485\n",
      "Gradient Descent(396/499): loss=0.32655290539266413, w0=0.06531851108999458, w1=-0.03769491037818233\n",
      "Gradient Descent(397/499): loss=0.3265290982250568, w0=0.06544433613421019, w1=-0.03771249188859476\n",
      "Gradient Descent(398/499): loss=0.32650536129212654, w0=0.06557006623694579, w1=-0.03772998425212673\n",
      "Gradient Descent(399/499): loss=0.32648169408084027, w0=0.0656957016424225, w1=-0.03774738784708832\n",
      "Gradient Descent(400/499): loss=0.326458096084718, w0=0.06582124259374884, w1=-0.037764703049520315\n",
      "Gradient Descent(401/499): loss=0.32643456680372257, w0=0.0659466893329283, w1=-0.037781930233213516\n",
      "Gradient Descent(402/499): loss=0.3264111057441542, w0=0.06607204210086672, w1=-0.03779906976972792\n",
      "Gradient Descent(403/499): loss=0.3263877124185448, w0=0.0661973011373797, w1=-0.037816122028411545\n",
      "Gradient Descent(404/499): loss=0.326364386345555, w0=0.06632246668119997, w1=-0.037833087376419154\n",
      "Gradient Descent(405/499): loss=0.3263411270498737, w0=0.06644753896998455, w1=-0.037849966178730696\n",
      "Gradient Descent(406/499): loss=0.32631793406211795, w0=0.06657251824032204, w1=-0.03786675879816953\n",
      "Gradient Descent(407/499): loss=0.3262948069187363, w0=0.06669740472773966, w1=-0.037883465595420496\n",
      "Gradient Descent(408/499): loss=0.3262717451619126, w0=0.0668221986667104, w1=-0.0379000869290477\n",
      "Gradient Descent(409/499): loss=0.3262487483394728, w0=0.06694690029065996, w1=-0.03791662315551216\n",
      "Gradient Descent(410/499): loss=0.32622581600479206, w0=0.06707150983197374, w1=-0.03793307462918922\n",
      "Gradient Descent(411/499): loss=0.3262029477167046, w0=0.06719602752200368, w1=-0.037949441702385754\n",
      "Gradient Descent(412/499): loss=0.3261801430394152, w0=0.06732045359107512, w1=-0.03796572472535722\n",
      "Gradient Descent(413/499): loss=0.32615740154241135, w0=0.06744478826849354, w1=-0.03798192404632446\n",
      "Gradient Descent(414/499): loss=0.32613472280037803, w0=0.06756903178255128, w1=-0.03799804001149036\n",
      "Gradient Descent(415/499): loss=0.32611210639311383, w0=0.06769318436053415, w1=-0.03801407296505631\n",
      "Gradient Descent(416/499): loss=0.32608955190544814, w0=0.06781724622872809, w1=-0.038030023249238445\n",
      "Gradient Descent(417/499): loss=0.32606705892716004, w0=0.06794121761242565, w1=-0.03804589120428377\n",
      "Gradient Descent(418/499): loss=0.32604462705289894, w0=0.06806509873593246, w1=-0.038061677168486026\n",
      "Gradient Descent(419/499): loss=0.3260222558821064, w0=0.06818888982257368, w1=-0.03807738147820146\n",
      "Gradient Descent(420/499): loss=0.32599994501893964, w0=0.06831259109470035, w1=-0.03809300446786436\n",
      "Gradient Descent(421/499): loss=0.32597769407219596, w0=0.06843620277369573, w1=-0.03810854647000243\n",
      "Gradient Descent(422/499): loss=0.32595550265523865, w0=0.06855972507998152, w1=-0.038124007815252055\n",
      "Gradient Descent(423/499): loss=0.3259333703859251, w0=0.06868315823302408, w1=-0.03813938883237328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(424/499): loss=0.32591129688653464, w0=0.06880650245134057, w1=-0.038154689848264735\n",
      "Gradient Descent(425/499): loss=0.32588928178369914, w0=0.06892975795250507, w1=-0.038169911187978356\n",
      "Gradient Descent(426/499): loss=0.32586732470833385, w0=0.06905292495315464, w1=-0.03818505317473394\n",
      "Gradient Descent(427/499): loss=0.32584542529557015, w0=0.06917600366899528, w1=-0.03820011612993354\n",
      "Gradient Descent(428/499): loss=0.32582358318468946, w0=0.06929899431480788, w1=-0.03821510037317573\n",
      "Gradient Descent(429/499): loss=0.3258017980190573, w0=0.06942189710445416, w1=-0.03823000622226967\n",
      "Gradient Descent(430/499): loss=0.3257800694460603, w0=0.06954471225088248, w1=-0.0382448339932491\n",
      "Gradient Descent(431/499): loss=0.325758397117043, w0=0.06966743996613363, w1=-0.03825958400038607\n",
      "Gradient Descent(432/499): loss=0.32573678068724604, w0=0.06979008046134662, w1=-0.03827425655620464\n",
      "Gradient Descent(433/499): loss=0.3257152198157458, w0=0.06991263394676432, w1=-0.03828885197149434\n",
      "Gradient Descent(434/499): loss=0.325693714165395, w0=0.0700351006317392, w1=-0.03830337055532357\n",
      "Gradient Descent(435/499): loss=0.32567226340276384, w0=0.07015748072473887, w1=-0.038317812615052765\n",
      "Gradient Descent(436/499): loss=0.32565086719808334, w0=0.07027977443335165, w1=-0.038332178456347524\n",
      "Gradient Descent(437/499): loss=0.3256295252251883, w0=0.0704019819642921, w1=-0.03834646838319152\n",
      "Gradient Descent(438/499): loss=0.32560823716146214, w0=0.07052410352340652, w1=-0.03836068269789931\n",
      "Gradient Descent(439/499): loss=0.32558700268778284, w0=0.07064613931567827, w1=-0.038374821701129005\n",
      "Gradient Descent(440/499): loss=0.32556582148846913, w0=0.07076808954523328, w1=-0.03838888569189482\n",
      "Gradient Descent(441/499): loss=0.32554469325122837, w0=0.07088995441534532, w1=-0.03840287496757945\n",
      "Gradient Descent(442/499): loss=0.3255236176671047, w0=0.07101173412844125, w1=-0.038416789823946404\n",
      "Gradient Descent(443/499): loss=0.3255025944304286, w0=0.07113342888610638, w1=-0.03843063055515211\n",
      "Gradient Descent(444/499): loss=0.32548162323876717, w0=0.07125503888908956, w1=-0.03844439745375797\n",
      "Gradient Descent(445/499): loss=0.3254607037928752, w0=0.07137656433730842, w1=-0.038458090810742245\n",
      "Gradient Descent(446/499): loss=0.3254398357966474, w0=0.07149800542985447, w1=-0.03847171091551186\n",
      "Gradient Descent(447/499): loss=0.3254190189570709, w0=0.07161936236499816, w1=-0.03848525805591406\n",
      "Gradient Descent(448/499): loss=0.32539825298417935, w0=0.07174063534019395, w1=-0.038498732518247945\n",
      "Gradient Descent(449/499): loss=0.32537753759100696, w0=0.07186182455208528, w1=-0.03851213458727592\n",
      "Gradient Descent(450/499): loss=0.32535687249354434, w0=0.07198293019650956, w1=-0.03852546454623497\n",
      "Gradient Descent(451/499): loss=0.32533625741069405, w0=0.07210395246850305, w1=-0.03853872267684791\n",
      "Gradient Descent(452/499): loss=0.32531569206422795, w0=0.07222489156230577, w1=-0.038551909259334424\n",
      "Gradient Descent(453/499): loss=0.32529517617874437, w0=0.07234574767136631, w1=-0.03856502457242208\n",
      "Gradient Descent(454/499): loss=0.3252747094816266, w0=0.07246652098834666, w1=-0.038578068893357166\n",
      "Gradient Descent(455/499): loss=0.3252542917030022, w0=0.07258721170512697, w1=-0.03859104249791546\n",
      "Gradient Descent(456/499): loss=0.32523392257570266, w0=0.07270782001281023, w1=-0.03860394566041288\n",
      "Gradient Descent(457/499): loss=0.3252136018352236, w0=0.07282834610172703, w1=-0.03861677865371604\n",
      "Gradient Descent(458/499): loss=0.3251933292196859, w0=0.07294879016144015, w1=-0.03862954174925268\n",
      "Gradient Descent(459/499): loss=0.32517310446979875, w0=0.07306915238074918, w1=-0.038642235217022\n",
      "Gradient Descent(460/499): loss=0.32515292732882034, w0=0.07318943294769513, w1=-0.038654859325604925\n",
      "Gradient Descent(461/499): loss=0.3251327975425222, w0=0.07330963204956492, w1=-0.03866741434217422\n",
      "Gradient Descent(462/499): loss=0.3251127148591527, w0=0.07342974987289595, w1=-0.03867990053250453\n",
      "Gradient Descent(463/499): loss=0.32509267902940153, w0=0.07354978660348051, w1=-0.03869231816098234\n",
      "Gradient Descent(464/499): loss=0.3250726898063642, w0=0.07366974242637021, w1=-0.03870466749061581\n",
      "Gradient Descent(465/499): loss=0.3250527469455088, w0=0.07378961752588045, w1=-0.03871694878304451\n",
      "Gradient Descent(466/499): loss=0.3250328502046412, w0=0.0739094120855947, w1=-0.038729162298549114\n",
      "Gradient Descent(467/499): loss=0.32501299934387196, w0=0.0740291262883689, w1=-0.038741308296060935\n",
      "Gradient Descent(468/499): loss=0.3249931941255842, w0=0.07414876031633569, w1=-0.03875338703317142\n",
      "Gradient Descent(469/499): loss=0.32497343431440145, w0=0.07426831435090872, w1=-0.0387653987661415\n",
      "Gradient Descent(470/499): loss=0.32495371967715536, w0=0.07438778857278688, w1=-0.03877734374991092\n",
      "Gradient Descent(471/499): loss=0.32493404998285597, w0=0.07450718316195848, w1=-0.03878922223810743\n",
      "Gradient Descent(472/499): loss=0.32491442500266, w0=0.0746264982977054, w1=-0.0388010344830559\n",
      "Gradient Descent(473/499): loss=0.32489484450984163, w0=0.07474573415860726, w1=-0.03881278073578736\n",
      "Gradient Descent(474/499): loss=0.3248753082797632, w0=0.0748648909225455, w1=-0.038824461246047944\n",
      "Gradient Descent(475/499): loss=0.3248558160898456, w0=0.07498396876670745, w1=-0.038836076262307756\n",
      "Gradient Descent(476/499): loss=0.3248363677195403, w0=0.07510296786759035, w1=-0.038847626031769646\n",
      "Gradient Descent(477/499): loss=0.3248169629503015, w0=0.07522188840100541, w1=-0.03885911080037792\n",
      "Gradient Descent(478/499): loss=0.32479760156555815, w0=0.07534073054208174, w1=-0.038870530812826945\n",
      "Gradient Descent(479/499): loss=0.3247782833506876, w0=0.07545949446527028, w1=-0.03888188631256971\n",
      "Gradient Descent(480/499): loss=0.3247590080929882, w0=0.0755781803443478, w1=-0.038893177541826265\n",
      "Gradient Descent(481/499): loss=0.3247397755816539, w0=0.07569678835242066, w1=-0.03890440474159211\n",
      "Gradient Descent(482/499): loss=0.32472058560774797, w0=0.07581531866192881, w1=-0.038915568151646505\n",
      "Gradient Descent(483/499): loss=0.3247014379641783, w0=0.07593377144464948, w1=-0.03892666801056069\n",
      "Gradient Descent(484/499): loss=0.32468233244567246, w0=0.07605214687170109, w1=-0.038937704555706054\n",
      "Gradient Descent(485/499): loss=0.3246632688487525, w0=0.07617044511354693, w1=-0.03894867802326221\n",
      "Gradient Descent(486/499): loss=0.32464424697171235, w0=0.07628866633999895, w1=-0.038959588648225\n",
      "Gradient Descent(487/499): loss=0.32462526661459273, w0=0.0764068107202215, w1=-0.03897043666441441\n",
      "Gradient Descent(488/499): loss=0.32460632757915886, w0=0.07652487842273493, w1=-0.03898122230448247\n",
      "Gradient Descent(489/499): loss=0.3245874296688775, w0=0.07664286961541933, w1=-0.038991945799921025\n",
      "Gradient Descent(490/499): loss=0.32456857268889416, w0=0.07676078446551811, w1=-0.03900260738106947\n",
      "Gradient Descent(491/499): loss=0.32454975644601114, w0=0.07687862313964162, w1=-0.039013207277122375\n",
      "Gradient Descent(492/499): loss=0.32453098074866626, w0=0.07699638580377073, w1=-0.039023745716137107\n",
      "Gradient Descent(493/499): loss=0.3245122454069107, w0=0.07711407262326037, w1=-0.03903422292504132\n",
      "Gradient Descent(494/499): loss=0.32449355023238846, w0=0.07723168376284305, w1=-0.03904463912964042\n",
      "Gradient Descent(495/499): loss=0.3244748950383156, w0=0.07734921938663238, w1=-0.03905499455462495\n",
      "Gradient Descent(496/499): loss=0.3244562796394601, w0=0.07746667965812648, w1=-0.0390652894235779\n",
      "Gradient Descent(497/499): loss=0.32443770385212173, w0=0.07758406474021153, w1=-0.03907552395898197\n",
      "Gradient Descent(498/499): loss=0.3244191674941117, w0=0.07770137479516506, w1=-0.03908569838222677\n",
      "Gradient Descent(499/499): loss=0.3244006703847347, w0=0.07781860998465946, w1=-0.039095812913615934\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEKCAYAAAAb7IIBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18006f4e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Jet  3 *****accuracy jet 76.1234434218\n",
      ">>>>>>>> Accuracy TOTAL  79.1732\n"
     ]
    }
   ],
   "source": [
    "deg=5;\n",
    "mask_jets_train = split_jets_mask(tx_train)\n",
    "mask_jets_test = split_jets_mask(tx_test)\n",
    "len_mask = len(mask_jets_train)\n",
    "\n",
    "y_preds_train = np.zeros(len(y_train))\n",
    "y_preds_test = np.zeros(tx_test.shape[0])\n",
    "\n",
    "\n",
    "\n",
    "for mask_jet_id in range(len_mask):\n",
    "    print('**** Starting Jet ', mask_jet_id, '*****')\n",
    "    tx_single_jet_train = tx_train[mask_jets_train[mask_jet_id]]\n",
    "    tx_single_jet_test =  tx_test[mask_jets_test[mask_jet_id]]\n",
    "    y_single_jet_train =   y_train[mask_jets_train[mask_jet_id]]\n",
    "    tx_single_jet_train, tx_single_jet_test, len_init = prepare_data(tx_single_jet_train, tx_single_jet_test, deg);\n",
    "    initial_w = np.zeros(tx_single_jet_train.shape[1]);\n",
    "    w, loss = func_GD (y=y_single_jet_train, tx=tx_single_jet_train, test_set=tx_single_jet_test, max_iters=500,gamma=0.01, initial_w=initial_w);\n",
    "    \n",
    "    y_pred_train = predict_labels(w, tx_single_jet_train)\n",
    "    y_pred_test = predict_labels(w, tx_single_jet_test)\n",
    "    y_preds_train[mask_jets_train[mask_jet_id]] = y_pred_train\n",
    "    y_preds_test[mask_jets_test[mask_jet_id]] = y_pred_test\n",
    "    \n",
    "    right_train = np.sum(y_single_jet_train == y_pred_train)/len(y_single_jet_train )*100\n",
    "    \n",
    "    print('**** Jet ', mask_jet_id, '*****accuracy jet',right_train)\n",
    "    \n",
    "\n",
    "    \n",
    "right_train_total = np.sum(y_train == y_preds_train)/len(y_train )*100\n",
    "print('>>>>>>>> Accuracy TOTAL ',right_train_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (568238,30) and (552,) not aligned: 30 (dim 1) != 552 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-b5cc0ace5cd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/matteociprian/Documents/GitHub/LMO_ML/project1/A_final/lib/proj1_helpers.py\u001b[0m in \u001b[0;36mpredict_labels\u001b[0;34m(weights, data)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;34m\"\"\"Generates class predictions given weights, and a test data matrix\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (568238,30) and (552,) not aligned: 30 (dim 1) != 552 (dim 0)"
     ]
    }
   ],
   "source": [
    "y_pred = predict_labels(w, tx_test)\n",
    "len(y_pred[y_pred==-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "deg=5;\n",
    "lambda_=1e-5\n",
    "mask_jets_train = split_jets_mask(tx_train)\n",
    "mask_jets_test = split_jets_mask(tx_test)\n",
    "len_mask = len(mask_jets_train)\n",
    "for mask_jet_id in range(len_mask):\n",
    "    print('**** Starting Jet ', mask_jet_id, '*****')\n",
    "    tx_single_jet_train = tx_train[mask_jets_train[mask_jet_id]]\n",
    "    tx_single_jet_test =  tx_test[mask_jets_test[mask_jet_id]]\n",
    "    y_single_jet_train =   y_train[mask_jets_train[mask_jet_id]]\n",
    "    tx_single_jet_train, tx_single_jet_test, len_init = prepare_data(tx_single_jet_train, tx_single_jet_test, deg);\n",
    "    \n",
    "    [w,loss]=func_ridge_regression (y=y_single_jet_train, tx=tx_single_jet_train, test_set=tx_single_jet_test, lambda_=lambda_);    \n",
    "    \n",
    "    y_pred_train = predict_labels(w, tx_single_jet_train)\n",
    "    y_pred_test = predict_labels(w, tx_single_jet_test)\n",
    "    y_preds_train[mask_jets_train[mask_jet_id]] = y_pred_train\n",
    "    y_preds_test[mask_jets_test[mask_jet_id]] = y_pred_test\n",
    "    \n",
    "    right_train = np.sum(y_single_jet_train == y_pred_train)/len(y_single_jet_train )*100\n",
    "    \n",
    "    print('**** Jet ', mask_jet_id, '*****accuracy jet',right_train)\n",
    "    \n",
    "\n",
    "    \n",
    "right_train_total = np.sum(y_train == y_preds_train)/len(y_train )*100\n",
    "print('>>>>>>>> Accuracy TOTAL ',right_train_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##  SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def func_SGD (y, tx, test_set, max_iters, gamma, initial_w, batch_size):\n",
    "    name = 'Stochastic_Gradient_descent'\n",
    "    \n",
    "    w,loss = least_squares_SGD(y, tx, initial_w, max_iters,\\\n",
    "                               gamma, batch_size)\n",
    "    \n",
    "    y_pred = predict_labels(w, test_set)\n",
    "    create_csv_submission(ids_test, y_pred, OUT_FOLDER+name)\n",
    "    \n",
    "    plt.plot(w, 'go')\n",
    "    plt.title('SGD: weights with loss:  '+str(loss),fontsize=15, fontweight='bold');\n",
    "    plt.show()\n",
    "    \n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "deg=5;\n",
    "lambda_=1e-5\n",
    "mask_jets_train = split_jets_mask(tx_train)\n",
    "mask_jets_test = split_jets_mask(tx_test)\n",
    "len_mask = len(mask_jets_train)\n",
    "for mask_jet_id in range(len_mask):\n",
    "    print('**** Starting Jet ', mask_jet_id, '*****')\n",
    "    tx_single_jet_train = tx_train[mask_jets_train[mask_jet_id]]\n",
    "    tx_single_jet_test =  tx_test[mask_jets_test[mask_jet_id]]\n",
    "    y_single_jet_train =   y_train[mask_jets_train[mask_jet_id]]\n",
    "    tx_single_jet_train, tx_single_jet_test, len_init = prepare_data(tx_single_jet_train, tx_single_jet_test, deg);\n",
    "    \n",
    "    initial_w = np.zeros(tx_single_jet_train.shape[1]);\n",
    "    w, loss = func_SGD (y=y_single_jet_train, tx=tx_single_jet_train, test_set=tx_single_jet_test, max_iters=500,gamma=0.00005, initial_w=initial_w,\\\n",
    "                   batch_size=1);    \n",
    "   \n",
    "    y_pred_train = predict_labels(w, tx_single_jet_train)\n",
    "    y_pred_test = predict_labels(w, tx_single_jet_test)\n",
    "    y_preds_train[mask_jets_train[mask_jet_id]] = y_pred_train\n",
    "    y_preds_test[mask_jets_test[mask_jet_id]] = y_pred_test\n",
    "    \n",
    "    right_train = np.sum(y_single_jet_train == y_pred_train)/len(y_single_jet_train )*100\n",
    "    \n",
    "    print('**** Jet ', mask_jet_id, '*****accuracy jet',right_train)\n",
    "    \n",
    "\n",
    "    \n",
    "right_train_total = np.sum(y_train == y_preds_train)/len(y_train )*100\n",
    "print('>>>>>>>> Accuracy TOTAL ',right_train_total)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "def cross_validation_logistic_regression(y,tx, k_fold, max_iters, gammas):\n",
    "    \n",
    "    accuracies_test=np.zeros(len(gammas))\n",
    "    accuracies_train=np.zeros(len(gammas))\n",
    "    acc_max=0;\n",
    "    gamma_best=0;\n",
    "    \n",
    "    for i, single_gamma in enumerate(gammas):\n",
    "        \n",
    "        seed=1;\n",
    "\n",
    "        # get k'th subgroup in test, others in train\n",
    "        k_indices = build_k_indices(y, k_fold, seed)\n",
    "        accuracy_train = np.zeros(k_fold)\n",
    "        accuracy_test = np.zeros(k_fold)\n",
    "\n",
    "\n",
    "        for k in range(k_fold):\n",
    "            #print('----- FOLD', k, '-----')\n",
    "            k_index = k_indices[k]\n",
    "            test_y = y[k_index]\n",
    "            test_tx = tx[k_index,:]\n",
    "            mask = np.ones(len(y), dtype=bool) # set all elements to True\n",
    "            mask[k_index] = False              # set test elements to False\n",
    "            train_tx = tx[mask,:]              # select only True elements (ie train elements)\n",
    "            train_y = y[mask]\n",
    "            initial_w = np.zeros(train_tx.shape[1]);\n",
    "            weights,loss = logistic_regression(train_y, train_tx, initial_w, max_iters, single_gamma)\n",
    "            # Compute the predictions\n",
    "            y_pred_train = predict_labels(weights, train_tx)\n",
    "            y_pred_test = predict_labels(weights, test_tx);\n",
    "            accuracy_train[k] = np.sum(y_pred_train == train_y)/len(train_y)\n",
    "            accuracy_test[k] = np.sum(y_pred_test == test_y)/len(test_y)\n",
    "            \n",
    "        accuracies_test[i]=np.mean(accuracy_test);\n",
    "        accuracies_train[i]=np.mean(accuracy_train);\n",
    "        \n",
    "        print('GAMMA:', single_gamma, '---','ACCURANCY TEST:',accuracies_test[i], '---','ACCURANCY TRAIN:',accuracies_train[i])\n",
    "        if (accuracies_test[i]>acc_max):\n",
    "                gamma_best=gammas[i];\n",
    "                acc_max=accuracies_test[i];\n",
    "    \n",
    "    return [gamma_best,acc_max]\n",
    "               \n",
    "        # Compute accuracy of the predictions\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def func_logistic (y, tx, test_set, max_iters, gamma, initial_w):\n",
    "    name = 'Logistic regression'\n",
    "    \n",
    "    #w,loss = logistic_regression(y, tx, initial_w, max_iters, gamma)\n",
    "    w,loss = logistic_regression(y, tx, initial_w, max_iters, gamma)\n",
    "    \n",
    "    loss=loss/len(y);\n",
    "    y_pred = predict_labels(w, test_set)\n",
    "    create_csv_submission(ids_test, y_pred, OUT_FOLDER+name)  \n",
    "    \n",
    "    plt.plot(w, 'go')\n",
    "    plt.title('logistic reg: weights with loss:  '+str(loss),fontsize=15, fontweight='bold');\n",
    "    plt.show()\n",
    "    \n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gammas=np.linspace(0.00001,0.9,10);\n",
    "k_fold=3;\n",
    "max_iters=1000;\n",
    "deg=5;\n",
    "\n",
    "\n",
    "mask_jets_train = split_jets_mask(tx_train)\n",
    "mask_jets_test = split_jets_mask(tx_test)\n",
    "len_mask = len(mask_jets_train)\n",
    "for mask_jet_id in range(len_mask):\n",
    "    print('**** Starting Jet ', mask_jet_id, '*****')\n",
    "    tx_single_jet_train = tx_train[mask_jets_train[mask_jet_id]]\n",
    "    tx_single_jet_test =  tx_test[mask_jets_test[mask_jet_id]]\n",
    "    y_single_jet_train =   y_train[mask_jets_train[mask_jet_id]]\n",
    "    tx_single_jet_train, tx_single_jet_test, len_init = prepare_data(tx_single_jet_train, tx_single_jet_test, deg);\n",
    "    \n",
    "    \n",
    "    \n",
    "    [best_gamma,acc_max]=cross_validation_logistic_regression(y_single_jet_train, tx_single_jet_train, k_fold, max_iters, gammas)\n",
    "    \n",
    "    print('BEST GAMMA:', best_gamma, '---','ACCURANCY:',acc_max)\n",
    "\n",
    "    initial_w = np.zeros(tx_single_jet_train.shape[1]);\n",
    "    w, loss = func_logistic (y=y_single_jet_train, tx=tx_single_jet_train, test_set=tx_single_jet_test, max_iters=1000,\\\n",
    "                         gamma=best_gamma, initial_w=initial_w);\n",
    "    \n",
    "    y_pred_train = predict_labels(w, tx_single_jet_train)\n",
    "    y_pred_test = predict_labels(w, tx_single_jet_test)\n",
    "    y_preds_train[mask_jets_train[mask_jet_id]] = y_pred_train\n",
    "    y_preds_test[mask_jets_test[mask_jet_id]] = y_pred_test\n",
    "    \n",
    "    right_train = np.sum(y_single_jet_train == y_pred_train)/len(y_single_jet_train )*100\n",
    "    \n",
    "    print('**** Jet ', mask_jet_id, '*****accuracy jet',right_train)\n",
    "    \n",
    "\n",
    "    \n",
    "right_train_total = np.sum(y_train == y_preds_train)/len(y_train )*100\n",
    "print('>>>>>>>> Accuracy TOTAL ',right_train_total)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOGISTIC REGULARIZED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "def cross_validation_logistic_regression_reg(y,tx, k_fold, max_iters, gammas,lambdas):\n",
    "    \n",
    "    acc_max=0;\n",
    "    accuracies_test=np.zeros([len(gammas),len(lambdas)]);\n",
    "    \n",
    "    accuracies_train=np.zeros([len(gammas),len(lambdas)]);\n",
    "    \n",
    "    \n",
    "    for i, single_gamma in enumerate(gammas):\n",
    "        for j, single_lambda in enumerate(lambdas):\n",
    "            seed=1;\n",
    "        \n",
    "            # get k'th subgroup in test, others in train\n",
    "            k_indices = build_k_indices(y, k_fold, seed)\n",
    "            accuracy_train = np.zeros(k_fold)\n",
    "            accuracy_test = np.zeros(k_fold)\n",
    "\n",
    "\n",
    "            for k in range(k_fold):\n",
    "                #print('----- FOLD', k, '-----')\n",
    "                k_index = k_indices[k]\n",
    "                test_y = y[k_index]\n",
    "                test_tx = tx[k_index,:]\n",
    "\n",
    "                mask = np.ones(len(y), dtype=bool) # set all elements to True\n",
    "                mask[k_index] = False              # set test elements to False\n",
    "                train_tx = tx[mask,:]              # select only True elements (ie train elements)\n",
    "                train_y = y[mask]\n",
    "                initial_w = np.zeros(train_tx.shape[1]);\n",
    "                weights,loss = reg_logistic_regression(train_y, train_tx,single_lambda, initial_w, max_iters, single_gamma)\n",
    "                # Compute the predictions\n",
    "                y_pred_train = predict_labels(weights, train_tx)\n",
    "                y_pred_test = predict_labels(weights, test_tx)\n",
    "                predictions=True;\n",
    "                accuracy_train[k] = np.sum(y_pred_train == train_y)/len(train_y)\n",
    "                accuracy_test[k] = np.sum(y_pred_test == test_y)/len(test_y)\n",
    "                \n",
    "                \n",
    "            accuracies_test[i,j]= np.mean(accuracy_test);\n",
    "            accuracies_train[i,j]= np.mean(accuracy_train);\n",
    "            print('GAMMA', single_gamma, '---','LAMBDA', single_lambda, '---ACCURANCY TEST:',accuracies_test[i,j],'---ACCURANCY TRAIN:',accuracies_train[i,j])\n",
    "            \n",
    "            if (accuracies_test[i,j]>acc_max):\n",
    "                gamma_best=gammas[i];\n",
    "                lambda_best=lambdas[j];\n",
    "                acc_max=accuracies_test[i,j];\n",
    "            \n",
    "            \n",
    "    return [gamma_best,lambda_best,acc_max]\n",
    "     \n",
    "    \n",
    "    \n",
    "            # Compute accuracy of the predictions\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def func_logistic_reg (y, tx, lambda_, test_set, max_iters, gamma, initial_w):\n",
    "    name = 'Logistic regression regularized'\n",
    "    \n",
    "    #w,loss = logistic_regression(y, tx, initial_w, max_iters, gamma)\n",
    "    w,loss = reg_logistic_regression(y, tx, lambda_, initial_w, max_iters, gamma)\n",
    "    \n",
    "    loss=loss/len(y);\n",
    "    y_pred = predict_labels(w, test_set)\n",
    "    create_csv_submission(ids_test, y_pred, OUT_FOLDER+name)\n",
    "    \n",
    "    plt.plot(w, 'go')\n",
    "    plt.title('logistic reg: weights with the normalized log-like:  '+str(loss),fontsize=15, fontweight='bold');\n",
    "    plt.show()\n",
    "    \n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test cross_validation\n",
    "\n",
    "gammas=np.linspace(0.000001,0.0001,2);\n",
    "lambdas=np.linspace(0.001,0.3,2);\n",
    "k_fold=6;\n",
    "max_iters=50;\n",
    "\n",
    "deg=2;\n",
    "\n",
    "mask_jets_train = split_jets_mask(tx_train)\n",
    "mask_jets_test = split_jets_mask(tx_test)\n",
    "len_mask = len(mask_jets_train)\n",
    "for mask_jet_id in range(len_mask):\n",
    "    print('**** Starting Jet ', mask_jet_id, '*****')\n",
    "    tx_single_jet_train = tx_train[mask_jets_train[mask_jet_id]]\n",
    "    tx_single_jet_test =  tx_test[mask_jets_test[mask_jet_id]]\n",
    "    y_single_jet_train =   y_train[mask_jets_train[mask_jet_id]]\n",
    "    tx_single_jet_train, tx_single_jet_test, len_init\n",
    "    = prepare_data(tx_single_jet_train, tx_single_jet_test, deg);\n",
    "    \n",
    "    [gamma_best,lambda_best,acc_max]= cross_validation_logistic_regression_reg(y_single_jet_train, tx_single_jet_train, k_fold, max_iters, gammas,lambdas)\n",
    "    print('BEST GAMMA', gamma_best, '---','BEST LAMBDA', lambda_best, '---ACCURANCY:',acc_max)\n",
    "    \n",
    "    initial_w = np.zeros(tx_single_jet_train.shape[1]);\n",
    "    w, loss = func_logistic_reg (y=y_single_jet_train, tx=tx_single_jet_train, lambda_=lambda_best, test_set=tx_single_jet_test , max_iters=max_iters,\\\n",
    "                         gamma=gamma_best, initial_w=initial_w);\n",
    "\n",
    "\n",
    "    \n",
    "    y_pred_train = predict_labels(w, tx_single_jet_train)\n",
    "    y_pred_test = predict_labels(w, tx_single_jet_test)\n",
    "    y_preds_train[mask_jets_train[mask_jet_id]] = y_pred_train\n",
    "    y_preds_test[mask_jets_test[mask_jet_id]] = y_pred_test\n",
    "    \n",
    "    right_train = np.sum(y_single_jet_train == y_pred_train)/len(y_single_jet_train )*100\n",
    "    \n",
    "    print('**** Jet ', mask_jet_id, '*****accuracy jet',right_train)\n",
    "    \n",
    "\n",
    "    \n",
    "right_train_total = np.sum(y_train == y_preds_train)/len(y_train )*100\n",
    "print('>>>>>>>> Accuracy TOTAL ',right_train_total)\n",
    "\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
