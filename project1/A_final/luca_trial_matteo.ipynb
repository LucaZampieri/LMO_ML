{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Project 1\n",
    "\n",
    "    We begin by loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from preprocessing_functions import *\n",
    "%matplotlib inline \n",
    "import numpy as np   # generic stuff\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#### REMOVE THIS LINE BEFORE SUBMISSION\n",
    "import pandas as pd\n",
    "#######################################################################\n",
    "\n",
    "from lib.proj1_helpers import * #the helper provided for the project\n",
    "from lib.costs import *\n",
    "\n",
    "# choose which implementations you would like\n",
    "from lib.implementations import *\n",
    "#from implementations import * #our implementations of the functions done by us\n",
    "\n",
    "\n",
    "import datetime\n",
    "from helpers import * #helpers of exo 2\n",
    "# Useful starting lines\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_FOLDER = '../data/' # get rid of the ..\n",
    "\n",
    "y_train, tx_train, ids_train = load_csv_data(DATA_FOLDER+'train.csv',sub_sample=False)\n",
    "\n",
    "y_test, tx_test, ids_test = load_csv_data(DATA_FOLDER+'test.csv',sub_sample=False)\n",
    "AAA = 0;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note that here we are only considering a sub_sample as the \"True\" value indicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 30)\n",
      "(568238, 30)\n",
      "(250000,)\n"
     ]
    }
   ],
   "source": [
    "print(tx_train.shape)\n",
    "print(tx_test.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>138.470</td>\n",
       "      <td>51.655</td>\n",
       "      <td>97.827</td>\n",
       "      <td>27.980</td>\n",
       "      <td>0.91</td>\n",
       "      <td>124.711</td>\n",
       "      <td>2.666</td>\n",
       "      <td>3.064</td>\n",
       "      <td>41.928</td>\n",
       "      <td>197.760</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.277</td>\n",
       "      <td>258.733</td>\n",
       "      <td>2.0</td>\n",
       "      <td>67.435</td>\n",
       "      <td>2.150</td>\n",
       "      <td>0.444</td>\n",
       "      <td>46.062</td>\n",
       "      <td>1.24</td>\n",
       "      <td>-2.475</td>\n",
       "      <td>113.497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>160.937</td>\n",
       "      <td>68.768</td>\n",
       "      <td>103.235</td>\n",
       "      <td>48.146</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.473</td>\n",
       "      <td>2.078</td>\n",
       "      <td>125.157</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.916</td>\n",
       "      <td>164.546</td>\n",
       "      <td>1.0</td>\n",
       "      <td>46.226</td>\n",
       "      <td>0.725</td>\n",
       "      <td>1.158</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>46.226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-999.000</td>\n",
       "      <td>162.172</td>\n",
       "      <td>125.953</td>\n",
       "      <td>35.635</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.148</td>\n",
       "      <td>9.336</td>\n",
       "      <td>197.814</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.186</td>\n",
       "      <td>260.414</td>\n",
       "      <td>1.0</td>\n",
       "      <td>44.251</td>\n",
       "      <td>2.053</td>\n",
       "      <td>-2.028</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>44.251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>143.905</td>\n",
       "      <td>81.417</td>\n",
       "      <td>80.943</td>\n",
       "      <td>0.414</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.310</td>\n",
       "      <td>0.414</td>\n",
       "      <td>75.968</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060</td>\n",
       "      <td>86.062</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>175.864</td>\n",
       "      <td>16.915</td>\n",
       "      <td>134.805</td>\n",
       "      <td>16.405</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.891</td>\n",
       "      <td>16.405</td>\n",
       "      <td>57.983</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.871</td>\n",
       "      <td>53.131</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0        1        2       3       4        5        6      7       8   \\\n",
       "0  138.470   51.655   97.827  27.980    0.91  124.711    2.666  3.064  41.928   \n",
       "1  160.937   68.768  103.235  48.146 -999.00 -999.000 -999.000  3.473   2.078   \n",
       "2 -999.000  162.172  125.953  35.635 -999.00 -999.000 -999.000  3.148   9.336   \n",
       "3  143.905   81.417   80.943   0.414 -999.00 -999.000 -999.000  3.310   0.414   \n",
       "4  175.864   16.915  134.805  16.405 -999.00 -999.000 -999.000  3.891  16.405   \n",
       "\n",
       "        9    ...        20       21   22       23       24       25       26  \\\n",
       "0  197.760   ...    -0.277  258.733  2.0   67.435    2.150    0.444   46.062   \n",
       "1  125.157   ...    -1.916  164.546  1.0   46.226    0.725    1.158 -999.000   \n",
       "2  197.814   ...    -2.186  260.414  1.0   44.251    2.053   -2.028 -999.000   \n",
       "3   75.968   ...     0.060   86.062  0.0 -999.000 -999.000 -999.000 -999.000   \n",
       "4   57.983   ...    -0.871   53.131  0.0 -999.000 -999.000 -999.000 -999.000   \n",
       "\n",
       "       27       28       29  \n",
       "0    1.24   -2.475  113.497  \n",
       "1 -999.00 -999.000   46.226  \n",
       "2 -999.00 -999.000   44.251  \n",
       "3 -999.00 -999.000    0.000  \n",
       "4 -999.00 -999.000    0.000  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# illegal: \n",
    "pd.DataFrame(tx_train).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for methods: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OUT_FOLDER = 'output/'\n",
    "\n",
    "def func_least_squares (y, tx, test_set, fct='mse'):\n",
    "    #name = 'least_squares'\n",
    "    w,loss = least_squares(y,tx,fct)\n",
    "    #y_pred = predict_labels(w, test_set)\n",
    "    #create_csv_submission(ids_test, y_pred, OUT_FOLDER+name)\n",
    "    \n",
    "    plt.plot(w, 'go')\n",
    "    plt.title('least squares weights for loss:  '+str(loss),fontsize=15, fontweight='bold');\n",
    "    plt.show()\n",
    "    \n",
    "    return w, loss\n",
    "\n",
    "def func_GD (y, tx, test_set, max_iters, gamma, initial_w):\n",
    "    w,loss = least_squares_GD(y, tx, initial_w, max_iters, gamma,fct='mse');\n",
    "    #y_pred = predict_labels(w, test_set)\n",
    "    #create_csv_submission(ids_test, y_pred, OUT_FOLDER+name)\n",
    "    \n",
    "    plt.plot(w, 'go')\n",
    "    plt.title('GD: weights with loss:  '+str(loss),fontsize=15, fontweight='bold');\n",
    "    plt.show()\n",
    "    \n",
    "    return w, loss\n",
    "\n",
    "########################  RIDGE REGRESSION #######################################################\n",
    "def func_ridge_regression (y, tx, test_set, lambda_):\n",
    "    name = 'Ridge_regression'\n",
    "    \n",
    "    w,loss = ridge_regression(y, tx, lambda_=lambda_, fct='mse');\n",
    "    \n",
    "    y_pred = predict_labels(w, test_set)\n",
    "    create_csv_submission(ids_test, y_pred, OUT_FOLDER+name)\n",
    "    \n",
    "    plt.plot(w, 'go')\n",
    "    plt.title('Ridge regression: weights ;loss:  '+str(loss),fontsize=15, fontweight='bold');\n",
    "    plt.show()\n",
    "    \n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LEAST-SQUARES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"Build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "def cross_validation_least_square(y, tx, k_fold, degs, seed):\n",
    "    \"\"\"Finds the best degree for Least Squares method with cross validation.\"\"\"\n",
    "    # Get k'th subgroup in test, others in train\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    \n",
    "    # Initialisation of the matrices of accuracies for all the degrees and different folds\n",
    "    accuracy_train = np.zeros([k_fold, len(degs)])\n",
    "    accuracy_test = np.zeros([k_fold, len(degs)])\n",
    "\n",
    "    # Loop over the different folds\n",
    "    for k in range(k_fold):\n",
    "        print('----- FOLD', k, '-----')\n",
    "        k_index = k_indices[k]\n",
    "        test_y = y[k_index]\n",
    "        test_tx = tx[k_index,:]\n",
    "        mask = np.ones(len(y), dtype=bool) # set all elements to True\n",
    "        mask[k_index] = False              # set test elements to False\n",
    "        train_tx = tx[mask,:]              # select only True elements (ie train elements)\n",
    "        train_y = y[mask]\n",
    "\n",
    "        len_init_data = 0\n",
    "\n",
    "        # Loop over the different degrees to try\n",
    "        for i, single_deg in enumerate(degs):\n",
    "            print('++++ Deg =', single_deg)\n",
    "            if i==0:\n",
    "                # Preprocess the data (cleaning and adding features)\n",
    "                train_tx, test_tx, len_init_data = prepare_data(train_tx, test_tx, single_deg);\n",
    "            else:\n",
    "                nb_cols_tx = train_tx.shape[1]\n",
    "                # Add the supplementary powers of the features with respect to the previous iteration and standardize\n",
    "                train_tx = add_powers(train_tx, single_deg, 0, len_init_data, features='x', current_max_deg=single_deg-1)\n",
    "                test_tx = add_powers(test_tx, single_deg, 0, len_init_data, features='x', current_max_deg=single_deg-1)\n",
    "                train_tx[:,nb_cols_tx:] = standardize(train_tx[:,nb_cols_tx:])[0]\n",
    "                test_tx[:,nb_cols_tx:] = standardize(test_tx[:,nb_cols_tx:])[0]\n",
    "                unique_cols = keep_unique_cols(train_tx)\n",
    "                train_tx = train_tx[:,unique_cols]\n",
    "                test_tx = test_tx[:,unique_cols]\n",
    "                \n",
    "            # Compute the weights with LS\n",
    "            weights, loss = least_squares(train_y, train_tx, fct='mse');\n",
    "\n",
    "            # Compute the predictions\n",
    "            y_pred_train = predict_labels(weights, train_tx)\n",
    "            y_pred_test = predict_labels(weights, test_tx);\n",
    "            accuracy_train[k, i] = np.sum(y_pred_train == train_y)/len(train_y)\n",
    "            accuracy_test[k, i] = np.sum(y_pred_test == test_y)/len(test_y)\n",
    "\n",
    "    accuracies_test = np.mean(accuracy_test, axis=0); # mean on the k's\n",
    "    accuracies_train = np.mean(accuracy_train, axis=0);\n",
    "\n",
    "    max_index, acc_max = max(enumerate(accuracies_test), key=operator.itemgetter(1))\n",
    "    print(accuracy_test)\n",
    "    print(accuracies_test)\n",
    "    print(k_fold)\n",
    "    print(len(degs))\n",
    "    print(max_index)\n",
    "    print(acc_max)\n",
    "    deg_best = degs[max_index]\n",
    "    \n",
    "    return deg_best, acc_max;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Starting Jet  0 *****\n",
      "----- FOLD 0 -----\n",
      "++++ Deg = 2\n",
      "++++ Deg = 3\n",
      "++++ Deg = 4\n",
      "++++ Deg = 5\n",
      "++++ Deg = 6\n",
      "++++ Deg = 7\n",
      "++++ Deg = 8\n",
      "++++ Deg = 9\n",
      "++++ Deg = 10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-d63810797048>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0my_single_jet_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask_jets_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask_jet_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mbest_deg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validation_least_square\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_single_jet_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx_single_jet_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_fold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Best degree = '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_deg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-678a387f66f5>\u001b[0m in \u001b[0;36mcross_validation_least_square\u001b[0;34m(y, tx, k_fold, degs, seed)\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mtrain_tx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnb_cols_tx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstandardize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_tx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnb_cols_tx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0mtest_tx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnb_cols_tx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstandardize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_tx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnb_cols_tx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                 \u001b[0munique_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeep_unique_cols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_tx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m                 \u001b[0mtrain_tx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_tx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0munique_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0mtest_tx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_tx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0munique_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/matteociprian/Documents/GitHub/LMO_ML/project1/A_final/preprocessing_functions.py\u001b[0m in \u001b[0;36mkeep_unique_cols\u001b[0;34m(tx)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0merase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mid_loop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m                     \u001b[0merase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                     \u001b[0mequal_to\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "OUT_FOLDER = 'output/'\n",
    "name = 'least_squares.csv'\n",
    "\n",
    "degs=range(2,14)\n",
    "k_fold=5\n",
    "seed=1;\n",
    "\n",
    "mask_jets_train = split_jets_mask(tx_train)\n",
    "mask_jets_test = split_jets_mask(tx_test)\n",
    "len_mask = len(mask_jets_train)\n",
    "y_preds_train = np.zeros(len(y_train))\n",
    "y_preds_test = np.zeros(tx_test.shape[0])\n",
    "\n",
    "\n",
    "for mask_jet_id in range(len_mask):\n",
    "    print('**** Starting Jet ', mask_jet_id, '*****')\n",
    "    tx_single_jet_train = tx_train[mask_jets_train[mask_jet_id]]\n",
    "    tx_single_jet_test = tx_test[mask_jets_test[mask_jet_id]]\n",
    "    y_single_jet_train = y_train[mask_jets_train[mask_jet_id]]\n",
    "    \n",
    "    best_deg, best_acc = cross_validation_least_square(y_single_jet_train, tx_single_jet_train, k_fold, degs,seed)\n",
    "    print('Best degree = ', best_deg)\n",
    "    \n",
    "    tx_single_jet_train, tx_single_jet_test,len_init_data = prepare_data(tx_single_jet_train,tx_single_jet_test, best_deg)\n",
    "    w, loss = func_least_squares(y=y_single_jet_train, tx=tx_single_jet_train, test_set=tx_single_jet_test, fct='mse')\n",
    "    \n",
    "    y_pred_train = predict_labels(w, tx_single_jet_train)\n",
    "    y_pred_test = predict_labels(w, tx_single_jet_test)\n",
    "    y_preds_train[mask_jets_train[mask_jet_id]] = y_pred_train\n",
    "    y_preds_test[mask_jets_test[mask_jet_id]] = y_pred_test\n",
    "    \n",
    "    right_train = np.sum(y_single_jet_train == y_pred_train)/len(y_single_jet_train)*100\n",
    "    \n",
    "    print('**** Jet ', mask_jet_id, '***** Accuracy jet', right_train)\n",
    "    \n",
    "\n",
    "create_csv_submission(ids_test, y_preds_test, OUT_FOLDER+name)\n",
    "right_train_total = np.sum(y_train == y_preds_train)/len(y_train)*100\n",
    "print('>>>>>>>> Accuracy TOTAL ',right_train_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(y_preds_test[y_preds_test==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(y_preds_test[y_preds_test==-1])+len(y_preds_test[y_preds_test==1])==tx_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_preds_test[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cross validation su gamma e degree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "\n",
    "\n",
    "def cross_validation_GD(y, tx, k_fold, max_iters, degs,gammas):            \n",
    "    seed=1;\n",
    "    \n",
    "    # get k'th subgroup in test, others in train\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    accuracy_train = np.zeros([k_fold, len(degs),len(gammas)])\n",
    "    accuracy_test = np.zeros([k_fold, len(degs),len(gammas)])\n",
    "\n",
    "    acc_max=0;\n",
    "    deg_best=0;\n",
    "    gammas_best=0;\n",
    "               \n",
    "    for k in range(k_fold):\n",
    "        print('----- FOLD', k, '-----')\n",
    "        k_index = k_indices[k]\n",
    "        test_y = y[k_index]\n",
    "        test_tx = tx[k_index,:]\n",
    "        mask = np.ones(len(y), dtype=bool) # set all elements to True\n",
    "        mask[k_index] = False              # set test elements to False\n",
    "        train_tx = tx[mask,:]              # select only True elements (ie train elements)\n",
    "        train_y = y[mask]\n",
    "\n",
    "        len_init_data = 0\n",
    "\n",
    "        for i, single_deg in enumerate(degs):\n",
    "            print('++++ Deg =', single_deg)\n",
    "            for j , single_gamma_ in  enumerate(gammas):\n",
    "                print('++++ gamma =', single_gamma_)\n",
    "                \n",
    "                if i==0 and j==0:\n",
    "                    train_tx, test_tx, len_init_data = prepare_data(train_tx, test_tx, single_deg);\n",
    "                else:\n",
    "                    shape_tx=train_tx.shape[1];\n",
    "                    print('ciaociaociao')\n",
    "                    train_tx = add_powers(train_tx, single_deg, 0, len_init_data, features='x', current_max_deg=single_deg-1)\n",
    "                    test_tx = add_powers(test_tx, single_deg, 0, len_init_data, features='x', current_max_deg=single_deg-1)\n",
    "                    train_tx[:,shape_tx:]=standardize(train_tx[:,shape_tx:])[0]\n",
    "                initial_w = np.zeros(train_tx.shape[1]);\n",
    "                \n",
    "                weights,loss = least_squares_GD(train_y,train_tx, initial_w, max_iters, single_gamma_,fct='mse');\n",
    "\n",
    "                # Compute the predictions\n",
    "                y_pred_train = predict_labels(weights, train_tx);\n",
    "                y_pred_test = predict_labels(weights, test_tx);\n",
    "                accuracy_train[k, i,j] = np.sum(y_pred_train == train_y)/len(train_y)\n",
    "                accuracy_test[k, i,j] = np.sum(y_pred_test == test_y)/len(test_y)\n",
    "        \n",
    "      \n",
    "                \n",
    "    accuracies_test=np.mean(accuracy_test, axis=0); # mean on the k's\n",
    "    accuracies_train=np.mean(accuracy_train, axis=0);\n",
    "\n",
    " \n",
    "    for i, single_deg in enumerate(degs):\n",
    "         for j , single_gamma_ in  enumerate(gammas):\n",
    "                if (accuracies_test[i,j]>acc_max):\n",
    "                    deg_best=degs[i];\n",
    "                    gammas_best=gammas[j];\n",
    "                    acc_mac=accuracies_test[i,j];\n",
    "    \n",
    "    \n",
    "                    \n",
    "        \n",
    "                \n",
    "    return [deg_best, gamma_best, acc_max];\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1]\n",
      " [2 2 2]]\n",
      "(2, 2)\n",
      "[[ 1 -1 -1]\n",
      " [ 2  1  1]]\n"
     ]
    }
   ],
   "source": [
    "cubic=np.array([[1,1,1],[2,2,2]])\n",
    "print(cubic)\n",
    "print (cubic[:,1:].shape)\n",
    "\n",
    "cubic[:,1:]=standardize(cubic[:,1:])[0];\n",
    "print(cubic)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Starting Jet  0 *****\n",
      "----- FOLD 0 -----\n",
      "++++ Deg = 4\n",
      "++++ gamma = 0.0001\n",
      "Gradient Descent(0/399): loss=0.4999999999999887, w0=-6.887232304259569e-08, w1=-3.5851239558151825e-05\n",
      "Gradient Descent(1/399): loss=0.4997886023020471, w0=-1.3279692118023493e-07, w1=-7.165426932699993e-05\n",
      "Gradient Descent(2/399): loss=0.49957776994619885, w0=-1.9180268329462098e-07, w1=-0.00010740917970274563\n",
      "Gradient Descent(3/399): loss=0.49936750087119885, w0=-2.459183816411012e-07, w1=-0.00014311606088238587\n",
      "Gradient Descent(4/399): loss=0.49915779302441243, w0=-2.951726722743844e-07, w1=-0.000178775002864219\n",
      "Gradient Descent(5/399): loss=0.498948644361784, w0=-3.395940954727797e-07, w1=-0.00021438609544834942\n",
      "Gradient Descent(6/399): loss=0.49874005284779666, w0=-3.792110761609197e-07, w1=-0.0002499494282371909\n",
      "Gradient Descent(7/399): loss=0.4985320164554317, w0=-4.1405192433098656e-07, w1=-0.00028546509063596823\n",
      "Gradient Descent(8/399): loss=0.4983245331661278, w0=-4.4414483546245015e-07, w1=-0.0003209331718532171\n",
      "Gradient Descent(9/399): loss=0.4981176009697414, w0=-4.6951789094031177e-07, w1=-0.00035635376090128356\n",
      "Gradient Descent(10/399): loss=0.4979112178645054, w0=-4.901990584718737e-07, w1=-0.000391726946596821\n",
      "Gradient Descent(11/399): loss=0.49770538185699026, w0=-5.062161925020312e-07, w1=-0.00042705281756128617\n",
      "Gradient Descent(12/399): loss=0.49750009096206405, w0=-5.175970346270864e-07, w1=-0.0004623314622214339\n",
      "Gradient Descent(13/399): loss=0.4972953432028535, w0=-5.24369214007105e-07, w1=-0.0004975629688098099\n",
      "Gradient Descent(14/399): loss=0.4970911366107044, w0=-5.265602477768134e-07, w1=-0.000532747425365243\n",
      "Gradient Descent(15/399): loss=0.49688746922514226, w0=-5.241975414550211e-07, w1=-0.0005678849197333352\n",
      "Gradient Descent(16/399): loss=0.49668433909383425, w0=-5.173083893526141e-07, w1=-0.0006029755395669503\n",
      "Gradient Descent(17/399): loss=0.4964817442725501, w0=-5.059199749790999e-07, w1=-0.0006380193723267019\n",
      "Gradient Descent(18/399): loss=0.4962796828251246, w0=-4.90059371447695e-07, w1=-0.0006730165052814394\n",
      "Gradient Descent(19/399): loss=0.4960781528234176, w0=-4.6975354187899173e-07, w1=-0.0007079670255087326\n",
      "Gradient Descent(20/399): loss=0.49587715234727864, w0=-4.4502933980319824e-07, w1=-0.0007428710198953553\n",
      "Gradient Descent(21/399): loss=0.4956766794845071, w0=-4.1591350956093465e-07, w1=-0.0007777285751377666\n",
      "Gradient Descent(22/399): loss=0.4954767323308153, w0=-3.8243268670263026e-07, w1=-0.0008125397777425925\n",
      "Gradient Descent(23/399): loss=0.4952773089897917, w0=-3.4461339838648637e-07, w1=-0.0008473047140271043\n",
      "Gradient Descent(24/399): loss=0.4950784075728629, w0=-3.024820637750462e-07, w1=-0.000882023470119697\n",
      "Gradient Descent(25/399): loss=0.4948800261992581, w0=-2.5606499443034053e-07, w1=-0.0009166961319603652\n",
      "Gradient Descent(26/399): loss=0.4946821629959701, w0=-2.053883947076479e-07, w1=-0.0009513227853011792\n",
      "Gradient Descent(27/399): loss=0.49448481609772077, w0=-1.5047836214785388e-07, w1=-0.0009859035157067575\n",
      "Gradient Descent(28/399): loss=0.49428798364692395, w0=-9.136088786841646e-08, w1=-0.0010204384085547404\n",
      "Gradient Descent(29/399): loss=0.49409166379364944, w0=-2.806185695295112e-08, w1=-0.0010549275490362602\n",
      "Gradient Descent(30/399): loss=0.49389585469558656, w0=3.93929511605685e-08, w1=-0.0010893710221564117\n",
      "Gradient Descent(31/399): loss=0.49370055451800926, w0=1.1097786229298799e-07, w1=-0.0011237689127347197\n",
      "Gradient Descent(32/399): loss=0.4935057614337398, w0=1.8666730713851785e-07, w1=-0.0011581213054056069\n",
      "Gradient Descent(33/399): loss=0.4933114736231136, w0=2.664358208806434e-07, w1=-0.001192428284618859\n",
      "Gradient Descent(34/399): loss=0.49311768927394417, w0=3.5025804280940357e-07, w1=-0.0012266899346400897\n",
      "Gradient Descent(35/399): loss=0.49292440658148806, w0=4.3810871594001577e-07, w1=-0.0012609063395512036\n",
      "Gradient Descent(36/399): loss=0.4927316237484097, w0=5.299626866328737e-07, w1=-0.0012950775832508575\n",
      "Gradient Descent(37/399): loss=0.49253933898474794, w0=6.25794904214862e-07, w1=-0.0013292037494549215\n",
      "Gradient Descent(38/399): loss=0.49234755050787926, w0=7.255804206020422e-07, w1=-0.0013632849216969375\n",
      "Gradient Descent(39/399): loss=0.49215625654248607, w0=8.292943899236579e-07, w1=-0.0013973211833285778\n",
      "Gradient Descent(40/399): loss=0.4919654553205213, w0=9.369120681474894e-07, w1=-0.0014313126175201007\n",
      "Gradient Descent(41/399): loss=0.49177514508117437, w0=1.048408812706515e-06, w1=-0.0014652593072608069\n",
      "Gradient Descent(42/399): loss=0.4915853240708382, w0=1.16376008212692e-06, w1=-0.0014991613353594923\n",
      "Gradient Descent(43/399): loss=0.49139599054307503, w0=1.282941435657409e-06, w1=-0.0015330187844449013\n",
      "Gradient Descent(44/399): loss=0.49120714275858346, w0=1.4059285328998346e-06, w1=-0.0015668317369661783\n",
      "Gradient Descent(45/399): loss=0.491018778985165, w0=1.532697133441154e-06, w1=-0.0016006002751933171\n",
      "Gradient Descent(46/399): loss=0.49083089749769127, w0=1.6632230964866587e-06, w1=-0.0016343244812176106\n",
      "Gradient Descent(47/399): loss=0.49064349657807066, w0=1.7974823804945325e-06, w1=-0.0016680044369520974\n",
      "Gradient Descent(48/399): loss=0.49045657451521646, w0=1.935451042811696e-06, w1=-0.0017016402241320089\n",
      "Gradient Descent(49/399): loss=0.4902701296050139, w0=2.077105239310945e-06, w1=-0.0017352319243152138\n",
      "Gradient Descent(50/399): loss=0.490084160150288, w0=2.2224212240293683e-06, w1=-0.0017687796188826622\n",
      "Gradient Descent(51/399): loss=0.4898986644607707, w0=2.3713753488080608e-06, w1=-0.0018022833890388276\n",
      "Gradient Descent(52/399): loss=0.4897136408530704, w0=2.5239440629331013e-06, w1=-0.0018357433158121487\n",
      "Gradient Descent(53/399): loss=0.4895290876506383, w0=2.680103912777819e-06, w1=-0.001869159480055469\n",
      "Gradient Descent(54/399): loss=0.48934500318373847, w0=2.8398315414463155e-06, w1=-0.0019025319624464754\n",
      "Gradient Descent(55/399): loss=0.4891613857894153, w0=3.00310368841826e-06, w1=-0.0019358608434881363\n",
      "Gradient Descent(56/399): loss=0.4889782338114624, w0=3.169897189194943e-06, w1=-0.0019691462035091372\n",
      "Gradient Descent(57/399): loss=0.48879554560039185, w0=3.340188974946584e-06, w1=-0.002002388122664316\n",
      "Gradient Descent(58/399): loss=0.48861331951340237, w0=3.513956072160888e-06, w1=-0.0020355866809350976\n",
      "Gradient Descent(59/399): loss=0.48843155391434956, w0=3.691175602292862e-06, w1=-0.002068741958129924\n",
      "Gradient Descent(60/399): loss=0.48825024717371457, w0=3.8718247814158575e-06, w1=-0.002101854033884689\n",
      "Gradient Descent(61/399): loss=0.4880693976685735, w0=4.05588091987386e-06, w1=-0.002134922987663166\n",
      "Gradient Descent(62/399): loss=0.4878890037825678, w0=4.243321421935014e-06, w1=-0.0021679488987574372\n",
      "Gradient Descent(63/399): loss=0.48770906390587343, w0=4.434123785446379e-06, w1=-0.002200931846288322\n",
      "Gradient Descent(64/399): loss=0.4875295764351707, w0=4.628265601489895e-06, w1=-0.0022338719092058028\n",
      "Gradient Descent(65/399): loss=0.4873505397736154, w0=4.825724554039599e-06, w1=-0.002266769166289451\n",
      "Gradient Descent(66/399): loss=0.4871719523308075, w0=5.026478419620024e-06, w1=-0.0022996236961488506\n",
      "Gradient Descent(67/399): loss=0.48699381252276386, w0=5.23050506696583e-06, w1=-0.002332435577224021\n",
      "Gradient Descent(68/399): loss=0.48681611877188613, w0=5.437782456682656e-06, w1=-0.0023652048877858393\n",
      "Gradient Descent(69/399): loss=0.48663886950693386, w0=5.648288640909134e-06, w1=-0.0023979317059364593\n",
      "Gradient Descent(70/399): loss=0.4864620631629938, w0=5.862001762980157e-06, w1=-0.002430616109609733\n",
      "Gradient Descent(71/399): loss=0.486285698181452, w0=6.0789000570912916e-06, w1=-0.002463258176571628\n",
      "Gradient Descent(72/399): loss=0.48610977300996455, w0=6.298961847964414e-06, w1=-0.002495857984420642\n",
      "Gradient Descent(73/399): loss=0.48593428610242895, w0=6.522165550514532e-06, w1=-0.0025284156105882228\n",
      "Gradient Descent(74/399): loss=0.48575923591895565, w0=6.748489669517769e-06, w1=-0.00256093113233918\n",
      "Gradient Descent(75/399): loss=0.4855846209258402, w0=6.9779127992805425e-06, w1=-0.002593404626772099\n",
      "Gradient Descent(76/399): loss=0.48541043959553376, w0=7.210413623309925e-06, w1=-0.002625836170819754\n",
      "Gradient Descent(77/399): loss=0.48523669040661666, w0=7.445970913985151e-06, w1=-0.002658225841249518\n",
      "Gradient Descent(78/399): loss=0.4850633718437695, w0=7.68456353223032e-06, w1=-0.0026905737146637743\n",
      "Gradient Descent(79/399): loss=0.48489048239774546, w0=7.92617042718823e-06, w1=-0.0027228798675003234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(80/399): loss=0.4847180205653432, w0=8.170770635895399e-06, w1=-0.0027551443760327927\n",
      "Gradient Descent(81/399): loss=0.48454598484937883, w0=8.41834328295822e-06, w1=-0.002787367316371041\n",
      "Gradient Descent(82/399): loss=0.48437437375865905, w0=8.66886758023028e-06, w1=-0.0028195487644615655\n",
      "Gradient Descent(83/399): loss=0.4842031858079534, w0=8.92232282649081e-06, w1=-0.0028516887960879055\n",
      "Gradient Descent(84/399): loss=0.4840324195179678, w0=9.1786884071243e-06, w1=-0.002883787486871045\n",
      "Gradient Descent(85/399): loss=0.4838620734153176, w0=9.437943793801222e-06, w1=-0.0029158449122698147\n",
      "Gradient Descent(86/399): loss=0.48369214603250016, w0=9.700068544159913e-06, w1=-0.0029478611475812944\n",
      "Gradient Descent(87/399): loss=0.4835226359078691, w0=9.965042301489575e-06, w1=-0.0029798362679412105\n",
      "Gradient Descent(88/399): loss=0.48335354158560695, w0=1.0232844794414407e-05, w1=-0.0030117703483243364\n",
      "Gradient Descent(89/399): loss=0.48318486161569973, w0=1.050345583657885e-05, w1=-0.0030436634635448885\n",
      "Gradient Descent(90/399): loss=0.4830165945539099, w0=1.077685532633397e-05, w1=-0.0030755156882569237\n",
      "Gradient Descent(91/399): loss=0.48284873896175096, w0=1.1053023246424927e-05, w1=-0.003107327096954734\n",
      "Gradient Descent(92/399): loss=0.48268129340646093, w0=1.1331939663679575e-05, w1=-0.003139097763973241\n",
      "Gradient Descent(93/399): loss=0.4825142564609769, w0=1.1613584728698168e-05, w1=-0.0031708277634883886\n",
      "Gradient Descent(94/399): loss=0.48234762670390907, w0=1.1897938675544162e-05, w1=-0.003202517169517535\n",
      "Gradient Descent(95/399): loss=0.4821814027195155, w0=1.2184981821436109e-05, w1=-0.0032341660559198453\n",
      "Gradient Descent(96/399): loss=0.48201558309767656, w0=1.2474694566440657e-05, w1=-0.0032657744963966767\n",
      "Gradient Descent(97/399): loss=0.4818501664338697, w0=1.2767057393166652e-05, w1=-0.0032973425644919727\n",
      "Gradient Descent(98/399): loss=0.481685151329144, w0=1.3062050866460295e-05, w1=-0.0033288703335926464\n",
      "Gradient Descent(99/399): loss=0.48152053639009595, w0=1.3359655633101407e-05, w1=-0.0033603578769289696\n",
      "Gradient Descent(100/399): loss=0.4813563202288432, w0=1.365985242150077e-05, w1=-0.003391805267574957\n",
      "Gradient Descent(101/399): loss=0.4811925014630012, w0=1.3962622041398545e-05, w1=-0.0034232125784487504\n",
      "Gradient Descent(102/399): loss=0.4810290787156574, w0=1.4267945383563745e-05, w1=-0.0034545798823130028\n",
      "Gradient Descent(103/399): loss=0.48086605061534826, w0=1.4575803419494799e-05, w1=-0.00348590725177526\n",
      "Gradient Descent(104/399): loss=0.48070341579603215, w0=1.4886177201121167e-05, w1=-0.003517194759288341\n",
      "Gradient Descent(105/399): loss=0.48054117289706877, w0=1.5199047860506024e-05, w1=-0.00354844247715072\n",
      "Gradient Descent(106/399): loss=0.4803793205631913, w0=1.5514396609549992e-05, w1=-0.0035796504775069044\n",
      "Gradient Descent(107/399): loss=0.48021785744448514, w0=1.5832204739695928e-05, w1=-0.0036108188323478118\n",
      "Gradient Descent(108/399): loss=0.48005678219636283, w0=1.6152453621634773e-05, w1=-0.0036419476135111477\n",
      "Gradient Descent(109/399): loss=0.4798960934795401, w0=1.647512470501242e-05, w1=-0.003673036892681782\n",
      "Gradient Descent(110/399): loss=0.479735789960013, w0=1.680019951813766e-05, w1=-0.0037040867413921223\n",
      "Gradient Descent(111/399): loss=0.4795758703090329, w0=1.7127659667691136e-05, w1=-0.003735097231022489\n",
      "Gradient Descent(112/399): loss=0.4794163332030855, w0=1.7457486838435348e-05, w1=-0.0037660684328014874\n",
      "Gradient Descent(113/399): loss=0.4792571773238648, w0=1.7789662792925688e-05, w1=-0.0037970004178063785\n",
      "Gradient Descent(114/399): loss=0.4790984013582521, w0=1.8124169371222493e-05, w1=-0.0038278932569634516\n",
      "Gradient Descent(115/399): loss=0.4789400039982916, w0=1.8460988490604128e-05, w1=-0.0038587470210483922\n",
      "Gradient Descent(116/399): loss=0.47878198394116817, w0=1.8800102145281104e-05, w1=-0.0038895617806866513\n",
      "Gradient Descent(117/399): loss=0.47862433988918407, w0=1.914149240611118e-05, w1=-0.003920337606353813\n",
      "Gradient Descent(118/399): loss=0.4784670705497365, w0=1.9485141420315515e-05, w1=-0.00395107456837596\n",
      "Gradient Descent(119/399): loss=0.4783101746352949, w0=1.98310314111958e-05, w1=-0.00398177273693004\n",
      "Gradient Descent(120/399): loss=0.4781536508633785, w0=2.0179144677852405e-05, w1=-0.004012432182044232\n",
      "Gradient Descent(121/399): loss=0.4779974979565341, w0=2.0529463594903554e-05, w1=-0.004043052973598305\n",
      "Gradient Descent(122/399): loss=0.4778417146423134, w0=2.0881970612205452e-05, w1=-0.004073635181323985\n",
      "Gradient Descent(123/399): loss=0.4776862996532515, w0=2.1236648254573442e-05, w1=-0.004104178874805315\n",
      "Gradient Descent(124/399): loss=0.4775312517268442, w0=2.159347912150415e-05, w1=-0.004134684123479013\n",
      "Gradient Descent(125/399): loss=0.47737656960552644, w0=2.195244588689861e-05, w1=-0.004165150996634837\n",
      "Gradient Descent(126/399): loss=0.47722225203665003, w0=2.2313531298786366e-05, w1=-0.004195579563415936\n",
      "Gradient Descent(127/399): loss=0.4770682977724634, w0=2.2676718179050587e-05, w1=-0.004225969892819215\n",
      "Gradient Descent(128/399): loss=0.4769147055700877, w0=2.3041989423154127e-05, w1=-0.004256322053695686\n",
      "Gradient Descent(129/399): loss=0.4767614741914973, w0=2.3409327999866575e-05, w1=-0.004286636114750826\n",
      "Gradient Descent(130/399): loss=0.47660860240349734, w0=2.3778716950992283e-05, w1=-0.004316912144544931\n",
      "Gradient Descent(131/399): loss=0.4764560889777034, w0=2.415013939109933e-05, w1=-0.004347150211493469\n",
      "Gradient Descent(132/399): loss=0.4763039326905186, w0=2.452357850724951e-05, w1=-0.004377350383867434\n",
      "Gradient Descent(133/399): loss=0.47615213232311493, w0=2.489901755872921e-05, w1=-0.004407512729793696\n",
      "Gradient Descent(134/399): loss=0.47600068666141027, w0=2.5276439876781304e-05, w1=-0.004437637317255351\n",
      "Gradient Descent(135/399): loss=0.47584959449604836, w0=2.5655828864337965e-05, w1=-0.004467724214092072\n",
      "Gradient Descent(136/399): loss=0.47569885462237826, w0=2.6037167995754452e-05, w1=-0.0044977734880004595\n",
      "Gradient Descent(137/399): loss=0.4755484658404335, w0=2.642044081654383e-05, w1=-0.0045277852065343845\n",
      "Gradient Descent(138/399): loss=0.47539842695491147, w0=2.680563094311263e-05, w1=-0.004557759437105339\n",
      "Gradient Descent(139/399): loss=0.4752487367751528, w0=2.7192722062497474e-05, w1=-0.0045876962469827785\n",
      "Gradient Descent(140/399): loss=0.4750993941151216, w0=2.7581697932102603e-05, w1=-0.004617595703294472\n",
      "Gradient Descent(141/399): loss=0.47495039779338477, w0=2.7972542379438382e-05, w1=-0.004647457873026839\n",
      "Gradient Descent(142/399): loss=0.47480174663309205, w0=2.836523930186069e-05, w1=-0.004677282823025299\n",
      "Gradient Descent(143/399): loss=0.47465343946195576, w0=2.8759772666311277e-05, w1=-0.004707070619994608\n",
      "Gradient Descent(144/399): loss=0.47450547511223146, w0=2.9156126509059017e-05, w1=-0.004736821330499203\n",
      "Gradient Descent(145/399): loss=0.47435785242069745, w0=2.9554284935442108e-05, w1=-0.004766535020963542\n",
      "Gradient Descent(146/399): loss=0.4742105702286353, w0=2.995423211961117e-05, w1=-0.00479621175767244\n",
      "Gradient Descent(147/399): loss=0.47406362738181007, w0=3.0355952304273253e-05, w1=-0.004825851606771412\n",
      "Gradient Descent(148/399): loss=0.4739170227304518, w0=3.07594298004368e-05, w1=-0.004855454634267003\n",
      "Gradient Descent(149/399): loss=0.47377075512923394, w0=3.116464898715745e-05, w1=-0.004885020906027134\n",
      "Gradient Descent(150/399): loss=0.47362482343725654, w0=3.157159431128481e-05, w1=-0.004914550487781429\n",
      "Gradient Descent(151/399): loss=0.4734792265180249, w0=3.198025028721009e-05, w1=-0.004944043445121554\n",
      "Gradient Descent(152/399): loss=0.47333396323943133, w0=3.239060149661464e-05, w1=-0.004973499843501548\n",
      "Gradient Descent(153/399): loss=0.4731890324737368, w0=3.280263258821943e-05, w1=-0.00500291974823816\n",
      "Gradient Descent(154/399): loss=0.4730444330975504, w0=3.3216328277535345e-05, w1=-0.005032303224511173\n",
      "Gradient Descent(155/399): loss=0.47290016399181145, w0=3.363167334661442e-05, w1=-0.005061650337363743\n",
      "Gradient Descent(156/399): loss=0.47275622404177114, w0=3.4048652643801954e-05, w1=-0.005090961151702724\n",
      "Gradient Descent(157/399): loss=0.4726126121369726, w0=3.446725108348951e-05, w1=-0.005120235732298998\n",
      "Gradient Descent(158/399): loss=0.47246932717123313, w0=3.488745364586878e-05, w1=-0.005149474143787805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(159/399): loss=0.4723263680426262, w0=3.530924537668634e-05, w1=-0.0051786764506690655\n",
      "Gradient Descent(160/399): loss=0.4721837336534616, w0=3.573261138699927e-05, w1=-0.005207842717307714\n",
      "Gradient Descent(161/399): loss=0.47204142291026857, w0=3.615753685293166e-05, w1=-0.005236973007934016\n",
      "Gradient Descent(162/399): loss=0.47189943472377655, w0=3.658400701543196e-05, w1=-0.0052660673866439\n",
      "Gradient Descent(163/399): loss=0.4717577680088971, w0=3.7012007180031225e-05, w1=-0.005295125917399276\n",
      "Gradient Descent(164/399): loss=0.4716164216847075, w0=3.744152271660219e-05, w1=-0.00532414866402836\n",
      "Gradient Descent(165/399): loss=0.47147539467443006, w0=3.7872539059119234e-05, w1=-0.005353135690225997\n",
      "Gradient Descent(166/399): loss=0.47133468590541683, w0=3.830504170541917e-05, w1=-0.0053820870595539805\n",
      "Gradient Descent(167/399): loss=0.47119429430912974, w0=3.8739016216962895e-05, w1=-0.005411002835441371\n",
      "Gradient Descent(168/399): loss=0.47105421882112436, w0=3.917444821859794e-05, w1=-0.0054398830811848205\n",
      "Gradient Descent(169/399): loss=0.47091445838103185, w0=3.961132339832178e-05, w1=-0.005468727859948883\n",
      "Gradient Descent(170/399): loss=0.47077501193254107, w0=4.004962750704603e-05, w1=-0.005497537234766341\n",
      "Gradient Descent(171/399): loss=0.4706358784233816, w0=4.048934635836153e-05, w1=-0.005526311268538513\n",
      "Gradient Descent(172/399): loss=0.47049705680530624, w0=4.093046582830417e-05, w1=-0.005555050024035577\n",
      "Gradient Descent(173/399): loss=0.4703585460340737, w0=4.137297185512162e-05, w1=-0.00558375356389688\n",
      "Gradient Descent(174/399): loss=0.47022034506943144, w0=4.181685043904089e-05, w1=-0.005612421950631254\n",
      "Gradient Descent(175/399): loss=0.47008245287509853, w0=4.2262087642036663e-05, w1=-0.005641055246617329\n",
      "Gradient Descent(176/399): loss=0.4699448684187493, w0=4.2708669587600533e-05, w1=-0.005669653514103846\n",
      "Gradient Descent(177/399): loss=0.4698075906719943, w0=4.315658246051101e-05, w1=-0.005698216815209966\n",
      "Gradient Descent(178/399): loss=0.4696706186103671, w0=4.360581250660434e-05, w1=-0.005726745211925586\n",
      "Gradient Descent(179/399): loss=0.4695339512133035, w0=4.405634603254622e-05, w1=-0.00575523876611164\n",
      "Gradient Descent(180/399): loss=0.46939758746412785, w0=4.4508169405604184e-05, w1=-0.005783697539500415\n",
      "Gradient Descent(181/399): loss=0.4692615263500347, w0=4.496126905342098e-05, w1=-0.005812121593695857\n",
      "Gradient Descent(182/399): loss=0.4691257668620734, w0=4.5415631463788605e-05, w1=-0.005840510990173878\n",
      "Gradient Descent(183/399): loss=0.468990307995131, w0=4.587124318442322e-05, w1=-0.005868865790282661\n",
      "Gradient Descent(184/399): loss=0.4688551487479157, w0=4.632809082274087e-05, w1=-0.0058971860552429695\n",
      "Gradient Descent(185/399): loss=0.46872028812294125, w0=4.678616104563395e-05, w1=-0.005925471846148446\n",
      "Gradient Descent(186/399): loss=0.46858572512651064, w0=4.7245440579248546e-05, w1=-0.0059537232239659215\n",
      "Gradient Descent(187/399): loss=0.46845145876869915, w0=4.7705916208762495e-05, w1=-0.005981940249535716\n",
      "Gradient Descent(188/399): loss=0.4683174880633397, w0=4.8167574778164295e-05, w1=-0.00601012298357194\n",
      "Gradient Descent(189/399): loss=0.4681838120280053, w0=4.863040319003277e-05, w1=-0.006038271486662797\n",
      "Gradient Descent(190/399): loss=0.4680504296839943, w0=4.909438840531755e-05, w1=-0.006066385819270887\n",
      "Gradient Descent(191/399): loss=0.467917340056314, w0=4.955951744312029e-05, w1=-0.0060944660417334975\n",
      "Gradient Descent(192/399): loss=0.4677845421736657, w0=5.002577738047671e-05, w1=-0.006122512214262913\n",
      "Gradient Descent(193/399): loss=0.4676520350684278, w0=5.0493155352139416e-05, w1=-0.006150524396946705\n",
      "Gradient Descent(194/399): loss=0.4675198177766411, w0=5.0961638550361434e-05, w1=-0.006178502649748033\n",
      "Gradient Descent(195/399): loss=0.46738788933799336, w0=5.143121422468062e-05, w1=-0.006206447032505942\n",
      "Gradient Descent(196/399): loss=0.46725624879580296, w0=5.190186968170474e-05, w1=-0.006234357604935654\n",
      "Gradient Descent(197/399): loss=0.46712489519700484, w0=5.2373592284897385e-05, w1=-0.006262234426628867\n",
      "Gradient Descent(198/399): loss=0.4669938275921339, w0=5.28463694543646e-05, w1=-0.006290077557054047\n",
      "Gradient Descent(199/399): loss=0.46686304503531084, w0=5.3320188666642325e-05, w1=-0.006317887055556721\n",
      "Gradient Descent(200/399): loss=0.4667325465842258, w0=5.3795037454484564e-05, w1=-0.006345662981359771\n",
      "Gradient Descent(201/399): loss=0.4666023313001254, w0=5.42709034066523e-05, w1=-0.006373405393563725\n",
      "Gradient Descent(202/399): loss=0.4664723982477954, w0=5.4747774167703206e-05, w1=-0.006401114351147047\n",
      "Gradient Descent(203/399): loss=0.46634274649554663, w0=5.522563743778206e-05, w1=-0.006428789912966427\n",
      "Gradient Descent(204/399): loss=0.4662133751152008, w0=5.570448097241192e-05, w1=-0.006456432137757073\n",
      "Gradient Descent(205/399): loss=0.46608428318207507, w0=5.618429258228611e-05, w1=-0.006484041084132995\n",
      "Gradient Descent(206/399): loss=0.46595546977496755, w0=5.6665060133060824e-05, w1=-0.006511616810587298\n",
      "Gradient Descent(207/399): loss=0.4658269339761424, w0=5.7146771545148576e-05, w1=-0.006539159375492465\n",
      "Gradient Descent(208/399): loss=0.4656986748713152, w0=5.7629414793512356e-05, w1=-0.006566668837100645\n",
      "Gradient Descent(209/399): loss=0.4655706915496392, w0=5.811297790746051e-05, w1=-0.006594145253543937\n",
      "Gradient Descent(210/399): loss=0.4654429831036898, w0=5.8597448970442355e-05, w1=-0.0066215886828346745\n",
      "Gradient Descent(211/399): loss=0.4653155486294513, w0=5.908281611984454e-05, w1=-0.006648999182865713\n",
      "Gradient Descent(212/399): loss=0.46518838722630135, w0=5.956906754678811e-05, w1=-0.006676376811410708\n",
      "Gradient Descent(213/399): loss=0.46506149799699836, w0=6.005619149592632e-05, w1=-0.006703721626124402\n",
      "Gradient Descent(214/399): loss=0.46493488004766537, w0=6.054417626524317e-05, w1=-0.0067310336845429015\n",
      "Gradient Descent(215/399): loss=0.4648085324877776, w0=6.103301020585263e-05, w1=-0.0067583130440839615\n",
      "Gradient Descent(216/399): loss=0.4646824544301478, w0=6.152268172179863e-05, w1=-0.006785559762047264\n",
      "Gradient Descent(217/399): loss=0.464556644990912, w0=6.201317926985568e-05, w1=-0.006812773895614697\n",
      "Gradient Descent(218/399): loss=0.4644311032895161, w0=6.250449135933035e-05, w1=-0.006839955501850634\n",
      "Gradient Descent(219/399): loss=0.46430582844870155, w0=6.29966065518633e-05, w1=-0.00686710463770221\n",
      "Gradient Descent(220/399): loss=0.4641808195944925, w0=6.348951346123214e-05, w1=-0.006894221359999602\n",
      "Gradient Descent(221/399): loss=0.46405607585618047, w0=6.398320075315486e-05, w1=-0.006921305725456301\n",
      "Gradient Descent(222/399): loss=0.4639315963663133, w0=6.447765714509416e-05, w1=-0.00694835779066939\n",
      "Gradient Descent(223/399): loss=0.46380738026067836, w0=6.497287140606226e-05, w1=-0.006975377612119818\n",
      "Gradient Descent(224/399): loss=0.46368342667829154, w0=6.546883235642659e-05, w1=-0.007002365246172676\n",
      "Gradient Descent(225/399): loss=0.46355973476138307, w0=6.596552886771604e-05, w1=-0.007029320749077468\n",
      "Gradient Descent(226/399): loss=0.4634363036553837, w0=6.646294986242801e-05, w1=-0.007056244176968383\n",
      "Gradient Descent(227/399): loss=0.46331313250891193, w0=6.696108431383606e-05, w1=-0.0070831355858645695\n",
      "Gradient Descent(228/399): loss=0.46319022047376046, w0=6.745992124579832e-05, w1=-0.007109995031670405\n",
      "Gradient Descent(229/399): loss=0.463067566704883, w0=6.795944973256654e-05, w1=-0.007136822570175766\n",
      "Gradient Descent(230/399): loss=0.46294517036038163, w0=6.845965889859582e-05, w1=-0.0071636182570563\n",
      "Gradient Descent(231/399): loss=0.4628230306014929, w0=6.896053791835507e-05, w1=-0.00719038214787369\n",
      "Gradient Descent(232/399): loss=0.4627011465925753, w0=6.946207601613809e-05, w1=-0.007217114298075928\n",
      "Gradient Descent(233/399): loss=0.462579517501097, w0=6.996426246587539e-05, w1=-0.007243814762997578\n",
      "Gradient Descent(234/399): loss=0.4624581424976212, w0=7.046708659094657e-05, w1=-0.007270483597860045\n",
      "Gradient Descent(235/399): loss=0.4623370207557953, w0=7.097053776399353e-05, w1=-0.007297120857771841\n",
      "Gradient Descent(236/399): loss=0.46221615145233647, w0=7.147460540673418e-05, w1=-0.007323726597728852\n",
      "Gradient Descent(237/399): loss=0.46209553376702023, w0=7.1979278989777e-05, w1=-0.007350300872614597\n",
      "Gradient Descent(238/399): loss=0.461975166882667, w0=7.248454803243606e-05, w1=-0.007376843737200497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(239/399): loss=0.46185504998512983, w0=7.29904021025469e-05, w1=-0.007403355246146138\n",
      "Gradient Descent(240/399): loss=0.46173518226328225, w0=7.349683081628289e-05, w1=-0.00742983545399953\n",
      "Gradient Descent(241/399): loss=0.46161556290900446, w0=7.40038238379724e-05, w1=-0.007456284415197373\n",
      "Gradient Descent(242/399): loss=0.4614961911171728, w0=7.45113708799165e-05, w1=-0.007482702184065315\n",
      "Gradient Descent(243/399): loss=0.46137706608564577, w0=7.501946170220744e-05, w1=-0.007509088814818214\n",
      "Gradient Descent(244/399): loss=0.4612581870152529, w0=7.552808611254762e-05, w1=-0.007535444361560397\n",
      "Gradient Descent(245/399): loss=0.4611395531097817, w0=7.603723396606937e-05, w1=-0.007561768878285919\n",
      "Gradient Descent(246/399): loss=0.46102116357596634, w0=7.654689516515527e-05, w1=-0.007588062418878821\n",
      "Gradient Descent(247/399): loss=0.46090301762347413, w0=7.705705965925915e-05, w1=-0.0076143250371133895\n",
      "Gradient Descent(248/399): loss=0.46078511446489534, w0=7.756771744472772e-05, w1=-0.007640556786654411\n",
      "Gradient Descent(249/399): loss=0.46066745331572956, w0=7.807885856462286e-05, w1=-0.007666757721057429\n",
      "Gradient Descent(250/399): loss=0.4605500333943743, w0=7.859047310854453e-05, w1=-0.007692927893769\n",
      "Gradient Descent(251/399): loss=0.4604328539221139, w0=7.910255121245429e-05, w1=-0.0077190673581269496\n",
      "Gradient Descent(252/399): loss=0.4603159141231058, w0=7.96150830584995e-05, w1=-0.007745176167360622\n",
      "Gradient Descent(253/399): loss=0.4601992132243712, w0=8.012805887483811e-05, w1=-0.00777125437459114\n",
      "Gradient Descent(254/399): loss=0.4600827504557812, w0=8.064146893546414e-05, w1=-0.007797302032831652\n",
      "Gradient Descent(255/399): loss=0.4599665250500464, w0=8.115530356003367e-05, w1=-0.007823319194987589\n",
      "Gradient Descent(256/399): loss=0.45985053624270456, w0=8.166955311369159e-05, w1=-0.007849305913856911\n",
      "Gradient Descent(257/399): loss=0.45973478327210937, w0=8.218420800689885e-05, w1=-0.007875262242130364\n",
      "Gradient Descent(258/399): loss=0.4596192653794192, w0=8.269925869526042e-05, w1=-0.007901188232391722\n",
      "Gradient Descent(259/399): loss=0.45950398180858465, w0=8.321469567935381e-05, w1=-0.007927083937118048\n",
      "Gradient Descent(260/399): loss=0.45938893180633866, w0=8.373050950455821e-05, w1=-0.007952949408679928\n",
      "Gradient Descent(261/399): loss=0.4592741146221836, w0=8.42466907608843e-05, w1=-0.007978784699341733\n",
      "Gradient Descent(262/399): loss=0.4591595295083812, w0=8.476323008280459e-05, w1=-0.008004589861261858\n",
      "Gradient Descent(263/399): loss=0.4590451757199403, w0=8.528011814908441e-05, w1=-0.008030364946492972\n",
      "Gradient Descent(264/399): loss=0.45893105251460675, w0=8.57973456826135e-05, w1=-0.008056110006982264\n",
      "Gradient Descent(265/399): loss=0.45881715915285043, w0=8.631490345023818e-05, w1=-0.008081825094571686\n",
      "Gradient Descent(266/399): loss=0.45870349489785717, w0=8.683278226259422e-05, w1=-0.008107510260998204\n",
      "Gradient Descent(267/399): loss=0.45859005901551425, w0=8.735097297394013e-05, w1=-0.008133165557894034\n",
      "Gradient Descent(268/399): loss=0.4584768507744017, w0=8.786946648199124e-05, w1=-0.008158791036786892\n",
      "Gradient Descent(269/399): loss=0.4583638694457809, w0=8.838825372775423e-05, w1=-0.008184386749100232\n",
      "Gradient Descent(270/399): loss=0.45825111430358323, w0=8.890732569536232e-05, w1=-0.008209952746153494\n",
      "Gradient Descent(271/399): loss=0.45813858462439916, w0=8.942667341191107e-05, w1=-0.008235489079162342\n",
      "Gradient Descent(272/399): loss=0.4580262796874688, w0=8.994628794729469e-05, w1=-0.008260995799238903\n",
      "Gradient Descent(273/399): loss=0.45791419877466866, w0=9.046616041404305e-05, w1=-0.00828647295739201\n",
      "Gradient Descent(274/399): loss=0.4578023411705038, w0=9.098628196715912e-05, w1=-0.008311920604527446\n",
      "Gradient Descent(275/399): loss=0.45769070616209495, w0=9.150664380395718e-05, w1=-0.008337338791448172\n",
      "Gradient Descent(276/399): loss=0.4575792930391692, w0=9.202723716390144e-05, w1=-0.008362727568854577\n",
      "Gradient Descent(277/399): loss=0.45746810109404845, w0=9.254805332844535e-05, w1=-0.008388086987344707\n",
      "Gradient Descent(278/399): loss=0.45735712962164043, w0=9.306908362087144e-05, w1=-0.008413417097414508\n",
      "Gradient Descent(279/399): loss=0.45724637791942624, w0=9.359031940613177e-05, w1=-0.008438717949458061\n",
      "Gradient Descent(280/399): loss=0.4571358452874519, w0=9.411175209068886e-05, w1=-0.008463989593767814\n",
      "Gradient Descent(281/399): loss=0.45702553102831645, w0=9.463337312235731e-05, w1=-0.008489232080534825\n",
      "Gradient Descent(282/399): loss=0.4569154344471623, w0=9.515517399014592e-05, w1=-0.008514445459848988\n",
      "Gradient Descent(283/399): loss=0.45680555485166524, w0=9.567714622410039e-05, w1=-0.008539629781699275\n",
      "Gradient Descent(284/399): loss=0.45669589155202406, w0=9.619928139514658e-05, w1=-0.008564785095973963\n",
      "Gradient Descent(285/399): loss=0.4565864438609495, w0=9.672157111493437e-05, w1=-0.00858991145246087\n",
      "Gradient Descent(286/399): loss=0.4564772110936557, w0=9.7244007035682e-05, w1=-0.008615008900847586\n",
      "Gradient Descent(287/399): loss=0.4563681925678488, w0=9.776658085002106e-05, w1=-0.008640077490721708\n",
      "Gradient Descent(288/399): loss=0.4562593876037175, w0=9.828928429084196e-05, w1=-0.008665117271571066\n",
      "Gradient Descent(289/399): loss=0.4561507955239229, w0=9.881210913114e-05, w1=-0.008690128292783955\n",
      "Gradient Descent(290/399): loss=0.45604241565358866, w0=9.9335047183862e-05, w1=-0.008715110603649366\n",
      "Gradient Descent(291/399): loss=0.455934247320291, w0=9.985809030175343e-05, w1=-0.008740064253357216\n",
      "Gradient Descent(292/399): loss=0.4558262898540491, w0=0.00010038123037720615, w1=-0.00876498929099857\n",
      "Gradient Descent(293/399): loss=0.4557185425873148, w0=0.00010090445934210665, w1=-0.008789885765565879\n",
      "Gradient Descent(294/399): loss=0.45561100485496375, w0=0.00010142776916768486, w1=-0.0088147537259532\n",
      "Gradient Descent(295/399): loss=0.455503675994284, w0=0.00010195115186436346, w1=-0.008839593220956427\n",
      "Gradient Descent(296/399): loss=0.45539655534496826, w0=0.00010247459948160785, w1=-0.00886440429927351\n",
      "Gradient Descent(297/399): loss=0.45528964224910284, w0=0.00010299810410777648, w1=-0.00888918700950469\n",
      "Gradient Descent(298/399): loss=0.45518293605115945, w0=0.00010352165786997186, w1=-0.008913941400152716\n",
      "Gradient Descent(299/399): loss=0.455076436097984, w0=0.00010404525293389208, w1=-0.008938667519623078\n",
      "Gradient Descent(300/399): loss=0.45497014173878814, w0=0.00010456888150368278, w1=-0.00896336541622422\n",
      "Gradient Descent(301/399): loss=0.45486405232513977, w0=0.00010509253582178977, w1=-0.008988035138167769\n",
      "Gradient Descent(302/399): loss=0.4547581672109532, w0=0.00010561620816881209, w1=-0.009012676733568762\n",
      "Gradient Descent(303/399): loss=0.4546524857524801, w0=0.00010613989086335564, w1=-0.00903729025044586\n",
      "Gradient Descent(304/399): loss=0.4545470073083003, w0=0.00010666357626188733, w1=-0.009061875736721573\n",
      "Gradient Descent(305/399): loss=0.45444173123931164, w0=0.00010718725675858975, w1=-0.009086433240222479\n",
      "Gradient Descent(306/399): loss=0.45433665690872227, w0=0.00010771092478521634, w1=-0.009110962808679448\n",
      "Gradient Descent(307/399): loss=0.4542317836820397, w0=0.00010823457281094716, w1=-0.009135464489727858\n",
      "Gradient Descent(308/399): loss=0.45412711092706304, w0=0.00010875819334224505, w1=-0.009159938330907816\n",
      "Gradient Descent(309/399): loss=0.45402263801387266, w0=0.00010928177892271245, w1=-0.009184384379664375\n",
      "Gradient Descent(310/399): loss=0.45391836431482224, w0=0.0001098053221329486, w1=-0.009208802683347754\n",
      "Gradient Descent(311/399): loss=0.4538142892045293, w0=0.00011032881559040734, w1=-0.009233193289213553\n",
      "Gradient Descent(312/399): loss=0.4537104120598656, w0=0.00011085225194925542, w1=-0.009257556244422973\n",
      "Gradient Descent(313/399): loss=0.45360673225994913, w0=0.00011137562390023129, w1=-0.009281891596043028\n",
      "Gradient Descent(314/399): loss=0.4535032491861346, w0=0.00011189892417050438, w1=-0.009306199391046764\n",
      "Gradient Descent(315/399): loss=0.4533999622220047, w0=0.00011242214552353494, w1=-0.009330479676313473\n",
      "Gradient Descent(316/399): loss=0.45329687075336134, w0=0.00011294528075893434, w1=-0.009354732498628908\n",
      "Gradient Descent(317/399): loss=0.4531939741682165, w0=0.00011346832271232587, w1=-0.009378957904685495\n",
      "Gradient Descent(318/399): loss=0.4530912718567841, w0=0.00011399126425520612, w1=-0.009403155941082549\n",
      "Gradient Descent(319/399): loss=0.4529887632114707, w0=0.00011451409829480674, w1=-0.009427326654326485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(320/399): loss=0.45288644762686703, w0=0.00011503681777395676, w1=-0.009451470090831032\n",
      "Gradient Descent(321/399): loss=0.45278432449973915, w0=0.00011555941567094543, w1=-0.009475586296917443\n",
      "Gradient Descent(322/399): loss=0.4526823932290204, w0=0.00011608188499938549, w1=-0.009499675318814706\n",
      "Gradient Descent(323/399): loss=0.4525806532158022, w0=0.000116604218808077, w1=-0.009523737202659758\n",
      "Gradient Descent(324/399): loss=0.4524791038633258, w0=0.0001171264101808716, w1=-0.00954777199449769\n",
      "Gradient Descent(325/399): loss=0.4523777445769738, w0=0.00011764845223653729, w1=-0.009571779740281961\n",
      "Gradient Descent(326/399): loss=0.4522765747642611, w0=0.0001181703381286237, w1=-0.009595760485874604\n",
      "Gradient Descent(327/399): loss=0.4521755938348282, w0=0.0001186920610453278, w1=-0.009619714277046436\n",
      "Gradient Descent(328/399): loss=0.4520748012004302, w0=0.0001192136142093602, w1=-0.009643641159477266\n",
      "Gradient Descent(329/399): loss=0.45197419627493096, w0=0.00011973499087781177, w1=-0.0096675411787561\n",
      "Gradient Descent(330/399): loss=0.4518737784742931, w0=0.00012025618434202092, w1=-0.009691414380381352\n",
      "Gradient Descent(331/399): loss=0.4517735472165708, w0=0.0001207771879274412, w1=-0.009715260809761046\n",
      "Gradient Descent(332/399): loss=0.4516735019219006, w0=0.00012129799499350947, w1=-0.009739080512213024\n",
      "Gradient Descent(333/399): loss=0.45157364201249395, w0=0.00012181859893351452, w1=-0.009762873532965153\n",
      "Gradient Descent(334/399): loss=0.45147396691262864, w0=0.0001223389931744662, w1=-0.009786639917155523\n",
      "Gradient Descent(335/399): loss=0.45137447604864134, w0=0.00012285917117696487, w1=-0.009810379709832659\n",
      "Gradient Descent(336/399): loss=0.45127516884891816, w0=0.00012337912643507157, w1=-0.00983409295595572\n",
      "Gradient Descent(337/399): loss=0.45117604474388795, w0=0.00012389885247617846, w1=-0.009857779700394705\n",
      "Gradient Descent(338/399): loss=0.45107710316601396, w0=0.00012441834286087974, w1=-0.00988143998793065\n",
      "Gradient Descent(339/399): loss=0.45097834354978483, w0=0.00012493759118284323, w1=-0.009905073863255835\n",
      "Gradient Descent(340/399): loss=0.45087976533170854, w0=0.00012545659106868206, w1=-0.009928681370973988\n",
      "Gradient Descent(341/399): loss=0.4507813679503021, w0=0.00012597533617782723, w1=-0.009952262555600477\n",
      "Gradient Descent(342/399): loss=0.45068315084608584, w0=0.00012649382020240028, w1=-0.00997581746156252\n",
      "Gradient Descent(343/399): loss=0.4505851134615748, w0=0.00012701203686708666, w1=-0.00999934613319938\n",
      "Gradient Descent(344/399): loss=0.4504872552412707, w0=0.00012752997992900942, w1=-0.01002284861476256\n",
      "Gradient Descent(345/399): loss=0.4503895756316535, w0=0.00012804764317760336, w1=-0.01004632495041601\n",
      "Gradient Descent(346/399): loss=0.4502920740811754, w0=0.0001285650204344898, w1=-0.010069775184236325\n",
      "Gradient Descent(347/399): loss=0.4501947500402517, w0=0.0001290821055533515, w1=-0.010093199360212938\n",
      "Gradient Descent(348/399): loss=0.4500976029612537, w0=0.00012959889241980835, w1=-0.010116597522248317\n",
      "Gradient Descent(349/399): loss=0.45000063229850057, w0=0.00013011537495129333, w1=-0.010139969714158167\n",
      "Gradient Descent(350/399): loss=0.4499038375082531, w0=0.00013063154709692885, w1=-0.010163315979671622\n",
      "Gradient Descent(351/399): loss=0.4498072180487039, w0=0.00013114740283740378, w1=-0.010186636362431445\n",
      "Gradient Descent(352/399): loss=0.44971077337997173, w0=0.00013166293618485067, w1=-0.010209930905994216\n",
      "Gradient Descent(353/399): loss=0.44961450296409333, w0=0.00013217814118272357, w1=-0.010233199653830535\n",
      "Gradient Descent(354/399): loss=0.44951840626501594, w0=0.00013269301190567623, w1=-0.010256442649325213\n",
      "Gradient Descent(355/399): loss=0.44942248274858965, w0=0.00013320754245944077, w1=-0.010279659935777462\n",
      "Gradient Descent(356/399): loss=0.44932673188256045, w0=0.00013372172698070676, w1=-0.010302851556401095\n",
      "Gradient Descent(357/399): loss=0.4492311531365624, w0=0.0001342355596370007, w1=-0.010326017554324714\n",
      "Gradient Descent(358/399): loss=0.4491357459821108, w0=0.0001347490346265661, w1=-0.010349157972591904\n",
      "Gradient Descent(359/399): loss=0.44904050989259425, w0=0.00013526214617824372, w1=-0.010372272854161426\n",
      "Gradient Descent(360/399): loss=0.4489454443432675, w0=0.0001357748885513525, w1=-0.010395362241907404\n",
      "Gradient Descent(361/399): loss=0.4488505488112451, w0=0.00013628725603557075, w1=-0.010418426178619522\n",
      "Gradient Descent(362/399): loss=0.44875582277549286, w0=0.0001367992429508179, w1=-0.010441464707003209\n",
      "Gradient Descent(363/399): loss=0.4486612657168212, w0=0.0001373108436471366, w1=-0.01046447786967983\n",
      "Gradient Descent(364/399): loss=0.44856687711787807, w0=0.0001378220525045752, w1=-0.010487465709186879\n",
      "Gradient Descent(365/399): loss=0.44847265646314244, w0=0.0001383328639330707, w1=-0.010510428267978163\n",
      "Gradient Descent(366/399): loss=0.44837860323891554, w0=0.00013884327237233225, w1=-0.010533365588423991\n",
      "Gradient Descent(367/399): loss=0.4482847169333156, w0=0.00013935327229172486, w1=-0.010556277712811367\n",
      "Gradient Descent(368/399): loss=0.4481909970362695, w0=0.00013986285819015366, w1=-0.010579164683344168\n",
      "Gradient Descent(369/399): loss=0.4480974430395066, w0=0.00014037202459594848, w1=-0.010602026542143343\n",
      "Gradient Descent(370/399): loss=0.4480040544365513, w0=0.00014088076606674896, w1=-0.010624863331247089\n",
      "Gradient Descent(371/399): loss=0.4479108307227161, w0=0.00014138907718939004, w1=-0.010647675092611041\n",
      "Gradient Descent(372/399): loss=0.44781777139509554, w0=0.0001418969525797878, w1=-0.010670461868108456\n",
      "Gradient Descent(373/399): loss=0.4477248759525573, w0=0.00014240438688282583, w1=-0.010693223699530402\n",
      "Gradient Descent(374/399): loss=0.4476321438957376, w0=0.0001429113747722418, w1=-0.01071596062858594\n",
      "Gradient Descent(375/399): loss=0.4475395747270334, w0=0.00014341791095051474, w1=-0.010738672696902304\n",
      "Gradient Descent(376/399): loss=0.4474471679505953, w0=0.00014392399014875247, w1=-0.010761359946025093\n",
      "Gradient Descent(377/399): loss=0.44735492307232105, w0=0.00014442960712657957, w1=-0.010784022417418444\n",
      "Gradient Descent(378/399): loss=0.44726283959984914, w0=0.0001449347566720257, w1=-0.010806660152465225\n",
      "Gradient Descent(379/399): loss=0.44717091704255163, w0=0.00014543943360141432, w1=-0.01082927319246721\n",
      "Gradient Descent(380/399): loss=0.4470791549115272, w0=0.00014594363275925192, w1=-0.010851861578645261\n",
      "Gradient Descent(381/399): loss=0.4469875527195957, w0=0.00014644734901811746, w1=-0.010874425352139516\n",
      "Gradient Descent(382/399): loss=0.4468961099812904, w0=0.00014695057727855237, w1=-0.010896964554009559\n",
      "Gradient Descent(383/399): loss=0.44680482621285167, w0=0.0001474533124689509, w1=-0.01091947922523461\n",
      "Gradient Descent(384/399): loss=0.4467137009322205, w0=0.0001479555495454508, w1=-0.010941969406713698\n",
      "Gradient Descent(385/399): loss=0.4466227336590323, w0=0.00014845728349182453, w1=-0.010964435139265847\n",
      "Gradient Descent(386/399): loss=0.4465319239146091, w0=0.00014895850931937076, w1=-0.010986876463630247\n",
      "Gradient Descent(387/399): loss=0.4464412712219554, w0=0.00014945922206680622, w1=-0.01100929342046644\n",
      "Gradient Descent(388/399): loss=0.44635077510574894, w0=0.00014995941680015813, w1=-0.01103168605035449\n",
      "Gradient Descent(389/399): loss=0.4462604350923367, w0=0.0001504590886126568, w1=-0.011054054393795172\n",
      "Gradient Descent(390/399): loss=0.4461702507097266, w0=0.00015095823262462876, w1=-0.011076398491210136\n",
      "Gradient Descent(391/399): loss=0.44608022148758275, w0=0.00015145684398339026, w1=-0.011098718382942092\n",
      "Gradient Descent(392/399): loss=0.44599034695721784, w0=0.00015195491786314106, w1=-0.011121014109254984\n",
      "Gradient Descent(393/399): loss=0.4459006266515873, w0=0.00015245244946485872, w1=-0.011143285710334164\n",
      "Gradient Descent(394/399): loss=0.4458110601052832, w0=0.00015294943401619325, w1=-0.011165533226286573\n",
      "Gradient Descent(395/399): loss=0.4457216468545278, w0=0.0001534458667713621, w1=-0.011187756697140906\n",
      "Gradient Descent(396/399): loss=0.4456323864371671, w0=0.00015394174301104545, w1=-0.011209956162847796\n",
      "Gradient Descent(397/399): loss=0.44554327839266494, w0=0.00015443705804228216, w1=-0.011232131663279984\n",
      "Gradient Descent(398/399): loss=0.4454543222620968, w0=0.00015493180719836576, w1=-0.011254283238232491\n",
      "Gradient Descent(399/399): loss=0.44536551758814347, w0=0.00015542598583874104, w1=-0.011276410927422795\n",
      "++++ gamma = 0.000268269579528\n",
      "ciaociaociao\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/399): loss=0.4999999999999887, w0=-1.8476349143751833e-07, w1=-9.617796961822002e-05\n",
      "Gradient Descent(1/399): loss=0.4994190213321271, w0=-3.31830276863767e-07, w1=-0.00019200435456225397\n",
      "Gradient Descent(2/399): loss=0.4988422477155444, w0=-4.417656812402293e-07, w1=-0.00028748094664720345\n",
      "Gradient Descent(3/399): loss=0.4982696374956127, w0=-5.15128927360894e-07, w1=-0.00038260952692916275\n",
      "Gradient Descent(4/399): loss=0.4977011494914685, w0=-5.524731964632497e-07, w1=-0.00047739186577970864\n",
      "Gradient Descent(5/399): loss=0.4971367429899027, w0=-5.543456882474625e-07, w1=-0.0005718297229598037\n",
      "Gradient Descent(6/399): loss=0.49657637773932795, w0=-5.212876803095354e-07, w1=-0.0006659248476931219\n",
      "Gradient Descent(7/399): loss=0.49602001394383466, w0=-4.5383458699416463e-07, w1=-0.0007596789787387992\n",
      "Gradient Descent(8/399): loss=0.4954676122573338, w0=-3.525160176729379e-07, w1=-0.0008530938444636123\n",
      "Gradient Descent(9/399): loss=0.49491913377778624, w0=-2.1785583445354215e-07, w1=-0.0009461711629135948\n",
      "Gradient Descent(10/399): loss=0.4943745400415152, w0=-5.037220932545146e-08, w1=-0.001038912641885092\n",
      "Gradient Descent(11/399): loss=0.4938337930176016, w0=1.4942231925234884e-07, w1=-0.0011313199789952616\n",
      "Gradient Descent(12/399): loss=0.49329685510236104, w0=3.810207903161071e-07, w1=-0.001223394861752021\n",
      "Gradient Descent(13/399): loss=0.49276368911390267, w0=6.439217649092568e-07, w1=-0.0013151389676234523\n",
      "Gradient Descent(14/399): loss=0.49223425828676276, w0=9.376292711360916e-07, w1=-0.0014065539641066657\n",
      "Gradient Descent(15/399): loss=0.49170852626662076, w0=1.2616527497519704e-06, w1=-0.0014976415087961234\n",
      "Gradient Descent(16/399): loss=0.4911864571050872, w0=1.6155070002847704e-06, w1=-0.0015884032494514354\n",
      "Gradient Descent(17/399): loss=0.49066801525457016, w0=1.9987121276824143e-06, w1=-0.0016788408240646262\n",
      "Gradient Descent(18/399): loss=0.4901531655632133, w0=2.4107934894813195e-06, w1=-0.001768955860926879\n",
      "Gradient Descent(19/399): loss=0.4896418732699084, w0=2.8512816434907397e-06, w1=-0.0018587499786947618\n",
      "Gradient Descent(20/399): loss=0.48913410399937685, w0=3.3197122959878695e-06, w1=-0.0019482247864559394\n",
      "Gradient Descent(21/399): loss=0.4886298237573248, w0=3.815626250418815e-06, w1=-0.0020373818837943767\n",
      "Gradient Descent(22/399): loss=0.4881289989256645, w0=4.338569356600464e-06, w1=-0.0021262228608550362\n",
      "Gradient Descent(23/399): loss=0.4876315962578055, w0=4.888092460418297e-06, w1=-0.0022147492984080754\n",
      "Gradient Descent(24/399): loss=0.48713758287401243, w0=5.463751354015451e-06, w1=-0.0023029627679125465\n",
      "Gradient Descent(25/399): loss=0.4866469262568287, w0=6.065106726468077e-06, w1=-0.002390864831579605\n",
      "Gradient Descent(26/399): loss=0.48615959424656496, w0=6.6917241149423655e-06, w1=-0.002478457042435231\n",
      "Gradient Descent(27/399): loss=0.48567555503685195, w0=7.343173856328468e-06, w1=-0.0025657409443824654\n",
      "Gradient Descent(28/399): loss=0.4851947771702556, w0=8.019031039346702e-06, w1=-0.002652718072263165\n",
      "Gradient Descent(29/399): loss=0.4847172295339538, w0=8.718875457121398e-06, w1=-0.002739389951919286\n",
      "Gradient Descent(30/399): loss=0.4842428813554749, w0=9.442291560217855e-06, w1=-0.0028257581002536923\n",
      "Gradient Descent(31/399): loss=0.48377170219849586, w0=1.0188868410137872e-05, w1=-0.0029118240252904975\n",
      "Gradient Descent(32/399): loss=0.4833036619586986, w0=1.0958199633269379e-05, w1=-0.002997589226234943\n",
      "Gradient Descent(33/399): loss=0.48283873085968587, w0=1.1749883375285744e-05, w1=-0.0030830551935328173\n",
      "Gradient Descent(34/399): loss=0.48237687944895175, w0=1.2563522255990334e-05, w1=-0.003168223408929419\n",
      "Gradient Descent(35/399): loss=0.4819180785939112, w0=1.3398723324602064e-05, w1=-0.0032530953455280668\n",
      "Gradient Descent(36/399): loss=0.4814622994779836, w0=1.4255098015477538e-05, w1=-0.003337672467848164\n",
      "Gradient Descent(37/399): loss=0.48100951359673016, w0=1.5132262104265609e-05, w1=-0.0034219562318828166\n",
      "Gradient Descent(38/399): loss=0.480559692754046, w0=1.6029835664490062e-05, w1=-0.0035059480851560108\n",
      "Gradient Descent(39/399): loss=0.48011280905840426, w0=1.694744302455631e-05, w1=-0.003589649466779355\n",
      "Gradient Descent(40/399): loss=0.47966883491915346, w0=1.7884712725177904e-05, w1=-0.0036730618075083896\n",
      "Gradient Descent(41/399): loss=0.47922774304286364, w0=1.8841277477218826e-05, w1=-0.003756186529798464\n",
      "Gradient Descent(42/399): loss=0.4787895064297251, w0=1.9816774119947453e-05, w1=-0.0038390250478601913\n",
      "Gradient Descent(43/399): loss=0.4783540983699953, w0=2.081084357969822e-05, w1=-0.003921578767714478\n",
      "Gradient Descent(44/399): loss=0.4779214924404961, w0=2.1823130828936986e-05, w1=-0.004003849087247139\n",
      "Gradient Descent(45/399): loss=0.4774916625011564, w0=2.285328484572614e-05, w1=-0.004085837396263091\n",
      "Gradient Descent(46/399): loss=0.47706458269160495, w0=2.3900958573585635e-05, w1=-0.004167545076540144\n",
      "Gradient Descent(47/399): loss=0.47664022742780804, w0=2.496580888174593e-05, w1=-0.004248973501882374\n",
      "Gradient Descent(48/399): loss=0.4762185713987528, w0=2.604749652578921e-05, w1=-0.004330124038173099\n",
      "Gradient Descent(49/399): loss=0.47579958956317764, w0=2.7145686108674986e-05, w1=-0.004410998043427449\n",
      "Gradient Descent(50/399): loss=0.4753832571463441, w0=2.8260046042146292e-05, w1=-0.004491596867844543\n",
      "Gradient Descent(51/399): loss=0.4749695496368554, w0=2.939024850851289e-05, w1=-0.004571921853859265\n",
      "Gradient Descent(52/399): loss=0.47455844278351583, w0=3.0535969422807686e-05, w1=-0.004651974336193654\n",
      "Gradient Descent(53/399): loss=0.47414991259223455, w0=3.169688839531279e-05, w1=-0.004731755641907904\n",
      "Gradient Descent(54/399): loss=0.47374393532296893, w0=3.2872688694451585e-05, w1=-0.00481126709045098\n",
      "Gradient Descent(55/399): loss=0.4733404874867108, w0=3.406305721004329e-05, w1=-0.004890509993710853\n",
      "Gradient Descent(56/399): loss=0.4729395458425129, w0=3.526768441691636e-05, w1=-0.004969485656064351\n",
      "Gradient Descent(57/399): loss=0.4725410873945547, w0=3.6486264338877396e-05, w1=-0.005048195374426649\n",
      "Gradient Descent(58/399): loss=0.4721450893892471, w0=3.77184945130319e-05, w1=-0.005126640438300368\n",
      "Gradient Descent(59/399): loss=0.47175152931237807, w0=3.8964075954453686e-05, w1=-0.005204822129824324\n",
      "Gradient Descent(60/399): loss=0.47136038488629345, w0=4.022271312119928e-05, w1=-0.005282741723821894\n",
      "Gradient Descent(61/399): loss=0.4709716340671169, w0=4.1494113879664196e-05, w1=-0.005360400487849034\n",
      "Gradient Descent(62/399): loss=0.4705852550420071, w0=4.277798947027754e-05, w1=-0.005437799682241929\n",
      "Gradient Descent(63/399): loss=0.4702012262264509, w0=4.407405447353182e-05, w1=-0.005514940560164286\n",
      "Gradient Descent(64/399): loss=0.4698195262615912, w0=4.538202677634459e-05, w1=-0.005591824367654278\n",
      "Gradient Descent(65/399): loss=0.46944013401159246, w0=4.670162753874872e-05, w1=-0.005668452343671137\n",
      "Gradient Descent(66/399): loss=0.46906302856103926, w0=4.8032581160908104e-05, w1=-0.00574482572014139\n",
      "Gradient Descent(67/399): loss=0.4686881892123697, w0=4.9374615250455616e-05, w1=-0.005820945722004766\n",
      "Gradient Descent(68/399): loss=0.4683155954833428, w0=5.072746059015016e-05, w1=-0.005896813567259745\n",
      "Gradient Descent(69/399): loss=0.4679452271045379, w0=5.209085110584977e-05, w1=-0.005972430467008778\n",
      "Gradient Descent(70/399): loss=0.46757706401688953, w0=5.346452383479761e-05, w1=-0.006047797625503167\n",
      "Gradient Descent(71/399): loss=0.4672110863692512, w0=5.4848218894217836e-05, w1=-0.006122916240187612\n",
      "Gradient Descent(72/399): loss=0.4668472745159942, w0=5.624167945021835e-05, w1=-0.006197787501744427\n",
      "Gradient Descent(73/399): loss=0.466485609014634, w0=5.764465168699736e-05, w1=-0.0062724125941374265\n",
      "Gradient Descent(74/399): loss=0.4661260706234923, w0=5.9056884776350924e-05, w1=-0.006346792694655491\n",
      "Gradient Descent(75/399): loss=0.46576864029938525, w0=6.047813084747842e-05, w1=-0.006420928973955803\n",
      "Gradient Descent(76/399): loss=0.46541329919534297, w0=6.190814495708308e-05, w1=-0.006494822596106772\n",
      "Gradient Descent(77/399): loss=0.4650600286583605, w0=6.334668505976476e-05, w1=-0.006568474718630632\n",
      "Gradient Descent(78/399): loss=0.4647088102271747, w0=6.4793511978702e-05, w1=-0.0066418864925457295\n",
      "Gradient Descent(79/399): loss=0.46435962563007216, w0=6.624838937662069e-05, w1=-0.0067150590624085035\n",
      "Gradient Descent(80/399): loss=0.46401245678272474, w0=6.771108372704639e-05, w1=-0.006787993566355148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(81/399): loss=0.46366728578605176, w0=6.918136428583773e-05, w1=-0.0068606911361429705\n",
      "Gradient Descent(82/399): loss=0.4633240949241114, w0=7.065900306299791e-05, w1=-0.006933152897191449\n",
      "Gradient Descent(83/399): loss=0.4629828666620184, w0=7.21437747947619e-05, w1=-0.007005379968622984\n",
      "Gradient Descent(84/399): loss=0.4626435836438874, w0=7.36354569159563e-05, w1=-0.0070773734633033485\n",
      "Gradient Descent(85/399): loss=0.46230622869080507, w0=7.513382953262957e-05, w1=-0.0071491344878818475\n",
      "Gradient Descent(86/399): loss=0.4619707847988251, w0=7.663867539494965e-05, w1=-0.007220664142831179\n",
      "Gradient Descent(87/399): loss=0.4616372351369902, w0=7.814977987036665e-05, w1=-0.007291963522487002\n",
      "Gradient Descent(88/399): loss=0.46130556304537995, w0=7.966693091703784e-05, w1=-0.007363033715087216\n",
      "Gradient Descent(89/399): loss=0.46097575203318186, w0=8.118991905751249e-05, w1=-0.007433875802810953\n",
      "Gradient Descent(90/399): loss=0.4606477857767879, w0=8.2718537352674e-05, w1=-0.007504490861817286\n",
      "Gradient Descent(91/399): loss=0.46032164811791454, w0=8.425258137593678e-05, w1=-0.007574879962283653\n",
      "Gradient Descent(92/399): loss=0.45999732306174673, w0=8.579184918769541e-05, w1=-0.007645044168443998\n",
      "Gradient Descent(93/399): loss=0.45967479477510537, w0=8.733614131002375e-05, w1=-0.007714984538626641\n",
      "Gradient Descent(94/399): loss=0.4593540475846376, w0=8.888526070162129e-05, w1=-0.007784702125291866\n",
      "Gradient Descent(95/399): loss=0.45903506597502985, w0=9.043901273300469e-05, w1=-0.007854197975069237\n",
      "Gradient Descent(96/399): loss=0.45871783458724325, w0=9.199720516194176e-05, w1=-0.007923473128794644\n",
      "Gradient Descent(97/399): loss=0.4584023382167715, w0=9.355964810912583e-05, w1=-0.007992528621547076\n",
      "Gradient Descent(98/399): loss=0.4580885618119195, w0=9.512615403408789e-05, w1=-0.008061365482685138\n",
      "Gradient Descent(99/399): loss=0.4577764904721052, w0=9.669653771134451e-05, w1=-0.008129984735883283\n",
      "Gradient Descent(100/399): loss=0.4574661094461806, w0=9.827061620677891e-05, w1=-0.008198387399167804\n",
      "Gradient Descent(101/399): loss=0.45715740413077416, w0=9.984820885425312e-05, w1=-0.008266574484952554\n",
      "Gradient Descent(102/399): loss=0.45685036006865565, w0=0.00010142913723244894, w1=-0.008334547000074402\n",
      "Gradient Descent(103/399): loss=0.45654496294711827, w0=0.00010301322514193543, w1=-0.008402305945828446\n",
      "Gradient Descent(104/399): loss=0.4562411985963825, w0=0.00010460029858246078, w1=-0.008469852318002962\n",
      "Gradient Descent(105/399): loss=0.45593905298802034, w0=0.0001061901857304663, w1=-0.00853718710691411\n",
      "Gradient Descent(106/399): loss=0.45563851223339646, w0=0.00010778271691682044, w1=-0.008604311297440373\n",
      "Gradient Descent(107/399): loss=0.45533956258213126, w0=0.00010937772460477076, w1=-0.00867122586905677\n",
      "Gradient Descent(108/399): loss=0.455042190420581, w0=0.00011097504336811147, w1=-0.008737931795868811\n",
      "Gradient Descent(109/399): loss=0.4547463822703375, w0=0.00011257450986956475, w1=-0.008804430046646198\n",
      "Gradient Descent(110/399): loss=0.4544521247867454, w0=0.0001141759628393736, w1=-0.008870721584856309\n",
      "Gradient Descent(111/399): loss=0.4541594047574383, w0=0.00011577924305410419, w1=-0.008936807368697423\n",
      "Gradient Descent(112/399): loss=0.4538682091008933, w0=0.00011738419331565569, w1=-0.009002688351131705\n",
      "Gradient Descent(113/399): loss=0.4535785248650003, w0=0.00011899065843047552, w1=-0.009068365479917974\n",
      "Gradient Descent(114/399): loss=0.45329033922565265, w0=0.00012059848518897806, w1=-0.00913383969764421\n",
      "Gradient Descent(115/399): loss=0.45300363948535227, w0=0.00012220752234516491, w1=-0.009199111941759854\n",
      "Gradient Descent(116/399): loss=0.45271841307183164, w0=0.00012381762059644454, w1=-0.00926418314460786\n",
      "Gradient Descent(117/399): loss=0.4524346475366945, w0=0.00012542863256364954, w1=-0.009329054233456525\n",
      "Gradient Descent(118/399): loss=0.4521523305540712, w0=0.00012704041277124958, w1=-0.009393726130531094\n",
      "Gradient Descent(119/399): loss=0.4518714499192905, w0=0.0001286528176277581, w1=-0.009458199753045137\n",
      "Gradient Descent(120/399): loss=0.4515919935475672, w0=0.00013026570540633077, w1=-0.009522476013231695\n",
      "Gradient Descent(121/399): loss=0.4513139494727073, w0=0.00013187893622555402, w1=-0.009586555818374224\n",
      "Gradient Descent(122/399): loss=0.45103730584582624, w0=0.00013349237203042174, w1=-0.009650440070837301\n",
      "Gradient Descent(123/399): loss=0.4507620509340845, w0=0.00013510587657349822, w1=-0.009714129668097118\n",
      "Gradient Descent(124/399): loss=0.4504881731194361, w0=0.00013671931539626565, w1=-0.009777625502771763\n",
      "Gradient Descent(125/399): loss=0.4502156608973956, w0=0.00013833255581065427, w1=-0.009840928462651288\n",
      "Gradient Descent(126/399): loss=0.44994450287581467, w0=0.00013994546688075353, w1=-0.009904039430727549\n",
      "Gradient Descent(127/399): loss=0.4496746877736801, w0=0.00014155791940470233, w1=-0.009966959285223858\n",
      "Gradient Descent(128/399): loss=0.4494062044199179, w0=0.00014316978589675675, w1=-0.010029688899624405\n",
      "Gradient Descent(129/399): loss=0.4491390417522195, w0=0.00014478094056953353, w1=-0.010092229142703487\n",
      "Gradient Descent(130/399): loss=0.44887318881587635, w0=0.00014639125931642736, w1=-0.010154580878554524\n",
      "Gradient Descent(131/399): loss=0.44860863476263063, w0=0.00014800061969420068, w1=-0.010216744966618866\n",
      "Gradient Descent(132/399): loss=0.4483453688495397, w0=0.000149608900905744, w1=-0.010278722261714413\n",
      "Gradient Descent(133/399): loss=0.4480833804378521, w0=0.00015121598378300518, w1=-0.01034051361406401\n",
      "Gradient Descent(134/399): loss=0.44782265899190055, w0=0.00015282175077008613, w1=-0.01040211986932367\n",
      "Gradient Descent(135/399): loss=0.4475631940780024, w0=0.00015442608590650504, w1=-0.010463541868610574\n",
      "Gradient Descent(136/399): loss=0.447304975363379, w0=0.00015602887481062282, w1=-0.01052478044853089\n",
      "Gradient Descent(137/399): loss=0.44704799261508277, w0=0.00015763000466323206, w1=-0.010585836441207397\n",
      "Gradient Descent(138/399): loss=0.44679223569894055, w0=0.0001592293641913068, w1=-0.010646710674306899\n",
      "Gradient Descent(139/399): loss=0.44653769457850667, w0=0.00016082684365191163, w1=-0.01070740397106747\n",
      "Gradient Descent(140/399): loss=0.44628435931402965, w0=0.0001624223348162688, w1=-0.010767917150325499\n",
      "Gradient Descent(141/399): loss=0.446032220061431, w0=0.00016401573095398126, w1=-0.010828251026542534\n",
      "Gradient Descent(142/399): loss=0.4457812670712946, w0=0.00016560692681741074, w1=-0.010888406409831962\n",
      "Gradient Descent(143/399): loss=0.44553149068786957, w0=0.00016719581862620892, w1=-0.010948384105985485\n",
      "Gradient Descent(144/399): loss=0.4452828813480835, w0=0.00016878230405200037, w1=-0.011008184916499417\n",
      "Gradient Descent(145/399): loss=0.4450354295805681, w0=0.0001703662822032158, w1=-0.011067809638600802\n",
      "Gradient Descent(146/399): loss=0.444789126004694, w0=0.00017194765361007406, w1=-0.011127259065273348\n",
      "Gradient Descent(147/399): loss=0.4445439613296205, w0=0.0001735263202097116, w1=-0.011186533985283175\n",
      "Gradient Descent(148/399): loss=0.444299926353352, w0=0.00017510218533145773, w1=-0.011245635183204395\n",
      "Gradient Descent(149/399): loss=0.444057011961809, w0=0.00017667515368225458, w1=-0.011304563439444507\n",
      "Gradient Descent(150/399): loss=0.44381520912790684, w0=0.00017824513133222003, w1=-0.011363319530269618\n",
      "Gradient Descent(151/399): loss=0.44357450891064687, w0=0.00017981202570035233, w1=-0.011421904227829493\n",
      "Gradient Descent(152/399): loss=0.4433349024542177, w0=0.00018137574554037526, w1=-0.011480318300182426\n",
      "Gradient Descent(153/399): loss=0.4430963809871067, w0=0.00018293620092672206, w1=-0.01153856251131994\n",
      "Gradient Descent(154/399): loss=0.4428589358212206, w0=0.00018449330324065712, w1=-0.011596637621191326\n",
      "Gradient Descent(155/399): loss=0.442622558351018, w0=0.00018604696515653384, w1=-0.011654544385727997\n",
      "Gradient Descent(156/399): loss=0.4423872400526512, w0=0.00018759710062818763, w1=-0.011712283556867695\n",
      "Gradient Descent(157/399): loss=0.4421529724831159, w0=0.00018914362487546245, w1=-0.011769855882578504\n",
      "Gradient Descent(158/399): loss=0.44191974727941397, w0=0.00019068645437086965, w1=-0.011827262106882726\n",
      "Gradient Descent(159/399): loss=0.4416875561577224, w0=0.00019222550682637804, w1=-0.011884502969880575\n",
      "Gradient Descent(160/399): loss=0.4414563909125748, w0=0.00019376070118033365, w1=-0.011941579207773714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(161/399): loss=0.44122624341604894, w0=0.00019529195758450802, w1=-0.01199849155288863\n",
      "Gradient Descent(162/399): loss=0.4409971056169667, w0=0.0001968191973912739, w1=-0.012055240733699854\n",
      "Gradient Descent(163/399): loss=0.44076896954010014, w0=0.0001983423431409068, w1=-0.012111827474853011\n",
      "Gradient Descent(164/399): loss=0.4405418272853884, w0=0.0001998613185490115, w1=-0.012168252497187724\n",
      "Gradient Descent(165/399): loss=0.44031567102716396, w0=0.00020137604849407225, w1=-0.012224516517760355\n",
      "Gradient Descent(166/399): loss=0.44009049301338454, w0=0.0002028864590051251, w1=-0.012280620249866592\n",
      "Gradient Descent(167/399): loss=0.43986628556487795, w0=0.00020439247724955185, w1=-0.012336564403063882\n",
      "Gradient Descent(168/399): loss=0.439643041074591, w0=0.00020589403152099364, w1=-0.012392349683193714\n",
      "Gradient Descent(169/399): loss=0.439420752006851, w0=0.00020739105122738368, w1=-0.012447976792403745\n",
      "Gradient Descent(170/399): loss=0.43919941089663184, w0=0.00020888346687909773, w1=-0.012503446429169781\n",
      "Gradient Descent(171/399): loss=0.43897901034883086, w0=0.00021037121007722094, w1=-0.012558759288317609\n",
      "Gradient Descent(172/399): loss=0.43875954303755327, w0=0.00021185421350193028, w1=-0.012613916061044669\n",
      "Gradient Descent(173/399): loss=0.4385410017054032, w0=0.00021333241090099134, w1=-0.012668917434941598\n",
      "Gradient Descent(174/399): loss=0.4383233791627843, w0=0.00021480573707836827, w1=-0.012723764094013615\n",
      "Gradient Descent(175/399): loss=0.4381066682872071, w0=0.00021627412788294585, w1=-0.012778456718701762\n",
      "Gradient Descent(176/399): loss=0.4378908620226051, w0=0.00021773752019736258, w1=-0.012832995985904008\n",
      "Gradient Descent(177/399): loss=0.43767595337865683, w0=0.00021919585192695378, w1=-0.012887382568996205\n",
      "Gradient Descent(178/399): loss=0.4374619354301179, w0=0.0002206490619888035, w1=-0.012941617137852905\n",
      "Gradient Descent(179/399): loss=0.4372488013161565, w0=0.00022209709030090427, w1=-0.012995700358868033\n",
      "Gradient Descent(180/399): loss=0.43703654423970134, w0=0.0002235398777714237, w1=-0.013049632894975427\n",
      "Gradient Descent(181/399): loss=0.43682515746679307, w0=0.00022497736628807668, w1=-0.013103415405669236\n",
      "Gradient Descent(182/399): loss=0.43661463432594294, w0=0.00022640949870760243, w1=-0.013157048547024177\n",
      "Gradient Descent(183/399): loss=0.4364049682075009, w0=0.00022783621884534523, w1=-0.013210532971715665\n",
      "Gradient Descent(184/399): loss=0.43619615256302935, w0=0.00022925747146493772, w1=-0.0132638693290398\n",
      "Gradient Descent(185/399): loss=0.43598818090468267, w0=0.00023067320226808607, w1=-0.013317058264933224\n",
      "Gradient Descent(186/399): loss=0.4357810468045949, w0=0.00023208335788445567, w1=-0.013370100421992848\n",
      "Gradient Descent(187/399): loss=0.43557474389427453, w0=0.00023348788586165668, w1=-0.013422996439495434\n",
      "Gradient Descent(188/399): loss=0.4353692658640048, w0=0.00023488673465532837, w1=-0.013475746953417063\n",
      "Gradient Descent(189/399): loss=0.4351646064622503, w0=0.00023627985361932105, w1=-0.013528352596452459\n",
      "Gradient Descent(190/399): loss=0.43496075949507285, w0=0.00023766719299597516, w1=-0.013580813998034198\n",
      "Gradient Descent(191/399): loss=0.4347577188255481, w0=0.00023904870390649595, w1=-0.013633131784351769\n",
      "Gradient Descent(192/399): loss=0.4345554783731962, w0=0.0002404243383414234, w1=-0.013685306578370535\n",
      "Gradient Descent(193/399): loss=0.43435403211341134, w0=0.00024179404915119604, w1=-0.013737338999850537\n",
      "Gradient Descent(194/399): loss=0.43415337407690197, w0=0.00024315779003680798, w1=-0.013789229665365202\n",
      "Gradient Descent(195/399): loss=0.433953498349136, w0=0.00024451551554055816, w1=-0.013840979188319905\n",
      "Gradient Descent(196/399): loss=0.4337543990697912, w0=0.00024586718103689097, w1=-0.013892588178970416\n",
      "Gradient Descent(197/399): loss=0.43355607043221256, w0=0.0002472127427233273, w1=-0.013944057244441234\n",
      "Gradient Descent(198/399): loss=0.43335850668287507, w0=0.0002485521576114852, w1=-0.013995386988743778\n",
      "Gradient Descent(199/399): loss=0.4331617021208527, w0=0.0002498853835181891, w1=-0.01404657801279448\n",
      "Gradient Descent(200/399): loss=0.4329656510972917, w0=0.0002512123790566672, w1=-0.014097630914432741\n",
      "Gradient Descent(201/399): loss=0.4327703480148922, w0=0.0002525331036278356, w1=-0.014148546288438781\n",
      "Gradient Descent(202/399): loss=0.4325757873273937, w0=0.00025384751741166844, w1=-0.014199324726551363\n",
      "Gradient Descent(203/399): loss=0.4323819635390646, w0=0.00025515558135865394, w1=-0.014249966817485405\n",
      "Gradient Descent(204/399): loss=0.43218887120420085, w0=0.0002564572571813343, w1=-0.014300473146949471\n",
      "Gradient Descent(205/399): loss=0.4319965049266275, w0=0.0002577525073459295, w1=-0.014350844297663149\n",
      "Gradient Descent(206/399): loss=0.43180485935920543, w0=0.0002590412950640441, w1=-0.014401080849374323\n",
      "Gradient Descent(207/399): loss=0.4316139292033453, w0=0.00026032358428445585, w1=-0.014451183378876312\n",
      "Gradient Descent(208/399): loss=0.4314237092085249, w0=0.00026159933968498576, w1=-0.01450115246002492\n",
      "Gradient Descent(209/399): loss=0.43123419417181175, w0=0.00026286852666444847, w1=-0.014550988663755357\n",
      "Gradient Descent(210/399): loss=0.43104537893739214, w0=0.00026413111133468244, w1=-0.014600692558099053\n",
      "Gradient Descent(211/399): loss=0.43085725839610445, w0=0.0002653870605126589, w1=-0.014650264708200366\n",
      "Gradient Descent(212/399): loss=0.4306698274849766, w0=0.0002666363417126692, w1=-0.014699705676333179\n",
      "Gradient Descent(213/399): loss=0.4304830811867697, w0=0.0002678789231385891, w1=-0.014749016021917388\n",
      "Gradient Descent(214/399): loss=0.4302970145295263, w0=0.0002691147736762204, w1=-0.014798196301535279\n",
      "Gradient Descent(215/399): loss=0.43011162258612273, w0=0.00027034386288570764, w1=-0.014847247068947803\n",
      "Gradient Descent(216/399): loss=0.42992690047382714, w0=0.0002715661609940306, w1=-0.014896168875110746\n",
      "Gradient Descent(217/399): loss=0.42974284335386126, w0=0.0002727816388875709, w1=-0.014944962268190786\n",
      "Gradient Descent(218/399): loss=0.4295594464309681, w0=0.00027399026810475237, w1=-0.014993627793581456\n",
      "Gradient Descent(219/399): loss=0.42937670495298197, w0=0.0002751920208287545, w1=-0.015042165993918992\n",
      "Gradient Descent(220/399): loss=0.42919461421040644, w0=0.0002763868698802981, w1=-0.015090577409098086\n",
      "Gradient Descent(221/399): loss=0.4290131695359938, w0=0.0002775747887105024, w1=-0.015138862576287538\n",
      "Gradient Descent(222/399): loss=0.42883236630432947, w0=0.00027875575139381343, w1=-0.015187022029945799\n",
      "Gradient Descent(223/399): loss=0.42865219993142323, w0=0.0002799297326210021, w1=-0.015235056301836419\n",
      "Gradient Descent(224/399): loss=0.4284726658743002, w0=0.0002810967076922322, w1=-0.015282965921043393\n",
      "Gradient Descent(225/399): loss=0.42829375963060146, w0=0.0002822566525101969, w1=-0.01533075141398641\n",
      "Gradient Descent(226/399): loss=0.42811547673818423, w0=0.000283409543573324, w1=-0.015378413304435996\n",
      "Gradient Descent(227/399): loss=0.4279378127747292, w0=0.0002845553579690477, w1=-0.015425952113528573\n",
      "Gradient Descent(228/399): loss=0.4277607633573509, w0=0.0002856940733671484, w1=-0.01547336835978141\n",
      "Gradient Descent(229/399): loss=0.4275843241422117, w0=0.00028682566801315786, w1=-0.015520662559107482\n",
      "Gradient Descent(230/399): loss=0.4274084908241417, w0=0.00028795012072183, w1=-0.015567835224830223\n",
      "Gradient Descent(231/399): loss=0.42723325913625954, w0=0.0002890674108706774, w1=-0.015614886867698206\n",
      "Gradient Descent(232/399): loss=0.42705862484960044, w0=0.0002901775183935709, w1=-0.01566181799589971\n",
      "Gradient Descent(233/399): loss=0.4268845837727461, w0=0.00029128042377440385, w1=-0.015708629115077203\n",
      "Gradient Descent(234/399): loss=0.4267111317514589, w0=0.0002923761080408191, w1=-0.015755320728341717\n",
      "Gradient Descent(235/399): loss=0.42653826466832023, w0=0.00029346455275799824, w1=-0.015801893336287152\n",
      "Gradient Descent(236/399): loss=0.4263659784423721, w0=0.00029454574002251295, w1=-0.015848347437004483\n",
      "Gradient Descent(237/399): loss=0.4261942690287629, w0=0.0002956196524562374, w1=-0.015894683526095868\n",
      "Gradient Descent(238/399): loss=0.4260231324183975, w0=0.0002966862732003216, w1=-0.01594090209668866\n",
      "Gradient Descent(239/399): loss=0.4258525646375883, w0=0.00029774558590922416, w1=-0.015987003639449354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(240/399): loss=0.4256825617477141, w0=0.00029879757474480546, w1=-0.01603298864259742\n",
      "Gradient Descent(241/399): loss=0.4255131198448783, w0=0.00029984222437047894, w1=-0.01607885759191907\n",
      "Gradient Descent(242/399): loss=0.42534423505957386, w0=0.000300879519945421, w1=-0.01612461097078091\n",
      "Gradient Descent(243/399): loss=0.42517590355634927, w0=0.00030190944711883875, w1=-0.016170249260143533\n",
      "Gradient Descent(244/399): loss=0.4250081215334804, w0=0.00030293199202429473, w1=-0.016215772938575006\n",
      "Gradient Descent(245/399): loss=0.4248408852226437, w0=0.0003039471412740885, w1=-0.016261182482264287\n",
      "Gradient Descent(246/399): loss=0.42467419088859326, w0=0.00030495488195369423, w1=-0.01630647836503454\n",
      "Gradient Descent(247/399): loss=0.4245080348288428, w0=0.00030595520161625376, w1=-0.01635166105835638\n",
      "Gradient Descent(248/399): loss=0.42434241337334766, w0=0.00030694808827712484, w1=-0.016396731031361023\n",
      "Gradient Descent(249/399): loss=0.4241773228841941, w0=0.00030793353040848377, w1=-0.016441688750853365\n",
      "Gradient Descent(250/399): loss=0.424012759755288, w0=0.000308911516933982, w1=-0.01648653468132497\n",
      "Gradient Descent(251/399): loss=0.4238487204120499, w0=0.0003098820372234559, w1=-0.016531269284966968\n",
      "Gradient Descent(252/399): loss=0.4236852013111111, w0=0.00031084508108769013, w1=-0.016575893021682896\n",
      "Gradient Descent(253/399): loss=0.4235221989400132, w0=0.0003118006387732325, w1=-0.01662040634910143\n",
      "Gradient Descent(254/399): loss=0.42335970981691196, w0=0.0003127487009572614, w1=-0.016664809722589057\n",
      "Gradient Descent(255/399): loss=0.4231977304902822, w0=0.0003136892587425045, w1=-0.01670910359526266\n",
      "Gradient Descent(256/399): loss=0.4230362575386288, w0=0.0003146223036522078, w1=-0.016753288418002023\n",
      "Gradient Descent(257/399): loss=0.422875287570196, w0=0.00031554782762515597, w1=-0.016797364639462253\n",
      "Gradient Descent(258/399): loss=0.42271481722268484, w0=0.0003164658230107418, w1=-0.016841332706086138\n",
      "Gradient Descent(259/399): loss=0.422554843162969, w0=0.00031737628256408554, w1=-0.016885193062116412\n",
      "Gradient Descent(260/399): loss=0.4223953620868176, w0=0.00031827919944120284, w1=-0.016928946149607958\n",
      "Gradient Descent(261/399): loss=0.42223637071861675, w0=0.0003191745671942215, w1=-0.01697259240843992\n",
      "Gradient Descent(262/399): loss=0.4220778658110982, w0=0.00032006237976664605, w1=-0.017016132276327745\n",
      "Gradient Descent(263/399): loss=0.42191984414506567, w0=0.00032094263148866963, w1=-0.017059566188835158\n",
      "Gradient Descent(264/399): loss=0.4217623025291297, w0=0.0003218153170725334, w1=-0.017102894579386043\n",
      "Gradient Descent(265/399): loss=0.42160523779944103, w0=0.0003226804316079323, w1=-0.017146117879276276\n",
      "Gradient Descent(266/399): loss=0.4214486468194277, w0=0.0003235379705574667, w1=-0.01718923651768546\n",
      "Gradient Descent(267/399): loss=0.4212925264795356, w0=0.00032438792975214044, w1=-0.017232250921688598\n",
      "Gradient Descent(268/399): loss=0.42113687369697095, w0=0.00032523030538690346, w1=-0.0172751615162677\n",
      "Gradient Descent(269/399): loss=0.4209816854154462, w0=0.0003260650940162396, w1=-0.017317968724323295\n",
      "Gradient Descent(270/399): loss=0.4208269586049269, w0=0.00032689229254979885, w1=-0.017360672966685902\n",
      "Gradient Descent(271/399): loss=0.42067269026138304, w0=0.00032771189824807326, w1=-0.017403274662127405\n",
      "Gradient Descent(272/399): loss=0.42051887740654115, w0=0.0003285239087181167, w1=-0.01744577422737237\n",
      "Gradient Descent(273/399): loss=0.4203655170876412, w0=0.0003293283219093075, w1=-0.017488172077109294\n",
      "Gradient Descent(274/399): loss=0.4202126063771928, w0=0.000330125136109154, w1=-0.017530468624001764\n",
      "Gradient Descent(275/399): loss=0.420060142372737, w0=0.00033091434993914214, w1=-0.01757266427869958\n",
      "Gradient Descent(276/399): loss=0.4199081221966079, w0=0.000331695962350625, w1=-0.017614759449849773\n",
      "Gradient Descent(277/399): loss=0.4197565429956991, w0=0.00033246997262075396, w1=-0.017656754544107596\n",
      "Gradient Descent(278/399): loss=0.4196054019412303, w0=0.00033323638034845086, w1=-0.017698649966147402\n",
      "Gradient Descent(279/399): loss=0.4194546962285176, w0=0.00033399518545042055, w1=-0.017740446118673488\n",
      "Gradient Descent(280/399): loss=0.4193044230767453, w0=0.0003347463881572041, w1=-0.017782143402430863\n",
      "Gradient Descent(281/399): loss=0.4191545797287415, w0=0.0003354899890092716, w1=-0.017823742216215948\n",
      "Gradient Descent(282/399): loss=0.4190051634507532, w0=0.0003362259888531545, w1=-0.0178652429568872\n",
      "Gradient Descent(283/399): loss=0.4188561715322272, w0=0.00033695438883761747, w1=-0.017906646019375704\n",
      "Gradient Descent(284/399): loss=0.4187076012855895, w0=0.0003376751904098681, w1=-0.017947951796695647\n",
      "Gradient Descent(285/399): loss=0.4185594500460301, w0=0.0003383883953118059, w1=-0.017989160679954787\n",
      "Gradient Descent(286/399): loss=0.41841171517128817, w0=0.0003390940055763087, w1=-0.01803027305836481\n",
      "Gradient Descent(287/399): loss=0.41826439404143945, w0=0.0003397920235235567, w1=-0.018071289319251647\n",
      "Gradient Descent(288/399): loss=0.4181174840586865, w0=0.00034048245175739416, w1=-0.018112209848065733\n",
      "Gradient Descent(289/399): loss=0.4179709826471507, w0=0.0003411652931617274, w1=-0.01815303502839218\n",
      "Gradient Descent(290/399): loss=0.41782488725266576, w0=0.00034184055089695993, w1=-0.018193765241960907\n",
      "Gradient Descent(291/399): loss=0.4176791953425749, w0=0.0003425082283964631, w1=-0.018234400868656707\n",
      "Gradient Descent(292/399): loss=0.417533904405528, w0=0.00034316832936308317, w1=-0.01827494228652924\n",
      "Gradient Descent(293/399): loss=0.41738901195128186, w0=0.0003438208577656835, w1=-0.018315389871802985\n",
      "Gradient Descent(294/399): loss=0.4172445155105028, w0=0.000344465817835722, w1=-0.018355743998887106\n",
      "Gradient Descent(295/399): loss=0.4171004126345704, w0=0.00034510321406386324, w1=-0.018396005040385285\n",
      "Gradient Descent(296/399): loss=0.4169567008953845, w0=0.0003457330511966252, w1=-0.018436173367105475\n",
      "Gradient Descent(297/399): loss=0.4168133778851712, w0=0.00034635533423305997, w1=-0.0184762493480696\n",
      "Gradient Descent(298/399): loss=0.41667044121629526, w0=0.0003469700684214683, w1=-0.018516233350523204\n",
      "Gradient Descent(299/399): loss=0.4165278885210694, w0=0.0003475772592561476, w1=-0.018556125739945025\n",
      "Gradient Descent(300/399): loss=0.41638571745157044, w0=0.0003481769124741731, w1=-0.018595926880056533\n",
      "Gradient Descent(301/399): loss=0.41624392567945223, w0=0.00034876903405221176, w1=-0.01863563713283139\n",
      "Gradient Descent(302/399): loss=0.416102510895766, w0=0.00034935363020336866, w1=-0.01867525685850486\n",
      "Gradient Descent(303/399): loss=0.4159614708107767, w0=0.00034993070737406566, w1=-0.018714786415583162\n",
      "Gradient Descent(304/399): loss=0.415820803153785, w0=0.00035050027224095185, w1=-0.01875422616085278\n",
      "Gradient Descent(305/399): loss=0.415680505672951, w0=0.00035106233170784557, w1=-0.018793576449389695\n",
      "Gradient Descent(306/399): loss=0.4155405761351173, w0=0.00035161689290270757, w1=-0.01883283763456856\n",
      "Gradient Descent(307/399): loss=0.41540101232563575, w0=0.0003521639631746454, w1=-0.018872010068071852\n",
      "Gradient Descent(308/399): loss=0.4152618120481956, w0=0.00035270355009094816, w1=-0.018911094099898933\n",
      "Gradient Descent(309/399): loss=0.41512297312465285, w0=0.00035323566143415157, w1=-0.01895009007837508\n",
      "Gradient Descent(310/399): loss=0.41498449339486165, w0=0.0003537603051991334, w1=-0.018988998350160442\n",
      "Gradient Descent(311/399): loss=0.41484637071650693, w0=0.00035427748959023837, w1=-0.01902781926025896\n",
      "Gradient Descent(312/399): loss=0.4147086029649398, w0=0.00035478722301843273, w1=-0.019066553152027235\n",
      "Gradient Descent(313/399): loss=0.41457118803301296, w0=0.0003552895140984877, w1=-0.019105200367183307\n",
      "Gradient Descent(314/399): loss=0.4144341238309193, w0=0.0003557843716461921, w1=-0.01914376124581544\n",
      "Gradient Descent(315/399): loss=0.4142974082860307, w0=0.0003562718046755937, w1=-0.019182236126390802\n",
      "Gradient Descent(316/399): loss=0.41416103934273957, w0=0.0003567518223962684, w1=-0.01922062534576413\n",
      "Gradient Descent(317/399): loss=0.41402501496230104, w0=0.0003572244342106181, w1=-0.019258929239186318\n",
      "Gradient Descent(318/399): loss=0.4138893331226775, w0=0.00035768964971119577, w1=-0.019297148140312963\n",
      "Gradient Descent(319/399): loss=0.4137539918183839, w0=0.0003581474786780587, w1=-0.019335282381212866\n",
      "Gradient Descent(320/399): loss=0.41361898906033556, w0=0.0003585979310761482, w1=-0.019373332292376474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(321/399): loss=0.41348432287569614, w0=0.00035904101705269714, w1=-0.019411298202724275\n",
      "Gradient Descent(322/399): loss=0.4133499913077281, w0=0.0003594767469346635, w1=-0.019449180439615146\n",
      "Gradient Descent(323/399): loss=0.413215992415644, w0=0.0003599051312261909, w1=-0.019486979328854643\n",
      "Gradient Descent(324/399): loss=0.4130823242744608, w0=0.00036032618060609486, w1=-0.019524695194703248\n",
      "Gradient Descent(325/399): loss=0.41294898497485305, w0=0.0003607399059253756, w1=-0.01956232835988457\n",
      "Gradient Descent(326/399): loss=0.41281597262300934, w0=0.00036114631820475607, w1=-0.019599879145593493\n",
      "Gradient Descent(327/399): loss=0.4126832853404896, w0=0.00036154542863224554, w1=-0.01963734787150427\n",
      "Gradient Descent(328/399): loss=0.4125509212640841, w0=0.0003619372485607286, w1=-0.019674734855778586\n",
      "Gradient Descent(329/399): loss=0.4124188785456734, w0=0.0003623217895055788, w1=-0.019712040415073545\n",
      "Gradient Descent(330/399): loss=0.41228715535208965, w0=0.00036269906314229724, w1=-0.01974926486454965\n",
      "Gradient Descent(331/399): loss=0.4121557498649797, w0=0.0003630690813041757, w1=-0.019786408517878693\n",
      "Gradient Descent(332/399): loss=0.41202466028066964, w0=0.0003634318559799836, w1=-0.019823471687251636\n",
      "Gradient Descent(333/399): loss=0.4118938848100297, w0=0.00036378739931167955, w1=-0.019860454683386413\n",
      "Gradient Descent(334/399): loss=0.4117634216783408, w0=0.00036413572359214613, w1=-0.019897357815535713\n",
      "Gradient Descent(335/399): loss=0.41163326912516385, w0=0.00036447684126294854, w1=-0.019934181391494697\n",
      "Gradient Descent(336/399): loss=0.41150342540420765, w0=0.0003648107649121165, w1=-0.019970925717608685\n",
      "Gradient Descent(337/399): loss=0.41137388878320147, w0=0.000365137507271949, w1=-0.020007591098780787\n",
      "Gradient Descent(338/399): loss=0.41124465754376527, w0=0.00036545708121684257, w1=-0.020044177838479494\n",
      "Gradient Descent(339/399): loss=0.41111572998128315, w0=0.00036576949976114105, w1=-0.020080686238746228\n",
      "Gradient Descent(340/399): loss=0.4109871044047787, w0=0.000366074776057009, w1=-0.020117116600202833\n",
      "Gradient Descent(341/399): loss=0.41085877913678953, w0=0.0003663729233923264, w1=-0.020153469222059038\n",
      "Gradient Descent(342/399): loss=0.4107307525132451, w0=0.00036666395518860604, w1=-0.02018974440211988\n",
      "Gradient Descent(343/399): loss=0.4106030228833437, w0=0.00036694788499893195, w1=-0.020225942436793064\n",
      "Gradient Descent(344/399): loss=0.4104755886094324, w0=0.00036722472650591996, w1=-0.020262063621096282\n",
      "Gradient Descent(345/399): loss=0.4103484480668873, w0=0.0003674944935196994, w1=-0.020298108248664522\n",
      "Gradient Descent(346/399): loss=0.4102215996439949, w0=0.00036775719997591584, w1=-0.020334076611757287\n",
      "Gradient Descent(347/399): loss=0.41009504174183475, w0=0.000368012859933755, w1=-0.020369969001265805\n",
      "Gradient Descent(348/399): loss=0.4099687727741633, w0=0.0003682614875739871, w1=-0.020405785706720192\n",
      "Gradient Descent(349/399): loss=0.4098427911672994, w0=0.00036850309719703206, w1=-0.02044152701629656\n",
      "Gradient Descent(350/399): loss=0.40971709536000894, w0=0.0003687377032210447, w1=-0.020477193216824088\n",
      "Gradient Descent(351/399): loss=0.40959168380339384, w0=0.0003689653201800203, w1=-0.02051278459379207\n",
      "Gradient Descent(352/399): loss=0.4094665549607786, w0=0.00036918596272192, w1=-0.020548301431356896\n",
      "Gradient Descent(353/399): loss=0.4093417073076008, w0=0.0003693996456068159, w1=-0.020583744012349005\n",
      "Gradient Descent(354/399): loss=0.4092171393313011, w0=0.0003696063837050559, w1=-0.020619112618279808\n",
      "Gradient Descent(355/399): loss=0.4090928495312142, w0=0.00036980619199544773, w1=-0.020654407529348544\n",
      "Gradient Descent(356/399): loss=0.40896883641846205, w0=0.0003699990855634619, w1=-0.020689629024449124\n",
      "Gradient Descent(357/399): loss=0.40884509851584705, w0=0.00037018507959945423, w1=-0.02072477738117692\n",
      "Gradient Descent(358/399): loss=0.40872163435774667, w0=0.00037036418939690683, w1=-0.020759852875835517\n",
      "Gradient Descent(359/399): loss=0.40859844249000854, w0=0.00037053643035068746, w1=-0.02079485578344343\n",
      "Gradient Descent(360/399): loss=0.40847552146984795, w0=0.0003707018179553278, w1=-0.020829786377740772\n",
      "Gradient Descent(361/399): loss=0.4083528698657446, w0=0.00037086036780331963, w1=-0.020864644931195903\n",
      "Gradient Descent(362/399): loss=0.4082304862573413, w0=0.0003710120955834293, w1=-0.020899431715012025\n",
      "Gradient Descent(363/399): loss=0.40810836923534377, w0=0.0003711570170790298, w1=-0.020934146999133735\n",
      "Gradient Descent(364/399): loss=0.4079865174014211, w0=0.000371295148166451, w1=-0.020968791052253553\n",
      "Gradient Descent(365/399): loss=0.4078649293681067, w0=0.00037142650481334694, w1=-0.02100336414181841\n",
      "Gradient Descent(366/399): loss=0.4077436037587013, w0=0.00037155110307708085, w1=-0.021037866534036102\n",
      "Gradient Descent(367/399): loss=0.4076225392071754, w0=0.00037166895910312734, w1=-0.02107229849388168\n",
      "Gradient Descent(368/399): loss=0.4075017343580747, w0=0.0003717800891234914, w1=-0.021106660285103843\n",
      "Gradient Descent(369/399): loss=0.40738118786642447, w0=0.0003718845094551446, w1=-0.021140952170231275\n",
      "Gradient Descent(370/399): loss=0.40726089839763535, w0=0.0003719822364984776, w1=-0.021175174410578937\n",
      "Gradient Descent(371/399): loss=0.4071408646274115, w0=0.0003720732867357699, w1=-0.021209327266254345\n",
      "Gradient Descent(372/399): loss=0.4070210852416569, w0=0.0003721576767296748, w1=-0.021243410996163784\n",
      "Gradient Descent(373/399): loss=0.40690155893638535, w0=0.00037223542312172196, w1=-0.021277425858018513\n",
      "Gradient Descent(374/399): loss=0.4067822844176295, w0=0.00037230654263083505, w1=-0.02131137210834092\n",
      "Gradient Descent(375/399): loss=0.40666326040135137, w0=0.0003723710520518657, w1=-0.021345250002470655\n",
      "Gradient Descent(376/399): loss=0.4065444856133531, w0=0.00037242896825414316, w1=-0.021379059794570696\n",
      "Gradient Descent(377/399): loss=0.40642595878919097, w0=0.00037248030818003933, w1=-0.02141280173763343\n",
      "Gradient Descent(378/399): loss=0.4063076786740857, w0=0.0003725250888435498, w1=-0.021446476083486647\n",
      "Gradient Descent(379/399): loss=0.40618964402283897, w0=0.00037256332732888943, w1=-0.02148008308279955\n",
      "Gradient Descent(380/399): loss=0.40607185359974635, w0=0.00037259504078910384, w1=-0.02151362298508868\n",
      "Gradient Descent(381/399): loss=0.4059543061785128, w0=0.00037262024644469527, w1=-0.02154709603872386\n",
      "Gradient Descent(382/399): loss=0.4058370005421699, w0=0.0003726389615822637, w1=-0.021580502490934052\n",
      "Gradient Descent(383/399): loss=0.40571993548299234, w0=0.0003726512035531624, w1=-0.021613842587813226\n",
      "Gradient Descent(384/399): loss=0.4056031098024156, w0=0.0003726569897721683, w1=-0.021647116574326174\n",
      "Gradient Descent(385/399): loss=0.4054865223109552, w0=0.00037265633771616674, w1=-0.021680324694314287\n",
      "Gradient Descent(386/399): loss=0.4053701718281252, w0=0.00037264926492285006, w1=-0.021713467190501313\n",
      "Gradient Descent(387/399): loss=0.4052540571823592, w0=0.0003726357889894311, w1=-0.021746544304499068\n",
      "Gradient Descent(388/399): loss=0.40513817721093104, w0=0.0003726159275713701, w1=-0.021779556276813133\n",
      "Gradient Descent(389/399): loss=0.40502253075987615, w0=0.0003725896983811157, w1=-0.021812503346848505\n",
      "Gradient Descent(390/399): loss=0.4049071166839143, w0=0.0003725571191868595, w1=-0.021845385752915208\n",
      "Gradient Descent(391/399): loss=0.40479193384637296, w0=0.00037251820781130464, w1=-0.0218782037322339\n",
      "Gradient Descent(392/399): loss=0.40467698111911055, w0=0.00037247298213044717, w1=-0.021910957520941424\n",
      "Gradient Descent(393/399): loss=0.40456225738244156, w0=0.0003724214600723715, w1=-0.021943647354096332\n",
      "Gradient Descent(394/399): loss=0.40444776152506157, w0=0.00037236365961605835, w1=-0.02197627346568439\n",
      "Gradient Descent(395/399): loss=0.40433349244397376, w0=0.0003722995987902063, w1=-0.02200883608862404\n",
      "Gradient Descent(396/399): loss=0.40421944904441487, w0=0.00037222929567206606, w1=-0.02204133545477183\n",
      "Gradient Descent(397/399): loss=0.4041056302397832, w0=0.0003721527683862875, w1=-0.02207377179492783\n",
      "Gradient Descent(398/399): loss=0.40399203495156705, w0=0.0003720700351037795, w1=-0.022106145338840994\n",
      "Gradient Descent(399/399): loss=0.4038786621092719, w0=0.0003719811140405823, w1=-0.02213845631521451\n",
      "++++ gamma = 0.000719685673001\n",
      "ciaociaociao\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/399): loss=0.4999999999999887, w0=-4.956642416006324e-07, w1=-0.00025801623469334025\n",
      "Gradient Descent(1/399): loss=0.4984074787246081, w0=-7.049990571385746e-07, w1=-0.0005134688741594068\n",
      "Gradient Descent(2/399): loss=0.49684616036082363, w0=-6.390997723981293e-07, w1=-0.0007663934648813193\n",
      "Gradient Descent(3/399): loss=0.4953152081988466, w0=-3.087418877945349e-07, w1=-0.0010168249711848443\n",
      "Gradient Descent(4/399): loss=0.49381381130984725, w0=2.7561029174925095e-07, w1=-0.0012647977862361211\n",
      "Gradient Descent(5/399): loss=0.4923411836481514, w0=1.1037952591443092e-06, w1=-0.0015103457428028025\n",
      "Gradient Descent(6/399): loss=0.4908965631888764, w0=2.165946133374344e-06, w1=-0.0017535021237842648\n",
      "Gradient Descent(7/399): loss=0.4894792110994442, w0=3.452482707482458e-06, w1=-0.0019942996725163743\n",
      "Gradient Descent(8/399): loss=0.4880884109434803, w0=4.954103710327425e-06, w1=-0.002232770602856182\n",
      "Gradient Descent(9/399): loss=0.48672346791567506, w0=6.66177927629495e-06, w1=-0.0024689466090517457\n",
      "Gradient Descent(10/399): loss=0.48538370810625237, w0=8.566743617309667e-06, w1=-0.0027028588754021976\n",
      "Gradient Descent(11/399): loss=0.4840684777937607, w0=1.0660487891648776e-05, w1=-0.0029345380857129823\n",
      "Gradient Descent(12/399): loss=0.4827771427649551, w0=1.2934753264210093e-05, w1=-0.003164014432551121\n",
      "Gradient Descent(13/399): loss=0.48150908766060396, w0=1.5381524153033538e-05, w1=-0.003391317626305211\n",
      "Gradient Descent(14/399): loss=0.48026371534610635, w0=1.7993021657018747e-05, w1=-0.0036164769040547243\n",
      "Gradient Descent(15/399): loss=0.47904044630585907, w0=2.07616971599203e-05, w1=-0.003839521038253121\n",
      "Gradient Descent(16/399): loss=0.4778387180603635, w0=2.368022610583681e-05, w1=-0.004060478345229097\n",
      "Gradient Descent(17/399): loss=0.4766579846051081, w0=2.6741501941542126e-05, w1=-0.004279376693510246\n",
      "Gradient Descent(18/399): loss=0.47549771587030876, w0=2.9938630221134414e-05, w1=-0.004496243511973263\n",
      "Gradient Descent(19/399): loss=0.4743573972006335, w0=3.326492286860288e-05, w1=-0.004711105797824727\n",
      "Gradient Descent(20/399): loss=0.4732365288540733, w0=3.671389259403303e-05, w1=-0.004923990124416416\n",
      "Gradient Descent(21/399): loss=0.4721346255191654, w0=4.0279247459288756e-05, w1=-0.005134922648898971\n",
      "Gradient Descent(22/399): loss=0.47105121584981224, w0=4.3954885589123396e-05, w1=-0.005343929119717663\n",
      "Gradient Descent(23/399): loss=0.4699858420169658, w0=4.773489002378331e-05, w1=-0.005551034883953907\n",
      "Gradient Descent(24/399): loss=0.46893805927649146, w0=5.161352370927547e-05, w1=-0.005756264894516086\n",
      "Gradient Descent(25/399): loss=0.4679074355525492, w0=5.558522462157504e-05, w1=-0.005959643717183133\n",
      "Gradient Descent(26/399): loss=0.46689355103586355, w0=5.9644601021151684e-05, w1=-0.0061611955375042785\n",
      "Gradient Descent(27/399): loss=0.46589599779628044, w0=6.378642683429206e-05, w1=-0.006360944167558237\n",
      "Gradient Descent(28/399): loss=0.4649143794090368, w0=6.800563715779247e-05, w1=-0.006558913052575057\n",
      "Gradient Descent(29/399): loss=0.4639483105941953, w0=7.229732388368985e-05, w1=-0.0067551252774237696\n",
      "Gradient Descent(30/399): loss=0.46299741686872076, w0=7.665673144079013e-05, w1=-0.006949603572968886\n",
      "Gradient Descent(31/399): loss=0.46206133421069584, w0=8.10792526498417e-05, w1=-0.00714237032229873\n",
      "Gradient Descent(32/399): loss=0.46113970873519994, w0=8.556042468928831e-05, w1=-0.007333447566828505\n",
      "Gradient Descent(33/399): loss=0.46023219638139223, w0=9.009592516861913e-05, w1=-0.0075228570122809445\n",
      "Gradient Descent(34/399): loss=0.4593384626103645, w0=9.468156830641574e-05, w1=-0.007710620034547288\n",
      "Gradient Descent(35/399): loss=0.45845818211334194, w0=9.931330121027454e-05, w1=-0.007896757685431306\n",
      "Gradient Descent(36/399): loss=0.45759103852983585, w0=0.00010398720025586084, w1=-0.008081290698278976\n",
      "Gradient Descent(37/399): loss=0.4567367241753636, w0=0.00010869946756242518, w1=-0.008264239493496396\n",
      "Gradient Descent(38/399): loss=0.45589493977837087, w0=0.0001134464275621859, w1=-0.008445624183958434\n",
      "Gradient Descent(39/399): loss=0.4550653942260064, w0=0.00011822452366105264, w1=-0.008625464580310544\n",
      "Gradient Descent(40/399): loss=0.45424780431841216, w0=0.0001230303149882341, w1=-0.008803780196166134\n",
      "Gradient Descent(41/399): loss=0.4534418945312106, w0=0.00012786047323234105, w1=-0.008980590253201824\n",
      "Gradient Descent(42/399): loss=0.45264739678588123, w0=0.00013271177956166022, w1=-0.009155913686152838\n",
      "Gradient Descent(43/399): loss=0.4518640502277289, w0=0.00013758112162633832, w1=-0.009329769147710752\n",
      "Gradient Descent(44/399): loss=0.45109160101116785, w0=0.000142465490640277, w1=-0.009502175013325754\n",
      "Gradient Descent(45/399): loss=0.4503298020920457, w0=0.00014736197854059958, w1=-0.009673149385915528\n",
      "Gradient Descent(46/399): loss=0.4495784130267522, w0=0.0001522677752226084, w1=-0.0098427101004828\n",
      "Gradient Descent(47/399): loss=0.44883719977786185, w0=0.00015718016584820893, w1=-0.010010874728643562\n",
      "Gradient Descent(48/399): loss=0.4481059345260755, w0=0.0001620965282258311, w1=-0.010177660583067924\n",
      "Gradient Descent(49/399): loss=0.4473843954882303, w0=0.00016701433025993238, w1=-0.010343084721835512\n",
      "Gradient Descent(50/399): loss=0.44667236674116007, w0=0.0001719311274682192, w1=-0.010507163952707264\n",
      "Gradient Descent(51/399): loss=0.4459696380511957, w0=0.00017684456056477402, w1=-0.010669914837315437\n",
      "Gradient Descent(52/399): loss=0.4452760047091046, w0=0.00018175235310732444, w1=-0.010831353695273622\n",
      "Gradient Descent(53/399): loss=0.4445912673702766, w0=0.00018665230920693882, w1=-0.010991496608208468\n",
      "Gradient Descent(54/399): loss=0.44391523189996873, w0=0.00019154231129847965, w1=-0.011150359423714831\n",
      "Gradient Descent(55/399): loss=0.44324770922343204, w0=0.00019642031797019062, w1=-0.011307957759235985\n",
      "Gradient Descent(56/399): loss=0.4425885151807495, w0=0.00020128436185083844, w1=-0.011464307005870507\n",
      "Gradient Descent(57/399): loss=0.44193747038621883, w0=0.00020613254755287236, w1=-0.011619422332107403\n",
      "Gradient Descent(58/399): loss=0.4412944000921255, w0=0.00021096304967010643, w1=-0.011773318687491026\n",
      "Gradient Descent(59/399): loss=0.4406591340567516, w0=0.00021577411082847022, w1=-0.011926010806217259\n",
      "Gradient Descent(60/399): loss=0.44003150641647404, w0=0.00022056403978841304, w1=-0.012077513210662458\n",
      "Gradient Descent(61/399): loss=0.4394113555618155, w0=0.00022533120959758473, w1=-0.012227840214846547\n",
      "Gradient Descent(62/399): loss=0.4387985240173086, w0=0.0002300740557924539, w1=-0.012377005927831693\n",
      "Gradient Descent(63/399): loss=0.43819285832504984, w0=0.0002347910746475604, w1=-0.012525024257057897\n",
      "Gradient Descent(64/399): loss=0.4375942089318096, w0=0.0002394808214711345, w1=-0.01267190891161684\n",
      "Gradient Descent(65/399): loss=0.4370024300795895, w0=0.0002441419089458485, w1=-0.0128176734054653\n",
      "Gradient Descent(66/399): loss=0.4364173796995001, w0=0.00024877300551350175, w1=-0.012962331060579367\n",
      "Gradient Descent(67/399): loss=0.4358389193088585, w0=0.0002533728338024702, w1=-0.013105895010050737\n",
      "Gradient Descent(68/399): loss=0.43526691391139016, w0=0.00025794016909678514, w1=-0.013248378201126272\n",
      "Gradient Descent(69/399): loss=0.4347012319004371, w0=0.00026247383784573544, w1=-0.013389793398192008\n",
      "Gradient Descent(70/399): loss=0.4341417449650731, w0=0.0002669727162129175, w1=-0.01353015318570277\n",
      "Gradient Descent(71/399): loss=0.4335883279990267, w0=0.0002714357286636866, w1=-0.013669469971058527\n",
      "Gradient Descent(72/399): loss=0.4330408590123255, w0=0.000275861846589991, w1=-0.013807755987428567\n",
      "Gradient Descent(73/399): loss=0.43249921904556754, w0=0.0002802500869715982, w1=-0.013945023296524596\n",
      "Gradient Descent(74/399): loss=0.43196329208673995, w0=0.0002845995110727494, w1=-0.014081283791323784\n",
      "Gradient Descent(75/399): loss=0.43143296499049527, w0=0.0002889092231733028, w1=-0.014216549198742812\n",
      "Gradient Descent(76/399): loss=0.4309081273998162, w0=0.0002931783693334548, w1=-0.014350831082263893\n",
      "Gradient Descent(77/399): loss=0.43038867166998135, w0=0.0002974061361911487, w1=-0.014484140844513781\n",
      "Gradient Descent(78/399): loss=0.4298744927947665, w0=0.0003015917497913081, w1=-0.014616489729796688\n",
      "Gradient Descent(79/399): loss=0.42936548833480603, w0=0.0003057344744460527, w1=-0.014747888826582085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(80/399): loss=0.4288615583480468, w0=0.0003098336116250784, w1=-0.014878349069948262\n",
      "Gradient Descent(81/399): loss=0.4283626053222288, w0=0.0003138884988754055, w1=-0.015007881243982571\n",
      "Gradient Descent(82/399): loss=0.4278685341093285, w0=0.0003178985087697189, w1=-0.015136495984139216\n",
      "Gradient Descent(83/399): loss=0.4273792518619014, w0=0.00032186304788254686, w1=-0.015264203779555431\n",
      "Gradient Descent(84/399): loss=0.4268946679712682, w0=0.0003257815557935437, w1=-0.0153910149753269\n",
      "Gradient Descent(85/399): loss=0.42641469400748266, w0=0.0003296535041171622, w1=-0.015516939774743225\n",
      "Gradient Descent(86/399): loss=0.425939243661029, w0=0.00033347839555802027, w1=-0.01564198824148423\n",
      "Gradient Descent(87/399): loss=0.42546823268619255, w0=0.00033725576299128567, w1=-0.01576617030177791\n",
      "Gradient Descent(88/399): loss=0.42500157884605505, w0=0.0003409851685674196, w1=-0.015889495746520748\n",
      "Gradient Descent(89/399): loss=0.42453920185906047, w0=0.00034466620284063907, w1=-0.016011974233361172\n",
      "Gradient Descent(90/399): loss=0.4240810233471081, w0=0.00034829848392047366, w1=-0.01613361528874687\n",
      "Gradient Descent(91/399): loss=0.4236269667851207, w0=0.00035188165664581006, w1=-0.01625442830993669\n",
      "Gradient Descent(92/399): loss=0.4231769574520465, w0=0.0003554153917808335, w1=-0.016374422566977773\n",
      "Gradient Descent(93/399): loss=0.4227309223832513, w0=0.00035889938523229065, w1=-0.016493607204648694\n",
      "Gradient Descent(94/399): loss=0.4222887903242544, w0=0.0003623333572875152, w1=-0.016611991244369156\n",
      "Gradient Descent(95/399): loss=0.4218504916857746, w0=0.0003657170518726703, w1=-0.016729583586076992\n",
      "Gradient Descent(96/399): loss=0.4214159585000387, w0=0.0003690502358306787, w1=-0.016846393010073048\n",
      "Gradient Descent(97/399): loss=0.4209851243783212, w0=0.0003723326982183238, w1=-0.016962428178834605\n",
      "Gradient Descent(98/399): loss=0.4205579244696751, w0=0.0003755642496220195, w1=-0.017077697638797934\n",
      "Gradient Descent(99/399): loss=0.42013429542081815, w0=0.0003787447214917607, w1=-0.01719220982211057\n",
      "Gradient Descent(100/399): loss=0.41971417533714017, w0=0.0003818739654927768, w1=-0.017305973048353917\n",
      "Gradient Descent(101/399): loss=0.4192975037447991, w0=0.00038495185287442703, w1=-0.017418995526236717\n",
      "Gradient Descent(102/399): loss=0.4188842215538721, w0=0.0003879782738558853, w1=-0.01753128535525999\n",
      "Gradient Descent(103/399): loss=0.4184742710225318, w0=0.00039095313702817617, w1=-0.017642850527353916\n",
      "Gradient Descent(104/399): loss=0.41806759572221547, w0=0.00039387636877213445, w1=-0.01775369892848729\n",
      "Gradient Descent(105/399): loss=0.4176641405037598, w0=0.00039674791269187306, w1=-0.01786383834024998\n",
      "Gradient Descent(106/399): loss=0.4172638514644713, w0=0.00039956772906335345, w1=-0.017973276441408965\n",
      "Gradient Descent(107/399): loss=0.4168666759161067, w0=0.0004023357942976653, w1=-0.018082020809438406\n",
      "Gradient Descent(108/399): loss=0.41647256235373387, w0=0.000405052100418631, w1=-0.018190078922024285\n",
      "Gradient Descent(109/399): loss=0.416081460425451, w0=0.0004077166545543623, w1=-0.01829745815854405\n",
      "Gradient Descent(110/399): loss=0.4156933209029362, w0=0.0004103294784424045, w1=-0.018404165801521752\n",
      "Gradient Descent(111/399): loss=0.415308095652804, w0=0.00041289060794811515, w1=-0.01851020903805916\n",
      "Gradient Descent(112/399): loss=0.4149257376087472, w0=0.00041540009259593213, w1=-0.018615594961243234\n",
      "Gradient Descent(113/399): loss=0.4145462007444378, w0=0.00041785799511319534, w1=-0.01872033057153047\n",
      "Gradient Descent(114/399): loss=0.41416944004717, w0=0.00042026439098619606, w1=-0.0188244227781085\n",
      "Gradient Descent(115/399): loss=0.4137954114922165, w0=0.00042261936802813526, w1=-0.018927878400235403\n",
      "Gradient Descent(116/399): loss=0.41342407201788745, w0=0.00042492302595868183, w1=-0.019030704168557084\n",
      "Gradient Descent(117/399): loss=0.41305537950126203, w0=0.00042717547599482875, w1=-0.019132906726403203\n",
      "Gradient Descent(118/399): loss=0.4126892927345796, w0=0.00042937684045275397, w1=-0.019234492631061972\n",
      "Gradient Descent(119/399): loss=0.4123257714022699, w0=0.0004315272523604002, w1=-0.019335468355034274\n",
      "Gradient Descent(120/399): loss=0.41196477605860277, w0=0.0004336268550804954, w1=-0.019435840287267428\n",
      "Gradient Descent(121/399): loss=0.4116062681059425, w0=0.0004356758019437426, w1=-0.019535614734368996\n",
      "Gradient Descent(122/399): loss=0.4112502097735854, w0=0.0004376742558919162, w1=-0.019634797921801013\n",
      "Gradient Descent(123/399): loss=0.41089656409716935, w0=0.0004396223891306068, w1=-0.01973339599505496\n",
      "Gradient Descent(124/399): loss=0.4105452948986343, w0=0.000441520382791365, w1=-0.019831415020807854\n",
      "Gradient Descent(125/399): loss=0.41019636676672033, w0=0.000443368426603001, w1=-0.01992886098805979\n",
      "Gradient Descent(126/399): loss=0.4098497450379889, w0=0.00044516671857180166, w1=-0.02002573980925326\n",
      "Gradient Descent(127/399): loss=0.4095053957783498, w0=0.0004469154646704354, w1=-0.020122057321374592\n",
      "Gradient Descent(128/399): loss=0.40916328576508343, w0=0.00044861487853531927, w1=-0.02021781928703783\n",
      "Gradient Descent(129/399): loss=0.4088233824693399, w0=0.0004502651811722292, w1=-0.02031303139555134\n",
      "Gradient Descent(130/399): loss=0.4084856540391051, w0=0.0004518666006699408, w1=-0.020407699263967502\n",
      "Gradient Descent(131/399): loss=0.40815006928262004, w0=0.0004534193719216919, w1=-0.020501828438115734\n",
      "Gradient Descent(132/399): loss=0.40781659765223943, w0=0.00045492373635426595, w1=-0.02059542439361919\n",
      "Gradient Descent(133/399): loss=0.4074852092287173, w0=0.00045637994166449825, w1=-0.0206884925368954\n",
      "Gradient Descent(134/399): loss=0.40715587470591036, w0=0.0004577882415630134, w1=-0.020781038206141116\n",
      "Gradient Descent(135/399): loss=0.406828565375884, w0=0.00045914889552500765, w1=-0.020873066672301703\n",
      "Gradient Descent(136/399): loss=0.40650325311441127, w0=0.00046046216854789366, w1=-0.020964583140025275\n",
      "Gradient Descent(137/399): loss=0.4061799103668555, w0=0.00046172833091563046, w1=-0.021055592748601897\n",
      "Gradient Descent(138/399): loss=0.4058585101344223, w0=0.0004629476579695667, w1=-0.021146100572888096\n",
      "Gradient Descent(139/399): loss=0.4055390259607742, w0=0.000464120429885628, w1=-0.021236111624216913\n",
      "Gradient Descent(140/399): loss=0.4052214319189955, w0=0.0004652469314576858, w1=-0.02132563085129382\n",
      "Gradient Descent(141/399): loss=0.4049057025988999, w0=0.00046632745188694724, w1=-0.021414663141078642\n",
      "Gradient Descent(142/399): loss=0.4045918130946686, w0=0.00046736228457721187, w1=-0.02150321331965385\n",
      "Gradient Descent(143/399): loss=0.4042797389928122, w0=0.00046835172693584246, w1=-0.02159128615307933\n",
      "Gradient Descent(144/399): loss=0.4039694563604469, w0=0.000469296080180304, w1=-0.02167888634823399\n",
      "Gradient Descent(145/399): loss=0.4036609417338754, w0=0.00047019564915012625, w1=-0.021766018553644323\n",
      "Gradient Descent(146/399): loss=0.40335417210746527, w0=0.00047105074212415096, w1=-0.021852687360300223\n",
      "Gradient Descent(147/399): loss=0.4030491249228155, w0=0.00047186167064292635, w1=-0.021938897302458245\n",
      "Gradient Descent(148/399): loss=0.4027457780582057, w0=0.00047262874933611757, w1=-0.022024652858432515\n",
      "Gradient Descent(149/399): loss=0.40244410981831646, w0=0.0004733522957548028, w1=-0.022109958451373524\n",
      "Gradient Descent(150/399): loss=0.4021440989242171, w0=0.00047403263020852956, w1=-0.022194818450035\n",
      "Gradient Descent(151/399): loss=0.40184572450361156, w0=0.0004746700756070088, w1=-0.02227923716952905\n",
      "Gradient Descent(152/399): loss=0.4015489660813352, w0=0.0004752649573063264, w1=-0.022363218872069806\n",
      "Gradient Descent(153/399): loss=0.40125380357009777, w0=0.00047581760295955714, w1=-0.022446767767705714\n",
      "Gradient Descent(154/399): loss=0.400960217261463, w0=0.00047632834237166594, w1=-0.02252988801504072\n",
      "Gradient Descent(155/399): loss=0.4006681878170603, w0=0.0004767975073585876, w1=-0.022612583721944492\n",
      "Gradient Descent(156/399): loss=0.4003776962600222, w0=0.00047722543161037636, w1=-0.022694858946251895\n",
      "Gradient Descent(157/399): loss=0.40008872396664, w0=0.0004776124505583206, w1=-0.022776717696451867\n",
      "Gradient Descent(158/399): loss=0.39980125265823496, w0=0.0004779589012459208, w1=-0.02285816393236591\n",
      "Gradient Descent(159/399): loss=0.39951526439323315, w0=0.00047826512220363086, w1=-0.02293920156581634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(160/399): loss=0.39923074155944704, w0=0.0004785314533272664, w1=-0.023019834461284482\n",
      "Gradient Descent(161/399): loss=0.39894766686654914, w0=0.0004787582357599845, w1=-0.023100066436558972\n",
      "Gradient Descent(162/399): loss=0.39866602333874, w0=0.00047894581177774397, w1=-0.023179901263374336\n",
      "Gradient Descent(163/399): loss=0.3983857943076006, w0=0.00047909452467815574, w1=-0.02325934266803999\n",
      "Gradient Descent(164/399): loss=0.3981069634051272, w0=0.000479204718672636, w1=-0.023338394332059854\n",
      "Gradient Descent(165/399): loss=0.3978295145569398, w0=0.00047927673878177723, w1=-0.023417059892742686\n",
      "Gradient Descent(166/399): loss=0.39755343197566684, w0=0.00047931093073385396, w1=-0.02349534294380334\n",
      "Gradient Descent(167/399): loss=0.39727870015449074, w0=0.00047930764086638226, w1=-0.023573247035955064\n",
      "Gradient Descent(168/399): loss=0.39700530386086097, w0=0.0004792672160306543, w1=-0.02365077567749298\n",
      "Gradient Descent(169/399): loss=0.39673322813036316, w0=0.0004791900034991714, w1=-0.02372793233486893\n",
      "Gradient Descent(170/399): loss=0.3964624582607418, w0=0.0004790763508759, w1=-0.02380472043325779\n",
      "Gradient Descent(171/399): loss=0.39619297980607304, w0=0.00047892660600927885, w1=-0.023881143357115383\n",
      "Gradient Descent(172/399): loss=0.39592477857108466, w0=0.00047874111690790494, w1=-0.023957204450728204\n",
      "Gradient Descent(173/399): loss=0.3956578406056157, w0=0.0004785202316588301, w1=-0.024032907018754976\n",
      "Gradient Descent(174/399): loss=0.39539215219921553, w0=0.0004782642983484001, w1=-0.024108254326760264\n",
      "Gradient Descent(175/399): loss=0.39512769987587826, w0=0.00047797366498557065, w1=-0.02418324960174022\n",
      "Gradient Descent(176/399): loss=0.3948644703889061, w0=0.00047764867942763644, w1=-0.024257896032640627\n",
      "Gradient Descent(177/399): loss=0.39460245071590166, w0=0.00047728968930831035, w1=-0.024332196770867322\n",
      "Gradient Descent(178/399): loss=0.394341628053884, w0=0.0004768970419680922, w1=-0.024406154930789157\n",
      "Gradient Descent(179/399): loss=0.39408198981452564, w0=0.0004764710843868676, w1=-0.024479773590233605\n",
      "Gradient Descent(180/399): loss=0.3938235236195068, w0=0.00047601216311867926, w1=-0.024553055790975113\n",
      "Gradient Descent(181/399): loss=0.3935662172959843, w0=0.00047552062422861395, w1=-0.02462600453921635\n",
      "Gradient Descent(182/399): loss=0.3933100588721731, w0=0.00047499681323175086, w1=-0.024698622806062435\n",
      "Gradient Descent(183/399): loss=0.3930550365730343, w0=0.00047444107503411703, w1=-0.024770913527988284\n",
      "Gradient Descent(184/399): loss=0.39280113881607087, w0=0.00047385375387559833, w1=-0.024842879607299136\n",
      "Gradient Descent(185/399): loss=0.39254835420722545, w0=0.00047323519327475467, w1=-0.024914523912584436\n",
      "Gradient Descent(186/399): loss=0.3922966715368781, w0=0.00047258573597549005, w1=-0.024985849279165123\n",
      "Gradient Descent(187/399): loss=0.3920460797759431, w0=0.00047190572389552913, w1=-0.025056858509534447\n",
      "Gradient Descent(188/399): loss=0.39179656807205976, w0=0.0004711954980766532, w1=-0.02512755437379244\n",
      "Gradient Descent(189/399): loss=0.39154812574587494, w0=0.00047045539863664945, w1=-0.025197939610074094\n",
      "Gradient Descent(190/399): loss=0.39130074228741807, w0=0.000469685764722929, w1=-0.025268016924971368\n",
      "Gradient Descent(191/399): loss=0.39105440735256214, w0=0.0004688869344677699, w1=-0.02533778899394915\n",
      "Gradient Descent(192/399): loss=0.39080911075957214, w0=0.0004680592449451429, w1=-0.025407258461755216\n",
      "Gradient Descent(193/399): loss=0.39056484248573364, w0=0.00046720303212907785, w1=-0.025476427942824325\n",
      "Gradient Descent(194/399): loss=0.39032159266406635, w0=0.000466318630853531, w1=-0.025545300021676497\n",
      "Gradient Descent(195/399): loss=0.3900793515801138, w0=0.0004654063747737135, w1=-0.025613877253309617\n",
      "Gradient Descent(196/399): loss=0.3898381096688118, w0=0.0004644665963288424, w1=-0.025682162163586403\n",
      "Gradient Descent(197/399): loss=0.38959785751143167, w0=0.0004634996267062775, w1=-0.02575015724961587\n",
      "Gradient Descent(198/399): loss=0.3893585858325948, w0=0.00046250579580700655, w1=-0.02581786498012933\n",
      "Gradient Descent(199/399): loss=0.38912028549736144, w0=0.000461485432212444, w1=-0.025885287795851065\n",
      "Gradient Descent(200/399): loss=0.3888829475083866, w0=0.00046043886315250815, w1=-0.02595242810986369\n",
      "Gradient Descent(201/399): loss=0.3886465630031444, w0=0.00045936641447494325, w1=-0.02601928830796837\n",
      "Gradient Descent(202/399): loss=0.38841112325121674, w0=0.00045826841061585296, w1=-0.02608587074903988\n",
      "Gradient Descent(203/399): loss=0.38817661965164973, w0=0.00045714517457141375, w1=-0.02615217776537666\n",
      "Gradient Descent(204/399): loss=0.38794304373036836, w0=0.0004559970278707364, w1=-0.02621821166304592\n",
      "Gradient Descent(205/399): loss=0.38771038713765504, w0=0.0004548242905498452, w1=-0.026283974722223824\n",
      "Gradient Descent(206/399): loss=0.38747864164568624, w0=0.0004536272811267449, w1=-0.02634946919753093\n",
      "Gradient Descent(207/399): loss=0.3872477991461267, w0=0.0004524063165775468, w1=-0.02641469731836284\n",
      "Gradient Descent(208/399): loss=0.38701785164778113, w0=0.00045116171231362524, w1=-0.026479661289216232\n",
      "Gradient Descent(209/399): loss=0.3867887912742998, w0=0.00044989378215977655, w1=-0.026544363290010285\n",
      "Gradient Descent(210/399): loss=0.3865606102619385, w0=0.0004486028383333546, w1=-0.02660880547640358\n",
      "Gradient Descent(211/399): loss=0.3863333009573685, w0=0.0004472891914243558, w1=-0.026672989980106573\n",
      "Gradient Descent(212/399): loss=0.3861068558155414, w0=0.0004459531503764282, w1=-0.026736918909189666\n",
      "Gradient Descent(213/399): loss=0.3858812673975989, w0=0.00044459502246877944, w1=-0.026800594348386962\n",
      "Gradient Descent(214/399): loss=0.38565652836883413, w0=0.00044321511329896017, w1=-0.026864018359395787\n",
      "Gradient Descent(215/399): loss=0.3854326314966987, w0=0.0004418137267664981, w1=-0.026927192981172007\n",
      "Gradient Descent(216/399): loss=0.38520956964885555, w0=0.00044039116505736, w1=-0.026990120230221227\n",
      "Gradient Descent(217/399): loss=0.3849873357912771, w0=0.0004389477286292193, w1=-0.027052802100885932\n",
      "Gradient Descent(218/399): loss=0.3847659229863859, w0=0.0004374837161975069, w1=-0.027115240565628626\n",
      "Gradient Descent(219/399): loss=0.38454532439124023, w0=0.000435999424722224, w1=-0.02717743757531103\n",
      "Gradient Descent(220/399): loss=0.3843255332557569, w0=0.00043449514939549594, w1=-0.027239395059469398\n",
      "Gradient Descent(221/399): loss=0.38410654292097846, w0=0.00043297118362984683, w1=-0.027301114926586007\n",
      "Gradient Descent(222/399): loss=0.38388834681737816, w0=0.00043142781904717455, w1=-0.027362599064356893\n",
      "Gradient Descent(223/399): loss=0.3836709384632025, w0=0.00042986534546840744, w1=-0.02742384933995586\n",
      "Gradient Descent(224/399): loss=0.3834543114628528, w0=0.0004282840509038234, w1=-0.02748486760029484\n",
      "Gradient Descent(225/399): loss=0.38323845950530155, w0=0.0004266842215440128, w1=-0.027545655672280674\n",
      "Gradient Descent(226/399): loss=0.3830233763625458, w0=0.0004250661417514677, w1=-0.02760621536306831\n",
      "Gradient Descent(227/399): loss=0.3828090558880945, w0=0.00042343009405277927, w1=-0.027666548460310535\n",
      "Gradient Descent(228/399): loss=0.38259549201548954, w0=0.00042177635913142686, w1=-0.02772665673240425\n",
      "Gradient Descent(229/399): loss=0.38238267875685994, w0=0.0004201052158211418, w1=-0.02778654192873336\n",
      "Gradient Descent(230/399): loss=0.38217061020150833, w0=0.0004184169410998295, w1=-0.027846205779908324\n",
      "Gradient Descent(231/399): loss=0.381959280514529, w0=0.0004167118100840342, w1=-0.027905649998002414\n",
      "Gradient Descent(232/399): loss=0.38174868393545536, w0=0.0004149900960239312, w1=-0.02796487627678472\n",
      "Gradient Descent(233/399): loss=0.3815388147769385, w0=0.00041325207029883096, w1=-0.028023886291949972\n",
      "Gradient Descent(234/399): loss=0.3813296674234543, w0=0.0004114980024131804, w1=-0.028082681701345213\n",
      "Gradient Descent(235/399): loss=0.38112123633004, w0=0.00040972815999304777, w1=-0.02814126414519336\n",
      "Gradient Descent(236/399): loss=0.38091351602105505, w0=0.0004079428087830762, w1=-0.028199635246313713\n",
      "Gradient Descent(237/399): loss=0.38070650108897364, w0=0.0004061422126438932, w1=-0.028257796610339465\n",
      "Gradient Descent(238/399): loss=0.38050018619320025, w0=0.0004043266335499615, w1=-0.028315749825932216\n",
      "Gradient Descent(239/399): loss=0.38029456605891165, w0=0.0004024963315878604, w1=-0.028373496464993594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(240/399): loss=0.38008963547592445, w0=0.0004006515649549823, w1=-0.028431038082873974\n",
      "Gradient Descent(241/399): loss=0.37988538929758736, w0=0.0003987925899586348, w1=-0.02848837621857836\n",
      "Gradient Descent(242/399): loss=0.37968182243969567, w0=0.00039691966101553416, w1=-0.028545512394969476\n",
      "Gradient Descent(243/399): loss=0.37947892987943066, w0=0.00039503303065167985, w1=-0.028602448118968095\n",
      "Gradient Descent(244/399): loss=0.37927670665432167, w0=0.00039313294950259783, w1=-0.028659184881750645\n",
      "Gradient Descent(245/399): loss=0.3790751478612283, w0=0.00039121966631394233, w1=-0.02871572415894415\n",
      "Gradient Descent(246/399): loss=0.37887424865534736, w0=0.0003892934279424443, w1=-0.028772067410818523\n",
      "Gradient Descent(247/399): loss=0.3786740042492381, w0=0.0003873544793571968, w1=-0.028828216082476252\n",
      "Gradient Descent(248/399): loss=0.3784744099118694, w0=0.0003854030636412662, w1=-0.028884171604039537\n",
      "Gradient Descent(249/399): loss=0.3782754609676875, w0=0.0003834394219936199, w1=-0.028939935390834892\n",
      "Gradient Descent(250/399): loss=0.3780771527957011, w0=0.0003814637937313605, w1=-0.028995508843575254\n",
      "Gradient Descent(251/399): loss=0.37787948082858924, w0=0.0003794764162922562, w1=-0.02905089334853965\n",
      "Gradient Descent(252/399): loss=0.37768244055182376, w0=0.00037747752523755937, w1=-0.029106090277750428\n",
      "Gradient Descent(253/399): loss=0.37748602750281346, w0=0.0003754673542551028, w1=-0.029161100989148123\n",
      "Gradient Descent(254/399): loss=0.37729023727006306, w0=0.00037344613516266593, w1=-0.02921592682676396\n",
      "Gradient Descent(255/399): loss=0.3770950654923525, w0=0.0003714140979116017, w1=-0.029270569120890037\n",
      "Gradient Descent(256/399): loss=0.3769005078579311, w0=0.000369371470590716, w1=-0.02932502918824725\n",
      "Gradient Descent(257/399): loss=0.3767065601037284, w0=0.00036731847943039084, w1=-0.029379308332150935\n",
      "Gradient Descent(258/399): loss=0.3765132180145824, w0=0.00036525534880694447, w1=-0.029433407842674317\n",
      "Gradient Descent(259/399): loss=0.37632047742248215, w0=0.0003631823012472191, w1=-0.029487328996809763\n",
      "Gradient Descent(260/399): loss=0.37612833420582725, w0=0.0003610995574333899, w1=-0.029541073058627878\n",
      "Gradient Descent(261/399): loss=0.3759367842887003, w0=0.0003590073362079871, w1=-0.029594641279434484\n",
      "Gradient Descent(262/399): loss=0.3757458236401564, w0=0.00035690585457912423, w1=-0.029648034897925495\n",
      "Gradient Descent(263/399): loss=0.37555544827352544, w0=0.00035479532772592537, w1=-0.029701255140339736\n",
      "Gradient Descent(264/399): loss=0.37536565424572854, w0=0.0003526759690041446, w1=-0.029754303220609728\n",
      "Gradient Descent(265/399): loss=0.3751764376566082, w0=0.000350547989951971, w1=-0.02980718034051046\n",
      "Gradient Descent(266/399): loss=0.3749877946482735, w0=0.0003484116002960123, w1=-0.029859887689806183\n",
      "Gradient Descent(267/399): loss=0.37479972140445483, w0=0.0003462670079574516, w1=-0.02991242644639527\n",
      "Gradient Descent(268/399): loss=0.3746122141498749, w0=0.0003441144190583696, w1=-0.029964797776453136\n",
      "Gradient Descent(269/399): loss=0.37442526914963015, w0=0.0003419540379282282, w1=-0.030017002834573266\n",
      "Gradient Descent(270/399): loss=0.37423888270858474, w0=0.00033978606711050754, w1=-0.030069042763906383\n",
      "Gradient Descent(271/399): loss=0.37405305117077625, w0=0.0003376107073694923, w1=-0.030120918696297773\n",
      "Gradient Descent(272/399): loss=0.37386777091883433, w0=0.00033542815769720087, w1=-0.030172631752422793\n",
      "Gradient Descent(273/399): loss=0.3736830383734089, w0=0.000333238615320452, w1=-0.03022418304192058\n",
      "Gradient Descent(274/399): loss=0.3734988499926094, w0=0.000331042275708064, w1=-0.03027557366352601\n",
      "Gradient Descent(275/399): loss=0.3733152022714571, w0=0.00032883933257818075, w1=-0.030326804705199904\n",
      "Gradient Descent(276/399): loss=0.373132091741345, w0=0.00032662997790572, w1=-0.030377877244257535\n",
      "Gradient Descent(277/399): loss=0.3729495149695113, w0=0.00032441440192993863, w1=-0.030428792347495424\n",
      "Gradient Descent(278/399): loss=0.3727674685585186, w0=0.0003221927931621104, w1=-0.030479551071316485\n",
      "Gradient Descent(279/399): loss=0.3725859491457483, w0=0.0003199653383933111, w1=-0.030530154461853507\n",
      "Gradient Descent(280/399): loss=0.37240495340289975, w0=0.0003177322227023073, w1=-0.030580603555091037\n",
      "Gradient Descent(281/399): loss=0.37222447803550246, w0=0.0003154936294635431, w1=-0.030630899376985647\n",
      "Gradient Descent(282/399): loss=0.37204451978243525, w0=0.000313249740355222, w1=-0.030681042943584636\n",
      "Gradient Descent(283/399): loss=0.37186507541545455, w0=0.0003110007353674781, w1=-0.030731035261143163\n",
      "Gradient Descent(284/399): loss=0.37168614173873393, w0=0.00030874679281063424, w1=-0.03078087732623987\n",
      "Gradient Descent(285/399): loss=0.3715077155884088, w0=0.00030648808932354145, w1=-0.03083057012589097\n",
      "Gradient Descent(286/399): loss=0.37132979383213116, w0=0.00030422479988199703, w1=-0.030880114637662857\n",
      "Gradient Descent(287/399): loss=0.3711523733686343, w0=0.0003019570978072368, w1=-0.03092951182978325\n",
      "Gradient Descent(288/399): loss=0.3709754511273014, w0=0.00029968515477449823, w1=-0.030978762661250877\n",
      "Gradient Descent(289/399): loss=0.3707990240677461, w0=0.00029740914082165045, w1=-0.031027868081943747\n",
      "Gradient Descent(290/399): loss=0.37062308917939835, w0=0.00029512922435788816, w1=-0.031076829032725985\n",
      "Gradient Descent(291/399): loss=0.37044764348109793, w0=0.00029284557217248557, w1=-0.031125646445553302\n",
      "Gradient Descent(292/399): loss=0.37027268402069735, w0=0.00029055834944360725, w1=-0.031174321243577094\n",
      "Gradient Descent(293/399): loss=0.37009820787466896, w0=0.00028826771974717265, w1=-0.031222854341247174\n",
      "Gradient Descent(294/399): loss=0.36992421214772164, w0=0.0002859738450657714, w1=-0.03127124664441318\n",
      "Gradient Descent(295/399): loss=0.36975069397242194, w0=0.00028367688579762564, w1=-0.031319499050424665\n",
      "Gradient Descent(296/399): loss=0.3695776505088244, w0=0.00028137700076559725, w1=-0.031367612448229915\n",
      "Gradient Descent(297/399): loss=0.36940507894410796, w0=0.00027907434722623646, w1=-0.03141558771847344\n",
      "Gradient Descent(298/399): loss=0.369232976492216, w0=0.0002767690808788693, w1=-0.03146342573359225\n",
      "Gradient Descent(299/399): loss=0.36906134039350663, w0=0.00027446135587472133, w1=-0.031511127357910874\n",
      "Gradient Descent(300/399): loss=0.3688901679144071, w0=0.0002721513248260742, w1=-0.031558693447735126\n",
      "Gradient Descent(301/399): loss=0.36871945634707426, w0=0.00026983913881545356, w1=-0.031606124851444706\n",
      "Gradient Descent(302/399): loss=0.3685492030090611, w0=0.00026752494740484505, w1=-0.03165342240958459\n",
      "Gradient Descent(303/399): loss=0.36837940524298907, w0=0.0002652088986449359, w1=-0.03170058695495523\n",
      "Gradient Descent(304/399): loss=0.3682100604162267, w0=0.00026289113908438, w1=-0.0317476193127016\n",
      "Gradient Descent(305/399): loss=0.36804116592057234, w0=0.0002605718137790844, w1=-0.03179452030040111\n",
      "Gradient Descent(306/399): loss=0.36787271917194386, w0=0.00025825106630151433, w1=-0.03184129072815041\n",
      "Gradient Descent(307/399): loss=0.36770471761007295, w0=0.0002559290387500147, w1=-0.03188793139865099\n",
      "Gradient Descent(308/399): loss=0.36753715869820314, w0=0.0002536058717581466, w1=-0.03193444310729381\n",
      "Gradient Descent(309/399): loss=0.36737003992279677, w0=0.0002512817045040356, w1=-0.03198082664224277\n",
      "Gradient Descent(310/399): loss=0.3672033587932422, w0=0.0002489566747197311, w1=-0.03202708278451709\n",
      "Gradient Descent(311/399): loss=0.36703711284156937, w0=0.0002466309187005737, w1=-0.03207321230807272\n",
      "Gradient Descent(312/399): loss=0.36687129962216897, w0=0.0002443045713145691, w1=-0.032119215979882626\n",
      "Gradient Descent(313/399): loss=0.36670591671151576, w0=0.00024197776601176692, w1=-0.03216509456001612\n",
      "Gradient Descent(314/399): loss=0.36654096170789685, w0=0.00023965063483364228, w1=-0.03221084880171713\n",
      "Gradient Descent(315/399): loss=0.3663764322311456, w0=0.00023732330842247845, w1=-0.03225647945148148\n",
      "Gradient Descent(316/399): loss=0.36621232592237657, w0=0.00023499591603074916, w1=-0.032301987249133225\n",
      "Gradient Descent(317/399): loss=0.36604864044373037, w0=0.0002326685855304984, w1=-0.03234737292789996\n",
      "Gradient Descent(318/399): loss=0.36588537347811567, w0=0.00023034144342271674, w1=-0.032392637214487194\n",
      "Gradient Descent(319/399): loss=0.36572252272896216, w0=0.00022801461484671197, w1=-0.03243778082915183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(320/399): loss=0.36556008591997236, w0=0.00022568822358947323, w1=-0.03248280448577462\n",
      "Gradient Descent(321/399): loss=0.36539806079488046, w0=0.0002233623920950264, w1=-0.03252770889193176\n",
      "Gradient Descent(322/399): loss=0.3652364451172131, w0=0.00022103724147378014, w1=-0.032572494748965584\n",
      "Gradient Descent(323/399): loss=0.3650752366700557, w0=0.0002187128915118606, w1=-0.03261716275205433\n",
      "Gradient Descent(324/399): loss=0.36491443325582174, w0=0.00021638946068043357, w1=-0.03266171359028104\n",
      "Gradient Descent(325/399): loss=0.3647540326960243, w0=0.00021406706614501318, w1=-0.03270614794670163\n",
      "Gradient Descent(326/399): loss=0.3645940328310536, w0=0.00021174582377475533, w1=-0.032750466498412005\n",
      "Gradient Descent(327/399): loss=0.36443443151995736, w0=0.000209425848151735, w1=-0.032794669916614415\n",
      "Gradient Descent(328/399): loss=0.3642752266402219, w0=0.00020710725258020612, w1=-0.03283875886668295\n",
      "Gradient Descent(329/399): loss=0.36411641608756096, w0=0.00020479014909584295, w1=-0.0328827340082282\n",
      "Gradient Descent(330/399): loss=0.3639579977757051, w0=0.0002024746484749617, w1=-0.0329265959951611\n",
      "Gradient Descent(331/399): loss=0.3637999696361942, w0=0.0002001608602437216, w1=-0.03297034547575601\n",
      "Gradient Descent(332/399): loss=0.36364232961817466, w0=0.00019784889268730397, w1=-0.033013983092712924\n",
      "Gradient Descent(333/399): loss=0.363485075688198, w0=0.00019553885285906872, w1=-0.033057509483219\n",
      "Gradient Descent(334/399): loss=0.3633282058300247, w0=0.00019323084658968713, w1=-0.03310092527900924\n",
      "Gradient Descent(335/399): loss=0.36317171804442805, w0=0.00019092497849624963, w1=-0.03314423110642646\n",
      "Gradient Descent(336/399): loss=0.36301561034900354, w0=0.00018862135199134849, w1=-0.03318742758648047\n",
      "Gradient Descent(337/399): loss=0.36285988077798004, w0=0.0001863200692921336, w1=-0.03323051533490653\n",
      "Gradient Descent(338/399): loss=0.36270452738203374, w0=0.0001840212314293412, w1=-0.033273494962223094\n",
      "Gradient Descent(339/399): loss=0.36254954822810465, w0=0.00018172493825629438, w1=-0.03331636707378883\n",
      "Gradient Descent(340/399): loss=0.3623949413992168, w0=0.00017943128845787466, w1=-0.03335913226985889\n",
      "Gradient Descent(341/399): loss=0.3622407049943004, w0=0.00017714037955946383, w1=-0.03340179114564053\n",
      "Gradient Descent(342/399): loss=0.3620868371280157, w0=0.00017485230793585552, w1=-0.033444344291348\n",
      "Gradient Descent(343/399): loss=0.3619333359305818, w0=0.00017256716882013546, w1=-0.033486792292256796\n",
      "Gradient Descent(344/399): loss=0.36178019954760543, w0=0.00017028505631252992, w1=-0.033529135728757174\n",
      "Gradient Descent(345/399): loss=0.3616274261399135, w0=0.00016800606338922177, w1=-0.03357137517640707\n",
      "Gradient Descent(346/399): loss=0.36147501388338804, w0=0.00016573028191113335, w1=-0.03361351120598431\n",
      "Gradient Descent(347/399): loss=0.36132296096880345, w0=0.00016345780263267556, w1=-0.033655544383538204\n",
      "Gradient Descent(348/399): loss=0.3611712656016657, w0=0.00016118871521046267, w1=-0.03369747527044049\n",
      "Gradient Descent(349/399): loss=0.3610199260020538, w0=0.0001589231082119922, w1=-0.033739304423435645\n",
      "Gradient Descent(350/399): loss=0.3608689404044644, w0=0.00015666106912428932, w1=-0.033781032394690595\n",
      "Gradient Descent(351/399): loss=0.3607183070576575, w0=0.00015440268436251543, w1=-0.033822659731843785\n",
      "Gradient Descent(352/399): loss=0.36056802422450507, w0=0.00015214803927854004, w1=-0.033864186978053656\n",
      "Gradient Descent(353/399): loss=0.36041809018184057, w0=0.0001498972181694758, w1=-0.03390561467204654\n",
      "Gradient Descent(354/399): loss=0.36026850322031345, w0=0.00014765030428617618, w1=-0.033946943348163915\n",
      "Gradient Descent(355/399): loss=0.36011926164424146, w0=0.00014540737984169508, w1=-0.03398817353640917\n",
      "Gradient Descent(356/399): loss=0.3599703637714686, w0=0.0001431685260197084, w1=-0.034029305762493696\n",
      "Gradient Descent(357/399): loss=0.35982180793322355, w0=0.00014093382298289658, w1=-0.03407034054788248\n",
      "Gradient Descent(358/399): loss=0.35967359247397934, w0=0.00013870334988128841, w1=-0.034111278409839095\n",
      "Gradient Descent(359/399): loss=0.359525715751317, w0=0.00013647718486056512, w1=-0.03415211986147019\n",
      "Gradient Descent(360/399): loss=0.3593781761357886, w0=0.0001342554050703247, w1=-0.034192865411769385\n",
      "Gradient Descent(361/399): loss=0.3592309720107835, w0=0.00013203808667230614, w1=-0.034233515565660656\n",
      "Gradient Descent(362/399): loss=0.3590841017723979, w0=0.0001298253048485731, w1=-0.0342740708240412\n",
      "Gradient Descent(363/399): loss=0.35893756382930175, w0=0.0001276171338096568, w1=-0.03431453168382373\n",
      "Gradient Descent(364/399): loss=0.3587913566026125, w0=0.000125413646802658, w1=-0.034354898637978325\n",
      "Gradient Descent(365/399): loss=0.3586454785257666, w0=0.0001232149161193073, w1=-0.03439517217557371\n",
      "Gradient Descent(366/399): loss=0.3584999280443955, w0=0.00012102101310398413, w1=-0.03443535278181806\n",
      "Gradient Descent(367/399): loss=0.35835470361620103, w0=0.00011883200816169372, w1=-0.03447544093809931\n",
      "Gradient Descent(368/399): loss=0.3582098037108344, w0=0.00011664797076600205, w1=-0.03451543712202497\n",
      "Gradient Descent(369/399): loss=0.35806522680977493, w0=0.00011446896946692838, w1=-0.03455534180746145\n",
      "Gradient Descent(370/399): loss=0.35792097140621193, w0=0.00011229507189879545, w1=-0.03459515546457292\n",
      "Gradient Descent(371/399): loss=0.3577770360049275, w0=0.00011012634478803674, w1=-0.0346348785598597\n",
      "Gradient Descent(372/399): loss=0.3576334191221804, w0=0.00010796285396096098, w1=-0.0346745115561962\n",
      "Gradient Descent(373/399): loss=0.35749011928559127, w0=0.00010580466435147359, w1=-0.034714054912868324\n",
      "Gradient Descent(374/399): loss=0.3573471350340316, w0=0.00010365184000875486, w1=-0.034753509085610555\n",
      "Gradient Descent(375/399): loss=0.3572044649175107, w0=0.00010150444410489473, w1=-0.03479287452664248\n",
      "Gradient Descent(376/399): loss=0.35706210749706624, w0=9.93625389424842e-05, w1=-0.03483215168470491\n",
      "Gradient Descent(377/399): loss=0.35692006134465615, w0=9.7226185962163e-05, w1=-0.03487134100509559\n",
      "Gradient Descent(378/399): loss=0.3567783250430514, w0=9.509544575012354e-05, w1=-0.03491044292970445\n",
      "Gradient Descent(379/399): loss=0.3566368971857292, w0=9.297037804557111e-05, w1=-0.03494945789704846\n",
      "Gradient Descent(380/399): loss=0.35649577637677016, w0=9.08510417481401e-05, w1=-0.03498838634230602\n",
      "Gradient Descent(381/399): loss=0.3563549612307529, w0=8.87374949252661e-05, w1=-0.03502722869735099\n",
      "Gradient Descent(382/399): loss=0.3562144503726548, w0=8.662979481951415e-05, w1=-0.03506598539078629\n",
      "Gradient Descent(383/399): loss=0.35607424243774877, w0=8.452799785586266e-05, w1=-0.035104656847977096\n",
      "Gradient Descent(384/399): loss=0.3559343360715056, w0=8.243215964894322e-05, w1=-0.03514324349108362\n",
      "Gradient Descent(385/399): loss=0.35579472992949523, w0=8.03423350102362e-05, w1=-0.03518174573909358\n",
      "Gradient Descent(386/399): loss=0.35565542267728945, w0=7.825857795522188e-05, w1=-0.03522016400785415\n",
      "Gradient Descent(387/399): loss=0.35551641299036585, w0=7.618094171048759e-05, w1=-0.03525849871010367\n",
      "Gradient Descent(388/399): loss=0.3553776995540149, w0=7.410947872079023e-05, w1=-0.03529675025550286\n",
      "Gradient Descent(389/399): loss=0.3552392810632436, w0=7.204424065607458e-05, w1=-0.03533491905066574\n",
      "Gradient Descent(390/399): loss=0.3551011562226861, w0=6.998527841844737e-05, w1=-0.03537300549919018\n",
      "Gradient Descent(391/399): loss=0.3549633237465095, w0=6.79326421491068e-05, w1=-0.035411010001688024\n",
      "Gradient Descent(392/399): loss=0.3548257823583268, w0=6.58863812352279e-05, w1=-0.03544893295581493\n",
      "Gradient Descent(393/399): loss=0.35468853079110474, w0=6.384654431680353e-05, w1=-0.0354867747562998\n",
      "Gradient Descent(394/399): loss=0.35455156778707786, w0=6.181317929344101e-05, w1=-0.03552453579497392\n",
      "Gradient Descent(395/399): loss=0.3544148920976604, w0=5.9786333331114745e-05, w1=-0.035562216460799705\n",
      "Gradient Descent(396/399): loss=0.3542785024833609, w0=5.776605286887429e-05, w1=-0.035599817139899126\n",
      "Gradient Descent(397/399): loss=0.3541423977136968, w0=5.5752383625508595e-05, w1=-0.0356373382155818\n",
      "Gradient Descent(398/399): loss=0.3540065765671106, w0=5.3745370606165854e-05, w1=-0.035674780068372755\n",
      "Gradient Descent(399/399): loss=0.3538710378308873, w0=5.174505810892944e-05, w1=-0.035712143076039864\n",
      "++++ gamma = 0.00193069772888\n",
      "ciaociaociao\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/399): loss=0.4999999999999887, w0=-1.32971637681253e-06, w1=-0.0006921790679257304\n",
      "Gradient Descent(1/399): loss=0.49566123808114854, w0=-4.905803824638662e-07, w1=-0.001365668719898139\n",
      "Gradient Descent(2/399): loss=0.4915511696237397, w0=2.2990377644345445e-06, w1=-0.0020211746622378884\n",
      "Gradient Descent(3/399): loss=0.48765336170872337, w0=6.837563907575084e-06, w1=-0.002659371046936368\n",
      "Gradient Descent(4/399): loss=0.48395273629627084, w0=1.2938990702200896e-05, w1=-0.0032809021000257117\n",
      "Gradient Descent(5/399): loss=0.48043544493667995, w0=2.0431739455360104e-05, w1=-0.0038863836538890273\n",
      "Gradient Descent(6/399): loss=0.47708875652669624, w0=2.915760613530035e-05, w1=-0.004476404589849278\n",
      "Gradient Descent(7/399): loss=0.47390095658510434, w0=3.897078516547981e-05, w1=-0.0050515281969221275\n",
      "Gradient Descent(8/399): loss=0.47086125671835877, w0=4.973696511369489e-05, w1=-0.005612293452199048\n",
      "Gradient Descent(9/399): loss=0.46795971311667955, w0=6.133249084289614e-05, w1=-0.006159216227939294\n",
      "Gradient Descent(10/399): loss=0.46518715306738956, w0=7.364358711047254e-05, w1=-0.006692790430090781\n",
      "Gradient Descent(11/399): loss=0.462535108598659, w0=8.656563898997719e-05, w1=-0.007213489072627992\n",
      "Gradient Descent(12/399): loss=0.4599957564761299, w0=0.0001000025248460624, w1=-0.00772176529178786\n",
      "Gradient Descent(13/399): loss=0.4575618638695408, w0=0.00011386599792220072, w1=-0.008218053304000184\n",
      "Gradient Descent(14/399): loss=0.4552267390885413, w0=0.00012807511290380722, w1=-0.008702769311045834\n",
      "Gradient Descent(15/399): loss=0.45298418685816566, w0=0.00014255569409869014, w1=-0.009176312355732037\n",
      "Gradient Descent(16/399): loss=0.4508284676664165, w0=0.00015723984213421684, w1=-0.009639065131148126\n",
      "Gradient Descent(17/399): loss=0.44875426077039765, w0=0.00017206547630792485, w1=-0.01009139474635574\n",
      "Gradient Descent(18/399): loss=0.44675663049455133, w0=0.00018697590994711774, w1=-0.010533653451173328\n",
      "Gradient Descent(19/399): loss=0.44483099549569866, w0=0.00020191945633473278, w1=-0.010966179322534969\n",
      "Gradient Descent(20/399): loss=0.4429731007056302, w0=0.0002168490629447961, w1=-0.011389296914736552\n",
      "Gradient Descent(21/399): loss=0.44117899169356367, w0=0.0002317219719023356, w1=-0.011803317875727647\n",
      "Gradient Descent(22/399): loss=0.43944499121854674, w0=0.00024649940474083906, w1=-0.01220854153146372\n",
      "Gradient Descent(23/399): loss=0.4377676777662826, w0=0.00026114626967628236, w1=-0.012605255440200053\n",
      "Gradient Descent(24/399): loss=0.43614386588637133, w0=0.0002756308897513714, w1=-0.012993735918484975\n",
      "Gradient Descent(25/399): loss=0.43457058816496386, w0=0.00028992475032783973, w1=-0.01337424854049503\n",
      "Gradient Descent(26/399): loss=0.4330450786846136, w0=0.00030400226451923187, w1=-0.01374704861224797\n",
      "Gradient Descent(27/399): loss=0.4315647578380103, w0=0.000317840555262347, w1=-0.014112381622130188\n",
      "Gradient Descent(28/399): loss=0.4301272183754854, w0=0.00033141925282310214, w1=-0.014470483669082993\n",
      "Gradient Descent(29/399): loss=0.428730212577933, w0=0.00034472030662264814, w1=-0.014821581869706358\n",
      "Gradient Descent(30/399): loss=0.42737164045725307, w0=0.00035772781035271664, w1=-0.015165894745458932\n",
      "Gradient Descent(31/399): loss=0.42604953889576, w0=0.00037042783942594146, w1=-0.015503632591058934\n",
      "Gradient Descent(32/399): loss=0.42476207164434776, w0=0.0003828082998777765, w1=-0.015834997825121388\n",
      "Gradient Descent(33/399): loss=0.4235075201066684, w0=0.00039485878790208905, w1=-0.016160185324002787\n",
      "Gradient Descent(34/399): loss=0.4222842748432801, w0=0.0004065704592629635, w1=-0.01647938273976442\n",
      "Gradient Descent(35/399): loss=0.4210908277357386, w0=0.0004179359078810984, w1=-0.016792770803109588\n",
      "Gradient Descent(36/399): loss=0.4199257647560065, w0=0.00042894905294477235, w1=-0.017100523612098\n",
      "Gradient Descent(37/399): loss=0.41878775929143786, w0=0.0004396050339430337, w1=-0.017402808907391987\n",
      "Gradient Descent(38/399): loss=0.4176755659799778, w0=0.00044990011306282694, w1=-0.017699788334743883\n",
      "Gradient Descent(39/399): loss=0.4165880150141973, w0=0.00045983158443249796, w1=-0.017991617695391666\n",
      "Gradient Descent(40/399): loss=0.415524006876362, w0=0.0004693976897317684, w1=-0.018278447184990426\n",
      "Gradient Descent(41/399): loss=0.41448250746999027, w0=0.00047859753972308625, w1=-0.018560421621670393\n",
      "Gradient Descent(42/399): loss=0.41346254361629997, w0=0.0004874310412914515, w1=-0.01883768066377776\n",
      "Gradient Descent(43/399): loss=0.4124631988866171, w0=0.0004958988296095997, w1=-0.019110359017822355\n",
      "Gradient Descent(44/399): loss=0.4114836097442498, w0=0.0005040022050729725, w1=-0.019378586637126057\n",
      "Gradient Descent(45/399): loss=0.41052296197154003, w0=0.0005117430746744009, w1=-0.01964248891163771\n",
      "Gradient Descent(46/399): loss=0.40958048735981983, w0=0.0005191238975120172, w1=-0.019902186849353843\n",
      "Gradient Descent(47/399): loss=0.40865546064182845, w0=0.0005261476341457532, w1=-0.02015779724975987\n",
      "Gradient Descent(48/399): loss=0.4077471966478234, w0=0.0005328176995380024, w1=-0.020409432869683223\n",
      "Gradient Descent(49/399): loss=0.4068550476681429, w0=0.000539137919332749, w1=-0.020657202581928248\n",
      "Gradient Descent(50/399): loss=0.4059784010063706, w0=0.0005451124892448104, w1=-0.020901211527042333\n",
      "Gradient Descent(51/399): loss=0.4051166767085328, w0=0.0005507459373469092, w1=-0.021141561258543606\n",
      "Gradient Descent(52/399): loss=0.4042693254549206, w0=0.0005560430890571807, w1=-0.021378349881922717\n",
      "Gradient Descent(53/399): loss=0.4034358266022022, w0=0.0005610090346435207, w1=-0.021611672187714367\n",
      "Gradient Descent(54/399): loss=0.4026156863644628, w0=0.0005656490990739759, w1=-0.02184161977891843\n",
      "Gradient Descent(55/399): loss=0.4018084361227169, w0=0.0005699688140542396, w1=-0.02206828119303573\n",
      "Gradient Descent(56/399): loss=0.4010136308532458, w0=0.0005739738921043216, w1=-0.022291742018969534\n",
      "Gradient Descent(57/399): loss=0.40023084766588557, w0=0.0005776702025366676, w1=-0.022512085009030736\n",
      "Gradient Descent(58/399): loss=0.3994596844440686, w0=0.0005810637492074787, w1=-0.022729390186272326\n",
      "Gradient Descent(59/399): loss=0.39869975857906703, w0=0.0005841606499217698, w1=-0.02294373494736714\n",
      "Gradient Descent(60/399): loss=0.39795070579147196, w0=0.0005869671173808746, w1=-0.023155194161231928\n",
      "Gradient Descent(61/399): loss=0.3972121790334736, w0=0.0005894894415686778, w1=-0.023363840263590445\n",
      "Gradient Descent(62/399): loss=0.39648384746600934, w0=0.0005917339734799036, w1=-0.02356974334765859\n",
      "Gradient Descent(63/399): loss=0.395765395505301, w0=0.0005937071101003267, w1=-0.02377297125112537\n",
      "Gradient Descent(64/399): loss=0.3950565219337134, w0=0.0005954152805548547, w1=-0.02397358963959491\n",
      "Gradient Descent(65/399): loss=0.39435693907026753, w0=0.0005968649333450805, w1=-0.024171662086646447\n",
      "Gradient Descent(66/399): loss=0.3936663719964791, w0=0.0005980625246031616, w1=-0.024367250150661685\n",
      "Gradient Descent(67/399): loss=0.39298455783353725, w0=0.0005990145072937693, w1=-0.024560413448561396\n",
      "Gradient Descent(68/399): loss=0.39231124506712434, w0=0.0005997273213003994, w1=-0.02475120972658648\n",
      "Gradient Descent(69/399): loss=0.39164619291647085, w0=0.0006002073843365675, w1=-0.02493969492825204\n",
      "Gradient Descent(70/399): loss=0.39098917074448314, w0=0.0006004610836263535, w1=-0.025125923259596894\n",
      "Gradient Descent(71/399): loss=0.3903399575060261, w0=0.0006004947683024247, w1=-0.025309947251845193\n",
      "Gradient Descent(72/399): loss=0.3896983412316579, w0=0.0006003147424730879, w1=-0.025491817821591184\n",
      "Gradient Descent(73/399): loss=0.38906411854431505, w0=0.0005999272589130989, w1=-0.025671584328613002\n",
      "Gradient Descent(74/399): loss=0.3884370942066342, w0=0.0005993385133359287, w1=-0.02584929463141644\n",
      "Gradient Descent(75/399): loss=0.3878170806967643, w0=0.000598554639207947, w1=-0.02602499514060485\n",
      "Gradient Descent(76/399): loss=0.38720389781068704, w0=0.0005975817030675638, w1=-0.02619873087016703\n",
      "Gradient Descent(77/399): loss=0.38659737228920377, w0=0.0005964257003147727, w1=-0.02637054548677058\n",
      "Gradient Descent(78/399): loss=0.3859973374678854, w0=0.0005950925514387843, w1=-0.026540481357144322\n",
      "Gradient Descent(79/399): loss=0.3854036329484079, w0=0.0005935880986535309, w1=-0.0267085795936295\n",
      "Gradient Descent(80/399): loss=0.3848161042898061, w0=0.0005919181029127762, w1=-0.026874880097975905\n",
      "Gradient Descent(81/399): loss=0.384234602718292, w0=0.0005900882412783895, w1=-0.027039421603455654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(82/399): loss=0.38365898485437644, w0=0.0005881041046170494, w1=-0.02720224171536403\n",
      "Gradient Descent(83/399): loss=0.3830891124561282, w0=0.000585971195602228, w1=-0.027363376949973765\n",
      "Gradient Descent(84/399): loss=0.38252485217748566, w0=0.0005836949269998021, w1=-0.02752286277200616\n",
      "Gradient Descent(85/399): loss=0.3819660753406169, w0=0.0005812806202170196, w1=-0.027680733630679653\n",
      "Gradient Descent(86/399): loss=0.38141265772139404, w0=0.0005787335040958566, w1=-0.02783702299439378\n",
      "Gradient Descent(87/399): loss=0.38086447934711687, w0=0.000576058713933011, w1=-0.027991763384103897\n",
      "Gradient Descent(88/399): loss=0.38032142430567817, w0=0.0005732612907099157, w1=-0.028144986405439717\n",
      "Gradient Descent(89/399): loss=0.37978338056542543, w0=0.0005703461805172237, w1=-0.028296722779618266\n",
      "Gradient Descent(90/399): loss=0.37925023980502565, w0=0.0005673182341592037, w1=-0.02844700237319983\n",
      "Gradient Descent(91/399): loss=0.37872189725268207, w0=0.000564182206924425, w1=-0.028595854226733265\n",
      "Gradient Descent(92/399): loss=0.378198251534108, w0=0.0005609427585099786, w1=-0.028743306582335062\n",
      "Gradient Descent(93/399): loss=0.3776792045286984, w0=0.000557604453087299, w1=-0.02888938691024477\n",
      "Gradient Descent(94/399): loss=0.3771646612333768, w0=0.0005541717594984164, w1=-0.0290341219343974\n",
      "Gradient Descent(95/399): loss=0.37665452963363727, w0=0.0005506490515721889, w1=-0.029177537657051823\n",
      "Gradient Descent(96/399): loss=0.3761487205813319, w0=0.0005470406085507296, w1=-0.02931965938251256\n",
      "Gradient Descent(97/399): loss=0.37564714767878393, w0=0.000543350615616882, w1=-0.029460511739980634\n",
      "Gradient Descent(98/399): loss=0.37514972716883555, w0=0.000539583164514178, w1=-0.0296001187055678\n",
      "Gradient Descent(99/399): loss=0.37465637783046946, w0=0.0005357422542512759, w1=-0.029738503623507013\n",
      "Gradient Descent(100/399): loss=0.3741670208796629, w0=0.0005318317918833847, w1=-0.029875689226590542\n",
      "Gradient Descent(101/399): loss=0.37368157987516326, w0=0.0005278555933636777, w1=-0.03001169765586592\n",
      "Gradient Descent(102/399): loss=0.37319998062888393, w0=0.0005238173844581477, w1=-0.03014655047961864\n",
      "Gradient Descent(103/399): loss=0.3727221511206521, w0=0.0005197208017177874, w1=-0.030280268711669306\n",
      "Gradient Descent(104/399): loss=0.37224802141704894, w0=0.0005155693935023816, w1=-0.0304128728290118\n",
      "Gradient Descent(105/399): loss=0.37177752359410454, w0=0.0005113666210505722, w1=-0.030544382788817995\n",
      "Gradient Descent(106/399): loss=0.3713105916636238, w0=0.000507115859591214, w1=-0.030674818044833387\n",
      "Gradient Descent(107/399): loss=0.37084716150293306, w0=0.0005028203994913698, w1=-0.030804197563187155\n",
      "Gradient Descent(108/399): loss=0.3703871707878569, w0=0.0004984834474366058, w1=-0.0309325398376391\n",
      "Gradient Descent(109/399): loss=0.3699305589287373, w0=0.00049410812763954, w1=-0.031059862904285043\n",
      "Gradient Descent(110/399): loss=0.3694772670093295, w0=0.0004896974830728757, w1=-0.03118618435574142\n",
      "Gradient Descent(111/399): loss=0.3690272377284115, w0=0.0004852544767234024, w1=-0.03131152135482888\n",
      "Gradient Descent(112/399): loss=0.368580415343961, w0=0.00048078199286369746, w1=-0.031435890647774016\n",
      "Gradient Descent(113/399): loss=0.3681367456197583, w0=0.0004762828383384833, w1=-0.03155930857694747\n",
      "Gradient Descent(114/399): loss=0.36769617577428315, w0=0.0004717597438628119, w1=-0.031681791093156074\n",
      "Gradient Descent(115/399): loss=0.36725865443178685, w0=0.00046721536532944697, w1=-0.031803353767505746\n",
      "Gradient Descent(116/399): loss=0.3668241315754204, w0=0.00046265228512300445, w1=-0.03192401180285149\n",
      "Gradient Descent(117/399): loss=0.36639255850231234, w0=0.00045807301343858806, w1=-0.03204378004484993\n",
      "Gradient Descent(118/399): loss=0.36596388778049804, w0=0.0004534799896028223, w1=-0.032162672992629385\n",
      "Gradient Descent(119/399): loss=0.36553807320759896, w0=0.00044887558339534196, w1=-0.03228070480909175\n",
      "Gradient Descent(120/399): loss=0.3651150697711689, w0=0.0004442620963689437, w1=-0.032397889330860054\n",
      "Gradient Descent(121/399): loss=0.36469483361062116, w0=0.00043964176316674215, w1=-0.032514240077884816\n",
      "Gradient Descent(122/399): loss=0.36427732198065643, w0=0.00043501675283480224, w1=-0.03262977026272198\n",
      "Gradient Descent(123/399): loss=0.36386249321611935, w0=0.00043038917012884053, w1=-0.0327444927994946\n",
      "Gradient Descent(124/399): loss=0.36345030669821293, w0=0.0004257610568137026, w1=-0.03285842031255006\n",
      "Gradient Descent(125/399): loss=0.36304072282200606, w0=0.000421134392954428, w1=-0.03297156514482399\n",
      "Gradient Descent(126/399): loss=0.3626337029651708, w0=0.0004165110981978176, w1=-0.03308393936592179\n",
      "Gradient Descent(127/399): loss=0.36222920945789283, w0=0.00041189303304350856, w1=-0.03319555477992824\n",
      "Gradient Descent(128/399): loss=0.36182720555390024, w0=0.00040728200010365197, w1=-0.03330642293295498\n",
      "Gradient Descent(129/399): loss=0.3614276554025564, w0=0.0004026797453503706, w1=-0.033416555120435704\n",
      "Gradient Descent(130/399): loss=0.361030524021971, w0=0.0003980879593502503, w1=-0.033525962394178176\n",
      "Gradient Descent(131/399): loss=0.36063577727308116, w0=0.00039350827848519187, w1=-0.03363465556918199\n",
      "Gradient Descent(132/399): loss=0.3602433818346616, w0=0.0003889422861590174, w1=-0.03374264523023064\n",
      "Gradient Descent(133/399): loss=0.35985330517921804, w0=0.0003843915139892895, w1=-0.033849941738266026\n",
      "Gradient Descent(134/399): loss=0.35946551554973016, w0=0.00037985744298385884, w1=-0.03395655523655343\n",
      "Gradient Descent(135/399): loss=0.3590799819372033, w0=0.0003753415047017144, w1=-0.034062495656644404\n",
      "Gradient Descent(136/399): loss=0.3586966740589964, w0=0.0003708450823977588, w1=-0.03416777272414499\n",
      "Gradient Descent(137/399): loss=0.3583155623378922, w0=0.00036636951215118275, w1=-0.03427239596429624\n",
      "Gradient Descent(138/399): loss=0.35793661788187775, w0=0.00036191608397715666, w1=-0.034376374707373744\n",
      "Gradient Descent(139/399): loss=0.3575598124646075, w0=0.0003574860429215985, w1=-0.03447971809391281\n",
      "Gradient Descent(140/399): loss=0.3571851185065192, w0=0.00035308059013881924, w1=-0.03458243507976534\n",
      "Gradient Descent(141/399): loss=0.35681250905657586, w0=0.0003487008839518809, w1=-0.034684534440994626\n",
      "Gradient Descent(142/399): loss=0.3564419577746094, w0=0.0003443480408955387, w1=-0.034786024778613645\n",
      "Gradient Descent(143/399): loss=0.35607343891424154, w0=0.0003400231367416694, w1=-0.03488691452317259\n",
      "Gradient Descent(144/399): loss=0.35570692730635883, w0=0.00033572720750711785, w1=-0.03498721193920086\n",
      "Gradient Descent(145/399): loss=0.35534239834311976, w0=0.0003314612504439216, w1=-0.035086925129508784\n",
      "Gradient Descent(146/399): loss=0.3549798279624743, w0=0.00032722622501189823, w1=-0.035186062039353885\n",
      "Gradient Descent(147/399): loss=0.3546191926331749, w0=0.00032302305383360454, w1=-0.035284630460476606\n",
      "Gradient Descent(148/399): loss=0.3542604693402611, w0=0.00031885262363169793, w1=-0.035382638035009975\n",
      "Gradient Descent(149/399): loss=0.3539036355709999, w0=0.0003147157861487511, w1=-0.0354800922592677\n",
      "Gradient Descent(150/399): loss=0.35354866930126305, w0=0.0003106133590495891, w1=-0.03557700048741493\n",
      "Gradient Descent(151/399): loss=0.353195548982328, w0=0.0003065461268062366, w1=-0.03567336993502572\n",
      "Gradient Descent(152/399): loss=0.3528442535280847, w0=0.0003025148415655768, w1=-0.035769207682531244\n",
      "Gradient Descent(153/399): loss=0.3524947623026345, w0=0.0002985202239998401, w1=-0.03586452067856247\n",
      "Gradient Descent(154/399): loss=0.35214705510826527, w0=0.0002945629641400529, w1=-0.03595931574319098\n",
      "Gradient Descent(155/399): loss=0.35180111217379295, w0=0.0002906437221925893, w1=-0.036053599571071474\n",
      "Gradient Descent(156/399): loss=0.35145691414325164, w0=0.00028676312933897935, w1=-0.036147378734489276\n",
      "Gradient Descent(157/399): loss=0.3511144420649247, w0=0.0002829217885191391, w1=-0.03624065968631622\n",
      "Gradient Descent(158/399): loss=0.35077367738070114, w0=0.00027912027519819393, w1=-0.03633344876287792\n",
      "Gradient Descent(159/399): loss=0.3504346019157481, w0=0.0002753591381170788, w1=-0.03642575218673557\n",
      "Gradient Descent(160/399): loss=0.3500971978684892, w0=0.00027163890002710275, w1=-0.03651757606938514\n",
      "Gradient Descent(161/399): loss=0.34976144780087454, w0=0.00026796005840867463, w1=-0.03660892641387674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(162/399): loss=0.3494273346289386, w0=0.00026432308617439104, w1=-0.03669980911735693\n",
      "Gradient Descent(163/399): loss=0.3490948416136295, w0=0.00026072843235669423, w1=-0.03679022997353653\n",
      "Gradient Descent(164/399): loss=0.3487639523519066, w0=0.000257176522780311, w1=-0.036880194675086464\n",
      "Gradient Descent(165/399): loss=0.3484346507680932, w0=0.0002536677607196881, w1=-0.03696970881596402\n",
      "Gradient Descent(166/399): loss=0.3481069211054786, w0=0.0002502025275416446, w1=-0.037058777893671964\n",
      "Gradient Descent(167/399): loss=0.3477807479181604, w0=0.00024678118333346165, w1=-0.03714740731145256\n",
      "Gradient Descent(168/399): loss=0.34745611606311827, w0=0.00024340406751663545, w1=-0.037235602380418904\n",
      "Gradient Descent(169/399): loss=0.3471330106925145, w0=0.0002400714994465192, w1=-0.03732336832162544\n",
      "Gradient Descent(170/399): loss=0.3468114172462105, w0=0.00023678377899808296, w1=-0.037410710268079794\n",
      "Gradient Descent(171/399): loss=0.3464913214444951, w0=0.0002335411871380198, w1=-0.03749763326669786\n",
      "Gradient Descent(172/399): loss=0.3461727092810173, w0=0.00023034398648342927, w1=-0.037584142280203944\n",
      "Gradient Descent(173/399): loss=0.34585556701591635, w0=0.00022719242184730806, w1=-0.0376702421889778\n",
      "Gradient Descent(174/399): loss=0.34553988116914236, w0=0.000224086720771079, w1=-0.03775593779285033\n",
      "Gradient Descent(175/399): loss=0.3452256385139655, w0=0.0002210270940443887, w1=-0.037841233812849595\n",
      "Gradient Descent(176/399): loss=0.34491282607066137, w0=0.00021801373621240417, w1=-0.03792613489289869\n",
      "Gradient Descent(177/399): loss=0.34460143110037383, w0=0.00021504682607083748, w1=-0.03801064560146718\n",
      "Gradient Descent(178/399): loss=0.34429144109914445, w0=0.00021212652714892752, w1=-0.038094770433177445\n",
      "Gradient Descent(179/399): loss=0.3439828437921081, w0=0.00020925298818060567, w1=-0.03817851381036754\n",
      "Gradient Descent(180/399): loss=0.34367562712784533, w0=0.00020642634356407167, w1=-0.038261880084611795\n",
      "Gradient Descent(181/399): loss=0.34336977927289014, w0=0.00020364671381000395, w1=-0.038344873538200645\n",
      "Gradient Descent(182/399): loss=0.3430652886063872, w0=0.0002009142059786276, w1=-0.03842749838558094\n",
      "Gradient Descent(183/399): loss=0.3427621437148931, w0=0.00019822891410586045, w1=-0.03850975877475798\n",
      "Gradient Descent(184/399): loss=0.3424603333873202, w0=0.00019559091961875643, w1=-0.038591658788660464\n",
      "Gradient Descent(185/399): loss=0.34215984661001464, w0=0.00019300029174046287, w1=-0.03867320244646962\n",
      "Gradient Descent(186/399): loss=0.3418606725619691, w0=0.00019045708788490646, w1=-0.03875439370491349\n",
      "Gradient Descent(187/399): loss=0.3415628006101633, w0=0.00018796135404141999, w1=-0.0388352364595276\n",
      "Gradient Descent(188/399): loss=0.34126622030502807, w0=0.0001855131251495195, w1=-0.03891573454588301\n",
      "Gradient Descent(189/399): loss=0.34097092137603385, w0=0.0001831124254640391, w1=-0.03899589174078273\n",
      "Gradient Descent(190/399): loss=0.34067689372739346, w0=0.0001807592689108286, w1=-0.039075711763427494\n",
      "Gradient Descent(191/399): loss=0.3403841274338823, w0=0.0001784536594332148, w1=-0.03915519827655189\n",
      "Gradient Descent(192/399): loss=0.3400926127367667, w0=0.0001761955913294268, w1=-0.039234354887531686\n",
      "Gradient Descent(193/399): loss=0.33980234003984244, w0=0.00017398504958118089, w1=-0.03931318514946328\n",
      "Gradient Descent(194/399): loss=0.3395132999055771, w0=0.00017182201017361927, w1=-0.03939169256221611\n",
      "Gradient Descent(195/399): loss=0.33922548305135414, w0=0.00016970644040679273, w1=-0.039469880573458814\n",
      "Gradient Descent(196/399): loss=0.338938880345816, w0=0.00016763829919887554, w1=-0.03954775257965999\n",
      "Gradient Descent(197/399): loss=0.3386534828053042, w0=0.0001656175373812972, w1=-0.039625311927064304\n",
      "Gradient Descent(198/399): loss=0.3383692815903908, w0=0.00016364409798597313, w1=-0.03970256191264463\n",
      "Gradient Descent(199/399): loss=0.3380862680025028, w0=0.00016171791652481313, w1=-0.039779505785031054\n",
      "Gradient Descent(200/399): loss=0.3378044334806338, w0=0.00015983892126168376, w1=-0.03985614674541729\n",
      "Gradient Descent(201/399): loss=0.3375237695981408, w0=0.00015800703347699756, w1=-0.03993248794844531\n",
      "Gradient Descent(202/399): loss=0.3372442680596266, w0=0.00015622216772509924, w1=-0.04000853250306872\n",
      "Gradient Descent(203/399): loss=0.3369659206979001, w0=0.00015448423208461567, w1=-0.040084283473395585\n",
      "Gradient Descent(204/399): loss=0.33668871947101836, w0=0.00015279312840193405, w1=-0.040159743879511295\n",
      "Gradient Descent(205/399): loss=0.3364126564594031, w0=0.00015114875252796918, w1=-0.04023491669828198\n",
      "Gradient Descent(206/399): loss=0.33613772386303364, w0=0.00014955099454837793, w1=-0.040309804864139175\n",
      "Gradient Descent(207/399): loss=0.33586391399871074, w0=0.0001479997390073765, w1=-0.0403844112698461\n",
      "Gradient Descent(208/399): loss=0.3355912192973917, w0=0.00014649486512531212, w1=-0.04045873876724627\n",
      "Gradient Descent(209/399): loss=0.33531963230159356, w0=0.00014503624701013904, w1=-0.04053279016799479\n",
      "Gradient Descent(210/399): loss=0.33504914566286415, w0=0.00014362375386294533, w1=-0.04060656824427294\n",
      "Gradient Descent(211/399): loss=0.33477975213931627, w0=0.00014225725017767365, w1=-0.040680075729486426\n",
      "Gradient Descent(212/399): loss=0.33451144459322657, w0=0.00014093659593517743, w1=-0.04075331531894786\n",
      "Gradient Descent(213/399): loss=0.3342442159886946, w0=0.00013966164679174987, w1=-0.04082628967054381\n",
      "Gradient Descent(214/399): loss=0.3339780593893619, w0=0.0001384322542622613, w1=-0.040899001405386944\n",
      "Gradient Descent(215/399): loss=0.3337129679561909, w0=0.00013724826589803723, w1=-0.04097145310845364\n",
      "Gradient Descent(216/399): loss=0.3334489349452972, w0=0.00013610952545960688, w1=-0.04104364732920743\n",
      "Gradient Descent(217/399): loss=0.3331859537058395, w0=0.00013501587308444921, w1=-0.041115586582208756\n",
      "Gradient Descent(218/399): loss=0.33292401767796187, w0=0.00013396714544986081, w1=-0.041187273347711356\n",
      "Gradient Descent(219/399): loss=0.3326631203907888, w0=0.0001329631759310673, w1=-0.04125871007224562\n",
      "Gradient Descent(220/399): loss=0.33240325546047006, w0=0.00013200379475469793, w1=-0.041329899169189374\n",
      "Gradient Descent(221/399): loss=0.33214441658827587, w0=0.00013108882914773936, w1=-0.0414008430193263\n",
      "Gradient Descent(222/399): loss=0.3318865975587398, w0=0.00013021810348208318, w1=-0.041471543971392424\n",
      "Gradient Descent(223/399): loss=0.33162979223784766, w0=0.00012939143941477857, w1=-0.041542004342611\n",
      "Gradient Descent(224/399): loss=0.33137399457127426, w0=0.00012860865602409954, w1=-0.04161222641921599\n",
      "Gradient Descent(225/399): loss=0.33111919858266037, w0=0.00012786956994153336, w1=-0.041682212456964525\n",
      "Gradient Descent(226/399): loss=0.3308653983719375, w0=0.0001271739954797947, w1=-0.041751964681638706\n",
      "Gradient Descent(227/399): loss=0.3306125881136919, w0=0.00012652174475696773, w1=-0.04182148528953685\n",
      "Gradient Descent(228/399): loss=0.3303607620555691, w0=0.00012591262781687577, w1=-0.041890776447954615\n",
      "Gradient Descent(229/399): loss=0.3301099145167202, w0=0.000125346452745776, w1=-0.0419598402956562\n",
      "Gradient Descent(230/399): loss=0.32986003988628587, w0=0.00012482302578547527, w1=-0.04202867894333586\n",
      "Gradient Descent(231/399): loss=0.32961113262191777, w0=0.0001243421514429591, w1=-0.042097294474070054\n",
      "Gradient Descent(232/399): loss=0.32936318724833674, w0=0.0001239036325966261, w1=-0.04216568894376039\n",
      "Gradient Descent(233/399): loss=0.3291161983559283, w0=0.00012350727059921583, w1=-0.042233864381567655\n",
      "Gradient Descent(234/399): loss=0.3288701605993711, w0=0.00012315286537751768, w1=-0.04230182279033714\n",
      "Gradient Descent(235/399): loss=0.3286250686962996, w0=0.00012284021552894523, w1=-0.042369566147015474\n",
      "Gradient Descent(236/399): loss=0.3283809174260011, w0=0.00012256911841505912, w1=-0.04243709640305919\n",
      "Gradient Descent(237/399): loss=0.3281377016281438, w0=0.0001223393702521196, w1=-0.042504415484835224\n",
      "Gradient Descent(238/399): loss=0.3278954162015365, w0=0.0001221507661987475, w1=-0.04257152529401355\n",
      "Gradient Descent(239/399): loss=0.3276540561029187, w0=0.00012200310044077088, w1=-0.04263842770795215\n",
      "Gradient Descent(240/399): loss=0.32741361634578064, w0=0.00012189616627333297, w1=-0.042705124580074506\n",
      "Gradient Descent(241/399): loss=0.32717409199921244, w0=0.00012182975618033446, w1=-0.042771617740239805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(242/399): loss=0.32693547818678115, w0=0.00012180366191128252, w1=-0.042837908995105985\n",
      "Gradient Descent(243/399): loss=0.3266977700854356, w0=0.00012181767455561619, w1=-0.04290400012848591\n",
      "Gradient Descent(244/399): loss=0.32646096292443744, w0=0.00012187158461457642, w1=-0.04296989290169668\n",
      "Gradient Descent(245/399): loss=0.3262250519843185, w0=0.00012196518207068796, w1=-0.04303558905390241\n",
      "Gradient Descent(246/399): loss=0.325990032595865, w0=0.00012209825645491772, w1=-0.0431010903024505\n",
      "Gradient Descent(247/399): loss=0.3257559001391233, w0=0.00012227059691157348, w1=-0.043166398343201615\n",
      "Gradient Descent(248/399): loss=0.32552265004243336, w0=0.00012248199226100434, w1=-0.04323151485085354\n",
      "Gradient Descent(249/399): loss=0.3252902777814827, w0=0.00012273223106016407, w1=-0.043296441479258964\n",
      "Gradient Descent(250/399): loss=0.3250587788783853, w0=0.00012302110166109565, w1=-0.043361179861737466\n",
      "Gradient Descent(251/399): loss=0.3248281489007826, w0=0.00012334839226739498, w1=-0.04342573161138172\n",
      "Gradient Descent(252/399): loss=0.3245983834609637, w0=0.00012371389098870912, w1=-0.04349009832135814\n",
      "Gradient Descent(253/399): loss=0.3243694782150114, w0=0.00012411738589332476, w1=-0.043554281565202024\n",
      "Gradient Descent(254/399): loss=0.3241414288619638, w0=0.00012455866505889902, w1=-0.043618282897107376\n",
      "Gradient Descent(255/399): loss=0.32391423114299944, w0=0.00012503751662138536, w1=-0.04368210385221149\n",
      "Gradient Descent(256/399): loss=0.3236878808406416, w0=0.00012555372882220512, w1=-0.04374574594687443\n",
      "Gradient Descent(257/399): loss=0.3234623737779797, w0=0.00012610709005371357, w1=-0.043809210678953554\n",
      "Gradient Descent(258/399): loss=0.3232377058179112, w0=0.00012669738890300943, w1=-0.04387249952807312\n",
      "Gradient Descent(259/399): loss=0.32301387286240146, w0=0.00012732441419413377, w1=-0.04393561395588916\n",
      "Gradient Descent(260/399): loss=0.3227908708517606, w0=0.00012798795502870475, w1=-0.04399855540634964\n",
      "Gradient Descent(261/399): loss=0.3225686957639378, w0=0.0001286878008250327, w1=-0.04406132530595016\n",
      "Gradient Descent(262/399): loss=0.32234734361383227, w0=0.00012942374135575834, w1=-0.04412392506398505\n",
      "Gradient Descent(263/399): loss=0.3221268104526206, w0=0.00013019556678405727, w1=-0.04418635607279425\n",
      "Gradient Descent(264/399): loss=0.32190709236710113, w0=0.00013100306769845107, w1=-0.044248619708005865\n",
      "Gradient Descent(265/399): loss=0.32168818547905165, w0=0.00013184603514626594, w1=-0.0443107173287745\n",
      "Gradient Descent(266/399): loss=0.321470085944604, w0=0.00013272426066577754, w1=-0.04437265027801558\n",
      "Gradient Descent(267/399): loss=0.32125278995363293, w0=0.00013363753631708027, w1=-0.04443441988263567\n",
      "Gradient Descent(268/399): loss=0.3210362937291596, w0=0.00013458565471171795, w1=-0.04449602745375881\n",
      "Gradient Descent(269/399): loss=0.32082059352676784, w0=0.00013556840904111223, w1=-0.04455747428694913\n",
      "Gradient Descent(270/399): loss=0.32060568563403585, w0=0.00013658559310382405, w1=-0.04461876166242964\n",
      "Gradient Descent(271/399): loss=0.3203915663699801, w0=0.0001376370013316819, w1=-0.04467989084529736\n",
      "Gradient Descent(272/399): loss=0.32017823208451257, w0=0.00013872242881481095, w1=-0.0447408630857349\n",
      "Gradient Descent(273/399): loss=0.31996567915791035, w0=0.00013984167132559486, w1=-0.04480167961921845\n",
      "Gradient Descent(274/399): loss=0.31975390400029835, w0=0.00014099452534160274, w1=-0.044862341666722434\n",
      "Gradient Descent(275/399): loss=0.31954290305114275, w0=0.00014218078806751115, w1=-0.04492285043492066\n",
      "Gradient Descent(276/399): loss=0.31933267277875715, w0=0.0001434002574560519, w1=-0.044983207116384274\n",
      "Gradient Descent(277/399): loss=0.3191232096798201, w0=0.00014465273222801465, w1=-0.045043412889776396\n",
      "Gradient Descent(278/399): loss=0.3189145102789036, w0=0.0001459380118913323, w1=-0.04510346892004364\n",
      "Gradient Descent(279/399): loss=0.31870657112801204, w0=0.0001472558967592776, w1=-0.045163376358604505\n",
      "Gradient Descent(280/399): loss=0.31849938880613266, w0=0.0001486061879677973, w1=-0.04522313634353471\n",
      "Gradient Descent(281/399): loss=0.318292959918795, w0=0.0001499886874920104, w1=-0.04528274999974956\n",
      "Gradient Descent(282/399): loss=0.3180872810976424, w0=0.00015140319816189581, w1=-0.04534221843918335\n",
      "Gradient Descent(283/399): loss=0.31788234900001067, w0=0.00015284952367719442, w1=-0.04540154276096599\n",
      "Gradient Descent(284/399): loss=0.31767816030851925, w0=0.00015432746862154972, w1=-0.045460724051596665\n",
      "Gradient Descent(285/399): loss=0.317474711730669, w0=0.00015583683847591028, w1=-0.045519763385114924\n",
      "Gradient Descent(286/399): loss=0.31727199999845096, w0=0.00015737743963121728, w1=-0.0455786618232689\n",
      "Gradient Descent(287/399): loss=0.3170700218679636, w0=0.00015894907940039907, w1=-0.04563742041568097\n",
      "Gradient Descent(288/399): loss=0.31686877411903813, w0=0.00016055156602969467, w1=-0.04569604020001081\n",
      "Gradient Descent(289/399): loss=0.31666825355487194, w0=0.0001621847087093269, w1=-0.04575452220211585\n",
      "Gradient Descent(290/399): loss=0.3164684570016717, w0=0.00016384831758354618, w1=-0.0458128674362093\n",
      "Gradient Descent(291/399): loss=0.3162693813083046, w0=0.00016554220376006434, w1=-0.045871076905015636\n",
      "Gradient Descent(292/399): loss=0.31607102334595405, w0=0.00016726617931889828, w1=-0.04592915159992376\n",
      "Gradient Descent(293/399): loss=0.31587338000778775, w0=0.00016902005732064222, w1=-0.045987092501137736\n",
      "Gradient Descent(294/399): loss=0.31567644820862967, w0=0.0001708036518141866, w1=-0.04604490057782525\n",
      "Gradient Descent(295/399): loss=0.3154802248846411, w0=0.00017261677784390184, w1=-0.04610257678826377\n",
      "Gradient Descent(296/399): loss=0.31528470699300803, w0=0.0001744592514563041, w1=-0.046160122079984475\n",
      "Gradient Descent(297/399): loss=0.31508989151163547, w0=0.00017633088970622, w1=-0.04621753738991401\n",
      "Gradient Descent(298/399): loss=0.31489577543884884, w0=0.00017823151066246676, w1=-0.04627482364451406\n",
      "Gradient Descent(299/399): loss=0.314702355793102, w0=0.0001801609334130631, w1=-0.046331981759918844\n",
      "Gradient Descent(300/399): loss=0.31450962961269036, w0=0.0001821189780699876, w1=-0.046389012642070515\n",
      "Gradient Descent(301/399): loss=0.3143175939554729, w0=0.00018410546577349817, w1=-0.04644591718685252\n",
      "Gradient Descent(302/399): loss=0.3141262458985973, w0=0.00018612021869602848, w1=-0.046502696280220995\n",
      "Gradient Descent(303/399): loss=0.31393558253823306, w0=0.00018816306004567487, w1=-0.04655935079833413\n",
      "Gradient Descent(304/399): loss=0.3137456009893096, w0=0.00019023381406928772, w1=-0.04661588160767968\n",
      "Gradient Descent(305/399): loss=0.31355629838526033, w0=0.00019233230605518082, w1=-0.04667228956520055\n",
      "Gradient Descent(306/399): loss=0.3133676718777716, w0=0.00019445836233547196, w1=-0.04672857551841849\n",
      "Gradient Descent(307/399): loss=0.31317971863653793, w0=0.0001966118102880669, w1=-0.046784740305556016\n",
      "Gradient Descent(308/399): loss=0.31299243584902225, w0=0.00019879247833829965, w1=-0.046840784755656506\n",
      "Gradient Descent(309/399): loss=0.3128058207202206, w0=0.00020100019596024088, w1=-0.04689670968870253\n",
      "Gradient Descent(310/399): loss=0.31261987047243234, w0=0.00020323479367768564, w1=-0.04695251591573245\n",
      "Gradient Descent(311/399): loss=0.3124345823450356, w0=0.00020549610306483263, w1=-0.04700820423895533\n",
      "Gradient Descent(312/399): loss=0.3122499535942669, w0=0.00020778395674666492, w1=-0.04706377545186418\n",
      "Gradient Descent(313/399): loss=0.31206598149300485, w0=0.00021009818839904393, w1=-0.0471192303393475\n",
      "Gradient Descent(314/399): loss=0.311882663330561, w0=0.00021243863274852593, w1=-0.04717456967779926\n",
      "Gradient Descent(315/399): loss=0.31169999641247126, w0=0.00021480512557191201, w1=-0.04722979423522729\n",
      "Gradient Descent(316/399): loss=0.31151797806029435, w0=0.0002171975036955409, w1=-0.04728490477136007\n",
      "Gradient Descent(317/399): loss=0.3113366056114144, w0=0.00021961560499433408, w1=-0.047339902037752034\n",
      "Gradient Descent(318/399): loss=0.31115587641884557, w0=0.00022205926839060272, w1=-0.047394786777887325\n",
      "Gradient Descent(319/399): loss=0.31097578785104363, w0=0.00022452833385262509, w1=-0.047449559727282084\n",
      "Gradient Descent(320/399): loss=0.3107963372917184, w0=0.00022702264239300328, w1=-0.04750422161358527\n",
      "Gradient Descent(321/399): loss=0.3106175221396525, w0=0.00022954203606680767, w1=-0.047558773156678065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(322/399): loss=0.3104393398085227, w0=0.00023208635796951726, w1=-0.04761321506877183\n",
      "Gradient Descent(323/399): loss=0.31026178772672436, w0=0.00023465545223476413, w1=-0.04766754805450472\n",
      "Gradient Descent(324/399): loss=0.310084863337201, w0=0.00023724916403188918, w1=-0.047721772811036886\n",
      "Gradient Descent(325/399): loss=0.30990856409727563, w0=0.00023986733956331708, w1=-0.0477758900281444\n",
      "Gradient Descent(326/399): loss=0.3097328874784868, w0=0.00024250982606175783, w1=-0.047829900388311766\n",
      "Gradient Descent(327/399): loss=0.3095578309664269, w0=0.0002451764717872414, w1=-0.04788380456682323\n",
      "Gradient Descent(328/399): loss=0.3093833920605846, w0=0.00024786712602399287, w1=-0.04793760323185275\n",
      "Gradient Descent(329/399): loss=0.3092095682741899, w0=0.00025058163907715436, w1=-0.04799129704455271\n",
      "Gradient Descent(330/399): loss=0.3090363571340613, w0=0.0002533198622693604, w1=-0.048044886659141445\n",
      "Gradient Descent(331/399): loss=0.308863756180459, w0=0.00025608164793717295, w1=-0.04809837272298949\n",
      "Gradient Descent(332/399): loss=0.3086917629669375, w0=0.00025886684942738207, w1=-0.04815175587670467\n",
      "Gradient Descent(333/399): loss=0.3085203750602026, w0=0.00026167532109317793, w1=-0.048205036754216016\n",
      "Gradient Descent(334/399): loss=0.3083495900399722, w0=0.00026450691829020046, w1=-0.04825821598285647\n",
      "Gradient Descent(335/399): loss=0.3081794054988372, w0=0.0002673614973724716, w1=-0.048311294183444514\n",
      "Gradient Descent(336/399): loss=0.30800981904212843, w0=0.0002702389156882155, w1=-0.04836427197036464\n",
      "Gradient Descent(337/399): loss=0.30784082828778253, w0=0.0002731390315755724, w1=-0.048417149951646726\n",
      "Gradient Descent(338/399): loss=0.30767243086621354, w0=0.0002760617043582108, w1=-0.04846992872904429\n",
      "Gradient Descent(339/399): loss=0.30750462442018534, w0=0.00027900679434084277, w1=-0.04852260889811175\n",
      "Gradient Descent(340/399): loss=0.307337406604686, w0=0.00028197416280464736, w1=-0.04857519104828055\n",
      "Gradient Descent(341/399): loss=0.3071707750868059, w0=0.00028496367200260645, w1=-0.04862767576293428\n",
      "Gradient Descent(342/399): loss=0.30700472754561786, w0=0.00028797518515475784, w1=-0.048680063619482794\n",
      "Gradient Descent(343/399): loss=0.30683926167205794, w0=0.00029100856644336963, w1=-0.048732355189435285\n",
      "Gradient Descent(344/399): loss=0.306674375168811, w0=0.00029406368100803995, w1=-0.048784551038472394\n",
      "Gradient Descent(345/399): loss=0.3065100657501961, w0=0.00029714039494072627, w1=-0.04883665172651734\n",
      "Gradient Descent(346/399): loss=0.3063463311420553, w0=0.0003002385752807084, w1=-0.048888657807806074\n",
      "Gradient Descent(347/399): loss=0.3061831690816442, w0=0.00030335809000948835, w1=-0.04894056983095652\n",
      "Gradient Descent(348/399): loss=0.30602057731752413, w0=0.0003064988080456313, w1=-0.04899238833903686\n",
      "Gradient Descent(349/399): loss=0.3058585536094564, w0=0.00030966059923955117, w1=-0.04904411386963288\n",
      "Gradient Descent(350/399): loss=0.30569709572829984, w0=0.00031284333436824364, w1=-0.049095746954914476\n",
      "Gradient Descent(351/399): loss=0.3055362014559071, w0=0.00031604688512997095, w1=-0.049147288121701216\n",
      "Gradient Descent(352/399): loss=0.3053758685850262, w0=0.00031927112413890075, w1=-0.04919873789152704\n",
      "Gradient Descent(353/399): loss=0.3052160949192012, w0=0.0003225159249197025, w1=-0.04925009678070412\n",
      "Gradient Descent(354/399): loss=0.3050568782726767, w0=0.0003257811619021047, w1=-0.049301365300385834\n",
      "Gradient Descent(355/399): loss=0.30489821647030213, w0=0.0003290667104154153, w1=-0.04935254395662892\n",
      "Gradient Descent(356/399): loss=0.3047401073474391, w0=0.00033237244668300845, w1=-0.04940363325045482\n",
      "Gradient Descent(357/399): loss=0.30458254874986973, w0=0.0003356982478167808, w1=-0.04945463367791017\n",
      "Gradient Descent(358/399): loss=0.3044255385337066, w0=0.00033904399181157853, w1=-0.04950554573012655\n",
      "Gradient Descent(359/399): loss=0.30426907456530433, w0=0.0003424095575395992, w1=-0.049556369893379366\n",
      "Gradient Descent(360/399): loss=0.3041131547211728, w0=0.0003457948247447701, w1=-0.049607106649146054\n",
      "Gradient Descent(361/399): loss=0.3039577768878915, w0=0.00034919967403710577, w1=-0.04965775647416343\n",
      "Gradient Descent(362/399): loss=0.3038029389620259, w0=0.0003526239868870465, w1=-0.0497083198404843\n",
      "Gradient Descent(363/399): loss=0.3036486388500441, w0=0.00035606764561978095, w1=-0.049758797215533404\n",
      "Gradient Descent(364/399): loss=0.3034948744682363, w0=0.00035953053340955366, w1=-0.04980918906216252\n",
      "Gradient Descent(365/399): loss=0.3033416437426348, w0=0.0003630125342739612, w1=-0.049859495838704915\n",
      "Gradient Descent(366/399): loss=0.30318894460893503, w0=0.00036651353306823734, w1=-0.04990971799902906\n",
      "Gradient Descent(367/399): loss=0.30303677501241877, w0=0.00037003341547953047, w1=-0.04995985599259166\n",
      "Gradient Descent(368/399): loss=0.30288513290787744, w0=0.0003735720680211741, w1=-0.05000991026448996\n",
      "Gradient Descent(369/399): loss=0.30273401625953833, w0=0.0003771293780269531, w1=-0.05005988125551342\n",
      "Gradient Descent(370/399): loss=0.3025834230409894, w0=0.0003807052336453664, w1=-0.05010976940219467\n",
      "Gradient Descent(371/399): loss=0.302433351235109, w0=0.0003842995238338894, w1=-0.050159575136859834\n",
      "Gradient Descent(372/399): loss=0.3022837988339921, w0=0.0003879121383532356, w1=-0.05020929888767817\n",
      "Gradient Descent(373/399): loss=0.30213476383888227, w0=0.0003915429677616208, w1=-0.05025894107871114\n",
      "Gradient Descent(374/399): loss=0.3019862442601009, w0=0.0003951919034090308, w1=-0.05030850212996074\n",
      "Gradient Descent(375/399): loss=0.3018382381169805, w0=0.00039885883743149306, w1=-0.05035798245741729\n",
      "Gradient Descent(376/399): loss=0.30169074343779756, w0=0.0004025436627453555, w1=-0.05040738247310657\n",
      "Gradient Descent(377/399): loss=0.3015437582597046, w0=0.0004062462730415723, w1=-0.05045670258513637\n",
      "Gradient Descent(378/399): loss=0.30139728062866855, w0=0.0004099665627799986, w1=-0.05050594319774242\n",
      "Gradient Descent(379/399): loss=0.3012513085994042, w0=0.00041370442718369514, w1=-0.05055510471133373\n",
      "Gradient Descent(380/399): loss=0.3011058402353123, w0=0.0004174597622332444, w1=-0.05060418752253739\n",
      "Gradient Descent(381/399): loss=0.30096087360841695, w0=0.0004212324646610787, w1=-0.050653192024242726\n",
      "Gradient Descent(382/399): loss=0.30081640679930505, w0=0.00042502243194582204, w1=-0.050702118605644955\n",
      "Gradient Descent(383/399): loss=0.30067243789706566, w0=0.00042882956230664616, w1=-0.050750967652288245\n",
      "Gradient Descent(384/399): loss=0.3005289649992303, w0=0.00043265375469764245, w1=-0.05079973954610824\n",
      "Gradient Descent(385/399): loss=0.3003859862117153, w0=0.00043649490880221, w1=-0.050848434665474035\n",
      "Gradient Descent(386/399): loss=0.30024349964876307, w0=0.00044035292502746107, w1=-0.05089705338522964\n",
      "Gradient Descent(387/399): loss=0.30010150343288594, w0=0.0004442277044986449, w1=-0.050945596076734864\n",
      "Gradient Descent(388/399): loss=0.29995999569481013, w0=0.00044811914905359073, w1=-0.050994063107905746\n",
      "Gradient Descent(389/399): loss=0.29981897457341966, w0=0.0004520271612371706, w1=-0.051042454843254426\n",
      "Gradient Descent(390/399): loss=0.29967843821570284, w0=0.00045595164429578286, w1=-0.05109077164392851\n",
      "Gradient Descent(391/399): loss=0.29953838477669825, w0=0.00045989250217185726, w1=-0.05113901386774998\n",
      "Gradient Descent(392/399): loss=0.29939881241944083, w0=0.00046384963949838205, w1=-0.05118718186925357\n",
      "Gradient Descent(393/399): loss=0.29925971931491085, w0=0.00046782296159345407, w1=-0.05123527599972466\n",
      "Gradient Descent(394/399): loss=0.2991211036419814, w0=0.00047181237445485216, w1=-0.05128329660723672\n",
      "Gradient Descent(395/399): loss=0.2989829635873683, w0=0.000475817784754635, w1=-0.05133124403668825\n",
      "Gradient Descent(396/399): loss=0.29884529734557874, w0=0.0004798390998337631, w1=-0.05137911862983927\n",
      "Gradient Descent(397/399): loss=0.29870810311886276, w0=0.00048387622769674666, w1=-0.05142692072534737\n",
      "Gradient Descent(398/399): loss=0.2985713791171639, w0=0.0004879290770063188, w1=-0.051474650658803266\n",
      "Gradient Descent(399/399): loss=0.2984351235580708, w0=0.0004919975570781353, w1=-0.05152230876276592\n",
      "++++ gamma = 0.00517947467923\n",
      "ciaociaociao\n",
      "Gradient Descent(0/399): loss=0.4999999999999887, w0=-3.567224532989568e-06, w1=-0.0018569058751049976\n",
      "Gradient Descent(1/399): loss=0.4883668541005887, w0=9.253030379499552e-06, w1=-0.0035775824179173207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(2/399): loss=0.47834437469355146, w0=3.415165544540191e-05, w1=-0.005176049378553354\n",
      "Gradient Descent(3/399): loss=0.4696315076044244, w0=6.770359796760337e-05, w1=-0.0066646134290960805\n",
      "Gradient Descent(4/399): loss=0.4619913027012247, w0=0.00010719263141369933, w1=-0.008054109915891864\n",
      "Gradient Descent(5/399): loss=0.45523583755314584, w0=0.00015047184424682138, w1=-0.009354105471537366\n",
      "Gradient Descent(6/399): loss=0.4492150582008624, w0=0.0001958522698058043, w1=-0.01057306863767165\n",
      "Gradient Descent(7/399): loss=0.44380840680806916, w0=0.0002420137410195521, w1=-0.011718514218699079\n",
      "Gradient Descent(8/399): loss=0.4389184664311481, w0=0.00028793334301212975, w1=-0.012797125959455407\n",
      "Gradient Descent(9/399): loss=0.4344660887258289, w0=0.0003328278379170281, w1=-0.013814861249750742\n",
      "Gradient Descent(10/399): loss=0.43038662703950703, w0=0.00037610721478760126, w1=-0.014777040853834234\n",
      "Gradient Descent(11/399): loss=0.42662700338006165, w0=0.00041733712374311693, w1=-0.015688426102876702\n",
      "Gradient Descent(12/399): loss=0.423143410886346, w0=0.00045620842629664094, w1=-0.01655328554227884\n",
      "Gradient Descent(13/399): loss=0.4198995047941295, w0=0.0004925124631777257, w1=-0.01737545266864721\n",
      "Gradient Descent(14/399): loss=0.4168649716067079, w0=0.0005261209300641235, w1=-0.018158376104677576\n",
      "Gradient Descent(15/399): loss=0.4140143928483806, w0=0.0005569694783816775, w1=-0.01890516332915399\n",
      "Gradient Descent(16/399): loss=0.4113263394336042, w0=0.000585044336569621, w1=-0.019618618892253487\n",
      "Gradient Descent(17/399): loss=0.4087826473552284, w0=0.0006103713876578507, w1=-0.020301277894293025\n",
      "Gradient Descent(18/399): loss=0.40636783646602936, w0=0.0006330072499652857, w1=-0.020955435381870967\n",
      "Gradient Descent(19/399): loss=0.4040686425596537, w0=0.0006530319956408487, w1=-0.021583172213462284\n",
      "Gradient Descent(20/399): loss=0.40187363942890086, w0=0.0006705432116268428, w1=-0.022186377862530516\n",
      "Gradient Descent(21/399): loss=0.3997729325786251, w0=0.0006856511633109955, w1=-0.022766770556649343\n",
      "Gradient Descent(22/399): loss=0.3977579101529839, w0=0.0006984748656694679, w1=-0.023325915093232886\n",
      "Gradient Descent(23/399): loss=0.3958210396651743, w0=0.0007091389024433436, w1=-0.02386523862407215\n",
      "Gradient Descent(24/399): loss=0.3939557014887405, w0=0.000717770862673853, w1=-0.0243860446602242\n",
      "Gradient Descent(25/399): loss=0.39215605193154235, w0=0.0007244992871846638, w1=-0.024889525514507668\n",
      "Gradient Descent(26/399): loss=0.39041691017969793, w0=0.0007294520364699419, w1=-0.02537677336980491\n",
      "Gradient Descent(27/399): loss=0.3887336645560852, w0=0.0007327550068098301, w1=-0.025848790136656183\n",
      "Gradient Descent(28/399): loss=0.3871021944533084, w0=0.000734531133987646, w1=-0.026306496242524666\n",
      "Gradient Descent(29/399): loss=0.3855188050263907, w0=0.0007348996342759269, w1=-0.026750738477020617\n",
      "Gradient Descent(30/399): loss=0.38398017230628945, w0=0.0007339754408273456, w1=-0.027182297001813467\n",
      "Gradient Descent(31/399): loss=0.38248329685325966, w0=0.0007318688005977162, w1=-0.027601891620535786\n",
      "Gradient Descent(32/399): loss=0.38102546443387364, w0=0.0007286850027177318, w1=-0.028010187392365177\n",
      "Gradient Descent(33/399): loss=0.3796042124965765, w0=0.0007245242140383703, w1=-0.02840779966288865\n",
      "Gradient Descent(34/399): loss=0.3782173014533114, w0=0.0007194814015791465, w1=-0.028795298577083524\n",
      "Gradient Descent(35/399): loss=0.37686268996104233, w0=0.0007136463249512494, w1=-0.02917321313160151\n",
      "Gradient Descent(36/399): loss=0.3755385135464342, w0=0.000707103584624435, w1=-0.029542034816860168\n",
      "Gradient Descent(37/399): loss=0.37424306603706703, w0=0.0006999327142510163, w1=-0.029902220893595625\n",
      "Gradient Descent(38/399): loss=0.37297478335928497, w0=0.0006922083072287544, w1=-0.03025419734339959\n",
      "Gradient Descent(39/399): loss=0.37173222934085304, w0=0.0006840001693393866, w1=-0.03059836152825659\n",
      "Gradient Descent(40/399): loss=0.3705140832197428, w0=0.0006753734906922708, w1=-0.03093508459013251\n",
      "Gradient Descent(41/399): loss=0.36931912861157634, w0=0.00066638903137552, w1=-0.03126471361817306\n",
      "Gradient Descent(42/399): loss=0.36814624372988636, w0=0.0006571033162051087, w1=-0.031587573607990575\n",
      "Gradient Descent(43/399): loss=0.36699439268728057, w0=0.0006475688347948879, w1=-0.03190396923479815\n",
      "Gradient Descent(44/399): loss=0.3658626177333409, w0=0.0006378342438715365, w1=-0.0322141864597468\n",
      "Gradient Descent(45/399): loss=0.3647500323078338, w0=0.000627944569348525, w1=-0.032518493986695034\n",
      "Gradient Descent(46/399): loss=0.36365581480652065, w0=0.0006179414061692354, w1=-0.03281714458475804\n",
      "Gradient Descent(47/399): loss=0.36257920297230073, w0=0.000607863114345798, w1=-0.03311037629031578\n",
      "Gradient Descent(48/399): loss=0.36151948883721285, w0=0.0005977450099691071, w1=-0.0333984135006802\n",
      "Gradient Descent(49/399): loss=0.36047601415146396, w0=0.0005876195502570712, w1=-0.033681467970309534\n",
      "Gradient Descent(50/399): loss=0.35944816624451864, w0=0.0005775165119511316, w1=-0.033959739719292496\n",
      "Gradient Descent(51/399): loss=0.3584353742707329, w0=0.0005674631625728084, w1=-0.03423341786278975\n",
      "Gradient Descent(52/399): loss=0.35743710579826693, w0=0.0005574844242187543, w1=-0.03450268136919989\n",
      "Gradient Descent(53/399): loss=0.35645286370530127, w0=0.0005476030297098558, w1=-0.03476769975399852\n",
      "Gradient Descent(54/399): loss=0.3554821833520581, w0=0.0005378396710218555, w1=-0.03502863371547044\n",
      "Gradient Descent(55/399): loss=0.35452463000096335, w0=0.0005282131400156405, w1=-0.03528563571790648\n",
      "Gradient Descent(56/399): loss=0.35357979646053955, w0=0.0005187404615580782, w1=-0.03553885052725835\n",
      "Gradient Descent(57/399): loss=0.3526473009314383, w0=0.0005094370191818738, w1=-0.035788415703730064\n",
      "Gradient Descent(58/399): loss=0.3517267850354393, w0=0.0005003166734778092, w1=-0.03603446205532489\n",
      "Gradient Descent(59/399): loss=0.3508179120103466, w0=0.0004913918734469289, w1=-0.03627711405595719\n",
      "Gradient Descent(60/399): loss=0.34992036505553203, w0=0.00048267376106554514, w1=-0.0365164902313725\n",
      "Gradient Descent(61/399): loss=0.34903384581447755, w0=0.0004741722693338108, w1=-0.036752703515792504\n",
      "Gradient Descent(62/399): loss=0.3481580729820627, w0=0.00046589621409034846, w1=-0.03698586158190968\n",
      "Gradient Descent(63/399): loss=0.3472927810255715, w0=0.00045785337988209766, w1=-0.03721606714659531\n",
      "Gradient Descent(64/399): loss=0.34643771900948134, w0=0.00045005060018107906, w1=-0.03744341825445112\n",
      "Gradient Descent(65/399): loss=0.345592649515055, w0=0.0004424938322389503, w1=-0.037668008541125904\n",
      "Gradient Descent(66/399): loss=0.34475734764661203, w0=0.00043518822686670726, w1=-0.037889927478131406\n",
      "Gradient Descent(67/399): loss=0.34393160011711743, w0=0.00042813819342122625, w1=-0.03810926060072413\n",
      "Gradient Descent(68/399): loss=0.34311520440639837, w0=0.00042134746027301515, w1=-0.03832608972026955\n",
      "Gradient Descent(69/399): loss=0.3423079679859199, w0=0.0004148191310209448, w1=-0.038540493122370484\n",
      "Gradient Descent(70/399): loss=0.3415097076045841, w0=0.00040855573671019367, w1=-0.03875254575192035\n",
      "Gradient Descent(71/399): loss=0.34072024863051725, w0=0.0004025592842994345, w1=-0.03896231938613372\n",
      "Gradient Descent(72/399): loss=0.33993942444424735, w0=0.0003968313016126559, w1=-0.03916988279650868\n",
      "Gradient Descent(73/399): loss=0.3391670758790795, w0=0.00039137287900012336, w1=-0.03937530190058823\n",
      "Gradient Descent(74/399): loss=0.3384030507048298, w0=0.00038618470792201005, w1=-0.039578639904308655\n",
      "Gradient Descent(75/399): loss=0.3376472031514176, w0=0.0003812671166572813, w1=-0.03977995743565204\n",
      "Gradient Descent(76/399): loss=0.33689939346910297, w0=0.0003766201033296059, w1=-0.03997931267025611\n",
      "Gradient Descent(77/399): loss=0.33615948752243296, w0=0.0003722433664314724, w1=-0.0401767614495767\n",
      "Gradient Descent(78/399): loss=0.3354273564152034, w0=0.00036813633301736615, w1=-0.040372357392146424\n",
      "Gradient Descent(79/399): loss=0.3347028761439689, w0=0.00036429818472686785, w1=-0.04056615199842587\n",
      "Gradient Descent(80/399): loss=0.3339859272778338, w0=0.00036072788178889595, w1=-0.04075819474970146\n",
      "Gradient Descent(81/399): loss=0.33327639466244774, w0=0.00035742418514906005, w1=-0.04094853320144548\n",
      "Gradient Descent(82/399): loss=0.3325741671462992, w0=0.00035438567685324064, w1=-0.04113721307151942\n",
      "Gradient Descent(83/399): loss=0.3318791373275483, w0=0.0003516107788120638, w1=-0.041324278323569934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(84/399): loss=0.3311912013197968, w0=0.00034909777006290915, w1=-0.041509771245938534\n",
      "Gradient Descent(85/399): loss=0.330510258535307, w0=0.00034684480263846563, w1=-0.04169373252638018\n",
      "Gradient Descent(86/399): loss=0.3298362114843145, w0=0.00034484991614363913, w1=-0.04187620132286242\n",
      "Gradient Descent(87/399): loss=0.32916896558917697, w0=0.0003431110511357961, w1=-0.042057215330695466\n",
      "Gradient Descent(88/399): loss=0.32850842901221383, w0=0.0003416260613969001, w1=-0.04223681084622423\n",
      "Gradient Descent(89/399): loss=0.3278545124961716, w0=0.00034039272518004396, w1=-0.04241502282729554\n",
      "Gradient Descent(90/399): loss=0.32720712921633976, w0=0.0003394087555071922, w1=-0.0425918849506979\n",
      "Gradient Descent(91/399): loss=0.32656619464342285, w0=0.00033867180958959943, w1=-0.042767429666756174\n",
      "Gradient Descent(92/399): loss=0.325931626416337, w0=0.0003381794974373652, w1=-0.04294168825125056\n",
      "Gradient Descent(93/399): loss=0.3253033442241705, w0=0.00033792938971988754, w1=-0.043114690854816576\n",
      "Gradient Descent(94/399): loss=0.3246812696966079, w0=0.00033791902493458765, w1=-0.043286466549971855\n",
      "Gradient Descent(95/399): loss=0.32406532630216833, w0=0.00033814591593717, w1=-0.043457043375905245\n",
      "Gradient Descent(96/399): loss=0.3234554392536632, w0=0.00033860755588284945, w1=-0.04362644838115415\n",
      "Gradient Descent(97/399): loss=0.3228515354203255, w0=0.0003393014236243956, w1=-0.04379470766428764\n",
      "Gradient Descent(98/399): loss=0.3222535432460991, w0=0.0003402249886095067, w1=-0.04396184641270479\n",
      "Gradient Descent(99/399): loss=0.32166139267362753, w0=0.0003413757153169155, w1=-0.04412788893965043\n",
      "Gradient Descent(100/399): loss=0.32107501507350605, w0=0.00034275106726773275, w1=-0.044292858719543866\n",
      "Gradient Descent(101/399): loss=0.3204943431784025, w0=0.0003443485106458347, w1=-0.044456778421709885\n",
      "Gradient Descent(102/399): loss=0.3199193110216799, w0=0.0003461655175585966, w1=-0.04461966994259565\n",
      "Gradient Descent(103/399): loss=0.3193498538801821, w0=0.00034819956896693844, w1=-0.04478155443655186\n",
      "Gradient Descent(104/399): loss=0.3187859082208714, w0=0.00035044815731148264, w1=-0.044942452345251696\n",
      "Gradient Descent(105/399): loss=0.31822741165102963, w0=0.000352908788859612, w1=-0.04510238342581652\n",
      "Gradient Descent(106/399): loss=0.3176743028717564, w0=0.00035557898579634386, w1=-0.045261366777713155\n",
      "Gradient Descent(107/399): loss=0.3171265216345208, w0=0.00035845628808020335, w1=-0.0454194208684838\n",
      "Gradient Descent(108/399): loss=0.3165840087005388, w0=0.0003615382550836687, w1=-0.04557656355836579\n",
      "Gradient Descent(109/399): loss=0.31604670580276617, w0=0.0003648224670362679, w1=-0.0457328121238554\n",
      "Gradient Descent(110/399): loss=0.31551455561031894, w0=0.000368306526287022, w1=-0.04588818328026658\n",
      "Gradient Descent(111/399): loss=0.3149875016951339, w0=0.0003719880584016485, w1=-0.04604269320333264\n",
      "Gradient Descent(112/399): loss=0.3144654885007152, w0=0.00037586471310874805, w1=-0.04619635754989632\n",
      "Gradient Descent(113/399): loss=0.3139484613128065, w0=0.0003799341651080975, w1=-0.046349191477731116\n",
      "Gradient Descent(114/399): loss=0.3134363662318508, w0=0.00038419411475315097, w1=-0.046501209664534235\n",
      "Gradient Descent(115/399): loss=0.31292915014711015, w0=0.00038864228861891086, w1=-0.04665242632612973\n",
      "Gradient Descent(116/399): loss=0.31242676071232256, w0=0.0003932764399654497, w1=-0.04680285523391794\n",
      "Gradient Descent(117/399): loss=0.3119291463227859, w0=0.00039809434910656217, w1=-0.04695250973160561\n",
      "Gradient Descent(118/399): loss=0.3114362560937656, w0=0.000403093823692274, w1=-0.04710140275124942\n",
      "Gradient Descent(119/399): loss=0.3109480398401335, w0=0.0004082726989132439, w1=-0.04724954682864351\n",
      "Gradient Descent(120/399): loss=0.31046444805714746, w0=0.0004136288376344547, w1=-0.04739695411808069\n",
      "Gradient Descent(121/399): loss=0.3099854319022921, w0=0.0004191601304649985, w1=-0.047543636406514767\n",
      "Gradient Descent(122/399): loss=0.309510943178106, w0=0.00042486449577021205, w1=-0.04768960512715074\n",
      "Gradient Descent(123/399): loss=0.3090409343159242, w0=0.0004307398796319147, w1=-0.047834871372487736\n",
      "Gradient Descent(124/399): loss=0.308575358360475, w0=0.00043678425576203466, w1=-0.04797944590683872\n",
      "Gradient Descent(125/399): loss=0.3081141689552676, w0=0.00044299562537447476, w1=-0.048123339178349565\n",
      "Gradient Descent(126/399): loss=0.30765732032871884, w0=0.00044937201701967426, w1=-0.0482665613305392\n",
      "Gradient Descent(127/399): loss=0.30720476728096746, w0=0.000455911486385952, w1=-0.048409122213381305\n",
      "Gradient Descent(128/399): loss=0.30675646517132743, w0=0.0004626121160713778, w1=-0.04855103139394722\n",
      "Gradient Descent(129/399): loss=0.3063123699063391, w0=0.00046947201532960637, w1=-0.04869229816662868\n",
      "Gradient Descent(130/399): loss=0.3058724379283745, w0=0.000476489319792815, w1=-0.048832931562958225\n",
      "Gradient Descent(131/399): loss=0.30543662620476386, w0=0.00048366219117462365, w1=-0.04897294036104416\n",
      "Gradient Descent(132/399): loss=0.3050048922174044, w0=0.0004909888169556267, w1=-0.0491123330946363\n",
      "Gradient Descent(133/399): loss=0.3045771939528219, w0=0.0004984674100539379, w1=-0.04925111806183796\n",
      "Gradient Descent(134/399): loss=0.3041534898926542, w0=0.0005060962084829447, w1=-0.049389303333478844\n",
      "Gradient Descent(135/399): loss=0.30373373900453066, w0=0.0005138734749982689, w1=-0.04952689676116294\n",
      "Gradient Descent(136/399): loss=0.3033179007333189, w0=0.0005217974967357576, w1=-0.049663905985004864\n",
      "Gradient Descent(137/399): loss=0.30290593499271923, w0=0.0005298665848421625, w1=-0.04980033844106743\n",
      "Gradient Descent(138/399): loss=0.3024978021571809, w0=0.0005380790741000137, w1=-0.04993620136851279\n",
      "Gradient Descent(139/399): loss=0.30209346305412255, w0=0.0005464333225480572, w1=-0.05007150181647872\n",
      "Gradient Descent(140/399): loss=0.30169287895643726, w0=0.000554927711098495, w1=-0.05020624665069143\n",
      "Gradient Descent(141/399): loss=0.3012960115752626, w0=0.0005635606431521503, w1=-0.05034044255982541\n",
      "Gradient Descent(142/399): loss=0.30090282305300475, w0=0.0005723305442125728, w1=-0.050474096061620746\n",
      "Gradient Descent(143/399): loss=0.3005132759565939, w0=0.000581235861499996, w1=-0.05060721350876756\n",
      "Gradient Descent(144/399): loss=0.3001273332709636, w0=0.000590275063565971, w1=-0.050739801094567\n",
      "Gradient Descent(145/399): loss=0.29974495839273807, w0=0.000599446639909415, w1=-0.050871864858377795\n",
      "Gradient Descent(146/399): loss=0.2993661151241155, w0=0.0006087491005947346, w1=-0.051003410690856864\n",
      "Gradient Descent(147/399): loss=0.298990767666936, w0=0.0006181809758726156, w1=-0.05113444433900231\n",
      "Gradient Descent(148/399): loss=0.2986188806169246, w0=0.0006277408158040037, w1=-0.05126497141100662\n",
      "Gradient Descent(149/399): loss=0.29825041895809834, w0=0.0006374271898877417, w1=-0.051394997380927544\n",
      "Gradient Descent(150/399): loss=0.29788534805733047, w0=0.0006472386866922746, w1=-0.05152452759318408\n",
      "Gradient Descent(151/399): loss=0.2975236336590603, w0=0.0006571739134917832, w1=-0.051653567266884234\n",
      "Gradient Descent(152/399): loss=0.2971652418801435, w0=0.0006672314959070613, w1=-0.05178212149999138\n",
      "Gradient Descent(153/399): loss=0.29681013920483323, w0=0.000677410077551409, w1=-0.05191019527333548\n",
      "Gradient Descent(154/399): loss=0.29645829247988975, w0=0.0006877083196817785, w1=-0.05203779345447528\n",
      "Gradient Descent(155/399): loss=0.2961096689098046, w0=0.0006981249008553713, w1=-0.05216492080141735\n",
      "Gradient Descent(156/399): loss=0.2957642360521413, w0=0.0007086585165918545, w1=-0.05229158196619751\n",
      "Gradient Descent(157/399): loss=0.2954219618129826, w0=0.0007193078790413372, w1=-0.05241778149833006\n",
      "Gradient Descent(158/399): loss=0.29508281444247897, w0=0.0007300717166582171, w1=-0.05254352384812998\n",
      "Gradient Descent(159/399): loss=0.2947467625304967, w0=0.000740948773880988, w1=-0.052668813369912935\n",
      "Gradient Descent(160/399): loss=0.29441377500235705, w0=0.0007519378108180736, w1=-0.05279365432507798\n",
      "Gradient Descent(161/399): loss=0.2940838211146667, w0=0.0007630376029397364, w1=-0.05291805088507736\n",
      "Gradient Descent(162/399): loss=0.29375687045123056, w0=0.0007742469407760886, w1=-0.05304200713427788\n",
      "Gradient Descent(163/399): loss=0.2934328929190481, w0=0.0007855646296212205, w1=-0.053165527072717975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(164/399): loss=0.29311185874438606, w0=0.0007969894892434426, w1=-0.05328861461876448\n",
      "Gradient Descent(165/399): loss=0.2927937384689275, w0=0.0008085203536016287, w1=-0.05341127361167301\n",
      "Gradient Descent(166/399): loss=0.29247850294599004, w0=0.0008201560705676315, w1=-0.053533507814055636\n",
      "Gradient Descent(167/399): loss=0.29216612333681574, w0=0.0008318955016547366, w1=-0.05365532091425933\n",
      "Gradient Descent(168/399): loss=0.29185657110692526, w0=0.0008437375217521051, w1=-0.05377671652865872\n",
      "Gradient Descent(169/399): loss=0.2915498180225368, w0=0.0008556810188651536, w1=-0.053897698203866376\n",
      "Gradient Descent(170/399): loss=0.2912458361470453, w0=0.0008677248938618064, w1=-0.05401826941886366\n",
      "Gradient Descent(171/399): loss=0.29094459783756255, w0=0.0008798680602245528, w1=-0.05413843358705538\n",
      "Gradient Descent(172/399): loss=0.2906460757415126, w0=0.0008921094438082356, w1=-0.054258194058250955\n",
      "Gradient Descent(173/399): loss=0.2903502427932837, w0=0.0009044479826034897, w1=-0.05437755412057494\n",
      "Gradient Descent(174/399): loss=0.2900570722109327, w0=0.0009168826265057491, w1=-0.05449651700230967\n",
      "Gradient Descent(175/399): loss=0.28976653749294246, w0=0.0009294123370897323, w1=-0.05461508587367242\n",
      "Gradient Descent(176/399): loss=0.2894786124150269, w0=0.0009420360873893178, w1=-0.05473326384852973\n",
      "Gradient Descent(177/399): loss=0.2891932710269874, w0=0.000954752861682716, w1=-0.05485105398605112\n",
      "Gradient Descent(178/399): loss=0.288910487649614, w0=0.0009675616552828398, w1=-0.05496845929230459\n",
      "Gradient Descent(179/399): loss=0.28863023687163325, w0=0.0009804614743327807, w1=-0.05508548272179601\n",
      "Gradient Descent(180/399): loss=0.28835249354670067, w0=0.0009934513356062867, w1=-0.05520212717895455\n",
      "Gradient Descent(181/399): loss=0.28807723279043557, w0=0.0010065302663131482, w1=-0.05531839551956616\n",
      "Gradient Descent(182/399): loss=0.2878044299774981, w0=0.001019697303909386, w1=-0.05543429055215705\n",
      "Gradient Descent(183/399): loss=0.2875340607387083, w0=0.0010329514959121434, w1=-0.05554981503932902\n",
      "Gradient Descent(184/399): loss=0.28726610095820276, w0=0.0010462918997191835, w1=-0.05566497169904844\n",
      "Gradient Descent(185/399): loss=0.28700052677063237, w0=0.0010597175824328844, w1=-0.05577976320589064\n",
      "Gradient Descent(186/399): loss=0.28673731455839657, w0=0.0010732276206886398, w1=-0.05589419219224135\n",
      "Gradient Descent(187/399): loss=0.2864764409489149, w0=0.001086821100487558, w1=-0.05600826124945672\n",
      "Gradient Descent(188/399): loss=0.2862178828119356, w0=0.001100497117033364, w1=-0.05612197292898359\n",
      "Gradient Descent(189/399): loss=0.2859616172568773, w0=0.0011142547745734046, w1=-0.056235329743441344\n",
      "Gradient Descent(190/399): loss=0.285707621630208, w0=0.0011280931862436576, w1=-0.05634833416766685\n",
      "Gradient Descent(191/399): loss=0.2854558735128561, w0=0.0011420114739176513, w1=-0.05646098863972387\n",
      "Gradient Descent(192/399): loss=0.2852063507176545, w0=0.0011560087680591955, w1=-0.05657329556187812\n",
      "Gradient Descent(193/399): loss=0.28495903128681765, w0=0.0011700842075788324, w1=-0.05668525730153944\n",
      "Gradient Descent(194/399): loss=0.2847138934894507, w0=0.001184236939693912, w1=-0.05679687619217207\n",
      "Gradient Descent(195/399): loss=0.2844709158190888, w0=0.0011984661197922027, w1=-0.05690815453417439\n",
      "Gradient Descent(196/399): loss=0.2842300769912672, w0=0.001212770911298944, w1=-0.0570190945957291\n",
      "Gradient Descent(197/399): loss=0.2839913559411218, w0=0.0012271504855472573, w1=-0.057129698613625046\n",
      "Gradient Descent(198/399): loss=0.2837547318210183, w0=0.0012416040216518221, w1=-0.0572399687940516\n",
      "Gradient Descent(199/399): loss=0.283520183998211, w0=0.0012561307063857358, w1=-0.057349907313366685\n",
      "Gradient Descent(200/399): loss=0.2832876920525292, w0=0.0012707297340604727, w1=-0.0574595163188394\n",
      "Gradient Descent(201/399): loss=0.2830572357740915, w0=0.0012854003064088594, w1=-0.05756879792936811\n",
      "Gradient Descent(202/399): loss=0.28282879516104803, w0=0.001300141632470985, w1=-0.05767775423617493\n",
      "Gradient Descent(203/399): loss=0.2826023504173492, w0=0.0013149529284829696, w1=-0.0577863873034775\n",
      "Gradient Descent(204/399): loss=0.2823778819505402, w0=0.0013298334177685117, w1=-0.05789469916913879\n",
      "Gradient Descent(205/399): loss=0.28215537036958277, w0=0.001344782330633139, w1=-0.05800269184529575\n",
      "Gradient Descent(206/399): loss=0.2819347964827019, w0=0.0013597989042610904, w1=-0.05811036731896767\n",
      "Gradient Descent(207/399): loss=0.281716141295257, w0=0.0013748823826147552, w1=-0.05821772755264477\n",
      "Gradient Descent(208/399): loss=0.28149938600763963, w0=0.0013900320163365996, w1=-0.05832477448485798\n",
      "Gradient Descent(209/399): loss=0.28128451201319316, w0=0.0014052470626535114, w1=-0.05843151003073041\n",
      "Gradient Descent(210/399): loss=0.2810715008961592, w0=0.0014205267852834946, w1=-0.05853793608251118\n",
      "Gradient Descent(211/399): loss=0.2808603344296452, w0=0.0014358704543446514, w1=-0.058644054510092344\n",
      "Gradient Descent(212/399): loss=0.2806509945736174, w0=0.001451277346266383, w1=-0.05874986716150942\n",
      "Gradient Descent(213/399): loss=0.28044346347291477, w0=0.0014667467437027494, w1=-0.058855375863426124\n",
      "Gradient Descent(214/399): loss=0.2802377234552875, w0=0.0014822779354479285, w1=-0.05896058242160391\n",
      "Gradient Descent(215/399): loss=0.28003375702945676, w0=0.0014978702163537098, w1=-0.05906548862135679\n",
      "Gradient Descent(216/399): loss=0.2798315468831958, w0=0.0015135228872489724, w1=-0.05917009622799206\n",
      "Gradient Descent(217/399): loss=0.27963107588143477, w0=0.0015292352548610837, w1=-0.05927440698723729\n",
      "Gradient Descent(218/399): loss=0.27943232706438453, w0=0.0015450066317391686, w1=-0.05937842262565416\n",
      "Gradient Descent(219/399): loss=0.27923528364568373, w0=0.0015608363361791931, w1=-0.05948214485103965\n",
      "Gradient Descent(220/399): loss=0.279039929010565, w0=0.001576723692150812, w1=-0.05958557535281485\n",
      "Gradient Descent(221/399): loss=0.27884624671404273, w0=0.0015926680292259287, w1=-0.05968871580240202\n",
      "Gradient Descent(222/399): loss=0.27865422047912025, w0=0.0016086686825089187, w1=-0.05979156785359022\n",
      "Gradient Descent(223/399): loss=0.2784638341950181, w0=0.0016247249925684677, w1=-0.05989413314288988\n",
      "Gradient Descent(224/399): loss=0.2782750719154207, w0=0.0016408363053709793, w1=-0.05999641328987677\n",
      "Gradient Descent(225/399): loss=0.2780879178567438, w0=0.0016570019722155055, w1=-0.06009840989752568\n",
      "Gradient Descent(226/399): loss=0.27790235639642014, w0=0.0016732213496701562, w1=-0.06020012455253423\n",
      "Gradient Descent(227/399): loss=0.27771837207120464, w0=0.001689493799509945, w1=-0.06030155882563708\n",
      "Gradient Descent(228/399): loss=0.27753594957549754, w0=0.0017058186886560304, w1=-0.0604027142719109\n",
      "Gradient Descent(229/399): loss=0.2773550737596875, w0=0.001722195389116309, w1=-0.06050359243107047\n",
      "Gradient Descent(230/399): loss=0.2771757296285114, w0=0.001738623277927326, w1=-0.06060419482775614\n",
      "Gradient Descent(231/399): loss=0.27699790233943333, w0=0.0017551017370974603, w1=-0.06070452297181295\n",
      "Gradient Descent(232/399): loss=0.2768215772010402, w0=0.0017716301535513487, w1=-0.0608045783585618\n",
      "Gradient Descent(233/399): loss=0.27664673967145603, w0=0.0017882079190755147, w1=-0.060904362469062806\n",
      "Gradient Descent(234/399): loss=0.27647337535677374, w0=0.0018048344302651646, w1=-0.06100387677037119\n",
      "Gradient Descent(235/399): loss=0.2763014700095019, w0=0.0018215090884721168, w1=-0.06110312271578595\n",
      "Gradient Descent(236/399): loss=0.27613100952703196, w0=0.0018382312997538338, w1=-0.0612021017450915\n",
      "Gradient Descent(237/399): loss=0.2759619799501191, w0=0.0018550004748235217, w1=-0.061300815284792635\n",
      "Gradient Descent(238/399): loss=0.275794367461381, w0=0.0018718160290012683, w1=-0.061399264748342904\n",
      "Gradient Descent(239/399): loss=0.27562815838381316, w0=0.0018886773821661885, w1=-0.06149745153636671\n",
      "Gradient Descent(240/399): loss=0.2754633391793197, w0=0.0019055839587095477, w1=-0.06159537703687534\n",
      "Gradient Descent(241/399): loss=0.2752998964472604, w0=0.0019225351874888354, w1=-0.06169304262547709\n",
      "Gradient Descent(242/399): loss=0.27513781692301353, w0=0.0019395305017827604, w1=-0.06179044966558168\n",
      "Gradient Descent(243/399): loss=0.27497708747655414, w0=0.0019565693392471417, w1=-0.06188759950859922\n",
      "Gradient Descent(244/399): loss=0.27481769511104814, w0=0.001973651141871669, w1=-0.06198449349413384\n",
      "Gradient Descent(245/399): loss=0.2746596269614606, w0=0.0019907753559375066, w1=-0.062081132950172174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(246/399): loss=0.27450287029318043, w0=0.002007941431975717, w1=-0.06217751919326687\n",
      "Gradient Descent(247/399): loss=0.27434741250065864, w0=0.002025148824726481, w1=-0.06227365352871533\n",
      "Gradient Descent(248/399): loss=0.27419324110606236, w0=0.0020423969930990905, w1=-0.06236953725073375\n",
      "Gradient Descent(249/399): loss=0.2740403437579425, w0=0.0020596854001326908, w1=-0.062465171642626736\n",
      "Gradient Descent(250/399): loss=0.27388870822991646, w0=0.0020770135129577535, w1=-0.06256055797695252\n",
      "Gradient Descent(251/399): loss=0.273738322419364, w0=0.0020943808027582577, w1=-0.06265569751568396\n",
      "Gradient Descent(252/399): loss=0.2735891743461394, w0=0.0021117867447345553, w1=-0.06275059151036556\n",
      "Gradient Descent(253/399): loss=0.2734412521512934, w0=0.0021292308180669087, w1=-0.06284524120226642\n",
      "Gradient Descent(254/399): loss=0.2732945440958137, w0=0.0021467125058796744, w1=-0.0629396478225295\n",
      "Gradient Descent(255/399): loss=0.2731490385593754, w0=0.0021642312952061173, w1=-0.06303381259231715\n",
      "Gradient Descent(256/399): loss=0.27300472403910614, w0=0.0021817866769538383, w1=-0.063127736722953\n",
      "Gradient Descent(257/399): loss=0.27286158914836467, w0=0.0021993781458707956, w1=-0.0632214214160606\n",
      "Gradient Descent(258/399): loss=0.2727196226155322, w0=0.002217005200511905, w1=-0.06331486786369843\n",
      "Gradient Descent(259/399): loss=0.27257881328281597, w0=0.0022346673432062014, w1=-0.06340807724849191\n",
      "Gradient Descent(260/399): loss=0.2724391501050676, w0=0.002252364080024545, w1=-0.0635010507437622\n",
      "Gradient Descent(261/399): loss=0.2723006221486117, w0=0.0022700949207478606, w1=-0.06359378951365188\n",
      "Gradient Descent(262/399): loss=0.27216321859008885, w0=0.0022878593788358895, w1=-0.06368629471324785\n",
      "Gradient Descent(263/399): loss=0.2720269287153092, w0=0.0023056569713964447, w1=-0.06377856748870124\n",
      "Gradient Descent(264/399): loss=0.2718917419181207, w0=0.0023234872191551527, w1=-0.0638706089773446\n",
      "Gradient Descent(265/399): loss=0.27175764769928656, w0=0.0023413496464256673, w1=-0.0639624203078065\n",
      "Gradient Descent(266/399): loss=0.2716246356653772, w0=0.0023592437810803467, w1=-0.06405400260012337\n",
      "Gradient Descent(267/399): loss=0.27149269552767175, w0=0.0023771691545213768, w1=-0.064145356965849\n",
      "Gradient Descent(268/399): loss=0.27136181710107377, w0=0.0023951253016523295, w1=-0.06423648450816152\n",
      "Gradient Descent(269/399): loss=0.27123199030303613, w0=0.0024131117608501475, w1=-0.06432738632196802\n",
      "Gradient Descent(270/399): loss=0.2711032051524982, w0=0.0024311280739375365, w1=-0.06441806349400687\n",
      "Gradient Descent(271/399): loss=0.27097545176883536, w0=0.0024491737861557623, w1=-0.0645085171029479\n",
      "Gradient Descent(272/399): loss=0.27084872037081786, w0=0.0024672484461378363, w1=-0.06459874821949033\n",
      "Gradient Descent(273/399): loss=0.27072300127558235, w0=0.0024853516058820804, w1=-0.06468875790645867\n",
      "Gradient Descent(274/399): loss=0.2705982848976132, w0=0.0025034828207260618, w1=-0.06477854721889659\n",
      "Gradient Descent(275/399): loss=0.27047456174773477, w0=0.0025216416493208872, w1=-0.06486811720415879\n",
      "Gradient Descent(276/399): loss=0.27035182243211575, w0=0.0025398276536058467, w1=-0.06495746890200105\n",
      "Gradient Descent(277/399): loss=0.2702300576512815, w0=0.002558040398783399, w1=-0.06504660334466827\n",
      "Gradient Descent(278/399): loss=0.27010925819913967, w0=0.0025762794532944878, w1=-0.06513552155698095\n",
      "Gradient Descent(279/399): loss=0.2699894149620135, w0=0.002594544388794182, w1=-0.06522422455641967\n",
      "Gradient Descent(280/399): loss=0.2698705189176879, w0=0.002612834780127631, w1=-0.06531271335320808\n",
      "Gradient Descent(281/399): loss=0.26975256113446344, w0=0.0026311502053063245, w1=-0.06540098895039413\n",
      "Gradient Descent(282/399): loss=0.2696355327702217, w0=0.0026494902454846547, w1=-0.06548905234392975\n",
      "Gradient Descent(283/399): loss=0.2695194250714996, w0=0.002667854484936766, w1=-0.06557690452274899\n",
      "Gradient Descent(284/399): loss=0.26940422937257497, w0=0.0026862425110336916, w1=-0.0656645464688446\n",
      "Gradient Descent(285/399): loss=0.2692899370945599, w0=0.0027046539142207646, w1=-0.06575197915734322\n",
      "Gradient Descent(286/399): loss=0.2691765397445046, w0=0.0027230882879953, w1=-0.0658392035565791\n",
      "Gradient Descent(287/399): loss=0.2690640289145112, w0=0.00274154522888454, w1=-0.06592622062816643\n",
      "Gradient Descent(288/399): loss=0.2689523962808562, w0=0.002760024336423854, w1=-0.06601303132707037\n",
      "Gradient Descent(289/399): loss=0.2688416336031222, w0=0.002778525213135192, w1=-0.06609963660167675\n",
      "Gradient Descent(290/399): loss=0.26873173272333906, w0=0.0027970474645057793, w1=-0.06618603739386049\n",
      "Gradient Descent(291/399): loss=0.26862268556513413, w0=0.002815590698967053, w1=-0.06627223463905277\n",
      "Gradient Descent(292/399): loss=0.26851448413289114, w0=0.002834154527873826, w1=-0.06635822926630706\n",
      "Gradient Descent(293/399): loss=0.26840712051091853, w0=0.002852738565483684, w1=-0.06644402219836398\n",
      "Gradient Descent(294/399): loss=0.26830058686262537, w0=0.002871342428936599, w1=-0.0665296143517149\n",
      "Gradient Descent(295/399): loss=0.26819487542970794, w0=0.0028899657382347654, w1=-0.06661500663666454\n",
      "Gradient Descent(296/399): loss=0.2680899785313423, w0=0.0029086081162226405, w1=-0.06670019995739257\n",
      "Gradient Descent(297/399): loss=0.2679858885633877, w0=0.0029272691885671985, w1=-0.06678519521201394\n",
      "Gradient Descent(298/399): loss=0.26788259799759706, w0=0.0029459485837383826, w1=-0.0668699932926384\n",
      "Gradient Descent(299/399): loss=0.26778009938083625, w0=0.0029646459329897555, w1=-0.06695459508542889\n",
      "Gradient Descent(300/399): loss=0.2676783853343113, w0=0.002983360870339345, w1=-0.06703900147065905\n",
      "Gradient Descent(301/399): loss=0.2675774485528035, w0=0.0030020930325506743, w1=-0.06712321332276973\n",
      "Gradient Descent(302/399): loss=0.26747728180391384, w0=0.003020842059113982, w1=-0.06720723151042457\n",
      "Gradient Descent(303/399): loss=0.26737787792731355, w0=0.00303960759222762, w1=-0.06729105689656478\n",
      "Gradient Descent(304/399): loss=0.2672792298340033, w0=0.0030583892767796273, w1=-0.06737469033846295\n",
      "Gradient Descent(305/399): loss=0.26718133050558157, w0=0.0030771867603294808, w1=-0.06745813268777602\n",
      "Gradient Descent(306/399): loss=0.26708417299351783, w0=0.003095999693090011, w1=-0.06754138479059747\n",
      "Gradient Descent(307/399): loss=0.2669877504184357, w0=0.0031148277279094885, w1=-0.06762444748750866\n",
      "Gradient Descent(308/399): loss=0.2668920559694028, w0=0.003133670520253868, w1=-0.0677073216136294\n",
      "Gradient Descent(309/399): loss=0.26679708290322823, w0=0.003152527728189196, w1=-0.06779000799866773\n",
      "Gradient Descent(310/399): loss=0.2667028245437667, w0=0.0031713990123641727, w1=-0.06787250746696899\n",
      "Gradient Descent(311/399): loss=0.26660927428123127, w0=0.003190284035992867, w1=-0.0679548208375641\n",
      "Gradient Descent(312/399): loss=0.2665164255715121, w0=0.0032091824648375804, w1=-0.06803694892421717\n",
      "Gradient Descent(313/399): loss=0.2664242719355028, w0=0.003228093967191861, w1=-0.06811889253547242\n",
      "Gradient Descent(314/399): loss=0.2663328069584338, w0=0.0032470182138636594, w1=-0.06820065247470032\n",
      "Gradient Descent(315/399): loss=0.2662420242892133, w0=0.003265954878158627, w1=-0.06828222954014324\n",
      "Gradient Descent(316/399): loss=0.26615191763977364, w0=0.003284903635863553, w1=-0.06836362452496025\n",
      "Gradient Descent(317/399): loss=0.2660624807844259, w0=0.0033038641652299366, w1=-0.06844483821727151\n",
      "Gradient Descent(318/399): loss=0.26597370755922106, w0=0.0033228361469576978, w1=-0.06852587140020178\n",
      "Gradient Descent(319/399): loss=0.2658855918613167, w0=0.003341819264179013, w1=-0.06860672485192354\n",
      "Gradient Descent(320/399): loss=0.2657981276483522, w0=0.003360813202442286, w1=-0.06868739934569941\n",
      "Gradient Descent(321/399): loss=0.2657113089378283, w0=0.0033798176496962398, w1=-0.06876789564992404\n",
      "Gradient Descent(322/399): loss=0.26562512980649544, w0=0.0033988322962741394, w1=-0.06884821452816538\n",
      "Gradient Descent(323/399): loss=0.26553958438974623, w0=0.0034178568348781336, w1=-0.06892835673920544\n",
      "Gradient Descent(324/399): loss=0.2654546668810167, w0=0.0034368909605637177, w1=-0.0690083230370805\n",
      "Gradient Descent(325/399): loss=0.26537037153119075, w0=0.0034559343707243153, w1=-0.06908811417112078\n",
      "Gradient Descent(326/399): loss=0.2652866926480142, w0=0.0034749867650759768, w1=-0.0691677308859896\n",
      "Gradient Descent(327/399): loss=0.26520362459551217, w0=0.0034940478456421927, w1=-0.06924717392172203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(328/399): loss=0.26512116179341444, w0=0.0035131173167388206, w1=-0.06932644401376303\n",
      "Gradient Descent(329/399): loss=0.2650392987165851, w0=0.003532194884959121, w1=-0.06940554189300517\n",
      "Gradient Descent(330/399): loss=0.26495802989446027, w0=0.0035512802591589052, w1=-0.06948446828582573\n",
      "Gradient Descent(331/399): loss=0.2648773499104895, w0=0.003570373150441791, w1=-0.0695632239141235\n",
      "Gradient Descent(332/399): loss=0.2647972534015846, w0=0.0035894732721445614, w1=-0.06964180949535505\n",
      "Gradient Descent(333/399): loss=0.26471773505757323, w0=0.0036085803398226337, w1=-0.06972022574257049\n",
      "Gradient Descent(334/399): loss=0.26463878962065857, w0=0.003627694071235626, w1=-0.06979847336444892\n",
      "Gradient Descent(335/399): loss=0.2645604118848851, w0=0.003646814186333028, w1=-0.06987655306533336\n",
      "Gradient Descent(336/399): loss=0.2644825966956089, w0=0.0036659404072399747, w1=-0.06995446554526527\n",
      "Gradient Descent(337/399): loss=0.2644053389489743, w0=0.0036850724582431133, w1=-0.07003221150001872\n",
      "Gradient Descent(338/399): loss=0.2643286335913957, w0=0.0037042100657765724, w1=-0.07010979162113405\n",
      "Gradient Descent(339/399): loss=0.2642524756190445, w0=0.003723352958408026, w1=-0.07018720659595123\n",
      "Gradient Descent(340/399): loss=0.26417686007734237, w0=0.0037425008668248516, w1=-0.07026445710764279\n",
      "Gradient Descent(341/399): loss=0.264101782060458, w0=0.0037616535238203836, w1=-0.07034154383524634\n",
      "Gradient Descent(342/399): loss=0.2640272367108113, w0=0.003780810664280258, w1=-0.07041846745369681\n",
      "Gradient Descent(343/399): loss=0.2639532192185806, w0=0.0037999720251688496, w1=-0.07049522863385821\n",
      "Gradient Descent(344/399): loss=0.2638797248212174, w0=0.0038191373455158, w1=-0.07057182804255509\n",
      "Gradient Descent(345/399): loss=0.2638067488029629, w0=0.0038383063664026314, w1=-0.07064826634260366\n",
      "Gradient Descent(346/399): loss=0.263734286494373, w0=0.0038574788309494535, w1=-0.0707245441928425\n",
      "Gradient Descent(347/399): loss=0.26366233327184585, w0=0.003876654484301754, w1=-0.07080066224816305\n",
      "Gradient Descent(348/399): loss=0.26359088455715557, w0=0.003895833073617276, w1=-0.07087662115953956\n",
      "Gradient Descent(349/399): loss=0.26351993581698974, w0=0.003915014348052982, w1=-0.07095242157405897\n",
      "Gradient Descent(350/399): loss=0.26344948256249323, w0=0.003934198058752099, w1=-0.07102806413495023\n",
      "Gradient Descent(351/399): loss=0.26337952034881507, w0=0.003953383958831252, w1=-0.07110354948161349\n",
      "Gradient Descent(352/399): loss=0.263310044774661, w0=0.003972571803367671, w1=-0.0711788782496488\n",
      "Gradient Descent(353/399): loss=0.26324105148185106, w0=0.003991761349386493, w1=-0.07125405107088471\n",
      "Gradient Descent(354/399): loss=0.2631725361548803, w0=0.004010952355848129, w1=-0.07132906857340639\n",
      "Gradient Descent(355/399): loss=0.26310449452048545, w0=0.004030144583635724, w1=-0.07140393138158352\n",
      "Gradient Descent(356/399): loss=0.26303692234721543, w0=0.004049337795542688, w1=-0.07147864011609792\n",
      "Gradient Descent(357/399): loss=0.26296981544500697, w0=0.004068531756260308, w1=-0.07155319539397086\n",
      "Gradient Descent(358/399): loss=0.2629031696647631, w0=0.0040877262323654375, w1=-0.07162759782859006\n",
      "Gradient Descent(359/399): loss=0.26283698089793794, w0=0.004106920992308262, w1=-0.07170184802973652\n",
      "Gradient Descent(360/399): loss=0.2627712450761247, w0=0.00412611580640014, w1=-0.07177594660361093\n",
      "Gradient Descent(361/399): loss=0.26270595817064746, w0=0.004145310446801523, w1=-0.07184989415285996\n",
      "Gradient Descent(362/399): loss=0.26264111619215913, w0=0.004164504687509944, w1=-0.07192369127660211\n",
      "Gradient Descent(363/399): loss=0.2625767151902412, w0=0.004183698304348084, w1=-0.07199733857045355\n",
      "Gradient Descent(364/399): loss=0.2625127512530092, w0=0.004202891074951908, w1=-0.07207083662655343\n",
      "Gradient Descent(365/399): loss=0.2624492205067218, w0=0.004222082778758882, w1=-0.07214418603358913\n",
      "Gradient Descent(366/399): loss=0.2623861191153941, w0=0.004241273196996251, w1=-0.07221738737682121\n",
      "Gradient Descent(367/399): loss=0.26232344328041424, w0=0.004260462112669395, w1=-0.07229044123810809\n",
      "Gradient Descent(368/399): loss=0.26226118924016484, w0=0.004279649310550253, w1=-0.07236334819593049\n",
      "Gradient Descent(369/399): loss=0.26219935326964827, w0=0.00429883457716582, w1=-0.07243610882541571\n",
      "Gradient Descent(370/399): loss=0.2621379316801154, w0=0.004318017700786709, w1=-0.07250872369836155\n",
      "Gradient Descent(371/399): loss=0.26207692081869804, w0=0.00433719847141579, w1=-0.07258119338326013\n",
      "Gradient Descent(372/399): loss=0.2620163170680462, w0=0.004356376680776883, w1=-0.07265351844532139\n",
      "Gradient Descent(373/399): loss=0.26195611684596737, w0=0.0043755521223035375, w1=-0.07272569944649641\n",
      "Gradient Descent(374/399): loss=0.2618963166050716, w0=0.004394724591127863, w1=-0.07279773694550053\n",
      "Gradient Descent(375/399): loss=0.2618369128324187, w0=0.004413893884069435, w1=-0.07286963149783617\n",
      "Gradient Descent(376/399): loss=0.26177790204916923, w0=0.004433059799624269, w1=-0.07294138365581555\n",
      "Gradient Descent(377/399): loss=0.26171928081024065, w0=0.00445222213795385, w1=-0.07301299396858314\n",
      "Gradient Descent(378/399): loss=0.26166104570396465, w0=0.00447138070087424, w1=-0.07308446298213786\n",
      "Gradient Descent(379/399): loss=0.26160319335174953, w0=0.004490535291845241, w1=-0.07315579123935517\n",
      "Gradient Descent(380/399): loss=0.26154572040774604, w0=0.004509685715959626, w1=-0.07322697928000892\n",
      "Gradient Descent(381/399): loss=0.26148862355851654, w0=0.004528831779932436, w1=-0.07329802764079292\n",
      "Gradient Descent(382/399): loss=0.2614318995227065, w0=0.004547973292090337, w1=-0.0733689368553425\n",
      "Gradient Descent(383/399): loss=0.2613755450507214, w0=0.004567110062361039, w1=-0.07343970745425567\n",
      "Gradient Descent(384/399): loss=0.2613195569244049, w0=0.004586241902262785, w1=-0.07351033996511423\n",
      "Gradient Descent(385/399): loss=0.2612639319567225, w0=0.004605368624893891, w1=-0.07358083491250464\n",
      "Gradient Descent(386/399): loss=0.26120866699144574, w0=0.004624490044922359, w1=-0.0736511928180387\n",
      "Gradient Descent(387/399): loss=0.26115375890284254, w0=0.004643605978575545, w1=-0.07372141420037406\n",
      "Gradient Descent(388/399): loss=0.2610992045953686, w0=0.004662716243629884, w1=-0.07379149957523459\n",
      "Gradient Descent(389/399): loss=0.2610450010033635, w0=0.004681820659400691, w1=-0.07386144945543045\n",
      "Gradient Descent(390/399): loss=0.26099114509074833, w0=0.0047009190467320015, w1=-0.07393126435087813\n",
      "Gradient Descent(391/399): loss=0.2609376338507276, w0=0.004720011227986488, w1=-0.07400094476862021\n",
      "Gradient Descent(392/399): loss=0.2608844643054945, w0=0.004739097027035429, w1=-0.07407049121284501\n",
      "Gradient Descent(393/399): loss=0.2608316335059381, w0=0.004758176269248737, w1=-0.07413990418490604\n",
      "Gradient Descent(394/399): loss=0.2607791385313537, w0=0.004777248781485048, w1=-0.0742091841833413\n",
      "Gradient Descent(395/399): loss=0.2607269764891579, w0=0.0047963143920818655, w1=-0.07427833170389238\n",
      "Gradient Descent(396/399): loss=0.2606751445146038, w0=0.004815372930845767, w1=-0.07434734723952345\n",
      "Gradient Descent(397/399): loss=0.2606236397705016, w0=0.004834424229042663, w1=-0.07441623128044006\n",
      "Gradient Descent(398/399): loss=0.2605724594469407, w0=0.004853468119388116, w1=-0.07448498431410776\n",
      "Gradient Descent(399/399): loss=0.26052160076101505, w0=0.00487250443603772, w1=-0.07455360682527061\n",
      "++++ gamma = 0.0138949549437\n",
      "ciaociaociao\n",
      "Gradient Descent(0/399): loss=0.4999999999999887, w0=-9.569778255469791e-06, w1=-0.004981513583374394\n",
      "Gradient Descent(1/399): loss=0.4702050327567676, w0=0.00010440196756509906, w1=-0.008970194759876298\n",
      "Gradient Descent(2/399): loss=0.4504523852736084, w0=0.00025667477838768675, w1=-0.012244727532261114\n",
      "Gradient Descent(3/399): loss=0.4362766646670367, w0=0.00040865572925190665, w1=-0.014990655159828337\n",
      "Gradient Descent(4/399): loss=0.4254307823213102, w0=0.0005433920180433771, w1=-0.017336360551128346\n",
      "Gradient Descent(5/399): loss=0.41670545897642813, w0=0.000654566238784759, w1=-0.019373048353671398\n",
      "Gradient Descent(6/399): loss=0.4094115328504695, w0=0.0007412032731187282, w1=-0.021166825222190353\n",
      "Gradient Descent(7/399): loss=0.4031333550947498, w0=0.0008049392161764175, w1=-0.022766436785230167\n",
      "Gradient Descent(8/399): loss=0.3976060664000249, w0=0.0008485543870747308, w1=-0.024208407386381706\n",
      "Gradient Descent(9/399): loss=0.3926522630120408, w0=0.0008751795195398042, w1=-0.02552053446039104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(10/399): loss=0.3881480082075751, w0=0.0008878780608523976, w1=-0.026724301819011205\n",
      "Gradient Descent(11/399): loss=0.3840037627718361, w0=0.0008894428607962192, w1=-0.027836566316153975\n",
      "Gradient Descent(12/399): loss=0.3801531495187397, w0=0.0008823145631677751, w1=-0.0288707489097332\n",
      "Gradient Descent(13/399): loss=0.37654599155183105, w0=0.0008685671987611727, w1=-0.02983768413528353\n",
      "Gradient Descent(14/399): loss=0.37314378653638564, w0=0.0008499287558107319, w1=-0.03074623215165148\n",
      "Gradient Descent(15/399): loss=0.3699166372533357, w0=0.0008278178725749435, w1=-0.031603724489474634\n",
      "Gradient Descent(16/399): loss=0.3668410957510523, w0=0.0008033859010040729, w1=-0.03241629243429306\n",
      "Gradient Descent(17/399): loss=0.36389860716950617, w0=0.0007775584901388628, w1=-0.033189111915101016\n",
      "Gradient Descent(18/399): loss=0.3610743630377551, w0=0.0007510737622868594, w1=-0.03392658848146998\n",
      "Gradient Descent(19/399): loss=0.3583564433781631, w0=0.0007245158602247219, w1=-0.034632498887139825\n",
      "Gradient Descent(20/399): loss=0.35573516773114866, w0=0.0006983436012832425, w1=-0.03531010092248835\n",
      "Gradient Descent(21/399): loss=0.35320260022636984, w0=0.0006729144794003501, w1=-0.035962219758250036\n",
      "Gradient Descent(22/399): loss=0.35075216985454016, w0=0.0006485044933071827, w1=-0.03659131670840002\n",
      "Gradient Descent(23/399): loss=0.3483783777856051, w0=0.0006253243619950678, w1=-0.03719954467173379\n",
      "Gradient Descent(24/399): loss=0.34607657096077443, w0=0.0006035326872140905, w1=-0.0377887933512347\n",
      "Gradient Descent(25/399): loss=0.34384276642767264, w0=0.0005832465788261092, w1=-0.03836072652841588\n",
      "Gradient Descent(26/399): loss=0.34167351469282087, w0=0.0005645501968537473, w1=-0.03891681308377208\n",
      "Gradient Descent(27/399): loss=0.33956579317423463, w0=0.0005475015978060299, w1=-0.039458353033491274\n",
      "Gradient Descent(28/399): loss=0.33751692293616664, w0=0.0005321382096053846, w1=-0.03998649954768785\n",
      "Gradient Descent(29/399): loss=0.335524503471765, w0=0.0005184812026251149, w1=-0.040502277692638984\n",
      "Gradient Descent(30/399): loss=0.33358636150255305, w0=0.0005065389751952231, w1=-0.041006600475145455\n",
      "Gradient Descent(31/399): loss=0.3317005106823995, w0=0.0004963099304610276, w1=-0.04150028264462418\n",
      "Gradient Descent(32/399): loss=0.32986511979805105, w0=0.00048778468708675685, w1=-0.04198405261622087\n",
      "Gradient Descent(33/399): loss=0.32807848760001934, w0=0.0004809478381291548, w1=-0.042458562807876174\n",
      "Gradient Descent(34/399): loss=0.3263390228152374, w0=0.00047577934954557815, w1=-0.0429243986300399\n",
      "Gradient Descent(35/399): loss=0.32464522821553676, w0=0.0004722556713701165, w1=-0.043382086324425716\n",
      "Gradient Descent(36/399): loss=0.3229956878656288, w0=0.00047035061980318764, w1=-0.043832099814826275\n",
      "Gradient Descent(37/399): loss=0.3213890568677488, w0=0.00047003607663576643, w1=-0.044274866706385585\n",
      "Gradient Descent(38/399): loss=0.3198240530702332, w0=0.0004712825429973751, w1=-0.04471077354826002\n",
      "Gradient Descent(39/399): loss=0.3182994503239388, w0=0.00047405957690479264, w1=-0.04514017045711824\n",
      "Gradient Descent(40/399): loss=0.3168140729611191, w0=0.000478336138110559, w1=-0.045563375184560995\n",
      "Gradient Descent(41/399): loss=0.3153667912420042, w0=0.0004840808589949737, w1=-0.04598067669963028\n",
      "Gradient Descent(42/399): loss=0.31395651756938336, w0=0.0004912622564621139, w1=-0.046392338347626266\n",
      "Gradient Descent(43/399): loss=0.3125822033144353, w0=0.0004998488967893737, w1=-0.046798600638080744\n",
      "Gradient Descent(44/399): loss=0.3112428361306039, w0=0.0005098095229819493, w1=-0.04719968370765226\n",
      "Gradient Descent(45/399): loss=0.30993743765854165, w0=0.0005211131522721139, w1=-0.047595789497681215\n",
      "Gradient Descent(46/399): loss=0.30866506154566686, w0=0.000533729149877827, w1=-0.04798710368099053\n",
      "Gradient Descent(47/399): loss=0.30742479171996995, w0=0.000547627283916788, w1=-0.048373797368094475\n",
      "Gradient Descent(48/399): loss=0.30621574087030906, w0=0.0005627777653976686, w1=-0.04875602861916725\n",
      "Gradient Descent(49/399): loss=0.30503704909534557, w0=0.0005791512764301318, w1=-0.04913394378482946\n",
      "Gradient Descent(50/399): loss=0.3038878826910548, w0=0.0005967189891699786, w1=-0.049507678695956\n",
      "Gradient Descent(51/399): loss=0.30276743305287424, w0=0.0006154525775140845, w1=-0.049877359720229456\n",
      "Gradient Descent(52/399): loss=0.3016749156733822, w0=0.0006353242231568952, w1=-0.05024310470100399\n",
      "Gradient Descent(53/399): loss=0.3006095692202113, w0=0.0006563066172964353, w1=-0.05060502379216236\n",
      "Gradient Descent(54/399): loss=0.29957065468191485, w0=0.0006783729590173537, w1=-0.050963220201003835\n",
      "Gradient Descent(55/399): loss=0.29855745457188576, w0=0.0007014969511689881, w1=-0.051317790849762075\n",
      "Gradient Descent(56/399): loss=0.29756927218231927, w0=0.0007256527943877929, w1=-0.05166882696509204\n",
      "Gradient Descent(57/399): loss=0.2966054308817152, w0=0.0007508151797777698, w1=-0.05201641460375977\n",
      "Gradient Descent(58/399): loss=0.2956652734506005, w0=0.0007769592806533607, w1=-0.05236063512179917\n",
      "Gradient Descent(59/399): loss=0.29474816145111915, w0=0.0008040607436614906, w1=-0.05270156559354787\n",
      "Gradient Descent(60/399): loss=0.2938534746268844, w0=0.0008320956795289642, w1=-0.05303927918622489\n",
      "Gradient Descent(61/399): loss=0.29298061033010736, w0=0.0008610406536248983, w1=-0.05337384549505392\n",
      "Gradient Descent(62/399): loss=0.2921289829735005, w0=0.0008908726764826383, w1=-0.05370533084335543\n",
      "Gradient Descent(63/399): loss=0.29129802350485207, w0=0.0009215691943895063, w1=-0.0540337985515197\n",
      "Gradient Descent(64/399): loss=0.29048717890248416, w0=0.0009531080801240179, w1=-0.054359309178321984\n",
      "Gradient Descent(65/399): loss=0.2896959116900679, w0=0.000985467623897444, w1=-0.05468192073764367\n",
      "Gradient Descent(66/399): loss=0.28892369946947827, w0=0.0010186265245386908, w1=-0.055001688893312375\n",
      "Gradient Descent(67/399): loss=0.28817003447054323, w0=0.0010525638809474273, w1=-0.055318667134464455\n",
      "Gradient Descent(68/399): loss=0.2874344231166848, w0=0.001087259183829506, w1=-0.055632906933559684\n",
      "Gradient Descent(69/399): loss=0.28671638560556567, w0=0.001122692307720371, w1=-0.055944457888936504\n",
      "Gradient Descent(70/399): loss=0.28601545550394863, w0=0.0011588435032958018, w1=-0.05625336785358245\n",
      "Gradient Descent(71/399): loss=0.2853311793560658, w0=0.0011956933899646427, w1=-0.05655968305160566\n",
      "Gradient Descent(72/399): loss=0.28466311630484886, w0=0.0012332229487347526, w1=-0.056863448183726184\n",
      "Gradient Descent(73/399): loss=0.2840108377254457, w0=0.001271413515341021, w1=-0.05716470652295824\n",
      "Gradient Descent(74/399): loss=0.2833739268704807, w0=0.0013102467736227268, w1=-0.057463500001523386\n",
      "Gradient Descent(75/399): loss=0.28275197852657286, w0=0.0013497047491365687, w1=-0.057759869289919094\n",
      "Gradient Descent(76/399): loss=0.2821445986816505, w0=0.0013897698029912559, w1=-0.05805385386896429\n",
      "Gradient Descent(77/399): loss=0.28155140420264047, w0=0.0014304246258894698, w1=-0.05834549209555274\n",
      "Gradient Descent(78/399): loss=0.28097202252313375, w0=0.0014716522323632286, w1=-0.05863482126276437\n",
      "Gradient Descent(79/399): loss=0.28040609134065336, w0=0.0015134359551891043, w1=-0.05892187765491338\n",
      "Gradient Descent(80/399): loss=0.2798532583231761, w0=0.0015557594399703172, w1=-0.05920669659804843\n",
      "Gradient Descent(81/399): loss=0.27931318082457324, w0=0.0015986066398734206, w1=-0.05948931250636412\n",
      "Gradient Descent(82/399): loss=0.2787855256086571, w0=0.0016419618105080146, w1=-0.05976975892493298\n",
      "Gradient Descent(83/399): loss=0.2782699685815353, w0=0.0016858095049387286, w1=-0.06004806856912303\n",
      "Gradient Descent(84/399): loss=0.2777661945319875, w0=0.0017301345688194882, w1=-0.0603242733610265\n",
      "Gradient Descent(85/399): loss=0.27727389687959336, w0=0.0017749221356408887, w1=-0.0605984044631904\n",
      "Gradient Descent(86/399): loss=0.27679277743035513, w0=0.001820157622082256, w1=-0.06087049230990867\n",
      "Gradient Descent(87/399): loss=0.2763225461395644, w0=0.0018658267234607367, w1=-0.061140566636307944\n",
      "Gradient Descent(88/399): loss=0.2758629208816773, w0=0.0019119154092704582, w1=-0.06140865650543436\n",
      "Gradient Descent(89/399): loss=0.27541362722697216, w0=0.0019584099188054874, w1=-0.06167479033352729\n",
      "Gradient Descent(90/399): loss=0.2749743982247681, w0=0.002005296756860941, w1=-0.06193899591364605\n",
      "Gradient Descent(91/399): loss=0.2745449741929987, w0=0.0020525626895071965, w1=-0.06220130043779868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(92/399): loss=0.2741251025139372, w0=0.0021001947399326973, w1=-0.062461730517706295\n",
      "Gradient Descent(93/399): loss=0.2737145374358788, w0=0.0021481801843513484, w1=-0.06272031220432286\n",
      "Gradient Descent(94/399): loss=0.273313039880596, w0=0.002196506547970969, w1=-0.06297707100621806\n",
      "Gradient Descent(95/399): loss=0.2729203772563842, w0=0.002245161601019679, w1=-0.0632320319069198\n",
      "Gradient Descent(96/399): loss=0.2725363232765265, w0=0.0022941333548274936, w1=-0.0634852193813035\n",
      "Gradient Descent(97/399): loss=0.27216065778300974, w0=0.002343410057960726, w1=-0.0637366574111064\n",
      "Gradient Descent(98/399): loss=0.27179316657533065, w0=0.0023929801924071387, w1=-0.06398636949963742\n",
      "Gradient Descent(99/399): loss=0.2714336412442358, w0=0.0024428324698100344, w1=-0.06423437868574614\n",
      "Gradient Descent(100/399): loss=0.27108187901024783, w0=0.0024929558277497453, w1=-0.06448070755710851\n",
      "Gradient Descent(101/399): loss=0.2707376825668283, w0=0.0025433394260711947, w1=-0.06472537826288097\n",
      "Gradient Descent(102/399): loss=0.2704008599280419, w0=0.002593972643256403, w1=-0.06496841252577007\n",
      "Gradient Descent(103/399): loss=0.2700712242805835, w0=0.0026448450728409796, w1=-0.06520983165356017\n",
      "Gradient Descent(104/399): loss=0.269748593840037, w0=0.0026959465198737945, w1=-0.06544965655013756\n",
      "Gradient Descent(105/399): loss=0.269432791711242, w0=0.002747266997419153, w1=-0.06568790772604616\n",
      "Gradient Descent(106/399): loss=0.26912364575264236, w0=0.0027987967231009156, w1=-0.06592460530860665\n",
      "Gradient Descent(107/399): loss=0.26882098844450103, w0=0.0028505261156880957, w1=-0.06615976905162785\n",
      "Gradient Descent(108/399): loss=0.26852465676086457, w0=0.0029024457917215495, w1=-0.06639341834473667\n",
      "Gradient Descent(109/399): loss=0.2682344920451676, w0=0.0029545465621814517, w1=-0.06662557222235095\n",
      "Gradient Descent(110/399): loss=0.2679503398893681, w0=0.003006819429195301, w1=-0.066856249372317\n",
      "Gradient Descent(111/399): loss=0.26767205001651395, w0=0.0030592555827862427, w1=-0.06708546814423187\n",
      "Gradient Descent(112/399): loss=0.2673994761666314, w0=0.0031118463976615514, w1=-0.067313246557469\n",
      "Gradient Descent(113/399): loss=0.2671324759858489, w0=0.0031645834300411202, w1=-0.06753960230892404\n",
      "Gradient Descent(114/399): loss=0.26687091091865217, w0=0.003217458414525856, w1=-0.06776455278049645\n",
      "Gradient Descent(115/399): loss=0.2666146461031852, w0=0.003270463261005874, w1=-0.067988115046321\n",
      "Gradient Descent(116/399): loss=0.2663635502695045, w0=0.0033235900516084037, w1=-0.06821030587976273\n",
      "Gradient Descent(117/399): loss=0.2661174956407046, w0=0.003376831037685335, w1=-0.06843114176018716\n",
      "Gradient Descent(118/399): loss=0.26587635783682667, w0=0.003430178636840314, w1=-0.06865063887951725\n",
      "Gradient Descent(119/399): loss=0.2656400157814738, w0=0.0034836254299953164, w1=-0.06886881314858746\n",
      "Gradient Descent(120/399): loss=0.2654083516110524, w0=0.0035371641584966184, w1=-0.06908568020330465\n",
      "Gradient Descent(121/399): loss=0.26518125058656417, w0=0.0035907877212600693, w1=-0.06930125541062468\n",
      "Gradient Descent(122/399): loss=0.26495860100787527, w0=0.003644489171955572, w1=-0.06951555387435326\n",
      "Gradient Descent(123/399): loss=0.26474029413039135, w0=0.0036982617162306624, w1=-0.06972859044077868\n",
      "Gradient Descent(124/399): loss=0.2645262240840681, w0=0.003752098708973066, w1=-0.0699403797041438\n",
      "Gradient Descent(125/399): loss=0.2643162877946925, w0=0.0038059936516121095, w1=-0.07015093601196411\n",
      "Gradient Descent(126/399): loss=0.2641103849073677, w0=0.003859940189458827, w1=-0.0703602734701982\n",
      "Gradient Descent(127/399): loss=0.26390841771213797, w0=0.003913932109084616, w1=-0.07056840594827661\n",
      "Gradient Descent(128/399): loss=0.26371029107169436, w0=0.00396796333573826, w1=-0.07077534708399474\n",
      "Gradient Descent(129/399): loss=0.26351591235110033, w0=0.004022027930801141, w1=-0.07098111028827504\n",
      "Gradient Descent(130/399): loss=0.2633251913494796, w0=0.004076120089280428, w1=-0.07118570874980348\n",
      "Gradient Descent(131/399): loss=0.26313804023361076, w0=0.004130234137340033, w1=-0.07138915543954502\n",
      "Gradient Descent(132/399): loss=0.262954373473373, w0=0.004184364529869112, w1=-0.07159146311514246\n",
      "Gradient Descent(133/399): loss=0.26277410777899274, w0=0.00423850584808784, w1=-0.07179264432520295\n",
      "Gradient Descent(134/399): loss=0.2625971620400372, w0=0.004292652797190245, w1=-0.07199271141347605\n",
      "Gradient Descent(135/399): loss=0.2624234572661063, w0=0.004346800204023796, w1=-0.07219167652292709\n",
      "Gradient Descent(136/399): loss=0.26225291652917593, w0=0.004400943014805471, w1=-0.07238955159970954\n",
      "Gradient Descent(137/399): loss=0.26208546490754314, w0=0.004455076292874033, w1=-0.07258634839703969\n",
      "Gradient Descent(138/399): loss=0.26192102943133044, w0=0.0045091952164781745, w1=-0.07278207847897679\n",
      "Gradient Descent(139/399): loss=0.2617595390295025, w0=0.004563295076600248, w1=-0.07297675322411197\n",
      "Gradient Descent(140/399): loss=0.26160092447835537, w0=0.004617371274815229, w1=-0.07317038382916863\n",
      "Gradient Descent(141/399): loss=0.2614451183514336, w0=0.004671419321184602, w1=-0.07336298131251723\n",
      "Gradient Descent(142/399): loss=0.2612920549708368, w0=0.004725434832184806, w1=-0.07355455651760721\n",
      "Gradient Descent(143/399): loss=0.2611416703598761, w0=0.004779413528669901, w1=-0.07374512011631844\n",
      "Gradient Descent(144/399): loss=0.26099390219704177, w0=0.0048333512338680894, w1=-0.07393468261223475\n",
      "Gradient Descent(145/399): loss=0.2608486897712449, w0=0.004887243871411733, w1=-0.07412325434384194\n",
      "Gradient Descent(146/399): loss=0.2607059739382998, w0=0.0049410874634004875, w1=-0.07431084548765239\n",
      "Gradient Descent(147/399): loss=0.2605656970786072, w0=0.00499487812849718, w1=-0.07449746606125857\n",
      "Gradient Descent(148/399): loss=0.26042780305600943, w0=0.005048612080056054, w1=-0.07468312592631735\n",
      "Gradient Descent(149/399): loss=0.26029223717778227, w0=0.005102285624282978, w1=-0.07486783479146732\n",
      "Gradient Descent(150/399): loss=0.2601589461557307, w0=0.00515589515842725, w1=-0.07505160221518083\n",
      "Gradient Descent(151/399): loss=0.2600278780683596, w0=0.005209437169004582, w1=-0.07523443760855271\n",
      "Gradient Descent(152/399): loss=0.25989898232408837, w0=0.005262908230050885, w1=-0.07541635023802742\n",
      "Gradient Descent(153/399): loss=0.25977220962547914, w0=0.0053163050014064465, w1=-0.0755973492280663\n",
      "Gradient Descent(154/399): loss=0.25964751193445135, w0=0.005369624227030101, w1=-0.07577744356375658\n",
      "Gradient Descent(155/399): loss=0.2595248424384553, w0=0.005422862733342995, w1=-0.07595664209336386\n",
      "Gradient Descent(156/399): loss=0.25940415551757656, w0=0.0054760174276015355, w1=-0.07613495353082932\n",
      "Gradient Descent(157/399): loss=0.2592854067125456, w0=0.00552908529629913, w1=-0.07631238645821334\n",
      "Gradient Descent(158/399): loss=0.2591685526936288, w0=0.005582063403596302, w1=-0.076488949328087\n",
      "Gradient Descent(159/399): loss=0.2590535512303731, w0=0.00563494888977879, w1=-0.07666465046587263\n",
      "Gradient Descent(160/399): loss=0.2589403611621841, w0=0.0056877389697432106, w1=-0.07683949807213489\n",
      "Gradient Descent(161/399): loss=0.2588289423697103, w0=0.005740430931509909, w1=-0.0770135002248236\n",
      "Gradient Descent(162/399): loss=0.2587192557470162, w0=0.005793022134762562, w1=-0.0771866648814697\n",
      "Gradient Descent(163/399): loss=0.25861126317451477, w0=0.005845510009414172, w1=-0.07735899988133525\n",
      "Gradient Descent(164/399): loss=0.25850492749264736, w0=0.0058978920541990125, w1=-0.07753051294751896\n",
      "Gradient Descent(165/399): loss=0.2584002124762813, w0=0.005950165835290177, w1=-0.07770121168901815\n",
      "Gradient Descent(166/399): loss=0.25829708280981123, w0=0.006002328984942286, w1=-0.07787110360274838\n",
      "Gradient Descent(167/399): loss=0.2581955040629422, w0=0.006054379200159004, w1=-0.07804019607552162\n",
      "Gradient Descent(168/399): loss=0.2580954426671358, w0=0.006106314241384955, w1=-0.07820849638598422\n",
      "Gradient Descent(169/399): loss=0.25799686589269993, w0=0.006158131931221657, w1=-0.07837601170651547\n",
      "Gradient Descent(170/399): loss=0.25789974182650727, w0=0.006209830153167088, w1=-0.0785427491050878\n",
      "Gradient Descent(171/399): loss=0.2578040393503207, w0=0.006261406850378518, w1=-0.07870871554708957\n",
      "Gradient Descent(172/399): loss=0.257709728119713, w0=0.0063128600244582115, w1=-0.07887391789711129\n",
      "Gradient Descent(173/399): loss=0.2576167785435608, w0=0.006364187734261641, w1=-0.0790383629206962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(174/399): loss=0.2575251617640996, w0=0.00641538809472784, w1=-0.07920205728605602\n",
      "Gradient Descent(175/399): loss=0.25743484963752195, w0=0.006466459275731529, w1=-0.07936500756575271\n",
      "Gradient Descent(176/399): loss=0.25734581471510565, w0=0.006517399500956646, w1=-0.07952722023834709\n",
      "Gradient Descent(177/399): loss=0.25725803022485627, w0=0.006568207046790941, w1=-0.079688701690015\n",
      "Gradient Descent(178/399): loss=0.2571714700536492, w0=0.0066188802412412645, w1=-0.07984945821613189\n",
      "Gradient Descent(179/399): loss=0.25708610872985915, w0=0.0066694174628692034, w1=-0.08000949602282632\n",
      "Gradient Descent(180/399): loss=0.2570019214064615, w0=0.006719817139746727, w1=-0.08016882122850351\n",
      "Gradient Descent(181/399): loss=0.2569188838445937, w0=0.006770077748431494, w1=-0.08032743986533913\n",
      "Gradient Descent(182/399): loss=0.25683697239756426, w0=0.006820197812961485, w1=-0.08048535788074444\n",
      "Gradient Descent(183/399): loss=0.25675616399529544, w0=0.006870175903868628, w1=-0.08064258113880313\n",
      "Gradient Descent(184/399): loss=0.256676436129189, w0=0.006920010637211087, w1=-0.08079911542168067\n",
      "Gradient Descent(185/399): loss=0.25659776683740343, w0=0.006969700673623897, w1=-0.0809549664310067\n",
      "Gradient Descent(186/399): loss=0.2565201346905293, w0=0.007019244717387612, w1=-0.08111013978923118\n",
      "Gradient Descent(187/399): loss=0.2564435187776546, w0=0.00706864151551466, w1=-0.08126464104095463\n",
      "Gradient Descent(188/399): loss=0.2563678986928067, w0=0.007117889856853094, w1=-0.08141847565423338\n",
      "Gradient Descent(189/399): loss=0.256293254521763, w0=0.007166988571207426, w1=-0.08157164902186012\n",
      "Gradient Descent(190/399): loss=0.2562195668292171, w0=0.007215936528476247, w1=-0.08172416646262036\n",
      "Gradient Descent(191/399): loss=0.25614681664629346, w0=0.007264732637806335, w1=-0.0818760332225254\n",
      "Gradient Descent(192/399): loss=0.25607498545839924, w0=0.007313375846762952, w1=-0.08202725447602219\n",
      "Gradient Descent(193/399): loss=0.25600405519340524, w0=0.0073618651405160445, w1=-0.08217783532718073\n",
      "Gradient Descent(194/399): loss=0.25593400821014545, w0=0.007410199541042067, w1=-0.08232778081085937\n",
      "Gradient Descent(195/399): loss=0.2558648272872277, w0=0.007458378106341135, w1=-0.0824770958938485\n",
      "Gradient Descent(196/399): loss=0.2557964956121458, w0=0.0075063999296692465, w1=-0.0826257854759932\n",
      "Gradient Descent(197/399): loss=0.25572899677068606, w0=0.007554264138785284, w1=-0.08277385439129514\n",
      "Gradient Descent(198/399): loss=0.2556623147366187, w0=0.007601969895212551, w1=-0.08292130740899432\n",
      "Gradient Descent(199/399): loss=0.2555964338616681, w0=0.007649516393514547, w1=-0.08306814923463092\n",
      "Gradient Descent(200/399): loss=0.2555313388657511, w0=0.007696902860584757, w1=-0.08321438451108779\n",
      "Gradient Descent(201/399): loss=0.25546701482748085, w0=0.007744128554950179, w1=-0.083360017819614\n",
      "Gradient Descent(202/399): loss=0.25540344717492375, w0=0.007791192766088334, w1=-0.08350505368082975\n",
      "Gradient Descent(203/399): loss=0.25534062167660565, w0=0.007838094813757534, w1=-0.08364949655571303\n",
      "Gradient Descent(204/399): loss=0.25527852443276, w0=0.00788483404734014, w1=-0.08379335084656858\n",
      "Gradient Descent(205/399): loss=0.25521714186681077, w0=0.007931409845198596, w1=-0.0839366208979792\n",
      "Gradient Descent(206/399): loss=0.2551564607170835, w0=0.007977821614043963, w1=-0.08407931099774013\n",
      "Gradient Descent(207/399): loss=0.2550964680287389, w0=0.008024068788316786, w1=-0.08422142537777659\n",
      "Gradient Descent(208/399): loss=0.2550371511459232, w0=0.008070150829579998, w1=-0.08436296821504483\n",
      "Gradient Descent(209/399): loss=0.25497849770412784, w0=0.008116067225923687, w1=-0.08450394363241724\n",
      "Gradient Descent(210/399): loss=0.25492049562275393, w0=0.008161817491381477, w1=-0.08464435569955162\n",
      "Gradient Descent(211/399): loss=0.2548631330978759, w0=0.00820740116535833, w1=-0.08478420843374494\n",
      "Gradient Descent(212/399): loss=0.2548063985951985, w0=0.008252817812069537, w1=-0.08492350580077211\n",
      "Gradient Descent(213/399): loss=0.2547502808432015, w0=0.008298067019990701, w1=-0.08506225171570976\n",
      "Gradient Descent(214/399): loss=0.25469476882646785, w0=0.008343148401318497, w1=-0.08520045004374562\n",
      "Gradient Descent(215/399): loss=0.2546398517791897, w0=0.00838806159144203, w1=-0.0853381046009735\n",
      "Gradient Descent(216/399): loss=0.25458551917884753, w0=0.008432806248424555, w1=-0.08547521915517446\n",
      "Gradient Descent(217/399): loss=0.25453176074005796, w0=0.008477382052495408, w1=-0.08561179742658413\n",
      "Gradient Descent(218/399): loss=0.25447856640858446, w0=0.008521788705551921, w1=-0.08574784308864669\n",
      "Gradient Descent(219/399): loss=0.25442592635550865, w0=0.008566025930671156, w1=-0.08588335976875568\n",
      "Gradient Descent(220/399): loss=0.25437383097155564, w0=0.00861009347163126, w1=-0.08601835104898183\n",
      "Gradient Descent(221/399): loss=0.25432227086157033, w0=0.008653991092442267, w1=-0.08615282046678831\n",
      "Gradient Descent(222/399): loss=0.2542712368391405, w0=0.00869771857688618, w1=-0.08628677151573352\n",
      "Gradient Descent(223/399): loss=0.25422071992136286, w0=0.008741275728066116, w1=-0.08642020764616175\n",
      "Gradient Descent(224/399): loss=0.2541707113237468, w0=0.008784662367964408, w1=-0.08655313226588178\n",
      "Gradient Descent(225/399): loss=0.25412120245525427, w0=0.008827878337009433, w1=-0.0866855487408339\n",
      "Gradient Descent(226/399): loss=0.25407218491347017, w0=0.008870923493651038, w1=-0.08681746039574537\n",
      "Gradient Descent(227/399): loss=0.25402365047990116, w0=0.008913797713944393, w1=-0.08694887051477461\n",
      "Gradient Descent(228/399): loss=0.253975591115398, w0=0.008956500891142099, w1=-0.08707978234214435\n",
      "Gradient Descent(229/399): loss=0.25392799895569923, w0=0.008999032935294417, w1=-0.08721019908276391\n",
      "Gradient Descent(230/399): loss=0.25388086630709217, w0=0.009041393772857443, w1=-0.08734012390284085\n",
      "Gradient Descent(231/399): loss=0.25383418564218885, w0=0.009083583346309087, w1=-0.08746955993048217\n",
      "Gradient Descent(232/399): loss=0.2537879495958124, w0=0.009125601613772709, w1=-0.08759851025628523\n",
      "Gradient Descent(233/399): loss=0.2537421509609924, w0=0.009167448548648269, w1=-0.0877269779339187\n",
      "Gradient Descent(234/399): loss=0.2536967826850654, w0=0.009209124139250834, w1=-0.08785496598069352\n",
      "Gradient Descent(235/399): loss=0.2536518378658775, w0=0.009250628388456318, w1=-0.08798247737812427\n",
      "Gradient Descent(236/399): loss=0.2536073097480875, w0=0.00929196131335431, w1=-0.08810951507248097\n",
      "Gradient Descent(237/399): loss=0.2535631917195658, w0=0.00933312294490785, w1=-0.08823608197533159\n",
      "Gradient Descent(238/399): loss=0.2535194773078894, w0=0.009374113327620035, w1=-0.08836218096407537\n",
      "Gradient Descent(239/399): loss=0.25347616017692665, w0=0.00941493251920731, w1=-0.08848781488246721\n",
      "Gradient Descent(240/399): loss=0.25343323412351293, w0=0.009455580590279318, w1=-0.0886129865411331\n",
      "Gradient Descent(241/399): loss=0.2533906930742131, w0=0.009496057624025191, w1=-0.08873769871807706\n",
      "Gradient Descent(242/399): loss=0.25334853108216726, w0=0.009536363715906158, w1=-0.08886195415917944\n",
      "Gradient Descent(243/399): loss=0.2533067423240206, w0=0.009576498973354337, w1=-0.0889857555786869\n",
      "Gradient Descent(244/399): loss=0.2532653210969319, w0=0.009616463515477614, w1=-0.08910910565969427\n",
      "Gradient Descent(245/399): loss=0.2532242618156603, w0=0.009656257472770465, w1=-0.08923200705461821\n",
      "Gradient Descent(246/399): loss=0.25318355900972844, w0=0.009695880986830633, w1=-0.08935446238566312\n",
      "Gradient Descent(247/399): loss=0.2531432073206589, w0=0.009735334210081532, w1=-0.08947647424527917\n",
      "Gradient Descent(248/399): loss=0.253103201499282, w0=0.00977461730550027, w1=-0.08959804519661278\n",
      "Gradient Descent(249/399): loss=0.2530635364031145, w0=0.00981373044635119, w1=-0.08971917777394957\n",
      "Gradient Descent(250/399): loss=0.25302420699380523, w0=0.009852673815924818, w1=-0.08983987448314995\n",
      "Gradient Descent(251/399): loss=0.2529852083346475, w0=0.009891447607282111, w1=-0.0899601378020776\n",
      "Gradient Descent(252/399): loss=0.2529465355881557, w0=0.009930052023003904, w1=-0.09007997018102065\n",
      "Gradient Descent(253/399): loss=0.2529081840137043, w0=0.00996848727494547, w1=-0.09019937404310613\n",
      "Gradient Descent(254/399): loss=0.2528701489652281, w0=0.010006753583996069, w1=-0.09031835178470739\n",
      "Gradient Descent(255/399): loss=0.2528324258889817, w0=0.01004485117984341, w1=-0.0904369057758449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(256/399): loss=0.25279501032135643, w0=0.010082780300742927, w1=-0.09055503836058044\n",
      "Gradient Descent(257/399): loss=0.2527578978867542, w0=0.010120541193291772, w1=-0.09067275185740477\n",
      "Gradient Descent(258/399): loss=0.2527210842955149, w0=0.01015813411220743, w1=-0.09079004855961892\n",
      "Gradient Descent(259/399): loss=0.25268456534189765, w0=0.010195559320110882, w1=-0.0909069307357093\n",
      "Gradient Descent(260/399): loss=0.25264833690211463, w0=0.010232817087314218, w1=-0.09102340062971659\n",
      "Gradient Descent(261/399): loss=0.25261239493241344, w0=0.010269907691612604, w1=-0.09113946046159863\n",
      "Gradient Descent(262/399): loss=0.25257673546720993, w0=0.010306831418080541, w1=-0.0912551124275873\n",
      "Gradient Descent(263/399): loss=0.2525413546172683, w0=0.010343588558872322, w1=-0.0913703587005397\n",
      "Gradient Descent(264/399): loss=0.2525062485679279, w0=0.010380179413026599, w1=-0.09148520143028345\n",
      "Gradient Descent(265/399): loss=0.252471413577375, w0=0.010416604286274984, w1=-0.09159964274395652\n",
      "Gradient Descent(266/399): loss=0.2524368459749584, w0=0.01045286349085462, w1=-0.09171368474634134\n",
      "Gradient Descent(267/399): loss=0.2524025421595485, w0=0.010488957345324628, w1=-0.09182732952019367\n",
      "Gradient Descent(268/399): loss=0.25236849859793753, w0=0.010524886174386355, w1=-0.09194057912656599\n",
      "Gradient Descent(269/399): loss=0.2523347118232806, w0=0.010560650308707367, w1=-0.09205343560512569\n",
      "Gradient Descent(270/399): loss=0.252301178433576, w0=0.010596250084749086, w1=-0.09216590097446811\n",
      "Gradient Descent(271/399): loss=0.25226789509018466, w0=0.010631685844598037, w1=-0.09227797723242448\n",
      "Gradient Descent(272/399): loss=0.25223485851638616, w0=0.010666957935800593, w1=-0.09238966635636489\n",
      "Gradient Descent(273/399): loss=0.2522020654959717, w0=0.010702066711201184, w1=-0.0925009703034964\n",
      "Gradient Descent(274/399): loss=0.25216951287187234, w0=0.010737012528783892, w1=-0.09261189101115624\n",
      "Gradient Descent(275/399): loss=0.2521371975448226, w0=0.010771795751517352, w1=-0.09272243039710046\n",
      "Gradient Descent(276/399): loss=0.2521051164720556, w0=0.010806416747202917, w1=-0.09283259035978765\n",
      "Gradient Descent(277/399): loss=0.25207326666603375, w0=0.010840875888326012, w1=-0.0929423727786584\n",
      "Gradient Descent(278/399): loss=0.2520416451932089, w0=0.01087517355191061, w1=-0.09305177951441002\n",
      "Gradient Descent(279/399): loss=0.25201024917281495, w0=0.010909310119376783, w1=-0.09316081240926699\n",
      "Gradient Descent(280/399): loss=0.25197907577569, w0=0.010943285976401255, w1=-0.09326947328724704\n",
      "Gradient Descent(281/399): loss=0.2519481222231284, w0=0.01097710151278091, w1=-0.09337776395442289\n",
      "Gradient Descent(282/399): loss=0.2519173857857606, w0=0.011010757122299174, w1=-0.0934856861991799\n",
      "Gradient Descent(283/399): loss=0.2518868637824619, w0=0.011044253202595259, w1=-0.09359324179246949\n",
      "Gradient Descent(284/399): loss=0.25185655357928777, w0=0.011077590155036163, w1=-0.09370043248805865\n",
      "Gradient Descent(285/399): loss=0.251826452588436, w0=0.011110768384591409, w1=-0.0938072600227753\n",
      "Gradient Descent(286/399): loss=0.25179655826723324, w0=0.011143788299710446, w1=-0.09391372611674983\n",
      "Gradient Descent(287/399): loss=0.2517668681171489, w0=0.011176650312202673, w1=-0.09401983247365281\n",
      "Gradient Descent(288/399): loss=0.2517373796828314, w0=0.011209354837120038, w1=-0.0941255807809288\n",
      "Gradient Descent(289/399): loss=0.2517080905511694, w0=0.011241902292642143, w1=-0.09423097271002652\n",
      "Gradient Descent(290/399): loss=0.25167899835037566, w0=0.011274293099963828, w1=-0.0943360099166254\n",
      "Gradient Descent(291/399): loss=0.2516501007490931, w0=0.011306527683185168, w1=-0.0944406940408584\n",
      "Gradient Descent(292/399): loss=0.2516213954555242, w0=0.011338606469203849, w1=-0.09454502670753144\n",
      "Gradient Descent(293/399): loss=0.25159288021657933, w0=0.01137052988760986, w1=-0.0946490095263392\n",
      "Gradient Descent(294/399): loss=0.251564552817049, w0=0.01140229837058248, w1=-0.09475264409207762\n",
      "Gradient Descent(295/399): loss=0.25153641107879326, w0=0.01143391235278949, w1=-0.09485593198485295\n",
      "Gradient Descent(296/399): loss=0.25150845285995377, w0=0.011465372271288576, w1=-0.09495887477028758\n",
      "Gradient Descent(297/399): loss=0.25148067605418195, w0=0.011496678565430895, w1=-0.09506147399972252\n",
      "Gradient Descent(298/399): loss=0.25145307858988897, w0=0.01152783167676672, w1=-0.09516373121041671\n",
      "Gradient Descent(299/399): loss=0.25142565842951214, w0=0.011558832048953185, w1=-0.09526564792574327\n",
      "Gradient Descent(300/399): loss=0.25139841356879905, w0=0.011589680127664025, w1=-0.0953672256553825\n",
      "Gradient Descent(301/399): loss=0.25137134203610967, w0=0.011620376360501313, w1=-0.09546846589551197\n",
      "Gradient Descent(302/399): loss=0.2513444418917353, w0=0.011650921196909143, w1=-0.09556937012899355\n",
      "Gradient Descent(303/399): loss=0.2513177112272337, w0=0.01168131508808922, w1=-0.09566993982555753\n",
      "Gradient Descent(304/399): loss=0.25129114816477965, w0=0.011711558486918308, w1=-0.0957701764419838\n",
      "Gradient Descent(305/399): loss=0.25126475085653227, w0=0.01174165184786753, w1=-0.09587008142228022\n",
      "Gradient Descent(306/399): loss=0.2512385174840167, w0=0.011771595626923446, w1=-0.09596965619785823\n",
      "Gradient Descent(307/399): loss=0.2512124462575208, w0=0.011801390281510902, w1=-0.0960689021877056\n",
      "Gradient Descent(308/399): loss=0.2511865354155059, w0=0.011831036270417596, w1=-0.09616782079855654\n",
      "Gradient Descent(309/399): loss=0.251160783224032, w0=0.011860534053720354, w1=-0.09626641342505919\n",
      "Gradient Descent(310/399): loss=0.2511351879761975, w0=0.011889884092713035, w1=-0.09636468144994038\n",
      "Gradient Descent(311/399): loss=0.25110974799158947, w0=0.011919086849836091, w1=-0.09646262624416789\n",
      "Gradient Descent(312/399): loss=0.25108446161575027, w0=0.011948142788607697, w1=-0.09656024916711017\n",
      "Gradient Descent(313/399): loss=0.2510593272196545, w0=0.011977052373556455, w1=-0.09665755156669355\n",
      "Gradient Descent(314/399): loss=0.2510343431991994, w0=0.012005816070155625, w1=-0.09675453477955698\n",
      "Gradient Descent(315/399): loss=0.2510095079747064, w0=0.012034434344758857, w1=-0.0968512001312044\n",
      "Gradient Descent(316/399): loss=0.25098481999043515, w0=0.012062907664537386, w1=-0.09694754893615476\n",
      "Gradient Descent(317/399): loss=0.250960277714109, w0=0.012091236497418681, w1=-0.09704358249808957\n",
      "Gradient Descent(318/399): loss=0.25093587963645025, w0=0.01211942131202649, w1=-0.09713930210999833\n",
      "Gradient Descent(319/399): loss=0.2509116242707283, w0=0.012147462577622285, w1=-0.09723470905432156\n",
      "Gradient Descent(320/399): loss=0.2508875101523165, w0=0.012175360764048048, w1=-0.09732980460309175\n",
      "Gradient Descent(321/399): loss=0.25086353583826027, w0=0.012203116341670404, w1=-0.09742459001807195\n",
      "Gradient Descent(322/399): loss=0.2508396999068552, w0=0.012230729781326033, w1=-0.09751906655089239\n",
      "Gradient Descent(323/399): loss=0.25081600095723444, w0=0.012258201554268381, w1=-0.09761323544318481\n",
      "Gradient Descent(324/399): loss=0.2507924376089658, w0=0.01228553213211561, w1=-0.09770709792671492\n",
      "Gradient Descent(325/399): loss=0.25076900850165884, w0=0.012312721986799776, w1=-0.09780065522351256\n",
      "Gradient Descent(326/399): loss=0.2507457122945793, w0=0.012339771590517211, w1=-0.09789390854600008\n",
      "Gradient Descent(327/399): loss=0.25072254766627466, w0=0.012366681415680078, w1=-0.09798685909711861\n",
      "Gradient Descent(328/399): loss=0.250699513314206, w0=0.012393451934869085, w1=-0.0980795080704524\n",
      "Gradient Descent(329/399): loss=0.2506766079543902, w0=0.01242008362078732, w1=-0.09817185665035126\n",
      "Gradient Descent(330/399): loss=0.2506538303210481, w0=0.012446576946215208, w1=-0.0982639060120511\n",
      "Gradient Descent(331/399): loss=0.25063117916626354, w0=0.01247293238396654, w1=-0.09835565732179265\n",
      "Gradient Descent(332/399): loss=0.2506086532596473, w0=0.012499150406845575, w1=-0.09844711173693833\n",
      "Gradient Descent(333/399): loss=0.25058625138801044, w0=0.012525231487605176, w1=-0.09853827040608729\n",
      "Gradient Descent(334/399): loss=0.2505639723550439, w0=0.012551176098905971, w1=-0.0986291344691887\n",
      "Gradient Descent(335/399): loss=0.25054181498100664, w0=0.012576984713276519, w1=-0.09871970505765339\n",
      "Gradient Descent(336/399): loss=0.2505197781024189, w0=0.012602657803074447, w1=-0.09880998329446365\n",
      "Gradient Descent(337/399): loss=0.25049786057176415, w0=0.012628195840448556, w1=-0.09889997029428142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(338/399): loss=0.2504760612571967, w0=0.012653599297301858, w1=-0.0989896671635548\n",
      "Gradient Descent(339/399): loss=0.250454379042256, w0=0.012678868645255546, w1=-0.09907907500062292\n",
      "Gradient Descent(340/399): loss=0.2504328128255869, w0=0.01270400435561386, w1=-0.0991681948958193\n",
      "Gradient Descent(341/399): loss=0.25041136152066734, w0=0.012729006899329834, w1=-0.09925702793157344\n",
      "Gradient Descent(342/399): loss=0.2503900240555407, w0=0.012753876746971928, w1=-0.09934557518251111\n",
      "Gradient Descent(343/399): loss=0.2503687993725544, w0=0.012778614368691479, w1=-0.09943383771555286\n",
      "Gradient Descent(344/399): loss=0.2503476864281047, w0=0.012803220234191018, w1=-0.09952181659001123\n",
      "Gradient Descent(345/399): loss=0.2503266841923868, w0=0.012827694812693368, w1=-0.09960951285768635\n",
      "Gradient Descent(346/399): loss=0.25030579164915023, w0=0.01285203857291157, w1=-0.09969692756296014\n",
      "Gradient Descent(347/399): loss=0.25028500779545987, w0=0.012876251983019572, w1=-0.09978406174288909\n",
      "Gradient Descent(348/399): loss=0.2502643316414619, w0=0.012900335510623685, w1=-0.09987091642729562\n",
      "Gradient Descent(349/399): loss=0.250243762210155, w0=0.0129242896227348, w1=-0.099957492638858\n",
      "Gradient Descent(350/399): loss=0.2502232985371662, w0=0.012948114785741328, w1=-0.10004379139319901\n",
      "Gradient Descent(351/399): loss=0.25020293967053225, w0=0.01297181146538287, w1=-0.10012981369897316\n",
      "Gradient Descent(352/399): loss=0.2501826846704848, w0=0.012995380126724578, w1=-0.10021556055795262\n",
      "Gradient Descent(353/399): loss=0.2501625326092409, w0=0.013018821234132222, w1=-0.10030103296511195\n",
      "Gradient Descent(354/399): loss=0.250142482570797, w0=0.013042135251247922, w1=-0.10038623190871133\n",
      "Gradient Descent(355/399): loss=0.2501225336507292, w0=0.013065322640966544, w1=-0.10047115837037876\n",
      "Gradient Descent(356/399): loss=0.2501026849559951, w0=0.013088383865412749, w1=-0.10055581332519088\n",
      "Gradient Descent(357/399): loss=0.2500829356047427, w0=0.013111319385918668, w1=-0.10064019774175265\n",
      "Gradient Descent(358/399): loss=0.2500632847261208, w0=0.013134129663002213, w1=-0.10072431258227572\n",
      "Gradient Descent(359/399): loss=0.2500437314600949, w0=0.01315681515634598, w1=-0.10080815880265581\n",
      "Gradient Descent(360/399): loss=0.25002427495726676, w0=0.01317937632477676, w1=-0.10089173735254871\n",
      "Gradient Descent(361/399): loss=0.2500049143786971, w0=0.01320181362624563, w1=-0.1009750491754453\n",
      "Gradient Descent(362/399): loss=0.24998564889573288, w0=0.01322412751780861, w1=-0.10105809520874535\n",
      "Gradient Descent(363/399): loss=0.24996647768983732, w0=0.013246318455607899, w1=-0.10114087638383029\n",
      "Gradient Descent(364/399): loss=0.24994739995242402, w0=0.013268386894853627, w1=-0.10122339362613479\n",
      "Gradient Descent(365/399): loss=0.24992841488469428, w0=0.013290333289806175, w1=-0.10130564785521738\n",
      "Gradient Descent(366/399): loss=0.2499095216974777, w0=0.013312158093759003, w1=-0.10138763998482994\n",
      "Gradient Descent(367/399): loss=0.2498907196110764, w0=0.013333861759021993, w1=-0.10146937092298616\n",
      "Gradient Descent(368/399): loss=0.2498720078551115, w0=0.013355444736905303, w1=-0.10155084157202901\n",
      "Gradient Descent(369/399): loss=0.24985338566837462, w0=0.013376907477703703, w1=-0.10163205282869715\n",
      "Gradient Descent(370/399): loss=0.24983485229867983, w0=0.013398250430681404, w1=-0.10171300558419043\n",
      "Gradient Descent(371/399): loss=0.24981640700272073, w0=0.013419474044057352, w1=-0.10179370072423431\n",
      "Gradient Descent(372/399): loss=0.24979804904592937, w0=0.013440578764990987, w1=-0.10187413912914346\n",
      "Gradient Descent(373/399): loss=0.24977977770233833, w0=0.013461565039568456, w1=-0.10195432167388425\n",
      "Gradient Descent(374/399): loss=0.24976159225444558, w0=0.01348243331278926, w1=-0.10203424922813646\n",
      "Gradient Descent(375/399): loss=0.2497434919930815, w0=0.01350318402855335, w1=-0.10211392265635395\n",
      "Gradient Descent(376/399): loss=0.24972547621727947, w0=0.01352381762964863, w1=-0.10219334281782455\n",
      "Gradient Descent(377/399): loss=0.24970754423414845, w0=0.013544334557738884, w1=-0.10227251056672897\n",
      "Gradient Descent(378/399): loss=0.2496896953587481, w0=0.013564735253352113, w1=-0.10235142675219888\n",
      "Gradient Descent(379/399): loss=0.24967192891396622, w0=0.013585020155869255, w1=-0.10243009221837414\n",
      "Gradient Descent(380/399): loss=0.24965424423039928, w0=0.013605189703513312, w1=-0.10250850780445915\n",
      "Gradient Descent(381/399): loss=0.24963664064623448, w0=0.013625244333338843, w1=-0.1025866743447784\n",
      "Gradient Descent(382/399): loss=0.2496191175071344, w0=0.013645184481221834, w1=-0.10266459266883118\n",
      "Gradient Descent(383/399): loss=0.24960167416612442, w0=0.01366501058184994, w1=-0.10274226360134552\n",
      "Gradient Descent(384/399): loss=0.24958430998348102, w0=0.013684723068713064, w1=-0.10281968796233125\n",
      "Gradient Descent(385/399): loss=0.24956702432662378, w0=0.013704322374094309, w1=-0.10289686656713241\n",
      "Gradient Descent(386/399): loss=0.24954981657000852, w0=0.013723808929061249, w1=-0.10297380022647881\n",
      "Gradient Descent(387/399): loss=0.24953268609502235, w0=0.013743183163457547, w1=-0.10305048974653681\n",
      "Gradient Descent(388/399): loss=0.24951563228988166, w0=0.013762445505894897, w1=-0.10312693592895943\n",
      "Gradient Descent(389/399): loss=0.2494986545495308, w0=0.01378159638374528, w1=-0.10320313957093565\n",
      "Gradient Descent(390/399): loss=0.2494817522755439, w0=0.013800636223133541, w1=-0.10327910146523907\n",
      "Gradient Descent(391/399): loss=0.24946492487602762, w0=0.013819565448930256, w1=-0.10335482240027578\n",
      "Gradient Descent(392/399): loss=0.24944817176552603, w0=0.01383838448474492, w1=-0.10343030316013163\n",
      "Gradient Descent(393/399): loss=0.2494314923649278, w0=0.013857093752919403, w1=-0.10350554452461869\n",
      "Gradient Descent(394/399): loss=0.24941488610137416, w0=0.013875693674521709, w1=-0.10358054726932109\n",
      "Gradient Descent(395/399): loss=0.249398352408169, w0=0.013894184669340004, w1=-0.10365531216564025\n",
      "Gradient Descent(396/399): loss=0.24938189072469114, w0=0.013912567155876925, w1=-0.10372983998083937\n",
      "Gradient Descent(397/399): loss=0.24936550049630762, w0=0.013930841551344145, w1=-0.1038041314780873\n",
      "Gradient Descent(398/399): loss=0.2493491811742884, w0=0.013949008271657213, w1=-0.1038781874165018\n",
      "Gradient Descent(399/399): loss=0.24933293221572322, w0=0.013967067731430634, w1=-0.10395200855119215\n",
      "++++ gamma = 0.0372759372031\n",
      "ciaociaociao\n",
      "Gradient Descent(0/399): loss=0.4999999999999887, w0=-2.567280388770815e-05, w1=-0.013363885544247322\n",
      "Gradient Descent(1/399): loss=0.43441466431056897, w0=0.0008780903417468266, w1=-0.01949318968193197\n",
      "Gradient Descent(2/399): loss=0.4104439244388066, w0=0.0010212860583369318, w1=-0.02393043708394589\n",
      "Gradient Descent(3/399): loss=0.3951609883139459, w0=0.0011818541756782, w1=-0.027147214663952972\n",
      "Gradient Descent(4/399): loss=0.38368862830686307, w0=0.0010314824173061575, w1=-0.029911266572639914\n",
      "Gradient Descent(5/399): loss=0.3742004411508754, w0=0.0010501871500342484, w1=-0.032096197493819235\n",
      "Gradient Descent(6/399): loss=0.3659029264706912, w0=0.0008691802317702445, w1=-0.03412609469195585\n",
      "Gradient Descent(7/399): loss=0.3584274743762378, w0=0.0008656290886104437, w1=-0.03581947351100125\n",
      "Gradient Descent(8/399): loss=0.351588726374289, w0=0.0007057026711305202, w1=-0.037474246948116074\n",
      "Gradient Descent(9/399): loss=0.3452829696252455, w0=0.0007093542217406775, w1=-0.038905020545009744\n",
      "Gradient Descent(10/399): loss=0.33944436476240136, w0=0.0005825302598755167, w1=-0.0403439482194948\n",
      "Gradient Descent(11/399): loss=0.3340256785718423, w0=0.0006017076025220219, w1=-0.041617079984681574\n",
      "Gradient Descent(12/399): loss=0.328989588554468, w0=0.0005094148005107764, w1=-0.04291556966587146\n",
      "Gradient Descent(13/399): loss=0.324304629828738, w0=0.000545973643568772, w1=-0.044082013986615584\n",
      "Gradient Descent(14/399): loss=0.31994322045406876, w0=0.00048636791517712996, w1=-0.04527836930763617\n",
      "Gradient Descent(15/399): loss=0.3158806385326659, w0=0.0005399407632073284, w1=-0.04636468500471457\n",
      "Gradient Descent(16/399): loss=0.3120944474837259, w0=0.000510179328233137, w1=-0.04748017991596121\n",
      "Gradient Descent(17/399): loss=0.308564140127166, w0=0.0005795926083034633, w1=-0.048501525509766955\n",
      "Gradient Descent(18/399): loss=0.3052708948716677, w0=0.0005766111711745239, w1=-0.04954927181190354\n",
      "Gradient Descent(19/399): loss=0.3021973930961468, w0=0.0006604034009099801, w1=-0.0505152406059394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(20/399): loss=0.2993276726606373, w0=0.0006811977985219996, w1=-0.05150420476811059\n",
      "Gradient Descent(21/399): loss=0.29664700470213895, w0=0.0007778281438836587, w1=-0.052421507481858566\n",
      "Gradient Descent(22/399): loss=0.2941417867858433, w0=0.000819561278033604, w1=-0.05335838447175135\n",
      "Gradient Descent(23/399): loss=0.29179944842798505, w0=0.0009275038452651712, w1=-0.05423211948752461\n",
      "Gradient Descent(24/399): loss=0.289608366526515, w0=0.00098754664896677, w1=-0.05512224119986937\n",
      "Gradient Descent(25/399): loss=0.28755778905540025, w0=0.0011053380470515924, w1=-0.05595652028186921\n",
      "Gradient Descent(26/399): loss=0.2856377658446231, w0=0.0011812827426154746, w1=-0.05680431935348817\n",
      "Gradient Descent(27/399): loss=0.28383908555004983, w0=0.0013075474110132435, w1=-0.05760259268744935\n",
      "Gradient Descent(28/399): loss=0.28215321809952065, w0=0.0013972073713278779, w1=-0.05841185583086432\n",
      "Gradient Descent(29/399): loss=0.28057226202688323, w0=0.00153067047264938, w1=-0.05917709275682036\n",
      "Gradient Descent(30/399): loss=0.279088896196962, w0=0.0016320727599052557, w1=-0.059951110777715755\n",
      "Gradient Descent(31/399): loss=0.27769633549406714, w0=0.00177156529411014, w1=-0.06068590860534082\n",
      "Gradient Descent(32/399): loss=0.27638829010171423, w0=0.0018829389869574353, w1=-0.061427573207397126\n",
      "Gradient Descent(33/399): loss=0.2751589280460228, w0=0.0020273978228315514, w1=-0.06213422836459856\n",
      "Gradient Descent(34/399): loss=0.2740028407124709, w0=0.002147160026267792, w1=-0.0628460998011715\n",
      "Gradient Descent(35/399): loss=0.2729150110771169, w0=0.0022956246419174368, w1=-0.06352665798272382\n",
      "Gradient Descent(36/399): loss=0.2718907844202685, w0=0.002422365445783311, w1=-0.06421101551925158\n",
      "Gradient Descent(37/399): loss=0.27092584131380987, w0=0.0025739726388808665, w1=-0.06486730925030847\n",
      "Gradient Descent(38/399): loss=0.2700161726936362, w0=0.002706439886914149, w1=-0.06552619066563291\n",
      "Gradient Descent(39/399): loss=0.26915805684640925, w0=0.0028604173462321246, w1=-0.06615986873683849\n",
      "Gradient Descent(40/399): loss=0.26834803815554364, w0=0.0029975017873467335, w1=-0.06679510229940126\n",
      "Gradient Descent(41/399): loss=0.2675829074652639, w0=0.003153161143779358, w1=-0.06740765357774856\n",
      "Gradient Descent(42/399): loss=0.26685968393400195, w0=0.0032938823237600107, w1=-0.06802088452603523\n",
      "Gradient Descent(43/399): loss=0.26617559825954346, w0=0.003450612096159208, w1=-0.06861365764647491\n",
      "Gradient Descent(44/399): loss=0.26552807716834037, w0=0.00359410519370416, w1=-0.06920637046229025\n",
      "Gradient Descent(45/399): loss=0.26491472907043945, w0=0.0037513638972418205, w1=-0.069780590369247\n",
      "Gradient Descent(46/399): loss=0.2643333307896437, w0=0.0038968676003530898, w1=-0.07035412771995472\n",
      "Gradient Descent(47/399): loss=0.263781815285935, w0=0.004054177180439142, w1=-0.0709109097236354\n",
      "Gradient Descent(48/399): loss=0.26325826029391375, w0=0.004201022626504095, w1=-0.07146648870454221\n",
      "Gradient Descent(49/399): loss=0.2627608778071518, w0=0.004357962309002251, w1=-0.07200685053272485\n",
      "Gradient Descent(50/399): loss=0.262288004343936, w0=0.0045055630652004, w1=-0.07254557668510092\n",
      "Gradient Descent(51/399): loss=0.26183809193500496, w0=0.0046617636651103975, w1=-0.07307044889213513\n",
      "Gradient Descent(52/399): loss=0.2614096997785458, w0=0.004809606697524973, w1=-0.07359332836689184\n",
      "Gradient Descent(53/399): loss=0.26100148651200905, w0=0.004964745396630318, w1=-0.07410356338016591\n",
      "Gradient Descent(54/399): loss=0.26061220305422106, w0=0.0051123829609168465, w1=-0.07461151354179726\n",
      "Gradient Descent(55/399): loss=0.2602406859748859, w0=0.005266178545001264, w1=-0.07510789356735996\n",
      "Gradient Descent(56/399): loss=0.25988585135187586, w0=0.00541322092445089, w1=-0.07560175227652437\n",
      "Gradient Descent(57/399): loss=0.25954668907975453, w0=0.005565429458824578, w1=-0.07608499624186671\n",
      "Gradient Descent(58/399): loss=0.2592222575957827, w0=0.005711538474152685, w1=-0.07656553001175861\n",
      "Gradient Descent(59/399): loss=0.2589116789922279, w0=0.005861949389740641, w1=-0.07703629969030053\n",
      "Gradient Descent(60/399): loss=0.258614134486182, w0=0.006006832606842577, w1=-0.07750421087800366\n",
      "Gradient Descent(61/399): loss=0.25832886022027185, w0=0.006155265166116306, w1=-0.07796311631370798\n",
      "Gradient Descent(62/399): loss=0.2580551433696717, w0=0.006298670731941922, w1=-0.07841904948063401\n",
      "Gradient Descent(63/399): loss=0.257792318532678, w0=0.006444970843250156, w1=-0.07886665381050169\n",
      "Gradient Descent(64/399): loss=0.25753976438383347, w0=0.006586682884913882, w1=-0.0793112013641714\n",
      "Gradient Descent(65/399): loss=0.25729690057016164, w0=0.006730720234426772, w1=-0.07974802511985396\n",
      "Gradient Descent(66/399): loss=0.25706318483253865, w0=0.006870554762077534, w1=-0.08018173233150937\n",
      "Gradient Descent(67/399): loss=0.25683811033557996, w0=0.007012220234028861, w1=-0.08060825728797107\n",
      "Gradient Descent(68/399): loss=0.2566212031906604, w0=0.0071500214934685545, w1=-0.08103162676595774\n",
      "Gradient Descent(69/399): loss=0.2564120201578435, w0=0.007289224851285188, w1=-0.08144829939433719\n",
      "Gradient Descent(70/399): loss=0.2562101465135537, w0=0.007424862077612115, w1=-0.08186179508121451\n",
      "Gradient Descent(71/399): loss=0.256015194071812, w0=0.007561529880616805, w1=-0.08226902965424447\n",
      "Gradient Descent(72/399): loss=0.25582679934775954, w0=0.007694894409149135, w1=-0.08267308040566274\n",
      "Gradient Descent(73/399): loss=0.2556446218530377, w0=0.007828968141658994, w1=-0.0830712617968012\n",
      "Gradient Descent(74/399): loss=0.25546834251336414, w0=0.007959970836996339, w1=-0.08346626459193351\n",
      "Gradient Descent(75/399): loss=0.25529766219936656, w0=0.008091405228722433, w1=-0.08385575080342254\n",
      "Gradient Descent(76/399): loss=0.2551323003623944, w0=0.008219974197006123, w1=-0.08424207362984337\n",
      "Gradient Descent(77/399): loss=0.2549719937676473, w0=0.008348735715635958, w1=-0.08462319807999455\n",
      "Gradient Descent(78/399): loss=0.25481649531752115, w0=0.008474814268871892, w1=-0.08500118253010688\n",
      "Gradient Descent(79/399): loss=0.25466557295860365, w0=0.008600879767558491, w1=-0.08537425612601762\n",
      "Gradient Descent(80/399): loss=0.2545190086662305, w0=0.00872442461228671, w1=-0.08574421973724736\n",
      "Gradient Descent(81/399): loss=0.2543765975009678, w0=0.008847780116463822, w1=-0.08610953275572432\n",
      "Gradient Descent(82/399): loss=0.254238146731798, w0=0.008968759742121355, w1=-0.08647177112256355\n",
      "Gradient Descent(83/399): loss=0.25410347502117525, w0=0.009089399361612722, w1=-0.08682959491914438\n",
      "Gradient Descent(84/399): loss=0.25397241166746726, w0=0.009207792606669186, w1=-0.08718438360160531\n",
      "Gradient Descent(85/399): loss=0.25384479590063586, w0=0.009325717560465354, w1=-0.0875349721651279\n",
      "Gradient Descent(86/399): loss=0.2537204762273082, w0=0.00944151233684468, w1=-0.08788256841516134\n",
      "Gradient Descent(87/399): loss=0.25359930982167544, w0=0.009556730079191552, w1=-0.08822615978325245\n",
      "Gradient Descent(88/399): loss=0.2534811619589179, w0=0.009669922237657128, w1=-0.08856680410810384\n",
      "Gradient Descent(89/399): loss=0.253365905488095, w0=0.00978244567524616, w1=-0.08890362165718137\n",
      "Gradient Descent(90/399): loss=0.25325342034166487, w0=0.009893037996348875, w1=-0.0892375392364337\n",
      "Gradient Descent(91/399): loss=0.25314359307900314, w0=0.010002884787429838, w1=-0.0895677928582884\n",
      "Gradient Descent(92/399): loss=0.25303631646148594, w0=0.010110886084324837, w1=-0.08989519482941832\n",
      "Gradient Descent(93/399): loss=0.25293148905687746, w0=0.010218078011488786, w1=-0.09021908200511958\n",
      "Gradient Descent(94/399): loss=0.252829014870927, w0=0.010323502332440917, w1=-0.09054016663072319\n",
      "Gradient Descent(95/399): loss=0.25272880300423495, w0=0.010428064741653396, w1=-0.09085787341144533\n",
      "Gradient Descent(96/399): loss=0.25263076733258727, w0=0.010530930661394957, w1=-0.09117282713983575\n",
      "Gradient Descent(97/399): loss=0.2525348262090875, w0=0.010632891960606784, w1=-0.0914845290432003\n",
      "Gradient Descent(98/399): loss=0.2524409021865412, w0=0.010733221950903453, w1=-0.09179352747280733\n",
      "Gradient Descent(99/399): loss=0.2523489217586527, w0=0.01083261316223739, w1=-0.09209939030245776\n",
      "Gradient Descent(100/399): loss=0.25225881511870596, w0=0.010930433033076378, w1=-0.09240259905934495\n",
      "Gradient Descent(101/399): loss=0.25217051593449324, w0=0.011027287393190276, w1=-0.09270277965469777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(102/399): loss=0.2520839611383438, w0=0.011122625796944148, w1=-0.09300035519153124\n",
      "Gradient Descent(103/399): loss=0.251999090731192, w0=0.011216978400712067, w1=-0.09329500211396402\n",
      "Gradient Descent(104/399): loss=0.25191584759969643, w0=0.011309866392465956, w1=-0.09358709243790225\n",
      "Gradient Descent(105/399): loss=0.2518341773454956, w0=0.011401753875604513, w1=-0.09387634659903296\n",
      "Gradient Descent(106/399): loss=0.2517540281257509, w0=0.011492224523575567, w1=-0.09416309193524158\n",
      "Gradient Descent(107/399): loss=0.25167535050418643, w0=0.011581684780279263, w1=-0.09444708717241462\n",
      "Gradient Descent(108/399): loss=0.2515980973118986, w0=0.011669772820915839, w1=-0.09472862056923187\n",
      "Gradient Descent(109/399): loss=0.2515222235172499, w0=0.011756844752957293, w1=-0.09500748417284478\n",
      "Gradient Descent(110/399): loss=0.2514476861042225, w0=0.011842586285891134, w1=-0.09528393205402068\n",
      "Gradient Descent(111/399): loss=0.2513744439586413, w0=0.011927309579994936, w1=-0.09555778525089471\n",
      "Gradient Descent(112/399): loss=0.2513024577617261, w0=0.012010741798540442, w1=-0.09582926791979036\n",
      "Gradient Descent(113/399): loss=0.2512316898904668, w0=0.012093156729156467, w1=-0.0960982263164016\n",
      "Gradient Descent(114/399): loss=0.25116210432435226, w0=0.012174317682514799, w1=-0.09636485841655605\n",
      "Gradient Descent(115/399): loss=0.25109366655801896, w0=0.012254464937402221, w1=-0.09662903240559621\n",
      "Gradient Descent(116/399): loss=0.2510263435194133, w0=0.012333393321140739, w1=-0.09689092334164104\n",
      "Gradient Descent(117/399): loss=0.2509601034930925, w0=0.012411313847430853, w1=-0.09715041847506418\n",
      "Gradient Descent(118/399): loss=0.2508949160483153, w0=0.012488048819175979, w1=-0.09740767279758225\n",
      "Gradient Descent(119/399): loss=0.25083075197159665, w0=0.01256378368781317, w1=-0.09766259012901055\n",
      "Gradient Descent(120/399): loss=0.2507675832034274, w0=0.012638364705422317, w1=-0.09791530788659279\n",
      "Gradient Descent(121/399): loss=0.250705382778874, w0=0.012711954992090798, w1=-0.0981657442856992\n",
      "Gradient Descent(122/399): loss=0.25064412477180414, w0=0.012784421671861085, w1=-0.0984140213471446\n",
      "Gradient Descent(123/399): loss=0.2505837842424896, w0=0.012855908352692595, w1=-0.09866006978839921\n",
      "Gradient Descent(124/399): loss=0.2505243371883652, w0=0.012926300345424377, w1=-0.0989039981377262\n",
      "Gradient Descent(125/399): loss=0.25046576049773317, w0=0.012995724205951137, w1=-0.09914574796568278\n",
      "Gradient Descent(126/399): loss=0.2504080319062174, w0=0.01306408108891677, w1=-0.09938541597237115\n",
      "Gradient Descent(127/399): loss=0.25035112995578673, w0=0.013131482644886242, w1=-0.09962295314548046\n",
      "Gradient Descent(128/399): loss=0.2502950339561793, w0=0.01319784382796181, w1=-0.09985844581213794\n",
      "Gradient Descent(129/399): loss=0.25023972394856925, w0=0.013263263256767268, w1=-0.10009185312690155\n",
      "Gradient Descent(130/399): loss=0.2501851806713321, w0=0.013327667901170088, w1=-0.10032325231634688\n",
      "Gradient Descent(131/399): loss=0.25013138552777, w0=0.013391144982774744, w1=-0.10055260961346782\n",
      "Gradient Descent(132/399): loss=0.25007832055567286, w0=0.013453631931014806, w1=-0.10077999425703912\n",
      "Gradient Descent(133/399): loss=0.2500259683985949, w0=0.013515205997358918, w1=-0.10100537861108275\n",
      "Gradient Descent(134/399): loss=0.24997431227874023, w0=0.013575813713160087, w1=-0.10122882489981502\n",
      "Gradient Descent(135/399): loss=0.2499233359713499, w0=0.013635523605141198, w1=-0.10145031079376346\n",
      "Gradient Descent(136/399): loss=0.2498730237805015, w0=0.013694290122219982, w1=-0.10166989235392924\n",
      "Gradient Descent(137/399): loss=0.24982336051622697, w0=0.013752174153427235, w1=-0.1018875518398938\n",
      "Gradient Descent(138/399): loss=0.24977433147286857, w0=0.013809137032134901, w1=-0.10210333989426636\n",
      "Gradient Descent(139/399): loss=0.24972592240859637, w0=0.013865232958600333, w1=-0.10231724274151434\n",
      "Gradient Descent(140/399): loss=0.2496781195260115, w0=0.013920429249539646, w1=-0.10252930625759\n",
      "Gradient Descent(141/399): loss=0.2496309094537738, w0=0.013974774244843221, w1=-0.10273952008894373\n",
      "Gradient Descent(142/399): loss=0.24958427922918647, w0=0.014028240458665414, w1=-0.10294792591524919\n",
      "Gradient Descent(143/399): loss=0.24953821628168016, w0=0.014080871093797312, w1=-0.1031545163328248\n",
      "Gradient Descent(144/399): loss=0.24949270841714402, w0=0.014132643176469286, w1=-0.10335932932433475\n",
      "Gradient Descent(145/399): loss=0.24944774380305113, w0=0.014183595403912999, w1=-0.10356236002550646\n",
      "Gradient Descent(146/399): loss=0.24940331095433066, w0=0.014233708716820266, w1=-0.10376364315910563\n",
      "Gradient Descent(147/399): loss=0.24935939871994217, w0=0.014283017858374405, w1=-0.10396317604350591\n",
      "Gradient Descent(148/399): loss=0.2493159962701122, w0=0.014331507162692882, w1=-0.10416099052434667\n",
      "Gradient Descent(149/399): loss=0.24927309308419252, w0=0.014379207900598493, w1=-0.10435708579264423\n",
      "Gradient Descent(150/399): loss=0.24923067893910467, w0=0.014426107345428688, w1=-0.10455149115217612\n",
      "Gradient Descent(151/399): loss=0.24918874389833623, w0=0.014472233716413029, w1=-0.10474420739731113\n",
      "Gradient Descent(152/399): loss=0.24914727830145778, w0=0.014517576830224373, w1=-0.10493526158368993\n",
      "Gradient Descent(153/399): loss=0.24910627275413083, w0=0.01456216222211194, w1=-0.10512465587518856\n",
      "Gradient Descent(154/399): loss=0.2490657181185783, w0=0.014605981907093436, w1=-0.10531241533671064\n",
      "Gradient Descent(155/399): loss=0.24902560550449226, w0=0.014649059057670923, w1=-0.10549854329864855\n",
      "Gradient Descent(156/399): loss=0.2489859262603537, w0=0.014691387586627715, w1=-0.10568306306079989\n",
      "Gradient Descent(157/399): loss=0.24894667196514297, w0=0.014732988584481983, w1=-0.10586597894393672\n",
      "Gradient Descent(158/399): loss=0.24890783442041792, w0=0.014773857599956336, w1=-0.10604731268059428\n",
      "Gradient Descent(159/399): loss=0.24886940564274024, w0=0.014814013887033671, w1=-0.10622706942915718\n",
      "Gradient Descent(160/399): loss=0.248831377856432, w0=0.01485345440236358, w1=-0.10640526952843395\n",
      "Gradient Descent(161/399): loss=0.24879374348664388, w0=0.014892196778024828, w1=-0.10658191884198816\n",
      "Gradient Descent(162/399): loss=0.248756495152718, w0=0.01493023918008476, w1=-0.10675703646717029\n",
      "Gradient Descent(163/399): loss=0.24871962566183176, w0=0.01496759780645467, w1=-0.10693062885797842\n",
      "Gradient Descent(164/399): loss=0.24868312800290582, w0=0.015004271859850754, w1=-0.10710271400396441\n",
      "Gradient Descent(165/399): loss=0.24864699534076468, w0=0.015040276268281222, w1=-0.1072732988502021\n",
      "Gradient Descent(166/399): loss=0.24861122101053465, w0=0.015075611120798275, w1=-0.10744240039581825\n",
      "Gradient Descent(167/399): loss=0.24857579851226913, w0=0.015110290219284448, w1=-0.10761002599098356\n",
      "Gradient Descent(168/399): loss=0.2485407215057894, w0=0.015144314408404531, w1=-0.10777619174751826\n",
      "Gradient Descent(169/399): loss=0.24850598380572855, w0=0.015177696489810105, w1=-0.10794090534634387\n",
      "Gradient Descent(170/399): loss=0.24847157937677092, w0=0.015210437950142455, w1=-0.10810418210261345\n",
      "Gradient Descent(171/399): loss=0.24843750232907563, w0=0.015242550701106035, w1=-0.10826602996376514\n",
      "Gradient Descent(172/399): loss=0.24840374691387637, w0=0.015274036772586038, w1=-0.10842646352799745\n",
      "Gradient Descent(173/399): loss=0.24837030751924838, w0=0.015304907282994627, w1=-0.10858549095381856\n",
      "Gradient Descent(174/399): loss=0.24833717866603497, w0=0.015335164719725701, w1=-0.10874312619261602\n",
      "Gradient Descent(175/399): loss=0.2483043550039261, w0=0.015364819492653893, w1=-0.10889937756615634\n",
      "Gradient Descent(176/399): loss=0.24827183130768138, w0=0.015393874472280387, w1=-0.1090542584407779\n",
      "Gradient Descent(177/399): loss=0.2482396024734923, w0=0.015422339434305309, w1=-0.10920777726032538\n",
      "Gradient Descent(178/399): loss=0.24820766351547466, w0=0.015450217567817516, w1=-0.10935994686050647\n",
      "Gradient Descent(179/399): loss=0.24817600956228858, w0=0.015477518079629781, w1=-0.10951077577182217\n",
      "Gradient Descent(180/399): loss=0.2481446358538782, w0=0.015504244421513599, w1=-0.10966027634733307\n",
      "Gradient Descent(181/399): loss=0.24811353773832498, w0=0.01553040528875369, w1=-0.10980845717377319\n",
      "Gradient Descent(182/399): loss=0.24808271066881338, w0=0.01555600434740779, w1=-0.10995533016389947\n",
      "Gradient Descent(183/399): loss=0.24805215020069948, w0=0.015581049831665599, w1=-0.11010090393459318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(184/399): loss=0.2480218519886812, w0=0.015605545580018159, w1=-0.11024518999570612\n",
      "Gradient Descent(185/399): loss=0.24799181178406432, w0=0.015629499409940812, w1=-0.11038819697194363\n",
      "Gradient Descent(186/399): loss=0.24796202543212098, w0=0.015652915296206115, w1=-0.11052993600331461\n",
      "Gradient Descent(187/399): loss=0.2479324888695364, w0=0.015675800678665815, w1=-0.11067041570328769\n",
      "Gradient Descent(188/399): loss=0.24790319812193992, w0=0.015698159637188357, w1=-0.11080964687128732\n",
      "Gradient Descent(189/399): loss=0.24787414930151858, w0=0.015719999268468066, w1=-0.11094763809331233\n",
      "Gradient Descent(190/399): loss=0.24784533860470623, w0=0.01574132373060838, w1=-0.11108439985412359\n",
      "Gradient Descent(191/399): loss=0.24781676230994953, w0=0.015762139807568333, w1=-0.11121994069846661\n",
      "Gradient Descent(192/399): loss=0.24778841677554497, w0=0.015782451712590604, w1=-0.11135427081943004\n",
      "Gradient Descent(193/399): loss=0.24776029843754455, w0=0.015802265943783627, w1=-0.11148739870884367\n",
      "Gradient Descent(194/399): loss=0.24773240380772754, w0=0.015821586749710397, w1=-0.1116193342885432\n",
      "Gradient Descent(195/399): loss=0.2477047294716371, w0=0.01584042036641814, w1=-0.11175008598761577\n",
      "Gradient Descent(196/399): loss=0.2476772720866766, w0=0.015858771060822046, w1=-0.11187966347480433\n",
      "Gradient Descent(197/399): loss=0.24765002838026567, w0=0.015876644827988146, w1=-0.11200807510821388\n",
      "Gradient Descent(198/399): loss=0.24762299514805297, w0=0.01589404593869486, w1=-0.1121353303196698\n",
      "Gradient Descent(199/399): loss=0.24759616925218286, w0=0.01591098016573443, w1=-0.11226143738942759\n",
      "Gradient Descent(200/399): loss=0.24756954761961522, w0=0.01592745177141456, w1=-0.11238640552682544\n",
      "Gradient Descent(201/399): loss=0.24754312724049574, w0=0.015943466322882516, w1=-0.1125102429285869\n",
      "Gradient Descent(202/399): loss=0.24751690516657404, w0=0.015959028063513552, w1=-0.11263295859445899\n",
      "Gradient Descent(203/399): loss=0.2474908785096708, w0=0.015974142369616928, w1=-0.11275456063297348\n",
      "Gradient Descent(204/399): loss=0.2474650444401878, w0=0.01598881345679933, w1=-0.11287505784583239\n",
      "Gradient Descent(205/399): loss=0.2474394001856632, w0=0.01600304652374121, w1=-0.11299445824959747\n",
      "Gradient Descent(206/399): loss=0.24741394302936853, w0=0.01601684575085517, w1=-0.11311277045828377\n",
      "Gradient Descent(207/399): loss=0.24738867030894632, w0=0.01603021617100002, w1=-0.11323000239346419\n",
      "Gradient Descent(208/399): loss=0.24736357941508685, w0=0.016043161923191857, w1=-0.11334616249077847\n",
      "Gradient Descent(209/399): loss=0.247338667790243, w0=0.016055687885043902, w1=-0.1134612585744452\n",
      "Gradient Descent(210/399): loss=0.24731393292738169, w0=0.016067798149032992, w1=-0.11357529891011843\n",
      "Gradient Descent(211/399): loss=0.24728937236877063, w0=0.01607949744702105, w1=-0.11368829122285848\n",
      "Gradient Descent(212/399): loss=0.24726498370479918, w0=0.016090789820720087, w1=-0.11380024361591047\n",
      "Gradient Descent(213/399): loss=0.24724076457283267, w0=0.0161016798647837, w1=-0.11391116371385428\n",
      "Gradient Descent(214/399): loss=0.24721671265609785, w0=0.01611217156672658, w1=-0.11402105946438564\n",
      "Gradient Descent(215/399): loss=0.24719282568259976, w0=0.016122269391699667, w1=-0.11412993839069473\n",
      "Gradient Descent(216/399): loss=0.2471691014240688, w0=0.016131977270272737, w1=-0.11423780829115428\n",
      "Gradient Descent(217/399): loss=0.2471455376949357, w0=0.016141299545062084, w1=-0.11434467658700864\n",
      "Gradient Descent(218/399): loss=0.24712213235133587, w0=0.01615024008753571, w1=-0.11445055093297454\n",
      "Gradient Descent(219/399): loss=0.24709888329013935, w0=0.016158803124092753, w1=-0.11455543864809581\n",
      "Gradient Descent(220/399): loss=0.24707578844800795, w0=0.01616699246545126, w1=-0.11465934724860562\n",
      "Gradient Descent(221/399): loss=0.24705284580047748, w0=0.01617481222753637, w1=-0.1147622839513494\n",
      "Gradient Descent(222/399): loss=0.24703005336106454, w0=0.016182266159105296, w1=-0.11486425613881145\n",
      "Gradient Descent(223/399): loss=0.24700740918039726, w0=0.016189358270844702, w1=-0.11496527092585906\n",
      "Gradient Descent(224/399): loss=0.24698491134536912, w0=0.01619609224871522, w1=-0.11506533556557479\n",
      "Gradient Descent(225/399): loss=0.24696255797831468, w0=0.016202472002951256, w1=-0.11516445707125277\n",
      "Gradient Descent(226/399): loss=0.24694034723620778, w0=0.016208501156202242, w1=-0.11526264257057718\n",
      "Gradient Descent(227/399): loss=0.2469182773098803, w0=0.016214183522638304, w1=-0.11535989897583002\n",
      "Gradient Descent(228/399): loss=0.2468963464232612, w0=0.016219522661357184, w1=-0.11545623329299544\n",
      "Gradient Descent(229/399): loss=0.2468745528326356, w0=0.016224522294499246, w1=-0.11555165233403539\n",
      "Gradient Descent(230/399): loss=0.24685289482592315, w0=0.016229185917603323, w1=-0.11564616298666136\n",
      "Gradient Descent(231/399): loss=0.24683137072197425, w0=0.016233517164500274, w1=-0.11573977196331701\n",
      "Gradient Descent(232/399): loss=0.24680997886988557, w0=0.01623751946736064, w1=-0.1158324860366275\n",
      "Gradient Descent(233/399): loss=0.24678871764833174, w0=0.016241196375146122, w1=-0.11592431182041119\n",
      "Gradient Descent(234/399): loss=0.24676758546491553, w0=0.016244551257016633, w1=-0.11601525597517844\n",
      "Gradient Descent(235/399): loss=0.2467465807555325, w0=0.016247587580255376, w1=-0.11610532501709071\n",
      "Gradient Descent(236/399): loss=0.2467257019837545, w0=0.01625030865150963, w1=-0.11619452549732368\n",
      "Gradient Descent(237/399): loss=0.24670494764022555, w0=0.016252717859351482, w1=-0.11628286383541196\n",
      "Gradient Descent(238/399): loss=0.24668431624207565, w0=0.016254818448530786, w1=-0.11637034647580553\n",
      "Gradient Descent(239/399): loss=0.2466638063323473, w0=0.016256613731675928, w1=-0.11645697974249229\n",
      "Gradient Descent(240/399): loss=0.24664341647943774, w0=0.016258106892351725, w1=-0.1165427699756525\n",
      "Gradient Descent(241/399): loss=0.24662314527655355, w0=0.016259301169830716, w1=-0.11662772340484734\n",
      "Gradient Descent(242/399): loss=0.24660299134118036, w0=0.01626019968728489, w1=-0.11671184626830623\n",
      "Gradient Descent(243/399): loss=0.24658295331456398, w0=0.016260805613057296, w1=-0.11679514470231507\n",
      "Gradient Descent(244/399): loss=0.2465630298612062, w0=0.016261122010784147, w1=-0.11687762484534815\n",
      "Gradient Descent(245/399): loss=0.2465432196683709, w0=0.016261151980159635, w1=-0.11695929274159145\n",
      "Gradient Descent(246/399): loss=0.24652352144560374, w0=0.016260898526193213, w1=-0.11704015443184924\n",
      "Gradient Descent(247/399): loss=0.24650393392426265, w0=0.016260364682079146, w1=-0.11712021586940062\n",
      "Gradient Descent(248/399): loss=0.2464844558570603, w0=0.0162595533951499, w1=-0.11719948299936511\n",
      "Gradient Descent(249/399): loss=0.24646508601761724, w0=0.016258467634129352, w1=-0.11727796168532034\n",
      "Gradient Descent(250/399): loss=0.24644582320002556, w0=0.016257110289654, w1=-0.1173556577785963\n",
      "Gradient Descent(251/399): loss=0.24642666621842424, w0=0.01625548426789838, w1=-0.11743257705428237\n",
      "Gradient Descent(252/399): loss=0.24640761390658278, w0=0.016253592403807006, w1=-0.11750872527173262\n",
      "Gradient Descent(253/399): loss=0.24638866511749685, w0=0.016251437542827263, w1=-0.1175841081187653\n",
      "Gradient Descent(254/399): loss=0.2463698187229916, w0=0.01624902246523162, w1=-0.11765873126449825\n",
      "Gradient Descent(255/399): loss=0.24635107361333541, w0=0.016246349957472178, w1=-0.11773260031069636\n",
      "Gradient Descent(256/399): loss=0.24633242869686311, w0=0.01624342274617922, w1=-0.11780572083791367\n",
      "Gradient Descent(257/399): loss=0.24631388289960754, w0=0.016240243560458698, w1=-0.11787809836307735\n",
      "Gradient Descent(258/399): loss=0.2462954351649392, w0=0.01623681507433334, w1=-0.11794973837978864\n",
      "Gradient Descent(259/399): loss=0.2462770844532155, w0=0.016233139961136067, w1=-0.11802064632134829\n",
      "Gradient Descent(260/399): loss=0.24625882974143765, w0=0.016229220843317113, w1=-0.11809082759595974\n",
      "Gradient Descent(261/399): loss=0.24624067002291536, w0=0.01622506033993948, w1=-0.11816028755450211\n",
      "Gradient Descent(262/399): loss=0.24622260430693976, w0=0.016220661022912693, w1=-0.1182290315212847\n",
      "Gradient Descent(263/399): loss=0.24620463161846345, w0=0.01621602545846828, w1=-0.11829706476596183\n",
      "Gradient Descent(264/399): loss=0.24618675099778814, w0=0.016211156169000467, w1=-0.11836439253040493\n",
      "Gradient Descent(265/399): loss=0.24616896150025974, w0=0.01620605566928783, w1=-0.11843102000423111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(266/399): loss=0.24615126219596967, w0=0.016200726433225756, w1=-0.11849695234828671\n",
      "Gradient Descent(267/399): loss=0.24613365216946445, w0=0.016195170925462774, w1=-0.11856219467332867\n",
      "Gradient Descent(268/399): loss=0.24611613051945985, w0=0.01618939157240073, w1=-0.11862675206055072\n",
      "Gradient Descent(269/399): loss=0.2460986963585634, w0=0.01618339078982921, w1=-0.11869062954301532\n",
      "Gradient Descent(270/399): loss=0.24608134881300242, w0=0.01617717095764891, w1=-0.11875383212359877\n",
      "Gradient Descent(271/399): loss=0.24606408702235832, w0=0.01617073444401322, w1=-0.11881636475882267\n",
      "Gradient Descent(272/399): loss=0.24604691013930638, w0=0.016164083583299675, w1=-0.11887823237454623\n",
      "Gradient Descent(273/399): loss=0.24602981732936235, w0=0.01615722069720299, w1=-0.11893943985189112\n",
      "Gradient Descent(274/399): loss=0.246012807770634, w0=0.01615014807553997, w1=-0.1189999920409675\n",
      "Gradient Descent(275/399): loss=0.24599588065357883, w0=0.0161428679946817, w1=-0.11905989374862488\n",
      "Gradient Descent(276/399): loss=0.24597903518076594, w0=0.016135382700830203, w1=-0.11911914975046194\n",
      "Gradient Descent(277/399): loss=0.24596227056664538, w0=0.016127694426128082, w1=-0.11917776478017031\n",
      "Gradient Descent(278/399): loss=0.24594558603732025, w0=0.016119805374091304, w1=-0.11923574354004637\n",
      "Gradient Descent(279/399): loss=0.2459289808303254, w0=0.01611171773369152, w1=-0.11929309069172443\n",
      "Gradient Descent(280/399): loss=0.2459124541944103, w0=0.0161034336666696, w1=-0.11934981086538084\n",
      "Gradient Descent(281/399): loss=0.24589600538932754, w0=0.01609495531984826, w1=-0.11940590865167915\n",
      "Gradient Descent(282/399): loss=0.2458796336856243, w0=0.016086284814086093, w1=-0.11946138860983266\n",
      "Gradient Descent(283/399): loss=0.24586333836444077, w0=0.016077424255045294, w1=-0.11951625526060669\n",
      "Gradient Descent(284/399): loss=0.24584711871731046, w0=0.016068375723576556, w1=-0.1195705130933845\n",
      "Gradient Descent(285/399): loss=0.24583097404596693, w0=0.01605914128513813, w1=-0.11962416656009149\n",
      "Gradient Descent(286/399): loss=0.24581490366215333, w0=0.016049722981428635, w1=-0.11967722008139108\n",
      "Gradient Descent(287/399): loss=0.24579890688743694, w0=0.01604012283862869, w1=-0.11972967804141292\n",
      "Gradient Descent(288/399): loss=0.24578298305302715, w0=0.016030342860122056, w1=-0.1197815447931891\n",
      "Gradient Descent(289/399): loss=0.2457671314995976, w0=0.016020385033709245, w1=-0.11983282465408354\n",
      "Gradient Descent(290/399): loss=0.24575135157711236, w0=0.01601025132527779, w1=-0.1198835219105648\n",
      "Gradient Descent(291/399): loss=0.24573564264465572, w0=0.015999943685118225, w1=-0.11993364081424687\n",
      "Gradient Descent(292/399): loss=0.24572000407026456, w0=0.015989464042421974, w1=-0.11998318558608279\n",
      "Gradient Descent(293/399): loss=0.24570443523076665, w0=0.01597881431081355, w1=-0.12003216041293843\n",
      "Gradient Descent(294/399): loss=0.24568893551161994, w0=0.015967996383570073, w1=-0.12008056945128026\n",
      "Gradient Descent(295/399): loss=0.24567350430675705, w0=0.015957012138468993, w1=-0.12012841682421382\n",
      "Gradient Descent(296/399): loss=0.24565814101843167, w0=0.015945863433636784, w1=-0.12017570662472966\n",
      "Gradient Descent(297/399): loss=0.2456428450570695, w0=0.015934552111798873, w1=-0.12022244291314695\n",
      "Gradient Descent(298/399): loss=0.24562761584112114, w0=0.01592307999667685, w1=-0.12026862971997333\n",
      "Gradient Descent(299/399): loss=0.24561245279691907, w0=0.015911448896716248, w1=-0.12031427104370175\n",
      "Gradient Descent(300/399): loss=0.24559735535853688, w0=0.01589966060196191, w1=-0.12035937085333301\n",
      "Gradient Descent(301/399): loss=0.24558232296765195, w0=0.01588771688732965, w1=-0.12040393308648031\n",
      "Gradient Descent(302/399): loss=0.24556735507341076, w0=0.015875619509898357, w1=-0.12044796165159721\n",
      "Gradient Descent(303/399): loss=0.245552451132297, w0=0.015863370211783207, w1=-0.12049146042635017\n",
      "Gradient Descent(304/399): loss=0.2455376106080024, w0=0.01585097071779093, w1=-0.12053443325958908\n",
      "Gradient Descent(305/399): loss=0.24552283297130045, w0=0.01583842273794488, w1=-0.12057688396995335\n",
      "Gradient Descent(306/399): loss=0.2455081176999226, w0=0.01582572796545674, w1=-0.12061881634761733\n",
      "Gradient Descent(307/399): loss=0.24549346427843644, w0=0.015812888078947374, w1=-0.12066023415309979\n",
      "Gradient Descent(308/399): loss=0.24547887219812803, w0=0.0157999047406942, w1=-0.12070114111881261\n",
      "Gradient Descent(309/399): loss=0.2454643409568844, w0=0.015786779598586164, w1=-0.12074154094804732\n",
      "Gradient Descent(310/399): loss=0.24544987005908023, w0=0.015773514284611236, w1=-0.12078143731635171\n",
      "Gradient Descent(311/399): loss=0.24543545901546596, w0=0.01576011041657894, w1=-0.12082083387067057\n",
      "Gradient Descent(312/399): loss=0.2454211073430588, w0=0.01574656959681698, w1=-0.12085973423057171\n",
      "Gradient Descent(313/399): loss=0.24540681456503488, w0=0.015732893413690555, w1=-0.12089814198752087\n",
      "Gradient Descent(314/399): loss=0.2453925802106251, w0=0.015719083440481053, w1=-0.12093606070597607\n",
      "Gradient Descent(315/399): loss=0.24537840381501158, w0=0.01570514123672763, w1=-0.12097349392277912\n",
      "Gradient Descent(316/399): loss=0.24536428491922727, w0=0.015691068347264404, w1=-0.12101044514813475\n",
      "Gradient Descent(317/399): loss=0.24535022307005694, w0=0.015676866303406572, w1=-0.12104691786510358\n",
      "Gradient Descent(318/399): loss=0.2453362178199409, w0=0.015662536622125516, w1=-0.12108291553048023\n",
      "Gradient Descent(319/399): loss=0.2453222687268794, w0=0.015648080807098886, w1=-0.12111844157437453\n",
      "Gradient Descent(320/399): loss=0.24530837535434025, w0=0.015633500348005706, w1=-0.12115349940100113\n",
      "Gradient Descent(321/399): loss=0.24529453727116743, w0=0.01561879672145737, w1=-0.12118809238833736\n",
      "Gradient Descent(322/399): loss=0.24528075405149197, w0=0.015603971390397113, w1=-0.12122222388883527\n",
      "Gradient Descent(323/399): loss=0.2452670252746445, w0=0.015589025804926746, w1=-0.12125589722914597\n",
      "Gradient Descent(324/399): loss=0.24525335052506908, w0=0.015573961401796836, w1=-0.12128911571076373\n",
      "Gradient Descent(325/399): loss=0.24523972939223956, w0=0.015558779605142145, w1=-0.12132188260980795\n",
      "Gradient Descent(326/399): loss=0.24522616147057663, w0=0.015543481826050608, w1=-0.1213542011776076\n",
      "Gradient Descent(327/399): loss=0.2452126463593675, w0=0.015528069463218771, w1=-0.12138607464053322\n",
      "Gradient Descent(328/399): loss=0.24519918366268612, w0=0.01551254390258923, w1=-0.12141750620052891\n",
      "Gradient Descent(329/399): loss=0.24518577298931613, w0=0.015496906517935914, w1=-0.12144849903498763\n",
      "Gradient Descent(330/399): loss=0.245172413952674, w0=0.015481158670560958, w1=-0.12147905629723715\n",
      "Gradient Descent(331/399): loss=0.24515910617073466, w0=0.015465301709818448, w1=-0.12150918111645284\n",
      "Gradient Descent(332/399): loss=0.2451458492659583, w0=0.015449336972862847, w1=-0.12153887659810303\n",
      "Gradient Descent(333/399): loss=0.24513264286521808, w0=0.015433265785118762, w1=-0.12156814582389418\n",
      "Gradient Descent(334/399): loss=0.2451194865997303, w0=0.015417089460074024, w1=-0.12159699185218059\n",
      "Gradient Descent(335/399): loss=0.24510638010498492, w0=0.015400809299702079, w1=-0.12162541771793747\n",
      "Gradient Descent(336/399): loss=0.24509332302067766, w0=0.015384426594293742, w1=-0.12165342643313924\n",
      "Gradient Descent(337/399): loss=0.24508031499064378, w0=0.015367942622837947, w1=-0.1216810209867566\n",
      "Gradient Descent(338/399): loss=0.24506735566279256, w0=0.015351358652886974, w1=-0.12170820434510703\n",
      "Gradient Descent(339/399): loss=0.24505444468904372, w0=0.015334675940900587, w1=-0.12173497945187266\n",
      "Gradient Descent(340/399): loss=0.24504158172526436, w0=0.015317895732140197, w1=-0.12176134922842617\n",
      "Gradient Descent(341/399): loss=0.24502876643120744, w0=0.015301019260980772, w1=-0.12178731657386636\n",
      "Gradient Descent(342/399): loss=0.24501599847045144, w0=0.015284047750829985, w1=-0.12181288436532242\n",
      "Gradient Descent(343/399): loss=0.2450032775103409, w0=0.015266982414411746, w1=-0.12183805545800461\n",
      "Gradient Descent(344/399): loss=0.24499060322192862, w0=0.015249824453706881, w1=-0.12186283268548931\n",
      "Gradient Descent(345/399): loss=0.24497797527991816, w0=0.015232575060211653, w1=-0.12188721885978265\n",
      "Gradient Descent(346/399): loss=0.24496539336260809, w0=0.015215235414896984, w1=-0.1219112167715885\n",
      "Gradient Descent(347/399): loss=0.24495285715183723, w0=0.015197806688444868, w1=-0.121934829190383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(348/399): loss=0.24494036633292993, w0=0.015180290041223565, w1=-0.1219580588646674\n",
      "Gradient Descent(349/399): loss=0.24492792059464444, w0=0.01516268662350448, w1=-0.12198090852205197\n",
      "Gradient Descent(350/399): loss=0.24491551962911945, w0=0.01514499757545102, w1=-0.12200338086949518\n",
      "Gradient Descent(351/399): loss=0.24490316313182453, w0=0.015127224027318227, w1=-0.12202547859339546\n",
      "Gradient Descent(352/399): loss=0.24489085080150882, w0=0.015109367099453305, w1=-0.12204720435981835\n",
      "Gradient Descent(353/399): loss=0.24487858234015306, w0=0.015091427902479979, w1=-0.12206856081459468\n",
      "Gradient Descent(354/399): loss=0.24486635745292024, w0=0.01507340753730899, w1=-0.12208955058353695\n",
      "Gradient Descent(355/399): loss=0.24485417584810915, w0=0.015055307095308881, w1=-0.122110176272543\n",
      "Gradient Descent(356/399): loss=0.24484203723710757, w0=0.015037127658324989, w1=-0.12213044046780254\n",
      "Gradient Descent(357/399): loss=0.24482994133434635, w0=0.015018870298838161, w1=-0.1221503457359053\n",
      "Gradient Descent(358/399): loss=0.24481788785725517, w0=0.015000536079990948, w1=-0.12216989462403885\n",
      "Gradient Descent(359/399): loss=0.24480587652621796, w0=0.014982126055735578, w1=-0.12218908966010034\n",
      "Gradient Descent(360/399): loss=0.24479390706453025, w0=0.014963641270866211, w1=-0.12220793335288636\n",
      "Gradient Descent(361/399): loss=0.24478197919835631, w0=0.014945082761157363, w1=-0.12222642819220768\n",
      "Gradient Descent(362/399): loss=0.2447700926566878, w0=0.014926451553401218, w1=-0.12224457664907179\n",
      "Gradient Descent(363/399): loss=0.2447582471713024, w0=0.014907748665537523, w1=-0.12226238117579996\n",
      "Gradient Descent(364/399): loss=0.24474644247672445, w0=0.014888975106695129, w1=-0.12227984420620332\n",
      "Gradient Descent(365/399): loss=0.24473467831018444, w0=0.014870131877314242, w1=-0.1222969681557015\n",
      "Gradient Descent(366/399): loss=0.24472295441158112, w0=0.014851219969191453, w1=-0.12231375542149286\n",
      "Gradient Descent(367/399): loss=0.24471127052344313, w0=0.01483224036559513, w1=-0.12233020838267433\n",
      "Gradient Descent(368/399): loss=0.24469962639089132, w0=0.014813194041313317, w1=-0.12234632940040617\n",
      "Gradient Descent(369/399): loss=0.2446880217616024, w0=0.014794081962762943, w1=-0.12236212081803265\n",
      "Gradient Descent(370/399): loss=0.24467645638577268, w0=0.014774905088040045, w1=-0.12237758496124178\n",
      "Gradient Descent(371/399): loss=0.2446649300160829, w0=0.014755664367023406, w1=-0.1223927241381865\n",
      "Gradient Descent(372/399): loss=0.2446534424076624, w0=0.01473636074142662, w1=-0.12240754063963977\n",
      "Gradient Descent(373/399): loss=0.24464199331805642, w0=0.014716995144896692, w1=-0.1224220367391159\n",
      "Gradient Descent(374/399): loss=0.24463058250719116, w0=0.014697568503067545, w1=-0.1224362146930214\n",
      "Gradient Descent(375/399): loss=0.24461920973734164, w0=0.014678081733654052, w1=-0.12245007674077618\n",
      "Gradient Descent(376/399): loss=0.2446078747730983, w0=0.014658535746506616, w1=-0.12246362510496042\n",
      "Gradient Descent(377/399): loss=0.24459657738133617, w0=0.014638931443701066, w1=-0.12247686199143547\n",
      "Gradient Descent(378/399): loss=0.24458531733118258, w0=0.014619269719594017, w1=-0.12248978958948699\n",
      "Gradient Descent(379/399): loss=0.24457409439398686, w0=0.014599551460908952, w1=-0.12250241007194534\n",
      "Gradient Descent(380/399): loss=0.24456290834328986, w0=0.014579777546792136, w1=-0.12251472559532527\n",
      "Gradient Descent(381/399): loss=0.24455175895479433, w0=0.014559948848895226, w1=-0.12252673829994556\n",
      "Gradient Descent(382/399): loss=0.2445406460063353, w0=0.014540066231431467, w1=-0.1225384503100655\n",
      "Gradient Descent(383/399): loss=0.2445295692778518, w0=0.014520130551255143, w1=-0.12254986373400366\n",
      "Gradient Descent(384/399): loss=0.2445185285513582, w0=0.014500142657917872, w1=-0.12256098066427135\n",
      "Gradient Descent(385/399): loss=0.2445075236109166, w0=0.014480103393745131, w1=-0.12257180317769042\n",
      "Gradient Descent(386/399): loss=0.24449655424260963, w0=0.014460013593892505, w1=-0.12258233333552379\n",
      "Gradient Descent(387/399): loss=0.24448562023451303, w0=0.014439874086419497, w1=-0.1225925731835922\n",
      "Gradient Descent(388/399): loss=0.2444747213766699, w0=0.01441968569234559, w1=-0.12260252475240195\n",
      "Gradient Descent(389/399): loss=0.2444638574610643, w0=0.014399449225721577, w1=-0.12261219005726062\n",
      "Gradient Descent(390/399): loss=0.24445302828159587, w0=0.014379165493685281, w1=-0.12262157109840208\n",
      "Gradient Descent(391/399): loss=0.24444223363405487, w0=0.014358835296530582, w1=-0.12263066986110102\n",
      "Gradient Descent(392/399): loss=0.244431473316097, w0=0.014338459427762724, w1=-0.12263948831579548\n",
      "Gradient Descent(393/399): loss=0.24442074712721948, w0=0.014318038674165166, w1=-0.12264802841820004\n",
      "Gradient Descent(394/399): loss=0.24441005486873757, w0=0.014297573815854435, w1=-0.12265629210942605\n",
      "Gradient Descent(395/399): loss=0.24439939634375996, w0=0.014277065626344914, w1=-0.12266428131609347\n",
      "Gradient Descent(396/399): loss=0.24438877135716675, w0=0.014256514872603142, w1=-0.12267199795044872\n",
      "Gradient Descent(397/399): loss=0.24437817971558595, w0=0.014235922315110734, w1=-0.12267944391047524\n",
      "Gradient Descent(398/399): loss=0.24436762122737207, w0=0.014215288707918034, w1=-0.12268662108000905\n",
      "Gradient Descent(399/399): loss=0.24435709570258307, w0=0.014194614798705268, w1=-0.12269353132884803\n",
      "++++ gamma = 0.1\n",
      "ciaociaociao\n",
      "Gradient Descent(0/399): loss=0.4999999999999887, w0=-6.887232304259569e-05, w1=-0.03585123955815182\n",
      "Gradient Descent(1/399): loss=0.4424094223297165, w0=0.006841488132927668, w1=-0.018993490468192546\n",
      "Gradient Descent(2/399): loss=0.5120062932015015, w0=-0.012844991396834794, w1=-0.05970685795247796\n",
      "Gradient Descent(3/399): loss=1.352053873242019, w0=0.038232937015697466, w1=0.008144312847180549\n",
      "Gradient Descent(4/399): loss=14.483068942756894, w0=-0.12342306959411156, w1=-0.17509149458766365\n",
      "Gradient Descent(5/399): loss=252.17386926516315, w0=0.4778375680234507, w1=0.3971471428882494\n",
      "Gradient Descent(6/399): loss=4839.210352994972, w0=-2.018637825831082, w1=-1.7744213142780467\n",
      "Gradient Descent(7/399): loss=96653.08086424429, w0=8.922746533547777, w1=7.139860334666214\n",
      "Gradient Descent(8/399): loss=1976795.6917851195, w0=-40.19876359811734, w1=-31.11612910301945\n",
      "Gradient Descent(9/399): loss=41026702.65597458, w0=182.72609312280503, w1=136.64853986877998\n",
      "Gradient Descent(10/399): loss=859082675.5807587, w0=-834.2420272742904, w1=-608.764917906975\n",
      "Gradient Descent(11/399): loss=18084789655.11732, w0=3818.4182288461006, w1=2732.158787047455\n",
      "Gradient Descent(12/399): loss=381907762938.2466, w0=-17505.698829657653, w1=-12336.797514207137\n",
      "Gradient Descent(13/399): loss=8079909901423.625, w0=80346.8832417267, w1=55956.50497559666\n",
      "Gradient Descent(14/399): loss=171129374756594.97, w0=-369079.44508035865, w1=-254695.65820384718\n",
      "Gradient Descent(15/399): loss=3626745487106869.5, w0=1696452.178215311, w1=1162436.6307692023\n",
      "Gradient Descent(16/399): loss=7.68899640462171e+16, w0=-7801331.468506646, w1=-5316537.524451827\n",
      "Gradient Descent(17/399): loss=1.63048011395877e+18, w0=35888246.092441924, w1=24355257.727417056\n",
      "Gradient Descent(18/399): loss=3.4579258718190354e+19, w0=-165141061.05679774, w1=-111711883.0712223\n",
      "Gradient Descent(19/399): loss=7.334111085686606e+20, w0=760062003.3192008, w1=512889076.4215716\n",
      "Gradient Descent(20/399): loss=1.5555991755977453e+22, w0=-3498747098.520977, w1=-2356502981.785475\n",
      "Gradient Descent(21/399): loss=3.299579871266138e+23, w0=16107539628.845356, w1=10833236219.197062\n",
      "Gradient Descent(22/399): loss=6.998836166818708e+24, w0=-74162852791.42204, w1=-49823767266.98586\n",
      "Gradient Descent(23/399): loss=1.4845561955584983e+26, w0=341487341374.6512, w1=229223347369.4399\n",
      "Gradient Descent(24/399): loss=3.1489775948582226e+27, w0=-1572484687415.5232, w1=-1054851180263.8113\n",
      "Gradient Descent(25/399): loss=6.679496555338798e+28, w0=7241295622662.873, w1=4855205765488.7295\n",
      "Gradient Descent(26/399): loss=1.4168328127023176e+30, w0=-33347239996302.664, w1=-22350556176574.69\n",
      "Gradient Descent(27/399): loss=3.005341989383303e+31, w0=153572698732572.1, w1=102900644102533.81\n",
      "Gradient Descent(28/399): loss=6.3748421408273876e+32, w0=-707255204758207.2, w1=-473789369378758.2\n",
      "Gradient Descent(29/399): loss=1.3522130154827737e+34, w0=3257199831534895.0, w1=2181630148936245.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(30/399): loss=2.868275551085233e+35, w0=-1.5000900186198636e+16, w1=-1.0046129390902024e+16\n",
      "Gradient Descent(31/399): loss=6.084104685775709e+36, w0=6.908659669324843e+16, w1=4.6262922621469464e+16\n",
      "Gradient Descent(32/399): loss=1.2905430996016169e+38, w0=-3.181800849682425e+17, w1=-2.130492821120532e+17\n",
      "Gradient Descent(33/399): loss=2.7374636699012644e+39, w0=1.4653934979162857e+18, w1=9.811531098915448e+17\n",
      "Gradient Descent(34/399): loss=5.806630951892993e+40, w0=-6.748963946878777e+18, w1=-4.518568764172535e+18\n",
      "Gradient Descent(35/399): loss=1.2316862433188203e+42, w0=3.10828736075555e+19, w1=2.0809931677291454e+19\n",
      "Gradient Descent(36/399): loss=2.612618273350248e+43, w0=-1.4315487338070037e+20, w1=-9.583954028640072e+19\n",
      "Gradient Descent(37/399): loss=5.541812542378454e+44, w0=6.593132657971154e+20, w1=4.413895806395646e+20\n",
      "Gradient Descent(38/399): loss=1.175513722948085e+46, w0=-3.036533150650998e+21, w1=-2.0328341161520938e+21\n",
      "Gradient Descent(39/399): loss=2.4934667210031295e+47, w0=1.3985070119749587e+22, w1=9.362325110194384e+21\n",
      "Gradient Descent(40/399): loss=5.289071639599875e+48, w0=-6.440974525508768e+22, w1=-4.311882876882099e+22\n",
      "Gradient Descent(41/399): loss=1.1219030351176403e+50, w0=2.966461748864849e+23, w1=1.985872250411574e+23\n",
      "Gradient Descent(42/399): loss=2.3797492383026672e+51, w0=-1.3662371975837578e+24, w1=-9.146111862396797e+23\n",
      "Gradient Descent(43/399): loss=5.047857311084162e+52, w0=6.292360506532154e+24, w1=4.2123297247964056e+24\n",
      "Gradient Descent(44/399): loss=1.070737329186495e+54, w0=-2.898018766177488e+25, w1=-1.9400311509659433e+25\n",
      "Gradient Descent(45/399): loss=2.2712179792814735e+55, w0=1.334716013045403e+26, w1=8.935017872960595e+25\n",
      "Gradient Descent(46/399): loss=4.817643850616579e+56, w0=-6.1471896197989535e+26, w1=-4.115119351891939e+26\n",
      "Gradient Descent(47/399): loss=1.0219050960124242e+58, w0=2.831159882314003e+27, w1=1.8952637910493294e+27\n",
      "Gradient Descent(48/399): loss=2.1676364165735103e+59, w0=-1.3039238148870975e+28, w1=-8.728850652572728e+27\n",
      "Gradient Descent(49/399): loss=4.5979295463157105e+60, w0=6.0053740961993145e+28, w1=4.020171572435974e+28\n",
      "Gradient Descent(50/399): loss=9.752999142901098e+61, w0=-2.7658456233227312e+29, w1=-1.8515362154604462e+29\n",
      "Gradient Descent(51/399): loss=2.0687788127997387e+63, w0=1.273842756423364e+30, w1=8.527464271018584e+29\n",
      "Gradient Descent(52/399): loss=4.38823557101381e+64, w0=-5.866832912590281e+30, w1=-3.9274228500096135e+30\n",
      "Gradient Descent(53/399): loss=9.308202166216443e+65, w0=2.7020390848082642e+31, w1=1.8088204525231978e+31\n",
      "Gradient Descent(54/399): loss=1.9744297261410913e+67, w0=-1.244455985585523e+32, w1=-8.330734436919643e+31\n",
      "Gradient Descent(55/399): loss=4.1881049356872417e+68, w0=5.73148896317416e+32, w1=3.836817534414578e+32\n",
      "Gradient Descent(56/399): loss=8.883690677921286e+69, w0=-2.6397049334230434e+33, w1=-1.767091431858744e+33\n",
      "Gradient Descent(57/399): loss=1.8843835403575582e+71, w0=1.2157472919523473e+34, w1=8.138547653317057e+33\n",
      "Gradient Descent(58/399): loss=3.997101492959155e+72, w0=-5.599267809498048e+34, w1=-3.7483040446669284e+34\n",
      "Gradient Descent(59/399): loss=8.478539534465095e+73, w0=2.578808963156715e+35, w1=1.72632564017111e+35\n",
      "Gradient Descent(60/399): loss=1.7984440165983868e+75, w0=-1.1877009487728249e+36, w1=-7.950796480568333e+35\n",
      "Gradient Descent(61/399): loss=3.8148089864897005e+76, w0=5.470097115835917e+36, w1=3.6618331981124426e+36\n",
      "Gradient Descent(62/399): loss=8.091865784584213e+77, w0=-2.519317892557e+37, w1=-1.6865005312006952e+37\n",
      "Gradient Descent(63/399): loss=1.7164238657195812e+79, w0=1.1603016396944444e+38, w1=7.76737744953718e+37\n",
      "Gradient Descent(64/399): loss=3.640830143802415e+80, w0=-5.343906382426542e+38, w1=-3.5773574709418515e+38\n",
      "Gradient Descent(65/399): loss=7.722826745049475e+81, w0=2.4611992667113554e+39, w1=1.6475942630181364e+39\n",
      "Gradient Descent(66/399): loss=1.638144340119158e+83, w0=-1.1335344223487306e+40, w1=-7.588190125295297e+39\n",
      "Gradient Descent(67/399): loss=3.474785810499536e+84, w0=5.22062680843391e+40, w1=3.494830663113703e+40\n",
      "Gradient Descent(68/399): loss=7.370618164191e+85, w0=-2.404421404909209e+41, w1=-1.6095855774841128e+41\n",
      "Gradient Descent(69/399): loss=1.5634348441894963e+87, w0=1.107384708033299e+42, w1=7.413136670667872e+41\n",
      "Gradient Descent(70/399): loss=3.316314123964845e+88, w0=-5.100191211677053e+42, w1=-3.4142077390509716e+42\n",
      "Gradient Descent(71/399): loss=7.034472469180614e+89, w0=2.3489533683479074e+43, w1=1.5724537415366246e+43\n",
      "Gradient Descent(72/399): loss=1.4921325625360046e+91, w0=-1.0818382484813752e+44, w1=-7.242121627324042e+43\n",
      "Gradient Descent(73/399): loss=3.1650697247516664e+92, w0=4.982533973467992e+44, w1=3.3354447449155808e+44\n",
      "Gradient Descent(74/399): loss=6.713657093250292e+93, w0=-2.2947649367558386e+45, w1=-1.536178515449152e+45\n",
      "Gradient Descent(75/399): loss=1.424082105151295e+95, w0=1.0568811258080073e+46, w1=7.075051791928211e+45\n",
      "Gradient Descent(76/399): loss=3.0207230039363257e+96, w0=-4.867590994812337e+46, w1=-3.258498759418612e+46\n",
      "Gradient Descent(77/399): loss=6.407472879199433e+97, w0=2.2418265890998245e+47, w1=1.5007401329341965e+47\n",
      "Gradient Descent(78/399): loss=1.359135168771717e+99, w0=-1.032499743933861e+48, w1=-6.911836134197062e+47\n",
      "Gradient Descent(79/399): loss=2.8829593847971576e+100, w0=4.755299657996944e+48, w1=3.183327859486425e+48\n",
      "Gradient Descent(80/399): loss=6.115252555712487e+101, w0=-2.190109486275834e+49, w1=-1.466119286527126e+49\n",
      "Gradient Descent(81/399): loss=1.2971502136780643e+103, w0=1.0086808207473193e+50, w1=6.752385733788259e+49\n",
      "Gradient Descent(82/399): loss=2.7514786372531386e+104, w0=-4.6455987909629884e+50, w1=-3.109891092671451e+50\n",
      "Gradient Descent(83/399): loss=5.836359283165727e+105, w0=2.139585454866122e+51, w1=1.432297115398807e+51\n",
      "Gradient Descent(84/399): loss=1.237992155236233e+107, w0=-9.854113806818717e+51, w1=-6.596613725994054e+51\n",
      "Gradient Descent(85/399): loss=2.625994223568666e+108, w0=4.5384286333246635e+52, w1=3.0381484527817794e+52\n",
      "Gradient Descent(86/399): loss=5.570185265754071e+109, w0=-2.0902269715605273e+53, w1=-1.3992551943567533e+53\n",
      "Gradient Descent(87/399): loss=1.1815320695054279e+111, w0=9.62678747566396e+53, w1=6.444435251876504e+53\n",
      "Gradient Descent(88/399): loss=2.5062326738979467e+112, w0=-4.433730803529145e+54, w1=-2.9680608571933762e+54\n",
      "Gradient Descent(89/399): loss=5.3161504269138215e+113, w0=2.0420071480628467e+55, w1=1.366975523493711e+55\n",
      "Gradient Descent(90/399): loss=1.1276469122725488e+115, w0=-9.404705376845855e+55, w1=-6.295767410935949e+55\n",
      "Gradient Descent(91/399): loss=2.3919329903085788e+116, w0=4.331448266929165e+56, w1=2.8995901251717406e+56\n",
      "Gradient Descent(92/399): loss=5.073701145153926e+117, w0=-1.9948997163998707e+57, w1=-1.3354405182458154e+57\n",
      "Gradient Descent(93/399): loss=1.0762192509003082e+119, w0=9.187746530135428e+57, w1=6.150529215471671e+57\n",
      "Gradient Descent(94/399): loss=2.282846077985302e+120, w0=-4.2315253046596203e+58, w1=-2.8326989569044033e+58\n",
      "Gradient Descent(95/399): loss=4.842309047541467e+121, w0=1.9488790145928306e+59, w1=1.3046329997541822e+59\n",
      "Gradient Descent(96/399): loss=1.0271370084046881e+123, w0=-8.975792746267304e+59, w1=-6.0086415462557095e+59\n",
      "Gradient Descent(97/399): loss=2.1787342023743446e+124, w0=4.133907483261024e+60, w1=2.767350913108693e+60\n",
      "Gradient Descent(98/399): loss=4.621469858211478e+125, w0=-1.903919972669821e+61, w1=-1.2745361854810012e+61\n",
      "Gradient Descent(99/399): loss=9.802932192041473e+126, w0=8.768728562526653e+61, w1=5.870027109342227e+61\n",
      "Gradient Descent(100/399): loss=2.079370471085419e+128, w0=-4.038541625016635e+62, w1=-2.703510395149681e+62\n",
      "Gradient Descent(101/399): loss=4.410702299391874e+129, w0=1.859998099005276e+63, w1=1.2451336800560485e+63\n",
      "Gradient Descent(102/399): loss=9.355857960080482e+130, w0=-8.566441179838259e+63, w1=-5.734610393922076e+63\n",
      "Gradient Descent(103/399): loss=1.984538339422043e+132, w0=3.945375778979379e+64, w1=2.641142625633852e+64\n",
      "Gradient Descent(104/399): loss=4.2095470425484156e+133, w0=-1.8170894669764453e+65, w1=-1.2164094663403816e+65\n",
      "Gradient Descent(105/399): loss=8.929173073365145e+134, w0=8.368820401312866e+65, w1=5.6023176311710995e+65\n",
      "Gradient Descent(106/399): loss=1.8940311384628638e+136, w0=-3.8543591926692316e+66, w1=-2.580213629458063e+66\n",
      "Gradient Descent(107/399): loss=4.017565707363942e+137, w0=1.7751707019287247e+67, w1=1.1883478966989147e+67\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(108/399): loss=8.521947651867076e+138, w0=-8.1757585722151e+67, w1=-5.473076754056938e+67\n",
      "Gradient Descent(109/399): loss=1.8076516246653597e+140, w0=3.7654422844250146e+68, w1=2.5206902152988507e+68\n",
      "Gradient Descent(110/399): loss=3.834339906370358e+141, w0=-1.7342189684417935e+69, w1=-1.1609336844753158e+69\n",
      "Gradient Descent(111/399): loss=8.133294223828107e+142, w0=7.98715052131668e+69, w1=5.346817358076877e+69\n",
      "Gradient Descent(112/399): loss=1.725211550010306e+144, w0=-3.6785766163940407e+70, w1=-2.4625399575302235e+70\n",
      "Gradient Descent(113/399): loss=3.6594703331512803e+145, w0=1.6942119578898694e+71, w1=1.1341518956641847e+71\n",
      "Gradient Descent(114/399): loss=7.762365791681803e+146, w0=-7.802893503603885e+71, w1=-5.223470662903448e+71\n",
      "Gradient Descent(115/399): loss=1.6465312517504415e+148, w0=3.5937148681453174e+72, w1=2.4057311785591783e+72\n",
      "Gradient Descent(116/399): loss=3.492575892128233e+149, w0=-1.6551278762889447e+73, w1=-1.107987940775518e+73\n",
      "Gradient Descent(117/399): loss=7.408353986179996e+150, w0=7.622887144306358e+73, w1=5.102969474915453e+73\n",
      "Gradient Descent(118/399): loss=1.57143926086901e+152, w0=-3.5108108108914805e+74, w1=-2.3502329315689447e+74\n",
      "Gradient Descent(119/399): loss=3.3332928680340412e+153, w0=1.6169454324243664e+75, w1=1.0824275668869624e+75\n",
      "Gradient Descent(120/399): loss=7.070487304703243e+154, w0=-7.44703338421737e+75, w1=-4.985248150593622e+75\n",
      "Gradient Descent(121/399): loss=1.4997719283950834e+156, w0=3.429819282305454e+76, w1=2.2960149836605516e+76\n",
      "Gradient Descent(122/399): loss=3.181274133263313e+157, w0=-1.579643826252366e+77, w1=-1.057456849879399e+77\n",
      "Gradient Descent(123/399): loss=6.74802942991491e+158, w0=7.275236426275948e+77, w1=4.870242560760987e+77\n",
      "Gradient Descent(124/399): loss=1.4313730687612063e+160, w0=-3.3506961619179957e+78, w1=-2.2430477993832868e+78\n",
      "Gradient Descent(125/399): loss=3.0361883913726713e+161, w0=1.5432027375691605e+79, w1=1.0330621868517508e+79\n",
      "Gradient Descent(126/399): loss=6.440277625094861e+162, w0=-7.107402683380743e+79, w1=-4.7578900556481594e+79\n",
      "Gradient Descent(127/399): loss=1.3660936194260829e+164, w0=3.273398347082971e+80, w1=2.1913025246450333e+80\n",
      "Gradient Descent(128/399): loss=2.8977194550819865e+165, w0=-1.507602314941394e+81, w1=-1.0092302887107332e+81\n",
      "Gradient Descent(129/399): loss=6.146561202656259e+166, w0=6.943440727408198e+81, w1=4.648129430764654e+81\n",
      "Gradient Descent(130/399): loss=1.3037913160206227e+168, w0=-3.1978837294969784e+82, w1=-2.1407509709939985e+82\n",
      "Gradient Descent(131/399): loss=2.7655655572032416e+169, w0=1.4728231648920284e+83, w1=9.859481729315439e+82\n",
      "Gradient Descent(132/399): loss=5.866240062507004e+170, w0=-6.783261239406525e+83, w1=-4.540900893557385e+83\n",
      "Gradient Descent(133/399): loss=1.2443303823093241e+172, w0=3.124111172260662e+84, w1=2.0913656002628414e+84\n",
      "Gradient Descent(134/399): loss=2.6394386929953985e+173, w0=-1.4388463413356383e+85, w1=-9.63203156485595e+84\n",
      "Gradient Descent(135/399): loss=5.598703297071365e+174, w0=6.626776960939113e+85, w1=4.436146030838505e+85\n",
      "Gradient Descent(136/399): loss=1.1875812342913254e+176, w0=-3.052040487469244e+86, w1=-2.04311950956719e+86\n",
      "Gradient Descent(137/399): loss=2.519063992511712e+177, w0=1.405653335257483e+87, w1=9.409828489313892e+86\n",
      "Gradient Descent(138/399): loss=5.343367859930671e+178, w0=-6.473902646549944e+87, w1=-4.333807776964561e+87\n",
      "Gradient Descent(139/399): loss=1.1334201977962365e+180, w0=2.981632414319956e+88, w1=1.9959864166501954e+88\n",
      "Gradient Descent(140/399): loss=2.4041791215720825e+181, w0=-1.3732260646306026e+89, w1=-9.192751456647359e+88\n",
      "Gradient Descent(141/399): loss=5.0996772951828327e+182, w0=6.324555017325795e+89, w1=4.233830382749675e+89\n",
      "Gradient Descent(142/399): loss=1.0817292389594337e+184, w0=-2.912848597724592e+90, w1=-1.9499406455650852e+90\n",
      "Gradient Descent(143/399): loss=2.2945337100546943e+185, w0=1.3415468645656026e+91, w1=8.980682213247683e+90\n",
      "Gradient Descent(144/399): loss=4.8671005247504485e+186, w0=-6.1786527155297106e+91, w1=-4.136159385096065e+91\n",
      "Gradient Descent(145/399): loss=1.0323957069892606e+188, w0=2.845651567415372e+92, w1=1.904957112688176e+92\n",
      "Gradient Descent(146/399): loss=2.1898888062611026e+189, w0=-1.3105984776875927e+93, w1=-8.773505233519848e+92\n",
      "Gradient Descent(147/399): loss=4.6451306909953706e+190, w0=6.036116260281114e+93, w1=4.0407415773249523e+93\n",
      "Gradient Descent(148/399): loss=9.853120886656773e+191, w0=-2.7800047175329402e+94, w1=-1.8610113130544164e+94\n",
      "Gradient Descent(149/399): loss=2.090016356165613e+193, w0=1.2803640447352016e+95, w1=8.571107656949811e+94\n",
      "Gradient Descent(150/399): loss=4.433284052117157e+194, w0=-5.896868004258157e+95, w1=-3.9475249801920585e+95\n",
      "Gradient Descent(151/399): loss=9.403757740352787e+195, w0=2.715872286685102e+96, w1=1.81807930700825e+96\n",
      "Gradient Descent(152/399): loss=1.9946987064141323e+197, w0=-1.2508270953763826e+97, w1=-8.3733792266230465e+96\n",
      "Gradient Descent(153/399): loss=4.23109892792809e+198, w0=5.760832091399172e+97, w1=3.856458813571681e+97\n",
      "Gradient Descent(154/399): loss=8.97488832792044e+199, w0=-2.653219338465666e+98, w1=-1.7761377071623072e+98\n",
      "Gradient Descent(155/399): loss=1.9037281299893014e+201, w0=1.2219715392361693e+99, w1=8.18021222916174e+98\n",
      "Gradient Descent(156/399): loss=4.038134693707445e+202, w0=-5.627934415579558e+99, w1=-3.767493468794004e+99\n",
      "Gradient Descent(157/399): loss=8.565578008565416e+203, w0=2.592011742422693e+100, w1=1.7351636656571158e+100\n",
      "Gradient Descent(158/399): loss=1.816906373508279e+205, w0=-1.1937816571313524e+101, w1=-7.991501436047366e+100\n",
      "Gradient Descent(159/399): loss=3.853970819942225e+206, w0=5.4981025802423776e+101, w1=3.6805804816205453e+101\n",
      "Gradient Descent(160/399): loss=8.174934766884116e+207, w0=-2.5322161554658443e+102, w1=-1.695134861714579e+102\n",
      "Gradient Descent(161/399): loss=1.7340442251665165e+209, w0=1.1662420925073937e+103, w1=7.807144046297124e+102\n",
      "Gradient Descent(162/399): loss=3.678205955861629e+210, w0=-5.371265858959894e+103, w1=-3.5956725058431045e+103\n",
      "Gradient Descent(163/399): loss=7.802107268882778e+211, w0=2.4738000037025022e+104, w1=1.656029489478682e+104\n",
      "Gradient Descent(164/399): loss=1.6549611023859578e+213, w0=-1.1393378430729024e+105, w1=-7.627039630462654e+104\n",
      "Gradient Descent(165/399): loss=3.5104570547678887e+214, w0=5.247355156905101e+105, w1=3.512723287491728e+105\n",
      "Gradient Descent(166/399): loss=7.44628300665386e+215, w0=-2.4167314646931003e+106, w1=-1.6178262461366266e+106\n",
      "Gradient Descent(167/399): loss=1.5794846582690439e+217, w0=1.113054252627039e+107, w1=7.451090075920736e+106\n",
      "Gradient Descent(168/399): loss=3.35035853925777e+218, w0=-5.126302973212052e+107, w1=-3.43168763963765e+107\n",
      "Gradient Descent(169/399): loss=7.106686527667084e+219, w0=2.360979450115668e+108, w1=1.580504320314046e+108\n",
      "Gradient Descent(170/399): loss=1.5074504060008273e+221, w0=-1.087377003075515e+109, w1=-7.279199533425861e+108\n",
      "Gradient Descent(171/399): loss=3.197561504514588e+222, w0=5.0080433642045154e+109, w1=3.352521417777498e+109\n",
      "Gradient Descent(172/399): loss=6.782577744814997e+223, w0=-2.3065135888303325e+110, w1=-1.544043380737946e+110\n",
      "Gradient Descent(173/399): loss=1.4387013603806624e+225, w0=1.0622921066307536e+111, w1=7.111274364896149e+110\n",
      "Gradient Descent(174/399): loss=3.051732957935481e+226, w0=-4.892511907472738e+111, w1=-3.2751814957854666e+111\n",
      "Gradient Descent(175/399): loss=6.4732503235317665e+227, w0=2.2533042103345448e+112, w1=1.5084235651611102e+112\n",
      "Gradient Descent(176/399): loss=1.3730876957022978e+229, w0=-1.0377858981919545e+113, w1=-6.947223092403583e+112\n",
      "Gradient Descent(177/399): loss=2.912555093436283e+230, w0=4.779645666779175e+113, w1=3.199625742420051e+113\n",
      "Gradient Descent(178/399): loss=6.1780301424686636e+231, w0=-2.2013223285999483e+114, w1=-1.4736254695421233e+114\n",
      "Gradient Descent(179/399): loss=1.310466419236723e+233, w0=1.013845027901015e+115, w1=6.78695634834084e+114\n",
      "Gradient Descent(180/399): loss=2.779724598852315e+234, w0=-4.6693831577738306e+115, w1=-3.1258129983730557e+115\n",
      "Gradient Descent(181/399): loss=5.896273824372509e+235, w0=2.1505396262820853e+116, w1=1.4396301374748895e+116\n",
      "Gradient Descent(182/399): loss=1.2507010596062114e+237, w0=-9.904564538701058e+116, w1=-6.630386826738156e+116\n",
      "Gradient Descent(183/399): loss=2.652951994926335e+238, w0=4.5616643145001507e+117, w1=3.053703053847727e+117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(184/399): loss=5.627367333965193e+239, w0=-2.1009284392944354e+118, w1=-1.4064190498620477e+118\n",
      "Gradient Descent(185/399): loss=1.1936613693704487e+241, w0=9.676074350770766e+118, w1=6.47742923570301e+118\n",
      "Gradient Descent(186/399): loss=2.5319610044424966e+242, w0=-4.4564304566740585e+119, w1=-2.983256626654414e+119\n",
      "Gradient Descent(187/399): loss=5.370724639768311e+243, w0=2.0524617417382842e+120, w1=1.373974114826686e+120\n",
      "Gradient Descent(188/399): loss=1.1392230411765643e+245, w0=-9.452855244247054e+120, w1=-6.328000250956794e+120\n",
      "Gradient Descent(189/399): loss=2.4164879501317433e+246, w0=4.353624257717505e+121, w1=2.914435340811396e+121\n",
      "Gradient Descent(190/399): loss=5.1257864369571524e+247, w0=-2.0051131311802854e+122, w1=-1.3422776578566222e+122\n",
      "Gradient Descent(191/399): loss=1.0872674368544445e+249, w0=9.234785619601138e+122, w1=6.1820184704439256e+122\n",
      "Gradient Descent(192/399): loss=2.3062811800364397e+250, w0=-4.253189713529611e+123, w1=-2.8472017056393816e+123\n",
      "Gradient Descent(193/399): loss=4.892018928460156e+251, w0=1.958856814269561e+124, w1=1.311312412176219e+124\n",
      "Gradient Descent(194/399): loss=1.0376813288670323e+253, w0=-9.0217466825056e+124, w1=-6.03940436998742e+124\n",
      "Gradient Descent(195/399): loss=2.2011005190819769e+254, w0=4.155072111978294e+125, w1=2.7815190953383378e+125\n",
      "Gradient Descent(196/399): loss=4.668912661647933e+255, w0=-1.9136675926867154e+126, w1=-1.2810615093401806e+126\n",
      "Gradient Descent(197/399): loss=9.903566535520203e+256, w0=8.813622379121012e+126, w1=5.900080259967908e+126\n",
      "Gradient Descent(198/399): loss=2.1007167456599396e+258, w0=-4.0592180030954473e+127, w1=-2.7173517290354395e+127\n",
      "Gradient Descent(199/399): loss=4.4559814180763724e+259, w0=1.8695208494169118e+128, w1=1.2515084700444403e+128\n",
      "Gradient Descent(200/399): loss=9.45190275617297e+260, w0=-8.61029933287464e+128, w1=-5.76397024300185e+128\n",
      "Gradient Descent(201/399): loss=2.0049110920825393e+262, w0=3.9655751699599574e+129, w1=2.654664651293244e+129\n",
      "Gradient Descent(202/399): loss=4.252761153864388e+263, w0=-1.8263925353397323e+130, w1=-1.2226371951489663e+130\n",
      "Gradient Descent(203/399): loss=9.020837633768448e+264, w0=8.411666782698654e+130, w1=5.631000172596219e+130\n",
      "Gradient Descent(204/399): loss=1.9134747678192265e+266, w0=-3.874092600252276e+131, w1=-2.5934237130676505e+131\n",
      "Gradient Descent(205/399): loss=4.0588089883969013e+267, w0=1.784259156128187e+132, w1=1.1944319569076892e+132\n",
      "Gradient Descent(206/399): loss=8.609431742372394e+268, w0=-8.217616522692194e+132, w1=-5.501097612756786e+132\n",
      "Gradient Descent(207/399): loss=1.8262085044767064e+270, w0=3.7847204584653095e+133, w1=2.5335955531050136e+133\n",
      "Gradient Descent(208/399): loss=3.873702239149259e+271, w0=-1.7430977594502052e+134, w1=-1.1668773904007626e+134\n",
      "Gradient Descent(209/399): loss=8.216788499673409e+272, w0=8.028042843175795e+134, w1=5.374191798528367e+134\n",
      "Gradient Descent(210/399): loss=1.7429221215307654e+274, w0=-3.697410058756216e+135, w1=-2.475147579768446e+135\n",
      "Gradient Descent(211/399): loss=3.6970375005296144e+275, w0=1.702885922465206e+136, w1=1.1399584851644416e+136\n",
      "Gradient Descent(212/399): loss=7.842052213048956e+276, w0=-7.842842473105758e+136, w1=-5.250213597445314e+136\n",
      "Gradient Descent(213/399): loss=1.6634341118632426e+278, w0=3.6121138384247175e+137, w1=2.4180479532834248e+137\n",
      "Gradient Descent(214/399): loss=3.528429764731776e+279, w0=-1.663601739609153e+138, w1=-1.113660577014098e+138\n",
      "Gradient Descent(215/399): loss=7.484406214743189e+280, w0=7.661914523816801e+138, w1=5.129095471871293e+138\n",
      "Gradient Descent(216/399): loss=1.5875712462012055e+282, w0=-3.5287853320030176e+139, w1=-2.3622655683929838e+139\n",
      "Gradient Descent(217/399): loss=3.3675115826824956e+283, w0=1.6252238106614247e+140, w1=1.0879693400558003e+140\n",
      "Gradient Descent(218/399): loss=7.143071082092107e+284, w0=-7.485160434062309e+140, w1=-5.010771442207893e+140\n",
      "Gradient Descent(219/399): loss=1.5151681955961092e+286, w0=3.447379145943611e+141, w1=2.3077700374128486e+141\n",
      "Gradient Descent(220/399): loss=3.213932263255003e+287, w0=-1.5877312290869538e+142, w1=-1.06287077888227e+142\n",
      "Gradient Descent(221/399): loss=6.81730293891737e+288, w0=7.312483916322945e+142, w1=4.895177050951996e+142\n",
      "Gradient Descent(222/399): loss=1.446067171120208e+290, w0=-3.367850933890866e+143, w1=-2.254531673677831e+143\n",
      "Gradient Descent(223/399): loss=3.067357109003106e+291, w0=1.5511035706473182e+144, w1=1.0383512209487986e+144\n",
      "Gradient Descent(224/399): loss=6.506391834387225e+292, w0=-7.143790904353632e+144, w1=-4.782249327582265e+144\n",
      "Gradient Descent(225/399): loss=1.3801175799950783e+294, w0=3.2901573725233645e+145, w1=2.202521475369701e+145\n",
      "Gradient Descent(226/399): loss=2.927466686750603e+295, w0=-1.5153208822745205e+146, w1=-1.0143973091250963e+146\n",
      "Gradient Descent(227/399): loss=6.2096601958110614e+296, w0=6.9789895019404e+146, w1=4.671926754255665e+146\n",
      "Gradient Descent(228/399): loss=1.3171756974027404e+298, w0=-3.2142561379531184e+147, w1=-2.1517111097184615e+147\n",
      "Gradient Descent(229/399): loss=2.79395613144624e+299, w0=1.4803636712015128e+148, w1=9.909959944188966e+147\n",
      "Gradient Descent(230/399): loss=5.926461351996247e+300, w0=-6.817989932839579e+148, w1=-4.5641492322954086e+148\n",
      "Gradient Descent(231/399): loss=1.2571043532642896e+302, w0=3.1401058826695615e+149, w1=2.1020728975678612e+149\n",
      "Gradient Descent(232/399): loss=2.666534481767473e+303, w0=-1.4462128943433827e+150, w1=-9.681345288675122e+149\n",
      "Gradient Descent(233/399): loss=5.656178123949314e+304, w0=6.660704491871977e+150, w1=4.458858049451949e+150\n",
      "Gradient Descent(234/399): loss=1.1997726333033308e+306, w0=-3.067666213015369e+151, w1=-2.053579798297139e+151\n",
      "Gradient Descent(235/399): loss=2.5449240460244393e+307, w0=1.4128499479236235e+152, w1=9.458004585933079e+151\n",
      "Gradient Descent(236/399): loss=inf, w0=-6.50704749714504e+152, w1=-4.355995847919213e+152\n",
      "Gradient Descent(237/399): loss=inf, w0=2.9968976671817235e+153, w1=2.0062053950904797e+153\n",
      "Gradient Descent(238/399): loss=inf, w0=-1.3802566573395716e+154, w1=-9.239816170193915e+153\n",
      "Gradient Descent(239/399): loss=inf, w0=6.356935243377106e+154, w1=4.255506593088717e+154\n",
      "Gradient Descent(240/399): loss=inf, w0=-2.927761693711446e+155, w1=-1.9599238805463648e+155\n",
      "Gradient Descent(241/399): loss=inf, w0=1.3484152672617664e+156, w1=9.026661182418309e+155\n",
      "Gradient Descent(242/399): loss=inf, w0=-6.210285956298946e+156, w1=-4.15733554302445e+156\n",
      "Gradient Descent(243/399): loss=inf, w0=2.8602206304978636e+157, w1=1.9147100426188658e+157\n",
      "Gradient Descent(244/399): loss=inf, w0=-1.3173084319616464e+158, w1=-8.818423505547753e+157\n",
      "Gradient Descent(245/399): loss=inf, w0=6.067019748106635e+158, w1=4.0614292186420385e+158\n",
      "Gradient Descent(246/399): loss=inf, w0=-2.7942376842682754e+159, w1=-1.8705392508833512e+159\n",
      "Gradient Descent(247/399): loss=inf, w0=1.286919205862408e+160, w1=8.614989701248834e+159\n",
      "Gradient Descent(248/399): loss=inf, w0=-5.927058573942423e+160, w1=-3.9677353745757935e+160\n",
      "Gradient Descent(249/399): loss=inf, w0=2.729776910540588e+161, w1=1.8273874431188225e+161\n",
      "Gradient Descent(250/399): loss=inf, w0=-1.2572310343078e+162, w1=-8.416248948117813e+161\n",
      "Gradient Descent(251/399): loss=inf, w0=5.790326189379454e+162, w1=3.876202970717739e+162\n",
      "Gradient Descent(252/399): loss=inf, w0=-2.66680319404249e+163, w1=-1.7852311122000602e+163\n",
      "Gradient Descent(253/399): loss=inf, w0=1.2282277445439358e+164, w1=8.222092981309804e+163\n",
      "Gradient Descent(254/399): loss=inf, w0=-5.656748108887448e+164, w1=-3.7867821444133916e+164\n",
      "Gradient Descent(255/399): loss=inf, w0=2.605282229582216e+165, w1=1.744047293291936e+165\n",
      "Gradient Descent(256/399): loss=inf, w0=-1.1998935369091103e+166, w1=-8.032416033561214e+165\n",
      "Gradient Descent(257/399): loss=inf, w0=5.526251565256104e+166, w1=3.6994241832988684e+166\n",
      "Gradient Descent(258/399): loss=inf, w0=-2.5451805033606566e+167, w1=-1.7038135513392698e+167\n",
      "Gradient Descent(259/399): loss=inf, w0=1.1722129762268654e+168, w1=7.847114777572463e+167\n",
      "Gradient Descent(260/399): loss=inf, w0=-5.398765469955099e+168, w1=-3.6140814987645573e+168\n",
      "Gradient Descent(261/399): loss=inf, w0=2.486465274714609e+169, w1=1.664507968845202e+169\n",
      "Gradient Descent(262/399): loss=inf, w0=-1.1451709833976118e+170, w1=-7.666088269720191e+169\n",
      "Gradient Descent(263/399): loss=inf, w0=5.274220374408317e+170, w1=3.5307076000311524e+170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(264/399): loss=inf, w0=-2.429104558281094e+171, w1=-1.626109133931574e+171\n",
      "Gradient Descent(265/399): loss=inf, w0=1.1187528271842348e+172, w1=7.489237895067736e+171\n",
      "Gradient Descent(266/399): loss=inf, w0=-5.152548432161323e+172, w1=-3.449257068823456e+172\n",
      "Gradient Descent(267/399): loss=inf, w0=2.373067106573315e+173, w1=1.5885961286747069e+173\n",
      "Gradient Descent(268/399): loss=inf, w0=-1.092944116187192e+174, w1=-7.316467313644131e+173\n",
      "Gradient Descent(269/399): loss=inf, w0=5.03368336192179e+174, w1=3.3696855346286873e+174\n",
      "Gradient Descent(270/399): loss=inf, w0=-2.3183223929583417e+175, w1=-1.5519485177102947e+175\n",
      "Gradient Descent(271/399): loss=inf, w0=1.0677307910047314e+176, w1=7.147682407962692e+175\n",
      "Gradient Descent(272/399): loss=inf, w0=-4.917560411452527e+176, w1=-3.291949650525459e+176\n",
      "Gradient Descent(273/399): loss=inf, w0=2.264840595027668e+177, w1=1.516146337101119e+177\n",
      "Gradient Descent(274/399): loss=inf, w0=-1.0430991165739775e+178, w1=-6.982791231749913e+177\n",
      "Gradient Descent(275/399): loss=inf, w0=4.804116322297345e+178, w1=3.216007069570328e+178\n",
      "Gradient Descent(276/399): loss=inf, w0=-2.212592578351312e+179, w1=-1.4811700834616518e+179\n",
      "Gradient Descent(277/399): loss=inf, w0=1.0190356746887065e+180, w1=6.821703959857594e+179\n",
      "Gradient Descent(278/399): loss=inf, w0=-4.693289295320892e+180, w1=-3.141816421729113e+180\n",
      "Gradient Descent(279/399): loss=inf, w0=2.1615498806067295e+181, w1=1.4470007033334816e+181\n",
      "Gradient Descent(280/399): loss=inf, w0=-9.955273566897177e+181, w1=-6.664332839330062e+181\n",
      "Gradient Descent(281/399): loss=inf, w0=4.585018957043128e+182, w1=3.0693372913404455e+182\n",
      "Gradient Descent(282/399): loss=inf, w0=-2.1116846960738045e+183, w1=-1.413619582805861e+183\n",
      "Gradient Descent(283/399): loss=inf, w0=9.725613563238218e+183, w1=6.510592141600355e+183\n",
      "Gradient Descent(284/399): loss=inf, w0=-4.479246326750777e+184, w1=-2.9985301950991515e+184\n",
      "Gradient Descent(285/399): loss=inf, w0=2.06296986048763e+185, w1=1.3810085373757202e+185\n",
      "Gradient Descent(286/399): loss=inf, w0=-9.501251627676228e+185, w1=-6.360398115789413e+185\n",
      "Gradient Descent(287/399): loss=inf, w0=4.375913784367263e+186, w1=2.9293565605475423e+186\n",
      "Gradient Descent(288/399): loss=inf, w0=-2.015378836240624e+187, w1=-1.349149802041588e+187\n",
      "Gradient Descent(289/399): loss=inf, w0=9.282065538121449e+187, w1=6.2136689430822214e+187\n",
      "Gradient Descent(290/399): loss=inf, w0=-4.274965039064015e+188, w1=-2.8617787050629363e+188\n",
      "Gradient Descent(291/399): loss=inf, w0=1.9688856979260548e+189, w1=1.3180260216259931e+189\n",
      "Gradient Descent(292/399): loss=inf, w0=-9.067935892047717e+189, w1=-6.070324692156932e+189\n",
      "Gradient Descent(293/399): loss=inf, w0=4.176345098595709e+190, w1=2.7957598153298322e+190\n",
      "Gradient Descent(294/399): loss=inf, w0=-1.9234651182150962e+191, w1=-1.287620241321205e+191\n",
      "Gradient Descent(295/399): loss=inf, w0=8.858746041447207e+191, w1=5.930287275641612e+191\n",
      "Gradient Descent(296/399): loss=inf, w0=-4.0800002393430686e+192, w1=-2.7312639272858444e+192\n",
      "Gradient Descent(297/399): loss=inf, w0=1.879092354059636e+193, w1=1.2579158974530097e+193\n",
      "Gradient Descent(298/399): loss=inf, w0=-8.654382029286146e+193, w1=-5.793480407576143e+193\n",
      "Gradient Descent(299/399): loss=inf, w0=3.985877977046672e+194, w1=2.6682559065299232e+194\n",
      "Gradient Descent(300/399): loss=inf, w0=-1.8357432332134008e+195, w1=-1.2288968084575752e+195\n",
      "Gradient Descent(301/399): loss=inf, w0=8.454732527426138e+195, w1=5.659829561854955e+195\n",
      "Gradient Descent(302/399): loss=inf, w0=-3.89392703821599e+196, w1=-2.606701429182946e+196\n",
      "Gradient Descent(303/399): loss=inf, w0=1.7933941410640408e+197, w1=1.2005471660665075e+197\n",
      "Gradient Descent(304/399): loss=inf, w0=-8.259688775977601e+197, w1=-5.5292619316286394e+197\n",
      "Gradient Descent(305/399): loss=inf, w0=3.8040973321978786e+198, w1=2.5465669631895174e+198\n",
      "Gradient Descent(306/399): loss=inf, w0=-1.7520220077689588e+199, w1=-1.1728515266951997e+199\n",
      "Gradient Descent(307/399): loss=inf, w0=8.069144524052645e+199, w1=5.401706389642193e+199\n",
      "Gradient Descent(308/399): loss=inf, w0=-3.71633992388955e+200, w1=-2.4878197500512974e+200\n",
      "Gradient Descent(309/399): loss=inf, w0=1.7116042956879377e+201, w1=1.1457948030298799e+201\n",
      "Gradient Descent(310/399): loss=inf, w0=-7.882995971884255e+201, w1=-5.277093449488097e+201\n",
      "Gradient Descent(311/399): loss=inf, w0=3.6306070070808876e+202, w1=2.430427786981662e+202\n",
      "Gradient Descent(312/399): loss=inf, w0=-1.6721189871056168e+203, w1=-1.1193622558087568e+203\n",
      "Gradient Descent(313/399): loss=inf, w0=7.70114171428095e+203, w1=5.155355227753232e+203\n",
      "Gradient Descent(314/399): loss=inf, w0=-3.546851878411906e+204, w1=-2.374359809472038e+204\n",
      "Gradient Descent(315/399): loss=inf, w0=1.6335445722373289e+205, w1=1.093539485792725e+205\n",
      "Gradient Descent(316/399): loss=inf, w0=-7.523482685385892e+205, w1=-5.036425407039356e+205\n",
      "Gradient Descent(317/399): loss=inf, w0=3.465028911930875e+206, w1=2.3195852742604593e+206\n",
      "Gradient Descent(318/399): loss=inf, w0=-1.5958600375114865e+207, w1=-1.0683124259213158e+207\n",
      "Gradient Descent(319/399): loss=inf, w0=7.34992210471043e+207, w1=4.9202391998361077e+207\n",
      "Gradient Descent(320/399): loss=inf, w0=-3.3850935342393536e+208, w1=-2.2660743426929675e+208\n",
      "Gradient Descent(321/399): loss=inf, w0=1.5590448541223833e+209, w1=1.0436673336496347e+209\n",
      "Gradient Descent(322/399): loss=inf, w0=-7.18036542441252e+209, w1=-4.806733313227987e+209\n",
      "Gradient Descent(323/399): loss=inf, w0=3.3070022002107153e+210, w1=2.213797864468934e+210\n",
      "Gradient Descent(324/399): loss=inf, w0=-1.5230789668470495e+211, w1=-1.0195907834620427e+211\n",
      "Gradient Descent(325/399): loss=inf, w0=7.014720277791317e+211, w1=4.695845914415147e+211\n",
      "Gradient Descent(326/399): loss=inf, w0=-3.230712369268665e+212, w1=-2.1627273617612375e+212\n",
      "Gradient Descent(327/399): loss=inf, w0=1.4879427831200613e+213, w1=9.960696595585155e+212\n",
      "Gradient Descent(328/399): loss=inf, w0=-6.852896428970058e+213, w1=-4.5875165970298266e+213\n",
      "Gradient Descent(329/399): loss=inf, w0=3.156182482213201e+214, w1=2.1128350137029393e+214\n",
      "Gradient Descent(330/399): loss=inf, w0=-1.4536171623604408e+215, w1=-9.730911487098166e+214\n",
      "Gradient Descent(331/399): loss=inf, w0=6.694805723739778e+215, w1=4.481686348229581e+215\n",
      "Gradient Descent(332/399): loss=inf, w0=-3.083371938580972e+216, w1=-2.0640936412316492e+216\n",
      "Gradient Descent(333/399): loss=inf, w0=1.4200834055447122e+217, w1=9.506427332773907e+216\n",
      "Gradient Descent(334/399): loss=inf, w0=-6.540362041536785e+217, w1=-4.378297516549947e+217\n",
      "Gradient Descent(335/399): loss=inf, w0=3.012241074528085e+218, w1=2.0164766922837303e+218\n",
      "Gradient Descent(336/399): loss=inf, w0=-1.3873232450205668e+219, w1=-9.287121843943586e+218\n",
      "Gradient Descent(337/399): loss=inf, w0=6.389481248528302e+219, w1=4.277293780498526e+219\n",
      "Gradient Descent(338/399): loss=inf, w0=-2.942751141223001e+220, w1=-1.9699582273298505e+220\n",
      "Gradient Descent(339/399): loss=inf, w0=1.3553188345554457e+221, w1=9.072875553037737e+220\n",
      "Gradient Descent(340/399): loss=inf, w0=-6.242081151780076e+221, w1=-4.1786201178735305e+221\n",
      "Gradient Descent(341/399): loss=inf, w0=2.874864283737961e+222, w1=1.924512905244386e+222\n",
      "Gradient Descent(342/399): loss=inf, w0=-1.3240527396147742e+223, w1=-8.863571748505808e+222\n",
      "Gradient Descent(343/399): loss=inf, w0=6.098081454481579e+223, w1=4.082222775790339e+223\n",
      "Gradient Descent(344/399): loss=inf, w0=-2.808543520427392e+224, w1=-1.88011596950073e+224\n",
      "Gradient Descent(345/399): loss=inf, w0=1.2935079278643802e+225, w1=8.6590964112372e+224\n",
      "Gradient Descent(346/399): loss=inf, w0=-5.95740371220381e+225, w1=-3.988049241399288e+225\n",
      "Gradient Descent(347/399): loss=inf, w0=2.743752722781977e+226, w1=1.8367432346850381e+226\n",
      "Gradient Descent(348/399): loss=inf, w0=-1.2636677598921074e+227, w1=-8.45933815244858e+226\n",
      "Gradient Descent(349/399): loss=inf, w0=5.819971290166544e+227, w1=3.896048213279157e+227\n",
      "Gradient Descent(350/399): loss=inf, w0=-2.6804565957475026e+228, w1=-1.7943710733211496e+228\n",
      "Gradient Descent(351/399): loss=inf, w0=1.2345159801433867e+229, w1=8.264188153004753e+228\n",
      "Gradient Descent(352/399): loss=inf, w0=-5.685709321491086e+229, w1=-3.806169573490481e+229\n",
      "Gradient Descent(353/399): loss=inf, w0=2.618620658497738e+230, w1=1.7529764029994109e+230\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(354/399): loss=inf, w0=-1.2060367080659757e+231, w1=-8.073540104138697e+230\n",
      "Gradient Descent(355/399): loss=inf, w0=5.554544666416299e+231, w1=3.718364360273542e+231\n",
      "Gradient Descent(356/399): loss=inf, w0=-2.5582112256508516e+232, w1=-1.7125366738024778e+232\n",
      "Gradient Descent(357/399): loss=inf, w0=1.1782144294589715e+233, w1=7.887290149539494e+232\n",
      "Gradient Descent(358/399): loss=inf, w0=-5.426405872455449e+233, w1=-3.632584741376372e+233\n",
      "Gradient Descent(359/399): loss=inf, w0=2.4991953889191738e+234, w1=1.6730298560210657e+234\n",
      "Gradient Descent(360/399): loss=inf, w0=-1.1510339880213526e+235, w1=-7.70533682877641e+234\n",
      "Gradient Descent(361/399): loss=inf, w0=5.301223135472034e+235, w1=3.548783987997797e+235\n",
      "Gradient Descent(362/399): loss=inf, w0=-2.4415409991822696e+236, w1=-1.6344344281532737e+236\n",
      "Gradient Descent(363/399): loss=inf, w0=1.1244805770955559e+237, w1=7.527581022027506e+236\n",
      "Gradient Descent(364/399): loss=inf, w0=-5.178928261653842e+237, w1=-3.4669164493318464e+237\n",
      "Gradient Descent(365/399): loss=inf, w0=2.3852166489735358e+238, w1=1.5967293651806192e+238\n",
      "Gradient Descent(366/399): loss=inf, w0=-1.0985397316014826e+239, w1=-7.353925896084025e+238\n",
      "Gradient Descent(367/399): loss=inf, w0=5.05945463036404e+239, w1=3.3869375276991727e+239\n",
      "Gradient Descent(368/399): loss=inf, w0=-2.330191655370943e+240, w1=-1.5598941271145325e+240\n",
      "Gradient Descent(369/399): loss=inf, w0=1.0731973201565583e+241, w1=7.184276851599904e+240\n",
      "Gradient Descent(370/399): loss=inf, w0=-4.942737157849251e+241, w1=-3.308803654252398e+241\n",
      "Gradient Descent(371/399): loss=inf, w0=2.276436043282302e+242, w1=1.5239086478071808e+242\n",
      "Gradient Descent(372/399): loss=inf, w0=-1.0484395373776524e+243, w1=-7.018541471558586e+242\n",
      "Gradient Descent(373/399): loss=inf, w0=4.8287122617850115e+243, w1=3.2324722652417664e+243\n",
      "Gradient Descent(374/399): loss=inf, w0=-2.2239205291162382e+244, w1=-1.4887533240203055e+244\n",
      "Gradient Descent(375/399): loss=inf, w0=1.0242528963604781e+245, w1=6.856629470928296e+244\n",
      "Gradient Descent(376/399): loss=inf, w0=-4.717317826638917e+245, w1=-3.1579017788282917e+245\n",
      "Gradient Descent(377/399): loss=inf, w0=2.172616504829831e+246, w1=1.454409004746285e+246\n",
      "Gradient Descent(378/399): loss=inf, w0=-1.000624221332604e+247, w1=-6.6984526474789354e+246\n",
      "Gradient Descent(379/399): loss=inf, w0=4.608493169833029e+247, w1=3.085051572431978e+247\n",
      "Gradient Descent(380/399): loss=inf, w0=-2.1224960223442624e+248, w1=-1.4208569807755643e+248\n",
      "Gradient Descent(381/399): loss=inf, w0=9.775406404758625e+248, w1=6.543924833733628e+248\n",
      "Gradient Descent(382/399): loss=inf, w0=-4.5021790086868174e+249, w1=-3.013881960602463e+249\n",
      "Gradient Descent(383/399): loss=inf, w0=2.0735317783200337e+250, w1=1.3880789745047221e+250\n",
      "Gradient Descent(384/399): loss=inf, w0=-9.549895789143889e+250, w1=-6.392961850028553e+250\n",
      "Gradient Descent(385/399): loss=inf, w0=4.3983174281224923e+251, w1=2.944354173400212e+251\n",
      "Gradient Descent(386/399): loss=inf, w0=-2.0256970992832493e+252, w1=-1.3560571299796652e+252\n",
      "Gradient Descent(387/399): loss=inf, w0=9.329587518643943e+252, w1=6.245481458655789e+252\n",
      "Gradient Descent(388/399): loss=inf, w0=-4.29685184911577e+253, w1=-2.876430335276393e+253\n",
      "Gradient Descent(389/399): loss=inf, w0=1.9789659270952658e+254, w1=1.324774003168533e+254\n",
      "Gradient Descent(390/399): loss=inf, w0=-9.114361579419892e+254, w1=-6.10140331906395e+254\n",
      "Gradient Descent(391/399): loss=inf, w0=4.197726997874007e+255, w1=2.8100734444400863e+255\n",
      "Gradient Descent(392/399): loss=inf, w0=-1.9333128047572768e+256, w1=-1.2942125524589664e+256\n",
      "Gradient Descent(393/399): loss=inf, w0=8.904100726253716e+256, w1=5.9606489440922725e+256\n",
      "Gradient Descent(394/399): loss=inf, w0=-4.100888875725721e+257, w1=-2.74524735270135e+257\n",
      "Gradient Descent(395/399): loss=inf, w0=1.8887128625426623e+258, w1=1.2643561293746739e+258\n",
      "Gradient Descent(396/399): loss=inf, w0=-8.698690418678643e+258, w1=-5.823141657214529e+258\n",
      "Gradient Descent(397/399): loss=inf, w0=4.006284729704483e+259, w1=2.6819167457793624e+259\n",
      "Gradient Descent(398/399): loss=inf, w0=-1.8451418044489425e+260, w1=-1.2351884695060532e+260\n",
      "Gradient Descent(399/399): loss=inf, w0=8.498018758582416e+260, w1=5.688806550769121e+260\n",
      "++++ Deg = 5\n",
      "++++ gamma = 0.0001\n",
      "ciaociaociao\n",
      "Gradient Descent(0/399): loss=0.4999999999999887, w0=-6.887232304259569e-08, w1=-3.5851239558151825e-05\n",
      "Gradient Descent(1/399): loss=0.49974832763835014, w0=-1.309546341420666e-07, w1=-7.164936218024027e-05\n",
      "Gradient Descent(2/399): loss=0.4994974420018251, w0=-1.8627978374193279e-07, w1=-0.0001073944803988531\n",
      "Gradient Descent(3/399): loss=0.4992473397750611, w0=-2.3488049324936566e-07, w1=-0.00014308670646006206\n",
      "Gradient Descent(4/399): loss=0.4989980176588128, w0=-2.767893555453136e-07, w1=-0.00017872615232426907\n",
      "Gradient Descent(5/399): loss=0.4987494723698662, w0=-3.12038835492409e-07, w1=-0.00021431292966704927\n",
      "Gradient Descent(6/399): loss=0.49850170064095234, w0=-3.4066127044072044e-07, w1=-0.0002498471498799912\n",
      "Gradient Descent(7/399): loss=0.4982546992206586, w0=-3.6268887073133037e-07, w1=-0.00028532892407153457\n",
      "Gradient Descent(8/399): loss=0.498008464873345, w0=-3.7815372019774024e-07, w1=-0.0003207583630678041\n",
      "Gradient Descent(9/399): loss=0.4977629943790566, w0=-3.8708777666514595e-07, w1=-0.00035613557741344143\n",
      "Gradient Descent(10/399): loss=0.49751828453343977, w0=-3.8952287244756924e-07, w1=-0.0003914606773724343\n",
      "Gradient Descent(11/399): loss=0.4972743321476564, w0=-3.854907148428508e-07, w1=-0.00042673377292894193\n",
      "Gradient Descent(12/399): loss=0.4970311340483009, w0=-3.750228866255299e-07, w1=-0.00046195497378811844\n",
      "Gradient Descent(13/399): loss=0.49678868707731627, w0=-3.581508465376279e-07, w1=-0.0004971243893769334\n",
      "Gradient Descent(14/399): loss=0.49654698809191083, w0=-3.3490592977731634e-07, w1=-0.0005322421288449893\n",
      "Gradient Descent(15/399): loss=0.4963060339644764, w0=-3.0531934848550727e-07, w1=-0.0005673083010653362\n",
      "Gradient Descent(16/399): loss=0.49606582158250545, w0=-2.69422192230361e-07, w1=-0.0006023230146352834\n",
      "Gradient Descent(17/399): loss=0.49582634784851, w0=-2.2724542848972429e-07, w1=-0.0006372863778772096\n",
      "Gradient Descent(18/399): loss=0.49558760967994087, w0=-1.7881990313151587e-07, w1=-0.0006721984988393681\n",
      "Gradient Descent(19/399): loss=0.49534960400910605, w0=-1.2417634089205783e-07, w1=-0.0007070594852966917\n",
      "Gradient Descent(20/399): loss=0.4951123277830923, w0=-6.334534585236953e-08, w1=-0.0007418694447515932\n",
      "Gradient Descent(21/399): loss=0.494875777963684, w0=3.6425980875756223e-09, w1=-0.0007766284844347635\n",
      "Gradient Descent(22/399): loss=0.49463995152728524, w0=7.675712673659662e-08, w1=-0.0008113367113059673\n",
      "Gradient Descent(23/399): loss=0.49440484546484054, w0=1.559679951420818e-07, w1=-0.0008459942320548364\n",
      "Gradient Descent(24/399): loss=0.4941704567817566, w0=2.412450771217489e-07, w1=-0.0008806011531016593\n",
      "Gradient Descent(25/399): loss=0.49393678249782536, w0=3.3255836479740984e-07, w1=-0.0009151575805981689\n",
      "Gradient Descent(26/399): loss=0.493703819647146, w0=4.2987796813071086e-07, w1=-0.0009496636204283274\n",
      "Gradient Descent(27/399): loss=0.4934715652780494, w0=5.331741144608507e-07, w1=-0.0009841193782091084\n",
      "Gradient Descent(28/399): loss=0.49324001645301974, w0=6.424171480442561e-07, w1=-0.0010185249592912767\n",
      "Gradient Descent(29/399): loss=0.4930091702486209, w0=7.575775295962131e-07, w1=-0.0010528804687601643\n",
      "Gradient Descent(30/399): loss=0.4927790237554201, w0=8.786258358344511e-07, w1=-0.0010871860114364464\n",
      "Gradient Descent(31/399): loss=0.49254957407791267, w0=1.0055327590246536e-06, w1=-0.0011214416918769112\n",
      "Gradient Descent(32/399): loss=0.4923208183344488, w0=1.1382691065279e-06, w1=-0.00115564761437523\n",
      "Gradient Descent(33/399): loss=0.4920927536571579, w0=1.2768058003500201e-06, w1=-0.0011898038829627235\n",
      "Gradient Descent(34/399): loss=0.49186537719187595, w0=1.4211138766928725e-06, w1=-0.0012239106014091253\n",
      "Gradient Descent(35/399): loss=0.49163868609807254, w0=1.5711644855075029e-06, w1=-0.0012579678732233437\n",
      "Gradient Descent(36/399): loss=0.49141267754877865, w0=1.7269288900492055e-06, w1=-0.0012919758016542203\n",
      "Gradient Descent(37/399): loss=0.49118734873051184, w0=1.888378466434464e-06, w1=-0.001325934489691286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(38/399): loss=0.49096269684320804, w0=2.0554847031997652e-06, w1=-0.0013598440400655147\n",
      "Gradient Descent(39/399): loss=0.4907387191001471, w0=2.228219200862279e-06, w1=-0.0013937045552500746\n",
      "Gradient Descent(40/399): loss=0.49051541272788385, w0=2.406553671482392e-06, w1=-0.0014275161374610761\n",
      "Gradient Descent(41/399): loss=0.490292774966176, w0=2.590459938228086e-06, w1=-0.0014612788886583191\n",
      "Gradient Descent(42/399): loss=0.4900708030679155, w0=2.779909934941161e-06, w1=-0.0014949929105460356\n",
      "Gradient Descent(43/399): loss=0.48984949429905733, w0=2.9748757057052853e-06, w1=-0.0015286583045736309\n",
      "Gradient Descent(44/399): loss=0.4896288459385518, w0=3.175329404415863e-06, w1=-0.001562275171936422\n",
      "Gradient Descent(45/399): loss=0.4894088552782738, w0=3.3812432943517157e-06, w1=-0.0015958436135763746\n",
      "Gradient Descent(46/399): loss=0.4891895196229562, w0=3.5925897477485767e-06, w1=-0.0016293637301828356\n",
      "Gradient Descent(47/399): loss=0.48897083629011984, w0=3.8093412453743705e-06, w1=-0.001662835622193265\n",
      "Gradient Descent(48/399): loss=0.48875280261000786, w0=4.031470376106281e-06, w1=-0.0016962593897939648\n",
      "Gradient Descent(49/399): loss=0.4885354159255167, w0=4.2589498365096105e-06, w1=-0.0017296351329208043\n",
      "Gradient Descent(50/399): loss=0.48831867359213044, w0=4.491752430418388e-06, w1=-0.001762962951259945\n",
      "Gradient Descent(51/399): loss=0.48810257297785326, w0=4.729851068517762e-06, w1=-0.0017962429442485617\n",
      "Gradient Descent(52/399): loss=0.4878871114631447, w0=4.973218767928135e-06, w1=-0.001829475211075561\n",
      "Gradient Descent(53/399): loss=0.48767228644085203, w0=5.221828651791048e-06, w1=-0.0018626598506822991\n",
      "Gradient Descent(54/399): loss=0.4874580953161466, w0=5.475653948856799e-06, w1=-0.001895796961763295\n",
      "Gradient Descent(55/399): loss=0.48724453550645896, w0=5.7346679930738026e-06, w1=-0.0019288866427669433\n",
      "Gradient Descent(56/399): loss=0.48703160444141225, w0=5.998844223179652e-06, w1=-0.001961928991896222\n",
      "Gradient Descent(57/399): loss=0.4868192995627607, w0=6.268156182293918e-06, w1=-0.0019949241071094024\n",
      "Gradient Descent(58/399): loss=0.48660761832432364, w0=6.542577517512636e-06, w1=-0.002027872086120751\n",
      "Gradient Descent(59/399): loss=0.4863965581919238, w0=6.822081979504494e-06, w1=-0.002060773026401234\n",
      "Gradient Descent(60/399): loss=0.48618611664332256, w0=7.106643422108729e-06, w1=-0.002093627025179217\n",
      "Gradient Descent(61/399): loss=0.4859762911681592, w0=7.396235801934671e-06, w1=-0.0021264341794411607\n",
      "Gradient Descent(62/399): loss=0.4857670792678864, w0=7.690833177962996e-06, w1=-0.0021591945859323197\n",
      "Gradient Descent(63/399): loss=0.4855584784557102, w0=7.990409711148624e-06, w1=-0.002191908341157434\n",
      "Gradient Descent(64/399): loss=0.4853504862565273, w0=8.294939664025285e-06, w1=-0.002224575541381419\n",
      "Gradient Descent(65/399): loss=0.48514310020686396, w0=8.604397400311735e-06, w1=-0.002257196282630056\n",
      "Gradient Descent(66/399): loss=0.4849363178548156, w0=8.918757384519611e-06, w1=-0.0022897706606906766\n",
      "Gradient Descent(67/399): loss=0.4847301367599858, w0=9.237994181562934e-06, w1=-0.002322298771112848\n",
      "Gradient Descent(68/399): loss=0.4845245544934259, w0=9.562082456369218e-06, w1=-0.0023547807092090542\n",
      "Gradient Descent(69/399): loss=0.48431956863757625, w0=9.890996973492233e-06, w1=-0.002387216570055376\n",
      "Gradient Descent(70/399): loss=0.4841151767862057, w0=1.0224712596726346e-05, w1=-0.0024196064484921674\n",
      "Gradient Descent(71/399): loss=0.48391137654435257, w0=1.0563204288722491e-05, w1=-0.002451950439124731\n",
      "Gradient Descent(72/399): loss=0.4837081655282664, w0=1.0906447110605735e-05, w1=-0.0024842486363239914\n",
      "Gradient Descent(73/399): loss=0.48350554136534873, w0=1.1254416221594424e-05, w1=-0.0025165011342271645\n",
      "Gradient Descent(74/399): loss=0.48330350169409625, w0=1.1607086878620928e-05, w1=-0.0025487080267384272\n",
      "Gradient Descent(75/399): loss=0.4831020441640409, w0=1.1964434435953953e-05, w1=-0.002580869407529583\n",
      "Gradient Descent(76/399): loss=0.48290116643569564, w0=1.2326434344822413e-05, w1=-0.002612985370040725\n",
      "Gradient Descent(77/399): loss=0.48270086618049346, w0=1.2693062153040891e-05, w1=-0.002645056007480899\n",
      "Gradient Descent(78/399): loss=0.4825011410807335, w0=1.306429350463663e-05, w1=-0.0026770814128287625\n",
      "Gradient Descent(79/399): loss=0.4823019888295241, w0=1.3440104139478088e-05, w1=-0.0027090616788332427\n",
      "Gradient Descent(80/399): loss=0.4821034071307254, w0=1.382046989290501e-05, w1=-0.0027409968980141907\n",
      "Gradient Descent(81/399): loss=0.48190539369889496, w0=1.4205366695360072e-05, w1=-0.0027728871626630353\n",
      "Gradient Descent(82/399): loss=0.4817079462592314, w0=1.4594770572022008e-05, w1=-0.002804732564843434\n",
      "Gradient Descent(83/399): loss=0.4815110625475195, w0=1.4988657642440298e-05, w1=-0.002836533196391921\n",
      "Gradient Descent(84/399): loss=0.48131474031007593, w0=1.5387004120171327e-05, w1=-0.002868289148918555\n",
      "Gradient Descent(85/399): loss=0.48111897730369335, w0=1.578978631241609e-05, w1=-0.002900000513807563\n",
      "Gradient Descent(86/399): loss=0.4809237712955875, w0=1.619698061965936e-05, w1=-0.0029316673822179836\n",
      "Gradient Descent(87/399): loss=0.4807291200633429, w0=1.6608563535310377e-05, w1=-0.002963289845084306\n",
      "Gradient Descent(88/399): loss=0.48053502139485926, w0=1.7024511645344997e-05, w1=-0.0029948679931171094\n",
      "Gradient Descent(89/399): loss=0.4803414730882971, w0=1.7444801627949338e-05, w1=-0.0030264019168036987\n",
      "Gradient Descent(90/399): loss=0.48014847295202717, w0=1.78694102531649e-05, w1=-0.0030578917064087384\n",
      "Gradient Descent(91/399): loss=0.47995601880457506, w0=1.8298314382535114e-05, w1=-0.0030893374519748856\n",
      "Gradient Descent(92/399): loss=0.47976410847457124, w0=1.87314909687534e-05, w1=-0.0031207392433234183\n",
      "Gradient Descent(93/399): loss=0.4795727398006972, w0=1.9168917055312632e-05, w1=-0.0031520971700548643\n",
      "Gradient Descent(94/399): loss=0.47938191063163427, w0=1.9610569776156087e-05, w1=-0.0031834113215496273\n",
      "Gradient Descent(95/399): loss=0.4791916188260124, w0=2.0056426355329784e-05, w1=-0.00321468178696861\n",
      "Gradient Descent(96/399): loss=0.4790018622523584, w0=2.050646410663631e-05, w1=-0.003245908655253837\n",
      "Gradient Descent(97/399): loss=0.4788126387890459, w0=2.0960660433290026e-05, w1=-0.0032770920151290717\n",
      "Gradient Descent(98/399): loss=0.4786239463242435, w0=2.141899282757371e-05, w1=-0.0033082319551004384\n",
      "Gradient Descent(99/399): loss=0.47843578275586557, w0=2.188143887049662e-05, w1=-0.003339328563457034\n",
      "Gradient Descent(100/399): loss=0.4782481459915221, w0=2.2347976231453943e-05, w1=-0.003370381928271544\n",
      "Gradient Descent(101/399): loss=0.47806103394846744, w0=2.2818582667887647e-05, w1=-0.003401392137400853\n",
      "Gradient Descent(102/399): loss=0.4778744445535531, w0=2.3293236024948747e-05, w1=-0.003432359278486655\n",
      "Gradient Descent(103/399): loss=0.4776883757431772, w0=2.3771914235160934e-05, w1=-0.0034632834389560618\n",
      "Gradient Descent(104/399): loss=0.4775028254632351, w0=2.425459531808559e-05, w1=-0.003494164706022207\n",
      "Gradient Descent(105/399): loss=0.4773177916690724, w0=2.4741257379988184e-05, w1=-0.0035250031666848513\n",
      "Gradient Descent(106/399): loss=0.47713327232543495, w0=2.5231878613506023e-05, w1=-0.003555798907730984\n",
      "Gradient Descent(107/399): loss=0.4769492654064219, w0=2.5726437297317376e-05, w1=-0.0035865520157354223\n",
      "Gradient Descent(108/399): loss=0.47676576889543687, w0=2.6224911795811954e-05, w1=-0.00361726257706141\n",
      "Gradient Descent(109/399): loss=0.47658278078514144, w0=2.672728055876272e-05, w1=-0.003647930677861213\n",
      "Gradient Descent(110/399): loss=0.4764002990774069, w0=2.7233522120999066e-05, w1=-0.0036785564040767143\n",
      "Gradient Descent(111/399): loss=0.4762183217832681, w0=2.7743615102081318e-05, w1=-0.003709139841440004\n",
      "Gradient Descent(112/399): loss=0.4760368469228762, w0=2.8257538205976562e-05, w1=-0.0037396810754739716\n",
      "Gradient Descent(113/399): loss=0.4758558725254517, w0=2.8775270220735807e-05, w1=-0.003770180191492894\n",
      "Gradient Descent(114/399): loss=0.47567539662923963, w0=2.929679001817246e-05, w1=-0.0038006372746030217\n",
      "Gradient Descent(115/399): loss=0.47549541728146233, w0=2.982207655354213e-05, w1=-0.0038310524097031626\n",
      "Gradient Descent(116/399): loss=0.475315932538274, w0=3.0351108865223705e-05, w1=-0.0038614256814852657\n",
      "Gradient Descent(117/399): loss=0.4751369404647155, w0=3.088386607440176e-05, w1=-0.003891757174435001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(118/399): loss=0.4749584391346693, w0=3.142032738475024e-05, w1=-0.00392204697283234\n",
      "Gradient Descent(119/399): loss=0.47478042663081377, w0=3.196047208211746e-05, w1=-0.00395229516075213\n",
      "Gradient Descent(120/399): loss=0.4746029010445796, w0=3.250427953421237e-05, w1=-0.003982501822064672\n",
      "Gradient Descent(121/399): loss=0.47442586047610413, w0=3.3051729190292084e-05, w1=-0.0040126670404362925\n",
      "Gradient Descent(122/399): loss=0.47424930303418833, w0=3.360280058085069e-05, w1=-0.004042790899329915\n",
      "Gradient Descent(123/399): loss=0.47407322683625175, w0=3.415747331730935e-05, w1=-0.004072873482005632\n",
      "Gradient Descent(124/399): loss=0.4738976300082898, w0=3.4715727091707625e-05, w1=-0.004102914871521268\n",
      "Gradient Descent(125/399): loss=0.47372251068482985, w0=3.5277541676396094e-05, w1=-0.004132915150732951\n",
      "Gradient Descent(126/399): loss=0.4735478670088877, w0=3.584289692373017e-05, w1=-0.004162874402295672\n",
      "Gradient Descent(127/399): loss=0.4733736971319251, w0=3.641177276576522e-05, w1=-0.0041927927086638506\n",
      "Gradient Descent(128/399): loss=0.47319999921380707, w0=3.698414921395284e-05, w1=-0.004222670152091893\n",
      "Gradient Descent(129/399): loss=0.47302677142275884, w0=3.7560006358838484e-05, w1=-0.004252506814634754\n",
      "Gradient Descent(130/399): loss=0.4728540119353239, w0=3.8139324369760187e-05, w1=-0.00428230277814849\n",
      "Gradient Descent(131/399): loss=0.4726817189363225, w0=3.872208349454861e-05, w1=-0.004312058124290818\n",
      "Gradient Descent(132/399): loss=0.4725098906188089, w0=3.9308264059228236e-05, w1=-0.004341772934521666\n",
      "Gradient Descent(133/399): loss=0.4723385251840306, w0=3.989784646771983e-05, w1=-0.004371447290103726\n",
      "Gradient Descent(134/399): loss=0.4721676208413861, w0=4.049081120154405e-05, w1=-0.004401081272103006\n",
      "Gradient Descent(135/399): loss=0.47199717580838474, w0=4.10871388195263e-05, w1=-0.0044306749613893725\n",
      "Gradient Descent(136/399): loss=0.4718271883106059, w0=4.1686809957502754e-05, w1=-0.0044602284386371024\n",
      "Gradient Descent(137/399): loss=0.4716576565816575, w0=4.228980532802758e-05, w1=-0.004489741784325425\n",
      "Gradient Descent(138/399): loss=0.4714885788631359, w0=4.2896105720081334e-05, w1=-0.004519215078739066\n",
      "Gradient Descent(139/399): loss=0.4713199534045861, w0=4.3505691998780554e-05, w1=-0.004548648401968787\n",
      "Gradient Descent(140/399): loss=0.4711517784634612, w0=4.41185451050885e-05, w1=-0.004578041833911926\n",
      "Gradient Descent(141/399): loss=0.470984052305083, w0=4.473464605552709e-05, w1=-0.004607395454272937\n",
      "Gradient Descent(142/399): loss=0.47081677320260285, w0=4.5353975941889976e-05, w1=-0.004636709342563922\n",
      "Gradient Descent(143/399): loss=0.4706499394369612, w0=4.597651593095677e-05, w1=-0.004665983578105167\n",
      "Gradient Descent(144/399): loss=0.4704835492968493, w0=4.660224726420847e-05, w1=-0.004695218240025677\n",
      "Gradient Descent(145/399): loss=0.47031760107867127, w0=4.723115125754397e-05, w1=-0.004724413407263705\n",
      "Gradient Descent(146/399): loss=0.47015209308650213, w0=4.786320930099775e-05, w1=-0.004753569158567278\n",
      "Gradient Descent(147/399): loss=0.46998702363205375, w0=4.849840285845872e-05, w1=-0.004782685572494732\n",
      "Gradient Descent(148/399): loss=0.46982239103463364, w0=4.913671346739012e-05, w1=-0.00481176272741523\n",
      "Gradient Descent(149/399): loss=0.4696581936211074, w0=4.977812273855064e-05, w1=-0.004840800701509292\n",
      "Gradient Descent(150/399): loss=0.46949442972586125, w0=5.04226123557166e-05, w1=-0.004869799572769312\n",
      "Gradient Descent(151/399): loss=0.46933109769076486, w0=5.107016407540527e-05, w1=-0.004898759419000086\n",
      "Gradient Descent(152/399): loss=0.469168195865133, w0=5.172075972659928e-05, w1=-0.004927680317819326\n",
      "Gradient Descent(153/399): loss=0.46900572260568896, w0=5.237438121047217e-05, w1=-0.004956562346658178\n",
      "Gradient Descent(154/399): loss=0.46884367627652734, w0=5.3031010500115034e-05, w1=-0.00498540558276174\n",
      "Gradient Descent(155/399): loss=0.4686820552490768, w0=5.3690629640264226e-05, w1=-0.005014210103189574\n",
      "Gradient Descent(156/399): loss=0.4685208579020651, w0=5.43532207470302e-05, w1=-0.00504297598481622\n",
      "Gradient Descent(157/399): loss=0.4683600826214806, w0=5.5018766007627406e-05, w1=-0.005071703304331708\n",
      "Gradient Descent(158/399): loss=0.4681997278005367, w0=5.5687247680105304e-05, w1=-0.005100392138242066\n",
      "Gradient Descent(159/399): loss=0.46803979183963723, w0=5.635864809308042e-05, w1=-0.0051290425628698275\n",
      "Gradient Descent(160/399): loss=0.4678802731463386, w0=5.703294964546949e-05, w1=-0.005157654654354539\n",
      "Gradient Descent(161/399): loss=0.46772117013531567, w0=5.771013480622369e-05, w1=-0.005186228488653263\n",
      "Gradient Descent(162/399): loss=0.46756248122832555, w0=5.839018611406386e-05, w1=-0.005214764141541085\n",
      "Gradient Descent(163/399): loss=0.4674042048541732, w0=5.90730861772169e-05, w1=-0.005243261688611608\n",
      "Gradient Descent(164/399): loss=0.4672463394486747, w0=5.975881767315313e-05, w1=-0.00527172120527746\n",
      "Gradient Descent(165/399): loss=0.46708888345462474, w0=6.044736334832471e-05, w1=-0.005300142766770787\n",
      "Gradient Descent(166/399): loss=0.4669318353217602, w0=6.113870601790515e-05, w1=-0.005328526448143752\n",
      "Gradient Descent(167/399): loss=0.46677519350672636, w0=6.18328285655298e-05, w1=-0.00535687232426903\n",
      "Gradient Descent(168/399): loss=0.4666189564730424, w0=6.252971394303743e-05, w1=-0.005385180469840301\n",
      "Gradient Descent(169/399): loss=0.46646312269106766, w0=6.322934517021282e-05, w1=-0.005413450959372741\n",
      "Gradient Descent(170/399): loss=0.46630769063796756, w0=6.393170533453035e-05, w1=-0.005441683867203517\n",
      "Gradient Descent(171/399): loss=0.46615265879767964, w0=6.463677759089865e-05, w1=-0.00546987926749227\n",
      "Gradient Descent(172/399): loss=0.4659980256608806, w0=6.534454516140627e-05, w1=-0.005498037234221607\n",
      "Gradient Descent(173/399): loss=0.46584378972495216, w0=6.60549913350683e-05, w1=-0.005526157841197586\n",
      "Gradient Descent(174/399): loss=0.4656899494939486, w0=6.67680994675741e-05, w1=-0.005554241162050198\n",
      "Gradient Descent(175/399): loss=0.46553650347856335, w0=6.748385298103592e-05, w1=-0.005582287270233851\n",
      "Gradient Descent(176/399): loss=0.46538345019609606, w0=6.820223536373861e-05, w1=-0.005610296239027855\n",
      "Gradient Descent(177/399): loss=0.4652307881704212, w0=6.892323016989031e-05, w1=-0.005638268141536896\n",
      "Gradient Descent(178/399): loss=0.4650785159319533, w0=6.964682101937406e-05, w1=-0.005666203050691519\n",
      "Gradient Descent(179/399): loss=0.46492663201761664, w0=7.037299159750049e-05, w1=-0.005694101039248603\n",
      "Gradient Descent(180/399): loss=0.46477513497081285, w0=7.110172565476145e-05, w1=-0.005721962179791835\n",
      "Gradient Descent(181/399): loss=0.4646240233413876, w0=7.18330070065846e-05, w1=-0.005749786544732186\n",
      "Gradient Descent(182/399): loss=0.46447329568560064, w0=7.256681953308901e-05, w1=-0.005777574206308384\n",
      "Gradient Descent(183/399): loss=0.46432295056609296, w0=7.33031471788417e-05, w1=-0.0058053252365873826\n",
      "Gradient Descent(184/399): loss=0.46417298655185657, w0=7.404197395261516e-05, w1=-0.0058330397074648314\n",
      "Gradient Descent(185/399): loss=0.46402340221820115, w0=7.478328392714582e-05, w1=-0.005860717690665544\n",
      "Gradient Descent(186/399): loss=0.4638741961467253, w0=7.552706123889351e-05, w1=-0.005888359257743967\n",
      "Gradient Descent(187/399): loss=0.46372536692528454, w0=7.627329008780182e-05, w1=-0.0059159644800846406\n",
      "Gradient Descent(188/399): loss=0.4635769131479599, w0=7.702195473705944e-05, w1=-0.005943533428902665\n",
      "Gradient Descent(189/399): loss=0.46342883341502933, w0=7.777303951286245e-05, w1=-0.005971066175244163\n",
      "Gradient Descent(190/399): loss=0.4632811263329347, w0=7.852652880417753e-05, w1=-0.005998562789986738\n",
      "Gradient Descent(191/399): loss=0.4631337905142536, w0=7.928240706250612e-05, w1=-0.0060260233438399384\n",
      "Gradient Descent(192/399): loss=0.46298682457766815, w0=8.004065880164948e-05, w1=-0.006053447907345709\n",
      "Gradient Descent(193/399): loss=0.46284022714793566, w0=8.080126859747473e-05, w1=-0.006080836550878853\n",
      "Gradient Descent(194/399): loss=0.4626939968558582, w0=8.156422108768183e-05, w1=-0.006108189344647484\n",
      "Gradient Descent(195/399): loss=0.462548132338254, w0=8.232950097157133e-05, w1=-0.006135506358693482\n",
      "Gradient Descent(196/399): loss=0.4624026322379268, w0=8.309709300981322e-05, w1=-0.006162787662892941\n",
      "Gradient Descent(197/399): loss=0.4622574952036371, w0=8.386698202421662e-05, w1=-0.006190033326956628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(198/399): loss=0.46211271989007374, w0=8.463915289750033e-05, w1=-0.006217243420430422\n",
      "Gradient Descent(199/399): loss=0.46196830495782326, w0=8.541359057306433e-05, w1=-0.006244418012695773\n",
      "Gradient Descent(200/399): loss=0.4618242490733424, w0=8.619028005476224e-05, w1=-0.00627155717297014\n",
      "Gradient Descent(201/399): loss=0.4616805509089294, w0=8.696920640667452e-05, w1=-0.00629866097030744\n",
      "Gradient Descent(202/399): loss=0.4615372091426946, w0=8.77503547528827e-05, w1=-0.006325729473598492\n",
      "Gradient Descent(203/399): loss=0.46139422245853257, w0=8.853371027724446e-05, w1=-0.006352762751571459\n",
      "Gradient Descent(204/399): loss=0.46125158954609397, w0=8.931925822316959e-05, w1=-0.006379760872792291\n",
      "Gradient Descent(205/399): loss=0.46110930910075765, w0=9.010698389339676e-05, w1=-0.00640672390566516\n",
      "Gradient Descent(206/399): loss=0.46096737982360236, w0=9.089687264977134e-05, w1=-0.006433651918432903\n",
      "Gradient Descent(207/399): loss=0.46082580042137844, w0=9.168890991302388e-05, w1=-0.006460544979177457\n",
      "Gradient Descent(208/399): loss=0.4606845696064817, w0=9.248308116254965e-05, w1=-0.006487403155820299\n",
      "Gradient Descent(209/399): loss=0.46054368609692475, w0=9.327937193618892e-05, w1=-0.006514226516122871\n",
      "Gradient Descent(210/399): loss=0.4604031486163097, w0=9.407776783000813e-05, w1=-0.006541015127687022\n",
      "Gradient Descent(211/399): loss=0.4602629558938013, w0=9.487825449808195e-05, w1=-0.006567769057955438\n",
      "Gradient Descent(212/399): loss=0.4601231066641004, w0=9.568081765227617e-05, w1=-0.006594488374212069\n",
      "Gradient Descent(213/399): loss=0.45998359966741587, w0=9.648544306203146e-05, w1=-0.006621173143582561\n",
      "Gradient Descent(214/399): loss=0.4598444336494387, w0=9.729211655414794e-05, w1=-0.0066478234330346825\n",
      "Gradient Descent(215/399): loss=0.45970560736131527, w0=9.81008240125706e-05, w1=-0.0066744393093787525\n",
      "Gradient Descent(216/399): loss=0.45956711955962026, w0=9.891155137817562e-05, w1=-0.006701020839268063\n",
      "Gradient Descent(217/399): loss=0.45942896900633123, w0=9.972428464855743e-05, w1=-0.0067275680891993065\n",
      "Gradient Descent(218/399): loss=0.45929115446880187, w0=0.0001005390098778167, w1=-0.0067540811255129935\n",
      "Gradient Descent(219/399): loss=0.45915367471973606, w0=0.00010135571317634905, w1=-0.00678056001439388\n",
      "Gradient Descent(220/399): loss=0.4590165285371619, w0=0.00010217438071063473, w1=-0.006807004821871382\n",
      "Gradient Descent(221/399): loss=0.45887971470440597, w0=0.00010299499870302894, w1=-0.006833415613819996\n",
      "Gradient Descent(222/399): loss=0.45874323201006795, w0=0.00010381755343155315, w1=-0.00685979245595972\n",
      "Gradient Descent(223/399): loss=0.45860707924799415, w0=0.00010464203122968708, w1=-0.006886135413856462\n",
      "Gradient Descent(224/399): loss=0.4584712552172535, w0=0.00010546841848616163, w1=-0.006912444552922462\n",
      "Gradient Descent(225/399): loss=0.4583357587221113, w0=0.0001062967016447525, w1=-0.006938719938416703\n",
      "Gradient Descent(226/399): loss=0.45820058857200474, w0=0.00010712686720407467, w1=-0.006964961635445322\n",
      "Gradient Descent(227/399): loss=0.45806574358151714, w0=0.00010795890171737767, w1=-0.00699116970896202\n",
      "Gradient Descent(228/399): loss=0.4579312225703539, w0=0.00010879279179234169, w1=-0.0070173442237684795\n",
      "Gradient Descent(229/399): loss=0.4577970243633171, w0=0.00010962852409087439, w1=-0.007043485244514761\n",
      "Gradient Descent(230/399): loss=0.4576631477902817, w0=0.0001104660853289086, w1=-0.007069592835699722\n",
      "Gradient Descent(231/399): loss=0.4575295916861706, w0=0.0001113054622762007, w1=-0.0070956670616714144\n",
      "Gradient Descent(232/399): loss=0.4573963548909298, w0=0.00011214664175612993, w1=-0.007121707986627495\n",
      "Gradient Descent(233/399): loss=0.4572634362495058, w0=0.00011298961064549833, w1=-0.007147715674615624\n",
      "Gradient Descent(234/399): loss=0.45713083461181925, w0=0.00011383435587433154, w1=-0.007173690189533872\n",
      "Gradient Descent(235/399): loss=0.4569985488327427, w0=0.00011468086442568039, w1=-0.0071996315951311195\n",
      "Gradient Descent(236/399): loss=0.4568665777720773, w0=0.00011552912333542316, w1=-0.007225539955007456\n",
      "Gradient Descent(237/399): loss=0.45673492029452634, w0=0.00011637911969206872, w1=-0.00725141533261458\n",
      "Gradient Descent(238/399): loss=0.4566035752696747, w0=0.00011723084063656039, w1=-0.007277257791256193\n",
      "Gradient Descent(239/399): loss=0.4564725415719639, w0=0.00011808427336208055, w1=-0.0073030673940884016\n",
      "Gradient Descent(240/399): loss=0.4563418180806683, w0=0.00011893940511385599, w1=-0.007328844204120107\n",
      "Gradient Descent(241/399): loss=0.4562114036798739, w0=0.00011979622318896407, w1=-0.007354588284213403\n",
      "Gradient Descent(242/399): loss=0.4560812972584528, w0=0.0001206547149361396, w1=-0.007380299697083967\n",
      "Gradient Descent(243/399): loss=0.45595149771004173, w0=0.00012151486775558245, w1=-0.00740597850530145\n",
      "Gradient Descent(244/399): loss=0.4558220039330189, w0=0.00012237666909876594, w1=-0.007431624771289868\n",
      "Gradient Descent(245/399): loss=0.455692814830482, w0=0.00012324010646824594, w1=-0.007457238557327994\n",
      "Gradient Descent(246/399): loss=0.4555639293102234, w0=0.00012410516741747073, w1=-0.007482819925549742\n",
      "Gradient Descent(247/399): loss=0.45543534628471005, w0=0.00012497183955059157, w1=-0.007508368937944553\n",
      "Gradient Descent(248/399): loss=0.45530706467105997, w0=0.00012584011052227403, w1=-0.0075338856563577855\n",
      "Gradient Descent(249/399): loss=0.4551790833910201, w0=0.0001267099680375101, w1=-0.007559370142491096\n",
      "Gradient Descent(250/399): loss=0.45505140137094424, w0=0.0001275813998514308, w1=-0.007584822457902824\n",
      "Gradient Descent(251/399): loss=0.454924017541771, w0=0.0001284543937691199, w1=-0.0076102426640083706\n",
      "Gradient Descent(252/399): loss=0.4547969308390021, w0=0.00012932893764542791, w1=-0.007635630822080586\n",
      "Gradient Descent(253/399): loss=0.4546701402026801, w0=0.00013020501938478717, w1=-0.007660986993250143\n",
      "Gradient Descent(254/399): loss=0.4545436445773673, w0=0.00013108262694102744, w1=-0.00768631123850592\n",
      "Gradient Descent(255/399): loss=0.45441744291212366, w0=0.00013196174831719225, w1=-0.007711603618695375\n",
      "Gradient Descent(256/399): loss=0.45429153416048557, w0=0.00013284237156535597, w1=-0.007736864194524925\n",
      "Gradient Descent(257/399): loss=0.4541659172804451, w0=0.00013372448478644162, w1=-0.007762093026560319\n",
      "Gradient Descent(258/399): loss=0.45404059123442636, w0=0.0001346080761300392, w1=-0.007787290175227013\n",
      "Gradient Descent(259/399): loss=0.4539155549892686, w0=0.00013549313379422511, w1=-0.007812455700810543\n",
      "Gradient Descent(260/399): loss=0.453790807516201, w0=0.00013637964602538174, w1=-0.007837589663456896\n",
      "Gradient Descent(261/399): loss=0.45366634779082426, w0=0.00013726760111801818, w1=-0.007862692123172883\n",
      "Gradient Descent(262/399): loss=0.453542174793089, w0=0.00013815698741459145, w1=-0.007887763139826505\n",
      "Gradient Descent(263/399): loss=0.4534182875072751, w0=0.00013904779330532844, w1=-0.007912802773147327\n",
      "Gradient Descent(264/399): loss=0.4532946849219705, w0=0.00013994000722804845, w1=-0.00793781108272684\n",
      "Gradient Descent(265/399): loss=0.4531713660300525, w0=0.00014083361766798657, w1=-0.007962788128018828\n",
      "Gradient Descent(266/399): loss=0.4530483298286654, w0=0.00014172861315761762, w1=-0.007987733968339735\n",
      "Gradient Descent(267/399): loss=0.4529255753192017, w0=0.00014262498227648085, w1=-0.008012648662869031\n",
      "Gradient Descent(268/399): loss=0.452803101507281, w0=0.0001435227136510052, w1=-0.008037532270649569\n",
      "Gradient Descent(269/399): loss=0.45268090740273004, w0=0.00014442179595433528, w1=-0.008062384850587953\n",
      "Gradient Descent(270/399): loss=0.45255899201956373, w0=0.00014532221790615815, w1=-0.008087206461454895\n",
      "Gradient Descent(271/399): loss=0.4524373543759635, w0=0.00014622396827253053, w1=-0.008111997161885575\n",
      "Gradient Descent(272/399): loss=0.45231599349425927, w0=0.00014712703586570677, w1=-0.008136757010380004\n",
      "Gradient Descent(273/399): loss=0.4521949084009094, w0=0.0001480314095439676, w1=-0.008161486065303375\n",
      "Gradient Descent(274/399): loss=0.4520740981264802, w0=0.00014893707821144942, w1=-0.008186184384886426\n",
      "Gradient Descent(275/399): loss=0.45195356170562756, w0=0.0001498440308179741, w1=-0.008210852027225789\n",
      "Gradient Descent(276/399): loss=0.45183329817707696, w0=0.00015075225635887974, w1=-0.008235489050284348\n",
      "Gradient Descent(277/399): loss=0.45171330658360465, w0=0.00015166174387485194, w1=-0.008260095511891595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(278/399): loss=0.45159358597201826, w0=0.00015257248245175554, w1=-0.008284671469743977\n",
      "Gradient Descent(279/399): loss=0.45147413539313774, w0=0.00015348446122046726, w1=-0.008309216981405248\n",
      "Gradient Descent(280/399): loss=0.4513549539017767, w0=0.00015439766935670894, w1=-0.008333732104306824\n",
      "Gradient Descent(281/399): loss=0.45123604055672295, w0=0.0001553120960808812, w1=-0.008358216895748125\n",
      "Gradient Descent(282/399): loss=0.4511173944207205, w0=0.00015622773065789803, w1=-0.00838267141289693\n",
      "Gradient Descent(283/399): loss=0.45099901456045033, w0=0.00015714456239702176, w1=-0.008407095712789721\n",
      "Gradient Descent(284/399): loss=0.45088090004651227, w0=0.00015806258065169886, w1=-0.008431489852332026\n",
      "Gradient Descent(285/399): loss=0.4507630499534058, w0=0.0001589817748193962, w1=-0.008455853888298772\n",
      "Gradient Descent(286/399): loss=0.4506454633595126, w0=0.00015990213434143808, w1=-0.008480187877334618\n",
      "Gradient Descent(287/399): loss=0.4505281393470774, w0=0.00016082364870284374, w1=-0.008504491875954307\n",
      "Gradient Descent(288/399): loss=0.45041107700219074, w0=0.0001617463074321656, w1=-0.008528765940543005\n",
      "Gradient Descent(289/399): loss=0.45029427541477013, w0=0.0001626701001013281, w1=-0.00855301012735664\n",
      "Gradient Descent(290/399): loss=0.45017773367854197, w0=0.00016359501632546701, w1=-0.008577224492522247\n",
      "Gradient Descent(291/399): loss=0.45006145089102434, w0=0.00016452104576276965, w1=-0.008601409092038298\n",
      "Gradient Descent(292/399): loss=0.44994542615350863, w0=0.0001654481781143154, w1=-0.008625563981775051\n",
      "Gradient Descent(293/399): loss=0.44982965857104207, w0=0.00016637640312391696, w1=-0.008649689217474877\n",
      "Gradient Descent(294/399): loss=0.44971414725240955, w0=0.00016730571057796227, w1=-0.008673784854752602\n",
      "Gradient Descent(295/399): loss=0.4495988913101173, w0=0.00016823609030525694, w1=-0.00869785094909584\n",
      "Gradient Descent(296/399): loss=0.44948388986037413, w0=0.0001691675321768673, w1=-0.008721887555865325\n",
      "Gradient Descent(297/399): loss=0.4493691420230743, w0=0.00017010002610596395, w1=-0.008745894730295248\n",
      "Gradient Descent(298/399): loss=0.44925464692178096, w0=0.00017103356204766626, w1=-0.008769872527493587\n",
      "Gradient Descent(299/399): loss=0.4491404036837086, w0=0.00017196812999888692, w1=-0.008793821002442434\n",
      "Gradient Descent(300/399): loss=0.4490264114397056, w0=0.00017290371999817753, w1=-0.008817740209998332\n",
      "Gradient Descent(301/399): loss=0.4489126693242374, w0=0.0001738403221255746, w1=-0.008841630204892599\n",
      "Gradient Descent(302/399): loss=0.4487991764753698, w0=0.000174777926502446, w1=-0.008865491041731658\n",
      "Gradient Descent(303/399): loss=0.44868593203475216, w0=0.00017571652329133837, w1=-0.008889322774997363\n",
      "Gradient Descent(304/399): loss=0.4485729351476002, w0=0.00017665610269582466, w1=-0.008913125459047325\n",
      "Gradient Descent(305/399): loss=0.4484601849626794, w0=0.00017759665496035255, w1=-0.00893689914811524\n",
      "Gradient Descent(306/399): loss=0.44834768063228886, w0=0.00017853817037009336, w1=-0.008960643896311208\n",
      "Gradient Descent(307/399): loss=0.44823542131224425, w0=0.00017948063925079147, w1=-0.00898435975762206\n",
      "Gradient Descent(308/399): loss=0.44812340616186197, w0=0.00018042405196861444, w1=-0.009008046785911678\n",
      "Gradient Descent(309/399): loss=0.44801163434394253, w0=0.0001813683989300035, w1=-0.00903170503492132\n",
      "Gradient Descent(310/399): loss=0.4479001050247537, w0=0.0001823136705815249, w1=-0.009055334558269937\n",
      "Gradient Descent(311/399): loss=0.44778881737401555, w0=0.0001832598574097215, w1=-0.009078935409454493\n",
      "Gradient Descent(312/399): loss=0.44767777056488295, w0=0.00018420694994096507, w1=-0.009102507641850282\n",
      "Gradient Descent(313/399): loss=0.44756696377393135, w0=0.00018515493874130922, w1=-0.00912605130871125\n",
      "Gradient Descent(314/399): loss=0.4474563961811392, w0=0.0001861038144163428, w1=-0.009149566463170307\n",
      "Gradient Descent(315/399): loss=0.4473460669698726, w0=0.0001870535676110438, w1=-0.009173053158239644\n",
      "Gradient Descent(316/399): loss=0.44723597532687, w0=0.00018800418900963384, w1=-0.00919651144681105\n",
      "Gradient Descent(317/399): loss=0.44712612044222544, w0=0.00018895566933543342, w1=-0.009219941381656221\n",
      "Gradient Descent(318/399): loss=0.44701650150937516, w0=0.00018990799935071726, w1=-0.009243343015427079\n",
      "Gradient Descent(319/399): loss=0.44690711772507924, w0=0.00019086116985657066, w1=-0.009266716400656077\n",
      "Gradient Descent(320/399): loss=0.44679796828940777, w0=0.0001918151716927461, w1=-0.009290061589756518\n",
      "Gradient Descent(321/399): loss=0.4466890524057255, w0=0.0001927699957375205, w1=-0.00931337863502286\n",
      "Gradient Descent(322/399): loss=0.446580369280676, w0=0.00019372563290755295, w1=-0.00933666758863102\n",
      "Gradient Descent(323/399): loss=0.4464719181241671, w0=0.00019468207415774305, w1=-0.009359928502638698\n",
      "Gradient Descent(324/399): loss=0.44636369814935484, w0=0.00019563931048108976, w1=-0.009383161428985667\n",
      "Gradient Descent(325/399): loss=0.44625570857262886, w0=0.00019659733290855073, w1=-0.009406366419494093\n",
      "Gradient Descent(326/399): loss=0.44614794861359797, w0=0.00019755613250890222, w1=-0.009429543525868831\n",
      "Gradient Descent(327/399): loss=0.4460404174950746, w0=0.0001985157003885995, w1=-0.009452692799697736\n",
      "Gradient Descent(328/399): loss=0.44593311444305944, w0=0.00019947602769163788, w1=-0.009475814292451961\n",
      "Gradient Descent(329/399): loss=0.44582603868672827, w0=0.000200437105599414, w1=-0.009498908055486269\n",
      "Gradient Descent(330/399): loss=0.44571918945841554, w0=0.00020139892533058804, w1=-0.009521974140039323\n",
      "Gradient Descent(331/399): loss=0.4456125659936011, w0=0.00020236147814094608, w1=-0.009545012597233999\n",
      "Gradient Descent(332/399): loss=0.44550616753089445, w0=0.00020332475532326314, w1=-0.009568023478077678\n",
      "Gradient Descent(333/399): loss=0.44539999331202196, w0=0.00020428874820716678, w1=-0.00959100683346255\n",
      "Gradient Descent(334/399): loss=0.44529404258181016, w0=0.00020525344815900106, w1=-0.009613962714165909\n",
      "Gradient Descent(335/399): loss=0.44518831458817354, w0=0.0002062188465816912, w1=-0.009636891170850453\n",
      "Gradient Descent(336/399): loss=0.44508280858209953, w0=0.00020718493491460858, w1=-0.009659792254064582\n",
      "Gradient Descent(337/399): loss=0.44497752381763334, w0=0.0002081517046334363, w1=-0.009682666014242692\n",
      "Gradient Descent(338/399): loss=0.4448724595518659, w0=0.00020911914725003526, w1=-0.009705512501705467\n",
      "Gradient Descent(339/399): loss=0.44476761504491746, w0=0.0002100872543123108, w1=-0.00972833176666018\n",
      "Gradient Descent(340/399): loss=0.44466298955992634, w0=0.00021105601740407966, w1=-0.00975112385920098\n",
      "Gradient Descent(341/399): loss=0.4445585823630319, w0=0.00021202542814493768, w1=-0.009773888829309192\n",
      "Gradient Descent(342/399): loss=0.4444543927233635, w0=0.00021299547819012774, w1=-0.009796626726853599\n",
      "Gradient Descent(343/399): loss=0.4443504199130248, w0=0.00021396615923040846, w1=-0.00981933760159074\n",
      "Gradient Descent(344/399): loss=0.4442466632070817, w0=0.00021493746299192307, w1=-0.009842021503165198\n",
      "Gradient Descent(345/399): loss=0.44414312188354704, w0=0.00021590938123606913, w1=-0.00986467848110989\n",
      "Gradient Descent(346/399): loss=0.44403979522336845, w0=0.00021688190575936843, w1=-0.009887308584846351\n",
      "Gradient Descent(347/399): loss=0.4439366825104136, w0=0.00021785502839333757, w1=-0.009909911863685027\n",
      "Gradient Descent(348/399): loss=0.44383378303145865, w0=0.00021882874100435893, w1=-0.00993248836682556\n",
      "Gradient Descent(349/399): loss=0.4437310960761721, w0=0.00021980303549355216, w1=-0.009955038143357069\n",
      "Gradient Descent(350/399): loss=0.4436286209371048, w0=0.00022077790379664613, w1=-0.009977561242258444\n",
      "Gradient Descent(351/399): loss=0.4435263569096737, w0=0.00022175333788385134, w1=-0.01000005771239862\n",
      "Gradient Descent(352/399): loss=0.44342430329215077, w0=0.00022272932975973285, w1=-0.010022527602536871\n",
      "Gradient Descent(353/399): loss=0.44332245938564924, w0=0.0002237058714630837, w1=-0.01004497096132308\n",
      "Gradient Descent(354/399): loss=0.44322082449410993, w0=0.00022468295506679868, w1=-0.010067387837298035\n",
      "Gradient Descent(355/399): loss=0.44311939792428945, w0=0.0002256605726777487, w1=-0.010089778278893697\n",
      "Gradient Descent(356/399): loss=0.4430181789857467, w0=0.00022663871643665561, w1=-0.010112142334433487\n",
      "Gradient Descent(357/399): loss=0.44291716699083006, w0=0.00022761737851796743, w1=-0.010134480052132563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(358/399): loss=0.4428163612546643, w0=0.00022859655112973404, w1=-0.0101567914800981\n",
      "Gradient Descent(359/399): loss=0.44271576109513877, w0=0.00022957622651348342, w1=-0.010179076666329567\n",
      "Gradient Descent(360/399): loss=0.44261536583289374, w0=0.00023055639694409826, w1=-0.010201335658719\n",
      "Gradient Descent(361/399): loss=0.44251517479130864, w0=0.00023153705472969303, w1=-0.010223568505051286\n",
      "Gradient Descent(362/399): loss=0.44241518729648965, w0=0.00023251819221149165, w1=-0.01024577525300443\n",
      "Gradient Descent(363/399): loss=0.44231540267725566, w0=0.0002334998017637054, w1=-0.010267955950149836\n",
      "Gradient Descent(364/399): loss=0.44221582026512785, w0=0.0002344818757934114, w1=-0.010290110643952575\n",
      "Gradient Descent(365/399): loss=0.4421164393943168, w0=0.0002354644067404316, w1=-0.010312239381771662\n",
      "Gradient Descent(366/399): loss=0.44201725940170933, w0=0.00023644738707721206, w1=-0.010334342210860326\n",
      "Gradient Descent(367/399): loss=0.4419182796268573, w0=0.0002374308093087028, w1=-0.01035641917836628\n",
      "Gradient Descent(368/399): loss=0.4418194994119652, w0=0.00023841466597223807, w1=-0.010378470331331999\n",
      "Gradient Descent(369/399): loss=0.4417209181018778, w0=0.00023939894963741706, w1=-0.010400495716694979\n",
      "Gradient Descent(370/399): loss=0.44162253504406823, w0=0.00024038365290598502, w1=-0.01042249538128801\n",
      "Gradient Descent(371/399): loss=0.4415243495886265, w0=0.00024136876841171487, w1=-0.010444469371839451\n",
      "Gradient Descent(372/399): loss=0.44142636108824684, w0=0.0002423542888202892, w1=-0.010466417734973485\n",
      "Gradient Descent(373/399): loss=0.44132856889821626, w0=0.0002433402068291828, w1=-0.010488340517210397\n",
      "Gradient Descent(374/399): loss=0.44123097237640274, w0=0.00024432651516754546, w1=-0.01051023776496683\n",
      "Gradient Descent(375/399): loss=0.4411335708832437, w0=0.0002453132065960854, w1=-0.01053210952455606\n",
      "Gradient Descent(376/399): loss=0.4410363637817337, w0=0.0002463002739069529, w1=-0.010553955842188252\n",
      "Gradient Descent(377/399): loss=0.44093935043741345, w0=0.0002472877099236247, w1=-0.010575776763970729\n",
      "Gradient Descent(378/399): loss=0.44084253021835806, w0=0.0002482755075007884, w1=-0.010597572335908231\n",
      "Gradient Descent(379/399): loss=0.44074590249516543, w0=0.00024926365952422766, w1=-0.010619342603903181\n",
      "Gradient Descent(380/399): loss=0.4406494666409453, w0=0.0002502521589107076, w1=-0.01064108761375594\n",
      "Gradient Descent(381/399): loss=0.4405532220313063, w0=0.00025124099860786064, w1=-0.010662807411165078\n",
      "Gradient Descent(382/399): loss=0.4404571680443471, w0=0.0002522301715940731, w1=-0.010684502041727622\n",
      "Gradient Descent(383/399): loss=0.44036130406064344, w0=0.0002532196708783716, w1=-0.010706171550939322\n",
      "Gradient Descent(384/399): loss=0.44026562946323655, w0=0.00025420948950031027, w1=-0.01072781598419491\n",
      "Gradient Descent(385/399): loss=0.44017014363762375, w0=0.0002551996205298586, w1=-0.010749435386788351\n",
      "Gradient Descent(386/399): loss=0.44007484597174584, w0=0.0002561900570672891, w1=-0.01077102980391311\n",
      "Gradient Descent(387/399): loss=0.4399797358559765, w0=0.0002571807922430658, w1=-0.010792599280662398\n",
      "Gradient Descent(388/399): loss=0.4398848126831111, w0=0.00025817181921773317, w1=-0.010814143862029434\n",
      "Gradient Descent(389/399): loss=0.4397900758483566, w0=0.0002591631311818052, w1=-0.010835663592907697\n",
      "Gradient Descent(390/399): loss=0.4396955247493196, w0=0.00026015472135565513, w1=-0.010857158518091182\n",
      "Gradient Descent(391/399): loss=0.439601158785996, w0=0.00026114658298940547, w1=-0.010878628682274646\n",
      "Gradient Descent(392/399): loss=0.43950697736075994, w0=0.00026213870936281835, w1=-0.010900074130053873\n",
      "Gradient Descent(393/399): loss=0.439412979878354, w0=0.0002631310937851865, w1=-0.010921494905925916\n",
      "Gradient Descent(394/399): loss=0.4393191657458767, w0=0.0002641237295952243, w1=-0.01094289105428935\n",
      "Gradient Descent(395/399): loss=0.4392255343727741, w0=0.00026511661016095976, w1=-0.010964262619444524\n",
      "Gradient Descent(396/399): loss=0.43913208517082736, w0=0.0002661097288796262, w1=-0.01098560964559381\n",
      "Gradient Descent(397/399): loss=0.4390388175541432, w0=0.0002671030791775549, w1=-0.011006932176841852\n",
      "Gradient Descent(398/399): loss=0.438945730939143, w0=0.0002680966545100679, w1=-0.011028230257195815\n",
      "Gradient Descent(399/399): loss=0.4388528247445532, w0=0.0002690904483613712, w1=-0.011049503930565628\n",
      "++++ gamma = 0.000268269579528\n",
      "ciaociaociao\n",
      "Gradient Descent(0/399): loss=0.4999999999999887, w0=-1.8476349143751833e-07, w1=-9.617796961822002e-05\n",
      "Gradient Descent(1/399): loss=0.49931935231145874, w0=-3.2202203497146546e-07, w1=-0.00019197072831855262\n",
      "Gradient Descent(2/399): loss=0.4986445021025432, w0=-4.1239493676116807e-07, w1=-0.0002873804937602492\n",
      "Gradient Descent(3/399): loss=0.49797538269476854, w0=-4.5649493125748015e-07, w1=-0.0003824094682075184\n",
      "Gradient Descent(4/399): loss=0.4973119282944376, w0=-4.5492825167781373e-07, w1=-0.0004770598386536913\n",
      "Gradient Descent(5/399): loss=0.49665407397953, w0=-4.082946996530082e-07, w1=-0.000571333776944223\n",
      "Gradient Descent(6/399): loss=0.4960017556868068, w0=-3.1718771405702487e-07, w1=-0.0006652334398985408\n",
      "Gradient Descent(7/399): loss=0.49535491019913314, w0=-1.8219443903020625e-07, w1=-0.0007587609694307529\n",
      "Gradient Descent(8/399): loss=0.494713475133006, w0=-3.895791206691425e-09, w1=-0.0008519184926692299\n",
      "Gradient Descent(9/399): loss=0.4940773889262911, w0=2.171334738434879e-07, w1=-0.0009447081220750704\n",
      "Gradient Descent(10/399): loss=0.49344659082616005, w0=4.803246959474201e-07, w1=-0.001037131955559465\n",
      "Gradient Descent(11/399): loss=0.49282102087722646, w0=7.851152454271846e-07, w1=-0.0011291920765999677\n",
      "Gradient Descent(12/399): loss=0.49220061990987674, w0=1.1309484589526714e-06, w1=-0.001220890554355691\n",
      "Gradient Descent(13/399): loss=0.4915853295287896, w0=1.5172735761394205e-06, w1=-0.0013122294437814307\n",
      "Gradient Descent(14/399): loss=0.4909750921016458, w0=1.9435456768817696e-06, w1=-0.0014032107857407375\n",
      "Gradient Descent(15/399): loss=0.49036985074801887, w0=2.409225619411794e-06, w1=-0.0014938366071179437\n",
      "Gradient Descent(16/399): loss=0.4897695493284484, w0=2.91377997907449e-06, w1=-0.0015841089209291553\n",
      "Gradient Descent(17/399): loss=0.4891741324336885, w0=3.456680987809993e-06, w1=-0.0016740297264322243\n",
      "Gradient Descent(18/399): loss=0.48858354537413196, w0=4.037406474333653e-06, w1=-0.00176360100923571\n",
      "Gradient Descent(19/399): loss=0.4879977341694025, w0=4.655439805004868e-06, w1=-0.0018528247414068394\n",
      "Gradient Descent(20/399): loss=0.4874166455381164, w0=5.310269825375815e-06, w1=-0.0019417028815784795\n",
      "Gradient Descent(21/399): loss=0.4868402268878087, w0=6.0013908024113236e-06, w1=-0.0020302373750551296\n",
      "Gradient Descent(22/399): loss=0.4862684263050183, w0=6.7283023673711625e-06, w1=-0.002118430153917947\n",
      "Gradient Descent(23/399): loss=0.485701192545535, w0=7.49050945934631e-06, w1=-0.002206283137128814\n",
      "Gradient Descent(24/399): loss=0.48513847502479907, w0=8.287522269440705e-06, w1=-0.0022937982306334573\n",
      "Gradient Descent(25/399): loss=0.48458022380845467, w0=9.118856185590253e-06, w1=-0.0023809773274636316\n",
      "Gradient Descent(26/399): loss=0.4840263896030549, w0=9.984031738010921e-06, w1=-0.002467822307838372\n",
      "Gradient Descent(27/399): loss=0.4834769237469121, w0=1.0882574545267814e-05, w1=-0.0025543350392643336\n",
      "Gradient Descent(28/399): loss=0.48293177820109146, w0=1.181401526095739e-05, w1=-0.002640517376635217\n",
      "Gradient Descent(29/399): loss=0.4823909055405523, w0=1.2777889520994846e-05, w1=-0.002726371162330304\n",
      "Gradient Descent(30/399): loss=0.4818542589454232, w0=1.3773737891499193e-05, w1=-0.0028118982263120955\n",
      "Gradient Descent(31/399): loss=0.48132179219241605, w0=1.4801105817268116e-05, w1=-0.0028971003862230736\n",
      "Gradient Descent(32/399): loss=0.4807934596463769, w0=1.5859543570835406e-05, w1=-0.0029819794474815965\n",
      "Gradient Descent(33/399): loss=0.48026921625196506, w0=1.6948606202103425e-05, w1=-0.003066537203376928\n",
      "Gradient Descent(34/399): loss=0.47974901752546745, w0=1.806785348854328e-05, w1=-0.003150775435163415\n",
      "Gradient Descent(35/399): loss=0.47923281954673413, w0=1.921684988595572e-05, w1=-0.0032346959121538252\n",
      "Gradient Descent(36/399): loss=0.4787205789512453, w0=2.0395164479785437e-05, w1=-0.003318300391811847\n",
      "Gradient Descent(37/399): loss=0.47821225292229774, w0=2.1602370936981995e-05, w1=-0.0034015906198437656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(38/399): loss=0.47770779918331474, w0=2.2838047458400356e-05, w1=-0.0034845683302893215\n",
      "Gradient Descent(39/399): loss=0.4772071759902724, w0=2.41017767317343e-05, w1=-0.00356723524561176\n",
      "Gradient Descent(40/399): loss=0.4767103421242453, w0=2.5393145884976048e-05, w1=-0.0036495930767870815\n",
      "Gradient Descent(41/399): loss=0.476217256884066, w0=2.671174644039545e-05, w1=-0.0037316435233924984\n",
      "Gradient Descent(42/399): loss=0.4757278800790977, w0=2.8057174269032262e-05, w1=-0.0038133882736941112\n",
      "Gradient Descent(43/399): loss=0.4752421720221178, w0=2.942902954569512e-05, w1=-0.0038948290047338043\n",
      "Gradient Descent(44/399): loss=0.4747600935223096, w0=3.082691670446084e-05, w1=-0.003975967382415378\n",
      "Gradient Descent(45/399): loss=0.47428160587836365, w0=3.225044439466787e-05, w1=-0.004056805061589922\n",
      "Gradient Descent(46/399): loss=0.4738066708716817, w0=3.3699225437397676e-05, w1=-0.004137343686140428\n",
      "Gradient Descent(47/399): loss=0.47333525075968663, w0=3.5172876782438056e-05, w1=-0.004217584889065673\n",
      "Gradient Descent(48/399): loss=0.4728673082692319, w0=3.66710194657224e-05, w1=-0.004297530292563351\n",
      "Gradient Descent(49/399): loss=0.47240280659011324, w0=3.819327856723897e-05, w1=-0.004377181508112486\n",
      "Gradient Descent(50/399): loss=0.471941709368677, w0=3.973928316940446e-05, w1=-0.004456540136555117\n",
      "Gradient Descent(51/399): loss=0.47148398070152714, w0=4.130866631589596e-05, w1=-0.004535607768177274\n",
      "Gradient Descent(52/399): loss=0.47102958512932463, w0=4.2901064970935846e-05, w1=-0.004614385982789243\n",
      "Gradient Descent(53/399): loss=0.47057848763068355, w0=4.451611997902391e-05, w1=-0.004692876349805132\n",
      "Gradient Descent(54/399): loss=0.4701306536161568, w0=4.615347602511126e-05, w1=-0.004771080428321745\n",
      "Gradient Descent(55/399): loss=0.46968604892231297, w0=4.7812781595210606e-05, w1=-0.00484899976719677\n",
      "Gradient Descent(56/399): loss=0.4692446398059023, w0=4.949368893743749e-05, w1=-0.004926635905126289\n",
      "Gradient Descent(57/399): loss=0.46880639293811005, w0=5.119585402347731e-05, w1=-0.005003990370721615\n",
      "Gradient Descent(58/399): loss=0.46837127539889556, w0=5.291893651047283e-05, w1=-0.00508106468258546\n",
      "Gradient Descent(59/399): loss=0.46793925467141606, w0=5.466259970332716e-05, w1=-0.005157860349387457\n",
      "Gradient Descent(60/399): loss=0.46751029863653343, w0=5.6426510517416954e-05, w1=-0.005234378869939011\n",
      "Gradient Descent(61/399): loss=0.46708437556740334, w0=5.8210339441711135e-05, w1=-0.005310621733267524\n",
      "Gradient Descent(62/399): loss=0.46666145412414417, w0=6.001376050228988e-05, w1=-0.005386590418689966\n",
      "Gradient Descent(63/399): loss=0.46624150334858555, w0=6.183645122625934e-05, w1=-0.005462286395885824\n",
      "Gradient Descent(64/399): loss=0.4658244926590938, w0=6.367809260605699e-05, w1=-0.005537711124969418\n",
      "Gradient Descent(65/399): loss=0.4654103918454751, w0=6.553836906414316e-05, w1=-0.0056128660565616\n",
      "Gradient Descent(66/399): loss=0.46499917106395466, w0=6.741696841807397e-05, w1=-0.005687752631860838\n",
      "Gradient Descent(67/399): loss=0.46459080083222637, w0=6.931358184595101e-05, w1=-0.005762372282713693\n",
      "Gradient Descent(68/399): loss=0.4641852520245794, w0=7.122790385224334e-05, w1=-0.005836726431684693\n",
      "Gradient Descent(69/399): loss=0.46378249586709447, w0=7.315963223397727e-05, w1=-0.005910816492125616\n",
      "Gradient Descent(70/399): loss=0.4633825039329116, w0=7.51084680472896e-05, w1=-0.005984643868244172\n",
      "Gradient Descent(71/399): loss=0.46298524813756525, w0=7.707411557433981e-05, w1=-0.006058209955172118\n",
      "Gradient Descent(72/399): loss=0.4625907007343911, w0=7.905628229057705e-05, w1=-0.006131516139032774\n",
      "Gradient Descent(73/399): loss=0.46219883430999686, w0=8.105467883235762e-05, w1=-0.006204563797007985\n",
      "Gradient Descent(74/399): loss=0.46180962177980023, w0=8.306901896490885e-05, w1=-0.006277354297404505\n",
      "Gradient Descent(75/399): loss=0.4614230363836336, w0=8.509901955063506e-05, w1=-0.006349888999719822\n",
      "Gradient Descent(76/399): loss=0.46103905168140985, w0=8.714440051776177e-05, w1=-0.006422169254707429\n",
      "Gradient Descent(77/399): loss=0.4606576415488532, w0=8.920488482931402e-05, w1=-0.00649419640444154\n",
      "Gradient Descent(78/399): loss=0.4602787801732918, w0=9.128019845242484e-05, w1=-0.006565971782381267\n",
      "Gradient Descent(79/399): loss=0.4599024420495104, w0=9.337007032796996e-05, w1=-0.0066374967134342515\n",
      "Gradient Descent(80/399): loss=0.4595286019756633, w0=9.547423234052502e-05, w1=-0.006708772514019764\n",
      "Gradient Descent(81/399): loss=0.4591572350492482, w0=9.759241928864127e-05, w1=-0.006779800492131275\n",
      "Gradient Descent(82/399): loss=0.4587883166631353, w0=9.972436885543618e-05, w1=-0.006850581947398494\n",
      "Gradient Descent(83/399): loss=0.45842182250165536, w0=0.00010186982157949527, w1=-0.0069211181711489045\n",
      "Gradient Descent(84/399): loss=0.4580577285367444, w0=0.00010402852082608128, w1=-0.006991410446468767\n",
      "Gradient Descent(85/399): loss=0.45769601102414315, w0=0.00010620021275864736, w1=-0.007061460048263627\n",
      "Gradient Descent(86/399): loss=0.4573366464996505, w0=0.00010838464631065056, w1=-0.007131268243318307\n",
      "Gradient Descent(87/399): loss=0.45697961177543195, w0=0.00011058157315766206, w1=-0.00720083629035641\n",
      "Gradient Descent(88/399): loss=0.45662488393638045, w0=0.00011279074768977095, w1=-0.007270165440099319\n",
      "Gradient Descent(89/399): loss=0.4562724403365295, w0=0.00011501192698427772, w1=-0.007339256935324711\n",
      "Gradient Descent(90/399): loss=0.45592225859551627, w0=0.00011724487077867451, w1=-0.007408112010924584\n",
      "Gradient Descent(91/399): loss=0.4555743165950976, w0=0.00011948934144390845, w1=-0.007476731893962805\n",
      "Gradient Descent(92/399): loss=0.4552285924757126, w0=0.00012174510395792503, w1=-0.007545117803732181\n",
      "Gradient Descent(93/399): loss=0.4548850646330975, w0=0.00012401192587948817, w1=-0.007613270951811056\n",
      "Gradient Descent(94/399): loss=0.45454371171494606, w0=0.00012628957732227378, w1=-0.007681192542119444\n",
      "Gradient Descent(95/399): loss=0.4542045126176182, w0=0.00012857783092923362, w1=-0.007748883770974696\n",
      "Gradient Descent(96/399): loss=0.45386744648289606, w0=0.00013087646184722654, w1=-0.007816345827146716\n",
      "Gradient Descent(97/399): loss=0.4535324926947851, w0=0.00013318524770191366, w1=-0.007883579891912712\n",
      "Gradient Descent(98/399): loss=0.4531996308763603, w0=0.0001355039685729148, w1=-0.007950587139111506\n",
      "Gradient Descent(99/399): loss=0.45286884088665846, w0=0.00013783240696922306, w1=-0.008017368735197391\n",
      "Gradient Descent(100/399): loss=0.4525401028176118, w0=0.00014017034780487443, w1=-0.008083925839293559\n",
      "Gradient Descent(101/399): loss=0.4522133969910285, w0=0.00014251757837486974, w1=-0.008150259603245075\n",
      "Gradient Descent(102/399): loss=0.4518887039556109, w0=0.0001448738883313459, w1=-0.008216371171671434\n",
      "Gradient Descent(103/399): loss=0.4515660044840201, w0=0.0001472390696599937, w1=-0.008282261682018675\n",
      "Gradient Descent(104/399): loss=0.4512452795699803, w0=0.00014961291665671917, w1=-0.008347932264611082\n",
      "Gradient Descent(105/399): loss=0.4509265104254214, w0=0.00015199522590454622, w1=-0.008413384042702456\n",
      "Gradient Descent(106/399): loss=0.4506096784776664, w0=0.00015438579625075695, w1=-0.008478618132526974\n",
      "Gradient Descent(107/399): loss=0.4502947653666536, w0=0.00015678442878426792, w1=-0.008543635643349633\n",
      "Gradient Descent(108/399): loss=0.4499817529422002, w0=0.0001591909268132389, w1=-0.008608437677516283\n",
      "Gradient Descent(109/399): loss=0.4496706232613036, w0=0.00016160509584291205, w1=-0.008673025330503258\n",
      "Gradient Descent(110/399): loss=0.44936135858548115, w0=0.00016402674355367849, w1=-0.008737399690966602\n",
      "Gradient Descent(111/399): loss=0.4490539413781435, w0=0.00016645567977936995, w1=-0.0088015618407909\n",
      "Gradient Descent(112/399): loss=0.4487483543020107, w0=0.00016889171648577306, w1=-0.008865512855137713\n",
      "Gradient Descent(113/399): loss=0.44844458021655786, w0=0.00017133466774936333, w1=-0.00892925380249362\n",
      "Gradient Descent(114/399): loss=0.4481426021755005, w0=0.00017378434973625684, w1=-0.008992785744717877\n",
      "Gradient Descent(115/399): loss=0.4478424034243129, w0=0.00017624058068137699, w1=-0.009056109737089688\n",
      "Gradient Descent(116/399): loss=0.44754396739778196, w0=0.00017870318086783378, w1=-0.0091192268283551\n",
      "Gradient Descent(117/399): loss=0.44724727771759404, w0=0.00018117197260651357, w1=-0.009182138060773518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(118/399): loss=0.4469523181899553, w0=0.0001836467802158766, w1=-0.00924484447016385\n",
      "Gradient Descent(119/399): loss=0.44665907280324557, w0=0.0001861274300019602, w1=-0.009307347085950277\n",
      "Gradient Descent(120/399): loss=0.44636752572570393, w0=0.00018861375023858533, w1=-0.009369646931207665\n",
      "Gradient Descent(121/399): loss=0.4460776613031472, w0=0.0001911055711477643, w1=-0.009431745022706604\n",
      "Gradient Descent(122/399): loss=0.4457894640567177, w0=0.00019360272488030695, w1=-0.009493642370958096\n",
      "Gradient Descent(123/399): loss=0.44550291868066566, w0=0.00019610504549662404, w1=-0.009555339980257881\n",
      "Gradient Descent(124/399): loss=0.44521801004015726, w0=0.0001986123689477245, w1=-0.009616838848730411\n",
      "Gradient Descent(125/399): loss=0.4449347231691174, w0=0.00020112453305640545, w1=-0.009678139968372478\n",
      "Gradient Descent(126/399): loss=0.44465304326810007, w0=0.00020364137749863208, w1=-0.009739244325096484\n",
      "Gradient Descent(127/399): loss=0.44437295570218666, w0=0.00020616274378510588, w1=-0.009800152898773382\n",
      "Gradient Descent(128/399): loss=0.44409444599891557, w0=0.00020868847524301866, w1=-0.009860866663275267\n",
      "Gradient Descent(129/399): loss=0.44381749984623886, w0=0.0002112184169979907, w1=-0.009921386586517632\n",
      "Gradient Descent(130/399): loss=0.4435421030905059, w0=0.00021375241595619077, w1=-0.009981713630501286\n",
      "Gradient Descent(131/399): loss=0.44326824173447676, w0=0.00021629032078663606, w1=-0.010041848751353947\n",
      "Gradient Descent(132/399): loss=0.44299590193536104, w0=0.0002188319819036703, w1=-0.010101792899371501\n",
      "Gradient Descent(133/399): loss=0.44272507000288336, w0=0.00022137725144961772, w1=-0.010161547019058938\n",
      "Gradient Descent(134/399): loss=0.4424557323973767, w0=0.0002239259832776112, w1=-0.010221112049170967\n",
      "Gradient Descent(135/399): loss=0.44218787572789964, w0=0.00022647803293459262, w1=-0.010280488922752302\n",
      "Gradient Descent(136/399): loss=0.4419214867503813, w0=0.00022903325764448364, w1=-0.010339678567177647\n",
      "Gradient Descent(137/399): loss=0.44165655236578943, w0=0.0002315915162915249, w1=-0.010398681904191348\n",
      "Gradient Descent(138/399): loss=0.44139305961832487, w0=0.0002341526694037819, w1=-0.010457499849946754\n",
      "Gradient Descent(139/399): loss=0.4411309956936395, w0=0.0002367165791368157, w1=-0.010516133315045247\n",
      "Gradient Descent(140/399): loss=0.44087034791707935, w0=0.00023928310925751687, w1=-0.01057458320457499\n",
      "Gradient Descent(141/399): loss=0.44061110375194973, w0=0.00024185212512810056, w1=-0.010632850418149348\n",
      "Gradient Descent(142/399): loss=0.4403532507978066, w0=0.00024442349369026116, w1=-0.01069093584994503\n",
      "Gradient Descent(143/399): loss=0.44009677678876724, w0=0.0002469970834494849, w1=-0.01074884038873992\n",
      "Gradient Descent(144/399): loss=0.43984166959184734, w0=0.0002495727644595186, w1=-0.010806564917950612\n",
      "Gradient Descent(145/399): loss=0.43958791720531776, w0=0.00025215040830699296, w1=-0.010864110315669665\n",
      "Gradient Descent(146/399): loss=0.43933550775708624, w0=0.00025472988809619823, w1=-0.010921477454702556\n",
      "Gradient Descent(147/399): loss=0.43908442950309684, w0=0.0002573110784340117, w1=-0.010978667202604342\n",
      "Gradient Descent(148/399): loss=0.4388346708257557, w0=0.0002598938554149744, w1=-0.011035680421716062\n",
      "Gradient Descent(149/399): loss=0.4385862202323729, w0=0.00026247809660651566, w1=-0.01109251796920082\n",
      "Gradient Descent(150/399): loss=0.4383390663536304, w0=0.00026506368103432426, w1=-0.01114918069707962\n",
      "Gradient Descent(151/399): loss=0.43809319794206547, w0=0.00026765048916786434, w1=-0.011205669452266904\n",
      "Gradient Descent(152/399): loss=0.4378486038705786, w0=0.00027023840290603445, w1=-0.011261985076605828\n",
      "Gradient Descent(153/399): loss=0.43760527313095865, w0=0.00027282730556296865, w1=-0.01131812840690325\n",
      "Gradient Descent(154/399): loss=0.43736319483242914, w0=0.0002754170818539775, w1=-0.011374100274964459\n",
      "Gradient Descent(155/399): loss=0.43712235820021283, w0=0.00027800761788162806, w1=-0.011429901507627642\n",
      "Gradient Descent(156/399): loss=0.43688275257411674, w0=0.000280598801121961, w1=-0.011485532926798069\n",
      "Gradient Descent(157/399): loss=0.43664436740713497, w0=0.0002831905204108436, w1=-0.011540995349482018\n",
      "Gradient Descent(158/399): loss=0.43640719226407043, w0=0.0002857826659304574, w1=-0.011596289587820458\n",
      "Gradient Descent(159/399): loss=0.43617121682017523, w0=0.00028837512919591834, w1=-0.011651416449122445\n",
      "Gradient Descent(160/399): loss=0.43593643085980854, w0=0.0002909678030420288, w1=-0.011706376735898283\n",
      "Gradient Descent(161/399): loss=0.43570282427511325, w0=0.00029356058161015983, w1=-0.011761171245892417\n",
      "Gradient Descent(162/399): loss=0.43547038706470853, w0=0.00029615336033526207, w1=-0.011815800772116088\n",
      "Gradient Descent(163/399): loss=0.4352391093324013, w0=0.00029874603593300426, w1=-0.011870266102879721\n",
      "Gradient Descent(164/399): loss=0.43500898128591425, w0=0.00030133850638703767, w1=-0.011924568021825082\n",
      "Gradient Descent(165/399): loss=0.43477999323563077, w0=0.0003039306709363857, w1=-0.01197870730795717\n",
      "Gradient Descent(166/399): loss=0.43455213559335565, w0=0.0003065224300629567, w1=-0.01203268473567589\n",
      "Gradient Descent(167/399): loss=0.434325398871093, w0=0.0003091136854791789, w1=-0.012086501074807462\n",
      "Gradient Descent(168/399): loss=0.4340997736798412, w0=0.00031170434011575684, w1=-0.012140157090635604\n",
      "Gradient Descent(169/399): loss=0.4338752507284006, w0=0.00031429429810954666, w1=-0.012193653543932467\n",
      "Gradient Descent(170/399): loss=0.43365182082220066, w0=0.0003168834647915505, w1=-0.01224699119098935\n",
      "Gradient Descent(171/399): loss=0.4334294748621401, w0=0.0003194717466750279, w1=-0.012300170783647165\n",
      "Gradient Descent(172/399): loss=0.43320820384344344, w0=0.00032205905144372284, w1=-0.012353193069326685\n",
      "Gradient Descent(173/399): loss=0.43298799885453176, w0=0.0003246452879402062, w1=-0.012406058791058546\n",
      "Gradient Descent(174/399): loss=0.43276885107590823, w0=0.0003272303661543313, w1=-0.012458768687513042\n",
      "Gradient Descent(175/399): loss=0.43255075177906016, w0=0.00032981419721180206, w1=-0.012511323493029674\n",
      "Gradient Descent(176/399): loss=0.43233369232537205, w0=0.00033239669336285215, w1=-0.012563723937646492\n",
      "Gradient Descent(177/399): loss=0.4321176641650566, w0=0.00033497776797103483, w1=-0.012615970747129206\n",
      "Gradient Descent(178/399): loss=0.43190265883609635, w0=0.00033755733550212114, w1=-0.01266806464300008\n",
      "Gradient Descent(179/399): loss=0.43168866796320166, w0=0.00034013531151310625, w1=-0.012720006342566604\n",
      "Gradient Descent(180/399): loss=0.4314756832567812, w0=0.00034271161264132254, w1=-0.012771796558949958\n",
      "Gradient Descent(181/399): loss=0.4312636965119253, w0=0.0003452861565936581, w1=-0.012823436001113254\n",
      "Gradient Descent(182/399): loss=0.4310526996074041, w0=0.00034785886213588014, w1=-0.01287492537388957\n",
      "Gradient Descent(183/399): loss=0.4308426845046774, w0=0.0003504296490820616, w1=-0.012926265378009769\n",
      "Gradient Descent(184/399): loss=0.43063364324691755, w0=0.0003529984382841103, w1=-0.012977456710130114\n",
      "Gradient Descent(185/399): loss=0.4304255679580461, w0=0.0003555651516213994, w1=-0.013028500062859676\n",
      "Gradient Descent(186/399): loss=0.430218450841782, w0=0.00035812971199049835, w1=-0.013079396124787524\n",
      "Gradient Descent(187/399): loss=0.430012284180702, w0=0.00036069204329500286, w1=-0.013130145580509733\n",
      "Gradient Descent(188/399): loss=0.42980706033531346, w0=0.0003632520704354636, w1=-0.013180749110656169\n",
      "Gradient Descent(189/399): loss=0.42960277174314043, w0=0.00036580971929941184, w1=-0.013231207391917084\n",
      "Gradient Descent(190/399): loss=0.4293994109178192, w0=0.0003683649167514815, w1=-0.013281521097069517\n",
      "Gradient Descent(191/399): loss=0.4291969704482072, w0=0.00037091759062362673, w1=-0.013331690895003485\n",
      "Gradient Descent(192/399): loss=0.42899544299750336, w0=0.0003734676697054335, w1=-0.013381717450747985\n",
      "Gradient Descent(193/399): loss=0.4287948213023784, w0=0.000376015083734525, w1=-0.01343160142549681\n",
      "Gradient Descent(194/399): loss=0.42859509817211905, w0=0.0003785597633870592, w1=-0.01348134347663415\n",
      "Gradient Descent(195/399): loss=0.4283962664877803, w0=0.000381101640268318, w1=-0.013530944257760039\n",
      "Gradient Descent(196/399): loss=0.4281983192013501, w0=0.0003836406469033871, w1=-0.01358040441871557\n",
      "Gradient Descent(197/399): loss=0.428001249334925, w0=0.0003861767167279257, w1=-0.013629724605607957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(198/399): loss=0.42780504997989577, w0=0.0003887097840790244, w1=-0.013678905460835396\n",
      "Gradient Descent(199/399): loss=0.4276097142961439, w0=0.00039123978418615166, w1=-0.013727947623111738\n",
      "Gradient Descent(200/399): loss=0.4274152355112476, w0=0.00039376665316218686, w1=-0.013776851727490992\n",
      "Gradient Descent(201/399): loss=0.4272216069196992, w0=0.0003962903279945397, w1=-0.013825618405391637\n",
      "Gradient Descent(202/399): loss=0.42702882188213226, w0=0.00039881074653635485, w1=-0.01387424828462075\n",
      "Gradient Descent(203/399): loss=0.4268368738245568, w0=0.0004013278474978008, w1=-0.01392274198939797\n",
      "Gradient Descent(204/399): loss=0.4266457562376076, w0=0.0004038415704374426, w1=-0.013971100140379278\n",
      "Gradient Descent(205/399): loss=0.4264554626757987, w0=0.00040635185575369707, w1=-0.01401932335468059\n",
      "Gradient Descent(206/399): loss=0.42626598675679006, w0=0.0004088586446763701, w1=-0.014067412245901198\n",
      "Gradient Descent(207/399): loss=0.4260773221606624, w0=0.0004113618792582748, w1=-0.014115367424147018\n",
      "Gradient Descent(208/399): loss=0.42588946262920013, w0=0.00041386150236693057, w1=-0.014163189496053684\n",
      "Gradient Descent(209/399): loss=0.42570240196518666, w0=0.00041635745767634047, w1=-0.014210879064809454\n",
      "Gradient Descent(210/399): loss=0.4255161340317045, w0=0.00041884968965884853, w1=-0.014258436730177964\n",
      "Gradient Descent(211/399): loss=0.4253306527514487, w0=0.00042133814357707427, w1=-0.014305863088520809\n",
      "Gradient Descent(212/399): loss=0.4251459521060449, w0=0.0004238227654759243, w1=-0.014353158732819952\n",
      "Gradient Descent(213/399): loss=0.42496202613537803, w0=0.00042630350217468085, w1=-0.014400324252699975\n",
      "Gradient Descent(214/399): loss=0.42477886893693, w0=0.0004287803012591652, w1=-0.01444736023445017\n",
      "Gradient Descent(215/399): loss=0.42459647466512557, w0=0.00043125311107397655, w1=-0.014494267261046462\n",
      "Gradient Descent(216/399): loss=0.4244148375306844, w0=0.0004337218807148046, w1=-0.01454104591217317\n",
      "Gradient Descent(217/399): loss=0.42423395179998424, w0=0.0004361865600208159, w1=-0.014587696764244618\n",
      "Gradient Descent(218/399): loss=0.42405381179443025, w0=0.0004386470995671124, w1=-0.014634220390426582\n",
      "Gradient Descent(219/399): loss=0.42387441188983327, w0=0.00044110345065726247, w1=-0.014680617360657577\n",
      "Gradient Descent(220/399): loss=0.4236957465157948, w0=0.00044355556531590264, w1=-0.014726888241669999\n",
      "Gradient Descent(221/399): loss=0.42351781015510137, w0=0.00044600339628141024, w1=-0.014773033597011102\n",
      "Gradient Descent(222/399): loss=0.423340597343125, w0=0.00044844689699864574, w1=-0.014819053987063827\n",
      "Gradient Descent(223/399): loss=0.4231641026672315, w0=0.0004508860216117642, w1=-0.014864949969067485\n",
      "Gradient Descent(224/399): loss=0.4229883207661976, w0=0.00045332072495709533, w1=-0.014910722097138276\n",
      "Gradient Descent(225/399): loss=0.422813246329633, w0=0.00045575096255609125, w1=-0.014956370922289674\n",
      "Gradient Descent(226/399): loss=0.42263887409741197, w0=0.0004581766906083413, w1=-0.01500189699245265\n",
      "Gradient Descent(227/399): loss=0.4224651988591105, w0=0.0004605978659846537, w1=-0.015047300852495765\n",
      "Gradient Descent(228/399): loss=0.422292215453451, w0=0.0004630144462202024, w1=-0.015092583044245095\n",
      "Gradient Descent(229/399): loss=0.4221199187677541, w0=0.0004654263895077398, w1=-0.015137744106504036\n",
      "Gradient Descent(230/399): loss=0.4219483037373976, w0=0.0004678336546908735, w1=-0.015182784575072947\n",
      "Gradient Descent(231/399): loss=0.4217773653452809, w0=0.0004702362012574074, w1=-0.015227704982768656\n",
      "Gradient Descent(232/399): loss=0.42160709862129775, w0=0.0004726339893327456, w1=-0.015272505859443833\n",
      "Gradient Descent(233/399): loss=0.42143749864181423, w0=0.0004750269796733594, w1=-0.015317187732006212\n",
      "Gradient Descent(234/399): loss=0.42126856052915407, w0=0.00047741513366031624, w1=-0.01536175112443768\n",
      "Gradient Descent(235/399): loss=0.42110027945109013, w0=0.00047979841329286996, w1=-0.015406196557813228\n",
      "Gradient Descent(236/399): loss=0.42093265062034135, w0=0.0004821767811821121, w1=-0.015450524550319764\n",
      "Gradient Descent(237/399): loss=0.42076566929407794, w0=0.0004845502005446835, w1=-0.01549473561727479\n",
      "Gradient Descent(238/399): loss=0.4205993307734307, w0=0.00048691863519654524, w1=-0.015538830271144947\n",
      "Gradient Descent(239/399): loss=0.420433630403007, w0=0.0004892820495468093, w1=-0.015582809021564427\n",
      "Gradient Descent(240/399): loss=0.420268563570414, w0=0.0004916404085916273, w1=-0.015626672375353246\n",
      "Gradient Descent(241/399): loss=0.4201041257057852, w0=0.0004939936779081371, w1=-0.015670420836535397\n",
      "Gradient Descent(242/399): loss=0.41994031228131623, w0=0.0004963418236484674, w1=-0.015714054906356866\n",
      "Gradient Descent(243/399): loss=0.41977711881080215, w0=0.0004986848125337986, w1=-0.01575757508330351\n",
      "Gradient Descent(244/399): loss=0.4196145408491849, w0=0.0005010226118484801, w1=-0.01580098186311882\n",
      "Gradient Descent(245/399): loss=0.41945257399210334, w0=0.000503355189434204, w1=-0.01584427573882157\n",
      "Gradient Descent(246/399): loss=0.41929121387544904, w0=0.0005056825136842329, w1=-0.0158874572007233\n",
      "Gradient Descent(247/399): loss=0.4191304561749299, w0=0.0005080045535376831, w1=-0.015930526736445697\n",
      "Gradient Descent(248/399): loss=0.41897029660563556, w0=0.0005103212784738622, w1=-0.01597348483093787\n",
      "Gradient Descent(249/399): loss=0.41881073092161114, w0=0.00051263265850666, w1=-0.01601633196649347\n",
      "Gradient Descent(250/399): loss=0.4186517549154348, w0=0.0005149386641789918, w1=-0.016059068622767705\n",
      "Gradient Descent(251/399): loss=0.4184933644178003, w0=0.0005172392665572959, w1=-0.016101695276794222\n",
      "Gradient Descent(252/399): loss=0.4183355552971059, w0=0.0005195344372260824, w1=-0.016144212403001883\n",
      "Gradient Descent(253/399): loss=0.41817832345904693, w0=0.0005218241482825326, w1=-0.016186620473231418\n",
      "Gradient Descent(254/399): loss=0.4180216648462133, w0=0.0005241083723311517, w1=-0.016228919956751946\n",
      "Gradient Descent(255/399): loss=0.4178655754376946, w0=0.0005263870824784705, w1=-0.0162711113202774\n",
      "Gradient Descent(256/399): loss=0.4177100512486851, w0=0.0005286602523277972, w1=-0.016313195027982813\n",
      "Gradient Descent(257/399): loss=0.4175550883300985, w0=0.0005309278559740203, w1=-0.016355171541520507\n",
      "Gradient Descent(258/399): loss=0.41740068276818426, w0=0.0005331898679984591, w1=-0.01639704132003616\n",
      "Gradient Descent(259/399): loss=0.41724683068415014, w0=0.0005354462634637638, w1=-0.016438804820184748\n",
      "Gradient Descent(260/399): loss=0.41709352823378765, w0=0.0005376970179088636, w1=-0.016480462496146403\n",
      "Gradient Descent(261/399): loss=0.4169407716071047, w0=0.0005399421073439629, w1=-0.016522014799642118\n",
      "Gradient Descent(262/399): loss=0.4167885570279599, w0=0.0005421815082455842, w1=-0.01656346217994938\n",
      "Gradient Descent(263/399): loss=0.4166368807537031, w0=0.0005444151975516584, w1=-0.016604805083917668\n",
      "Gradient Descent(264/399): loss=0.41648573907481923, w0=0.000546643152656662, w1=-0.01664604395598385\n",
      "Gradient Descent(265/399): loss=0.41633512831457753, w0=0.0005488653514067988, w1=-0.01668717923818747\n",
      "Gradient Descent(266/399): loss=0.41618504482868435, w0=0.0005510817720952287, w1=-0.016728211370185927\n",
      "Gradient Descent(267/399): loss=0.4160354850049389, w0=0.0005532923934573411, w1=-0.01676914078926955\n",
      "Gradient Descent(268/399): loss=0.4158864452628962, w0=0.0005554971946660727, w1=-0.016809967930376563\n",
      "Gradient Descent(269/399): loss=0.4157379220535312, w0=0.00055769615532727, w1=-0.016850693226107938\n",
      "Gradient Descent(270/399): loss=0.41558991185890826, w0=0.0005598892554750956, w1=-0.01689131710674216\n",
      "Gradient Descent(271/399): loss=0.41544241119185443, w0=0.0005620764755674773, w1=-0.016931840000249877\n",
      "Gradient Descent(272/399): loss=0.4152954165956369, w0=0.0005642577964816018, w1=-0.016972262332308443\n",
      "Gradient Descent(273/399): loss=0.41514892464364317, w0=0.000566433199509449, w1=-0.017012584526316373\n",
      "Gradient Descent(274/399): loss=0.41500293193906646, w0=0.0005686026663533704, w1=-0.017052807003407684\n",
      "Gradient Descent(275/399): loss=0.4148574351145943, w0=0.0005707661791217077, w1=-0.017092930182466135\n",
      "Gradient Descent(276/399): loss=0.4147124308321004, w0=0.0005729237203244545, w1=-0.017132954480139385\n",
      "Gradient Descent(277/399): loss=0.41456791578234153, w0=0.0005750752728689574, w1=-0.01717288031085303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(278/399): loss=0.41442388668465624, w0=0.0005772208200556589, w1=-0.01721270808682454\n",
      "Gradient Descent(279/399): loss=0.41428034028666777, w0=0.0005793603455738802, w1=-0.01725243821807713\n",
      "Gradient Descent(280/399): loss=0.41413727336399164, w0=0.0005814938334976436, w1=-0.017292071112453497\n",
      "Gradient Descent(281/399): loss=0.4139946827199458, w0=0.0005836212682815352, w1=-0.017331607175629482\n",
      "Gradient Descent(282/399): loss=0.41385256518526325, w0=0.0005857426347566058, w1=-0.017371046811127628\n",
      "Gradient Descent(283/399): loss=0.41371091761781015, w0=0.0005878579181263113, w1=-0.017410390420330654\n",
      "Gradient Descent(284/399): loss=0.41356973690230536, w0=0.0005899671039624912, w1=-0.01744963840249482\n",
      "Gradient Descent(285/399): loss=0.41342901995004483, w0=0.0005920701782013852, w1=-0.01748879115476321\n",
      "Gradient Descent(286/399): loss=0.4132887636986275, w0=0.0005941671271396873, w1=-0.017527849072178914\n",
      "Gradient Descent(287/399): loss=0.4131489651116871, w0=0.0005962579374306378, w1=-0.017566812547698134\n",
      "Gradient Descent(288/399): loss=0.4130096211786232, w0=0.0005983425960801517, w1=-0.017605681972203174\n",
      "Gradient Descent(289/399): loss=0.4128707289143401, w0=0.000600421090442984, w1=-0.017644457734515357\n",
      "Gradient Descent(290/399): loss=0.41273228535898465, w0=0.0006024934082189315, w1=-0.01768314022140786\n",
      "Gradient Descent(291/399): loss=0.4125942875776897, w0=0.0006045595374490699, w1=-0.017721729817618437\n",
      "Gradient Descent(292/399): loss=0.4124567326603191, w0=0.0006066194665120275, w1=-0.017760226905862063\n",
      "Gradient Descent(293/399): loss=0.41231961772121717, w0=0.0006086731841202935, w1=-0.017798631866843502\n",
      "Gradient Descent(294/399): loss=0.41218293989895965, w0=0.0006107206793165609, w1=-0.017836945079269778\n",
      "Gradient Descent(295/399): loss=0.4120466963561086, w0=0.0006127619414701056, w1=-0.017875166919862545\n",
      "Gradient Descent(296/399): loss=0.41191088427896916, w0=0.0006147969602731979, w1=-0.017913297763370407\n",
      "Gradient Descent(297/399): loss=0.4117755008773504, w0=0.0006168257257375493, w1=-0.01795133798258112\n",
      "Gradient Descent(298/399): loss=0.4116405433843275, w0=0.0006188482281907922, w1=-0.017989287948333717\n",
      "Gradient Descent(299/399): loss=0.4115060090560087, w0=0.0006208644582729939, w1=-0.018027148029530572\n",
      "Gradient Descent(300/399): loss=0.4113718951713022, w0=0.0006228744069332027, w1=-0.01806491859314934\n",
      "Gradient Descent(301/399): loss=0.4112381990316886, w0=0.0006248780654260268, w1=-0.018102600004254857\n",
      "Gradient Descent(302/399): loss=0.41110491796099374, w0=0.0006268754253082467, w1=-0.018140192626010923\n",
      "Gradient Descent(303/399): loss=0.4109720493051664, w0=0.0006288664784354583, w1=-0.018177696819692026\n",
      "Gradient Descent(304/399): loss=0.4108395904320554, w0=0.000630851216958749, w1=-0.01821511294469498\n",
      "Gradient Descent(305/399): loss=0.41070753873119314, w0=0.0006328296333214046, w1=-0.018252441358550475\n",
      "Gradient Descent(306/399): loss=0.41057589161357844, w0=0.0006348017202556476, w1=-0.018289682416934558\n",
      "Gradient Descent(307/399): loss=0.4104446465114635, w0=0.0006367674707794071, w1=-0.01832683647368003\n",
      "Gradient Descent(308/399): loss=0.41031380087814273, w0=0.0006387268781931183, w1=-0.018363903880787753\n",
      "Gradient Descent(309/399): loss=0.4101833521877447, w0=0.0006406799360765533, w1=-0.018400884988437918\n",
      "Gradient Descent(310/399): loss=0.4100532979350254, w0=0.0006426266382856811, w1=-0.018437780145001182\n",
      "Gradient Descent(311/399): loss=0.40992363563516504, w0=0.0006445669789495588, w1=-0.018474589697049768\n",
      "Gradient Descent(312/399): loss=0.4097943628235661, w0=0.0006465009524672507, w1=-0.01851131398936848\n",
      "Gradient Descent(313/399): loss=0.40966547705565437, w0=0.0006484285535047781, w1=-0.01854795336496563\n",
      "Gradient Descent(314/399): loss=0.409536975906683, w0=0.0006503497769920969, w1=-0.018584508165083904\n",
      "Gradient Descent(315/399): loss=0.4094088569715354, w0=0.0006522646181201051, w1=-0.01862097872921115\n",
      "Gradient Descent(316/399): loss=0.4092811178645362, w0=0.0006541730723376777, w1=-0.018657365395091084\n",
      "Gradient Descent(317/399): loss=0.4091537562192584, w0=0.0006560751353487308, w1=-0.01869366849873394\n",
      "Gradient Descent(318/399): loss=0.40902676968833673, w0=0.0006579708031093127, w1=-0.01872988837442703\n",
      "Gradient Descent(319/399): loss=0.4089001559432811, w0=0.0006598600718247232, w1=-0.018766025354745246\n",
      "Gradient Descent(320/399): loss=0.4087739126742933, w0=0.0006617429379466605, w1=-0.01880207977056147\n",
      "Gradient Descent(321/399): loss=0.4086480375900854, w0=0.000663619398170395, w1=-0.018838051951056946\n",
      "Gradient Descent(322/399): loss=0.4085225284176991, w0=0.0006654894494319697, w1=-0.01887394222373154\n",
      "Gradient Descent(323/399): loss=0.4083973829023296, w0=0.0006673530889054281, w1=-0.018909750914413968\n",
      "Gradient Descent(324/399): loss=0.4082725988071489, w0=0.0006692103140000678, w1=-0.018945478347271928\n",
      "Gradient Descent(325/399): loss=0.4081481739131324, w0=0.0006710611223577204, w1=-0.018981124844822186\n",
      "Gradient Descent(326/399): loss=0.4080241060188882, w0=0.0006729055118500583, w1=-0.01901669072794056\n",
      "Gradient Descent(327/399): loss=0.40790039294048575, w0=0.0006747434805759253, w1=-0.01905217631587188\n",
      "Gradient Descent(328/399): loss=0.4077770325112895, w0=0.0006765750268586945, w1=-0.019087581926239845\n",
      "Gradient Descent(329/399): loss=0.407654022581792, w0=0.0006784001492436507, w1=-0.019122907875056817\n",
      "Gradient Descent(330/399): loss=0.40753136101945053, w0=0.0006802188464953972, w1=-0.019158154476733573\n",
      "Gradient Descent(331/399): loss=0.40740904570852454, w0=0.0006820311175952891, w1=-0.01919332204408897\n",
      "Gradient Descent(332/399): loss=0.40728707454991536, w0=0.000683836961738889, w1=-0.019228410888359547\n",
      "Gradient Descent(333/399): loss=0.407165445461007, w0=0.0006856363783334485, w1=-0.019263421319209054\n",
      "Gradient Descent(334/399): loss=0.4070441563755104, w0=0.0006874293669954134, w1=-0.01929835364473795\n",
      "Gradient Descent(335/399): loss=0.4069232052433077, w0=0.0006892159275479524, w1=-0.019333208171492797\n",
      "Gradient Descent(336/399): loss=0.40680259003029884, w0=0.0006909960600185102, w1=-0.019367985204475604\n",
      "Gradient Descent(337/399): loss=0.40668230871824995, w0=0.0006927697646363834, w1=-0.01940268504715312\n",
      "Gradient Descent(338/399): loss=0.40656235930464346, w0=0.0006945370418303198, w1=-0.019437308001466057\n",
      "Gradient Descent(339/399): loss=0.40644273980252965, w0=0.0006962978922261408, w1=-0.01947185436783824\n",
      "Gradient Descent(340/399): loss=0.40632344824038064, w0=0.0006980523166443864, w1=-0.019506324445185706\n",
      "Gradient Descent(341/399): loss=0.4062044826619441, w0=0.0006998003160979833, w1=-0.01954071853092575\n",
      "Gradient Descent(342/399): loss=0.40608584112610036, w0=0.0007015418917899346, w1=-0.019575036920985882\n",
      "Gradient Descent(343/399): loss=0.40596752170672157, w0=0.0007032770451110322, w1=-0.01960927990981276\n",
      "Gradient Descent(344/399): loss=0.4058495224925299, w0=0.0007050057776375911, w1=-0.019643447790381033\n",
      "Gradient Descent(345/399): loss=0.40573184158695963, w0=0.0007067280911292055, w1=-0.019677540854202136\n",
      "Gradient Descent(346/399): loss=0.4056144771080195, w0=0.0007084439875265263, w1=-0.019711559391333032\n",
      "Gradient Descent(347/399): loss=0.4054974271881573, w0=0.0007101534689490597, w1=-0.019745503690384886\n",
      "Gradient Descent(348/399): loss=0.4053806899741256, w0=0.0007118565376929882, w1=-0.019779374038531684\n",
      "Gradient Descent(349/399): loss=0.40526426362684864, w0=0.0007135531962290107, w1=-0.01981317072151879\n",
      "Gradient Descent(350/399): loss=0.40514814632129137, w0=0.0007152434472002052, w1=-0.019846894023671464\n",
      "Gradient Descent(351/399): loss=0.40503233624632984, w0=0.0007169272934199109, w1=-0.01988054422790329\n",
      "Gradient Descent(352/399): loss=0.40491683160462233, w0=0.0007186047378696312, w1=-0.01991412161572458\n",
      "Gradient Descent(353/399): loss=0.40480163061248164, w0=0.0007202757836969571, w1=-0.0199476264672507\n",
      "Gradient Descent(354/399): loss=0.4046867314997508, w0=0.0007219404342135097, w1=-0.01998105906121035\n",
      "Gradient Descent(355/399): loss=0.4045721325096779, w0=0.000723598692892904, w1=-0.020014419674953796\n",
      "Gradient Descent(356/399): loss=0.40445783189879314, w0=0.0007252505633687311, w1=-0.020047708584461022\n",
      "Gradient Descent(357/399): loss=0.4043438279367878, w0=0.0007268960494325606, w1=-0.020080926064349858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(358/399): loss=0.4042301189063925, w0=0.000728535155031962, w1=-0.020114072387884027\n",
      "Gradient Descent(359/399): loss=0.4041167031032603, w0=0.0007301678842685455, w1=-0.02014714782698116\n",
      "Gradient Descent(360/399): loss=0.40400357883584714, w0=0.0007317942413960216, w1=-0.02018015265222075\n",
      "Gradient Descent(361/399): loss=0.40389074442529604, w0=0.0007334142308182791, w1=-0.020213087132852036\n",
      "Gradient Descent(362/399): loss=0.4037781982053219, w0=0.0007350278570874829, w1=-0.02024595153680187\n",
      "Gradient Descent(363/399): loss=0.4036659385220976, w0=0.0007366351249021886, w1=-0.020278746130682493\n",
      "Gradient Descent(364/399): loss=0.40355396373414143, w0=0.0007382360391054767, w1=-0.02031147117979929\n",
      "Gradient Descent(365/399): loss=0.4034422722122044, w0=0.0007398306046831047, w1=-0.02034412694815848\n",
      "Gradient Descent(366/399): loss=0.40333086233916177, w0=0.0007414188267616762, w1=-0.020376713698474744\n",
      "Gradient Descent(367/399): loss=0.4032197325099022, w0=0.0007430007106068293, w1=-0.02040923169217883\n",
      "Gradient Descent(368/399): loss=0.4031088811312205, w0=0.0007445762616214409, w1=-0.020441681189425086\n",
      "Gradient Descent(369/399): loss=0.4029983066217108, w0=0.0007461454853438502, w1=-0.020474062449098943\n",
      "Gradient Descent(370/399): loss=0.40288800741166053, w0=0.0007477083874460981, w1=-0.020506375728824362\n",
      "Gradient Descent(371/399): loss=0.40277798194294623, w0=0.0007492649737321846, w1=-0.02053862128497122\n",
      "Gradient Descent(372/399): loss=0.4026682286689294, w0=0.0007508152501363428, w1=-0.020570799372662655\n",
      "Gradient Descent(373/399): loss=0.40255874605435493, w0=0.0007523592227213296, w1=-0.020602910245782355\n",
      "Gradient Descent(374/399): loss=0.402449532575249, w0=0.0007538968976767337, w1=-0.020634954156981803\n",
      "Gradient Descent(375/399): loss=0.40234058671881945, w0=0.0007554282813172995, w1=-0.020666931357687473\n",
      "Gradient Descent(376/399): loss=0.40223190698335665, w0=0.0007569533800812672, w1=-0.02069884209810798\n",
      "Gradient Descent(377/399): loss=0.40212349187813423, w0=0.0007584722005287304, w1=-0.020730686627241177\n",
      "Gradient Descent(378/399): loss=0.40201533992331345, w0=0.000759984749340008, w1=-0.020762465192881222\n",
      "Gradient Descent(379/399): loss=0.40190744964984687, w0=0.0007614910333140334, w1=-0.020794178041625574\n",
      "Gradient Descent(380/399): loss=0.4017998195993818, w0=0.0007629910593667589, w1=-0.020825825418881963\n",
      "Gradient Descent(381/399): loss=0.40169244832416834, w0=0.0007644848345295756, w1=-0.020857407568875292\n",
      "Gradient Descent(382/399): loss=0.4015853343869644, w0=0.0007659723659477495, w1=-0.02088892473465453\n",
      "Gradient Descent(383/399): loss=0.40147847636094525, w0=0.0007674536608788721, w1=-0.020920377158099523\n",
      "Gradient Descent(384/399): loss=0.40137187282961073, w0=0.0007689287266913266, w1=-0.020951765079927773\n",
      "Gradient Descent(385/399): loss=0.40126552238669644, w0=0.0007703975708627694, w1=-0.020983088739701183\n",
      "Gradient Descent(386/399): loss=0.40115942363608376, w0=0.0007718602009786262, w1=-0.02101434837583274\n",
      "Gradient Descent(387/399): loss=0.40105357519171114, w0=0.0007733166247306028, w1=-0.021045544225593172\n",
      "Gradient Descent(388/399): loss=0.4009479756774877, w0=0.0007747668499152111, w1=-0.021076676525117546\n",
      "Gradient Descent(389/399): loss=0.40084262372720586, w0=0.0007762108844323093, w1=-0.02110774550941183\n",
      "Gradient Descent(390/399): loss=0.4007375179844557, w0=0.0007776487362836565, w1=-0.02113875141235941\n",
      "Gradient Descent(391/399): loss=0.40063265710254126, w0=0.0007790804135714809, w1=-0.021169694466727576\n",
      "Gradient Descent(392/399): loss=0.4005280397443951, w0=0.0007805059244970641, w1=-0.021200574904173933\n",
      "Gradient Descent(393/399): loss=0.40042366458249623, w0=0.0007819252773593369, w1=-0.02123139295525281\n",
      "Gradient Descent(394/399): loss=0.4003195302987881, w0=0.0007833384805534912, w1=-0.021262148849421604\n",
      "Gradient Descent(395/399): loss=0.4002156355845969, w0=0.0007847455425696038, w1=-0.02129284281504709\n",
      "Gradient Descent(396/399): loss=0.40011197914055074, w0=0.0007861464719912754, w1=-0.021323475079411684\n",
      "Gradient Descent(397/399): loss=0.40000855967650073, w0=0.000787541277494282, w1=-0.02135404586871966\n",
      "Gradient Descent(398/399): loss=0.39990537591144204, w0=0.0007889299678452401, w1=-0.021384555408103356\n",
      "Gradient Descent(399/399): loss=0.3998024265734349, w0=0.0007903125519002853, w1=-0.021415003921629303\n",
      "++++ gamma = 0.000719685673001\n",
      "ciaociaociao\n",
      "Gradient Descent(0/399): loss=0.4999999999999887, w0=-4.956642416006324e-07, w1=-0.00025801623469334025\n",
      "Gradient Descent(1/399): loss=0.4981636926600653, w0=-6.592428244550372e-07, w1=-0.0005132390320667247\n",
      "Gradient Descent(2/399): loss=0.49636985462678807, w0=-5.023864021255259e-07, w1=-0.0007657121062230711\n",
      "Gradient Descent(3/399): loss=0.49461715796180317, w0=-3.6411766528501595e-08, w1=-0.001015478343433184\n",
      "Gradient Descent(4/399): loss=0.4929043225953098, w0=7.276884414134556e-07, w1=-0.0012625798203835144\n",
      "Gradient Descent(5/399): loss=0.49123011440583964, w0=1.77923633316711e-06, w1=-0.0015070578219548085\n",
      "Gradient Descent(6/399): loss=0.48959334338597016, w0=3.1078596746321834e-06, w1=-0.0017489528585458455\n",
      "Gradient Descent(7/399): loss=0.4879928618895978, w0=4.703483077900507e-06, w1=-0.0019883046829559615\n",
      "Gradient Descent(8/399): loss=0.4864275629566458, w0=6.5563194722433176e-06, w1=-0.002225152306839602\n",
      "Gradient Descent(9/399): loss=0.48489637871132085, w0=8.65686184426596e-06, w1=-0.0024595340167456498\n",
      "Gradient Descent(10/399): loss=0.48339827883025843, w0=1.099587523757615e-05, w1=-0.002691487389753857\n",
      "Gradient Descent(11/399): loss=0.4819322690771088, w0=1.3564389002701583e-05, w1=-0.002921049308720282\n",
      "Gradient Descent(12/399): loss=0.4804973899003079, w0=1.6353689288364875e-05, w1=-0.003148255977143198\n",
      "Gradient Descent(13/399): loss=0.47909271509096607, w0=1.935531176557998e-05, w1=-0.0033731429336605803\n",
      "Gradient Descent(14/399): loss=0.4777173504979749, w0=2.2561034576374532e-05, w1=-0.003595745066189881\n",
      "Gradient Descent(15/399): loss=0.47637043279759483, w0=2.5962871499268374e-05, w1=-0.003816096625720436\n",
      "Gradient Descent(16/399): loss=0.4750511283149361, w0=2.9553065323949988e-05, w1=-0.004034231239768504\n",
      "Gradient Descent(17/399): loss=0.47375863189488987, w0=3.3324081427890595e-05, w1=-0.004250181925504605\n",
      "Gradient Descent(18/399): loss=0.4724921658201884, w0=3.726860154792081e-05, w1=-0.004463981102562494\n",
      "Gradient Descent(19/399): loss=0.47125097877441136, w0=4.1379517740067484e-05, w1=-0.004675660605538796\n",
      "Gradient Descent(20/399): loss=0.4700343448478592, w0=4.564992652120975e-05, w1=-0.004885251696192036\n",
      "Gradient Descent(21/399): loss=0.4688415625843293, w0=5.0073123186363027e-05, w1=-0.005092785075349492\n",
      "Gradient Descent(22/399): loss=0.4676719540669336, w0=5.464259629563901e-05, w1=-0.005298290894530054\n",
      "Gradient Descent(23/399): loss=0.4665248640411888, w0=5.93520223251588e-05, w1=-0.005501798767290962\n",
      "Gradient Descent(24/399): loss=0.46539965907370506, w0=6.419526047641563e-05, w1=-0.005703337780306079\n",
      "Gradient Descent(25/399): loss=0.46429572674488234, w0=6.916634763879356e-05, w1=-0.00590293650418308\n",
      "Gradient Descent(26/399): loss=0.46321247487410233, w0=7.425949350014993e-05, w1=-0.006100623004026719\n",
      "Gradient Descent(27/399): loss=0.4621493307759842, w0=7.946907580056183e-05, w1=-0.006296424849755086\n",
      "Gradient Descent(28/399): loss=0.4611057405463358, w0=8.47896357245213e-05, w1=-0.006490369126175569\n",
      "Gradient Descent(29/399): loss=0.4600811683765099, w0=9.021587342704147e-05, w1=-0.006682482442826999\n",
      "Gradient Descent(30/399): loss=0.4590750958949273, w0=9.574264368930431e-05, w1=-0.006872790943594274\n",
      "Gradient Descent(31/399): loss=0.4580870215345959, w0=0.00010136495169964444, w1=-0.007061320316101537\n",
      "Gradient Descent(32/399): loss=0.45711645992550853, w0=0.00010707794895581755, w1=-0.007248095800889815\n",
      "Gradient Descent(33/399): loss=0.45616294131085827, w0=0.00011287692928465284, w1=-0.007433142200384824\n",
      "Gradient Descent(34/399): loss=0.45522601098605275, w0=0.00011875732497533085, w1=-0.007616483887660482\n",
      "Gradient Descent(35/399): loss=0.4543052287595707, w0=0.00012471470302266588, w1=-0.0077981448150034795\n",
      "Gradient Descent(36/399): loss=0.45340016843473263, w0=0.00013074476147690344, w1=-0.007978148522284133\n",
      "Gradient Descent(37/399): loss=0.4525104173115157, w0=0.00013684332589667018, w1=-0.00815651814513854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(38/399): loss=0.45163557570756996, w0=0.00014300634590183373, w1=-0.008333276422966931\n",
      "Gradient Descent(39/399): loss=0.4507752564976434, w0=0.00014922989182314711, w1=-0.008508445706752963\n",
      "Gradient Descent(40/399): loss=0.4499290846706493, w0=0.0001555101514456636, w1=-0.008682047966708545\n",
      "Gradient Descent(41/399): loss=0.44909669690365245, w0=0.0001618434268430148, w1=-0.008854104799748658\n",
      "Gradient Descent(42/399): loss=0.44827774115207847, w0=0.00016822613129974755, w1=-0.009024637436800497\n",
      "Gradient Descent(43/399): loss=0.44747187625548157, w0=0.00017465478631901435, w1=-0.009193666749951136\n",
      "Gradient Descent(44/399): loss=0.4466787715582396, w0=0.00018112601871300598, w1=-0.009361213259437775\n",
      "Gradient Descent(45/399): loss=0.4458981065445691, w0=0.00018763655777360677, w1=-0.009527297140484553\n",
      "Gradient Descent(46/399): loss=0.4451295704872805, w0=0.00019418323252083986, w1=-0.009691938229989734\n",
      "Gradient Descent(47/399): loss=0.44437286210972443, w0=0.00020076296902675407, w1=-0.009855156033067015\n",
      "Gradient Descent(48/399): loss=0.44362768926039275, w0=0.00020737278781248473, w1=-0.010016969729444566\n",
      "Gradient Descent(49/399): loss=0.4428937685996738, w0=0.00021400980131629753, w1=-0.010177398179725324\n",
      "Gradient Descent(50/399): loss=0.4421708252982738, w0=0.00022067121143050085, w1=-0.010336459931511948\n",
      "Gradient Descent(51/399): loss=0.4414585927468429, w0=0.00022735430710518143, w1=-0.010494173225399759\n",
      "Gradient Descent(52/399): loss=0.44075681227636043, w0=0.000234056462016789, w1=-0.010650556000840875\n",
      "Gradient Descent(53/399): loss=0.44006523288885346, w0=0.00024077513229966052, w1=-0.010805625901882696\n",
      "Gradient Descent(54/399): loss=0.4393836109980447, w0=0.00024750785433863906, w1=-0.010959400282783763\n",
      "Gradient Descent(55/399): loss=0.4387117101795365, w0=0.0002542522426210036, w1=-0.011111896213509952\n",
      "Gradient Descent(56/399): loss=0.43804930093015965, w0=0.0002610059876459847, w1=-0.01126313048511389\n",
      "Gradient Descent(57/399): loss=0.43739616043612795, w0=0.00026776685389019825, w1=-0.011413119615000371\n",
      "Gradient Descent(58/399): loss=0.43675207234965674, w0=0.00027453267782738306, w1=-0.011561879852080518\n",
      "Gradient Descent(59/399): loss=0.43611682657371476, w0=0.0002813013660008826, w1=-0.0117094271818173\n",
      "Gradient Descent(60/399): loss=0.4354902190545958, w0=0.00028807089314735997, w1=-0.011855777331165022\n",
      "Gradient Descent(61/399): loss=0.4348720515820078, w0=0.00029483930037028423, w1=-0.012000945773405233\n",
      "Gradient Descent(62/399): loss=0.4342621315963899, w0=0.0003016046933617746, w1=-0.012144947732881545\n",
      "Gradient Descent(63/399): loss=0.43366027200317725, w0=0.0003083652406714326, w1=-0.012287798189635685\n",
      "Gradient Descent(64/399): loss=0.43306629099374994, w0=0.00031511917202083653, w1=-0.012429511883947116\n",
      "Gradient Descent(65/399): loss=0.43248001187280993, w0=0.0003218647766624145, w1=-0.012570103320778452\n",
      "Gradient Descent(66/399): loss=0.43190126289193764, w0=0.00032860040178145323, w1=-0.012709586774128857\n",
      "Gradient Descent(67/399): loss=0.4313298770890978, w0=0.0003353244509400381, w1=-0.012847976291297563\n",
      "Gradient Descent(68/399): loss=0.4307656921338623, w0=0.000342035382561758, w1=-0.012985285697059559\n",
      "Gradient Descent(69/399): loss=0.43020855017813886, w0=0.00034873170845604445, w1=-0.013121528597755477\n",
      "Gradient Descent(70/399): loss=0.4296582977121953, w0=0.0003554119923810501, w1=-0.013256718385297643\n",
      "Gradient Descent(71/399): loss=0.4291147854257764, w0=0.0003620748486440048, w1=-0.01339086824109419\n",
      "Gradient Descent(72/399): loss=0.4285778680741267, w0=0.0003687189407380201, w1=-0.013523991139893107\n",
      "Gradient Descent(73/399): loss=0.42804740434873084, w0=0.00037534298001434547, w1=-0.013656099853548041\n",
      "Gradient Descent(74/399): loss=0.4275232567525961, w0=0.00038194572438910736, w1=-0.013787206954707603\n",
      "Gradient Descent(75/399): loss=0.4270052914799072, w0=0.0003885259770835949, w1=-0.013917324820429915\n",
      "Gradient Descent(76/399): loss=0.42649337829988887, w0=0.0003950825853971813, w1=-0.01404646563572407\n",
      "Gradient Descent(77/399): loss=0.4259873904447198, w0=0.0004016144395119992, w1=-0.014174641397020146\n",
      "Gradient Descent(78/399): loss=0.42548720450134786, w0=0.00040812047132851337, w1=-0.014301863915569365\n",
      "Gradient Descent(79/399): loss=0.4249927003070617, w0=0.0004145996533311607, w1=-0.014428144820775944\n",
      "Gradient Descent(80/399): loss=0.424503760848676, w0=0.00042105099748325085, w1=-0.014553495563462172\n",
      "Gradient Descent(81/399): loss=0.42402027216520394, w0=0.00042747355415034604, w1=-0.014677927419068166\n",
      "Gradient Descent(82/399): loss=0.42354212325387974, w0=0.0004338664110513597, w1=-0.014801451490787768\n",
      "Gradient Descent(83/399): loss=0.42306920597941267, w0=0.0004402286922366379, w1=-0.014924078712641984\n",
      "Gradient Descent(84/399): loss=0.4226014149863551, w0=0.0004465595570923071, w1=-0.015045819852491316\n",
      "Gradient Descent(85/399): loss=0.42213864761446396, w0=0.0004528581993701942, w1=-0.01516668551498835\n",
      "Gradient Descent(86/399): loss=0.42168080381695316, w0=0.00045912384624264283, w1=-0.015286686144471897\n",
      "Gradient Descent(87/399): loss=0.421227786081525, w0=0.00046535575738157193, w1=-0.015405832027803936\n",
      "Gradient Descent(88/399): loss=0.42077949935408343, w0=0.0004715532240611386, w1=-0.015524133297150636\n",
      "Gradient Descent(89/399): loss=0.42033585096503046, w0=0.0004777155682833879, w1=-0.015641599932708646\n",
      "Gradient Descent(90/399): loss=0.4198967505580485, w0=0.00048384214192628773, w1=-0.01575824176537783\n",
      "Gradient Descent(91/399): loss=0.41946211002128353, w0=0.000489932325913566, w1=-0.01587406847938163\n",
      "Gradient Descent(92/399): loss=0.4190318434208389, w0=0.0004959855294057815, w1=-0.015989089614836135\n",
      "Gradient Descent(93/399): loss=0.4186058669364956, w0=0.0005020011890120775, w1=-0.01610331457026901\n",
      "Gradient Descent(94/399): loss=0.4181840987995813, w0=0.0005079787680220832, w1=-0.016216752605089313\n",
      "Gradient Descent(95/399): loss=0.4177664592329078, w0=0.000513917755657439, w1=-0.01632941284200927\n",
      "Gradient Descent(96/399): loss=0.41735287039270585, w0=0.0005198176663424428, w1=-0.01644130426941902\n",
      "Gradient Descent(97/399): loss=0.4169432563124805, w0=0.0005256780389933219, w1=-0.016552435743715337\n",
      "Gradient Descent(98/399): loss=0.4165375428487248, w0=0.0005314984363256522, w1=-0.016662815991585308\n",
      "Gradient Descent(99/399): loss=0.4161356576284195, w0=0.0005372784441794601, w1=-0.016772453612245896\n",
      "Gradient Descent(100/399): loss=0.41573752999825764, w0=0.0005430176708615525, w1=-0.016881357079640346\n",
      "Gradient Descent(101/399): loss=0.4153430909755333, w0=0.0005487157465046352, w1=-0.016989534744592322\n",
      "Gradient Descent(102/399): loss=0.414952273200632, w0=0.0005543723224427905, w1=-0.017096994836918667\n",
      "Gradient Descent(103/399): loss=0.4145650108910718, w0=0.0005599870706028976, w1=-0.017203745467501672\n",
      "Gradient Descent(104/399): loss=0.4141812397970328, w0=0.00056555968291159, w1=-0.01730979463032167\n",
      "Gradient Descent(105/399): loss=0.4138008971583277, w0=0.0005710898707173536, w1=-0.01741515020445079\n",
      "Gradient Descent(106/399): loss=0.413423921662761, w0=0.0005765773642273837, w1=-0.017519819956008716\n",
      "Gradient Descent(107/399): loss=0.4130502534058281, w0=0.0005820219119588258, w1=-0.01762381154008117\n",
      "Gradient Descent(108/399): loss=0.41267983385170615, w0=0.0005874232802040354, w1=-0.017727132502601974\n",
      "Gradient Descent(109/399): loss=0.4123126057954921, w0=0.0005927812525095046, w1=-0.01782979028219937\n",
      "Gradient Descent(110/399): loss=0.4119485133266456, w0=0.0005980956291681087, w1=-0.017931792212007386\n",
      "Gradient Descent(111/399): loss=0.41158750179359205, w0=0.0006033662267243388, w1=-0.018033145521442952\n",
      "Gradient Descent(112/399): loss=0.411229517769447, w0=0.0006085928774921924, w1=-0.018133857337949456\n",
      "Gradient Descent(113/399): loss=0.4108745090188226, w0=0.0006137754290854046, w1=-0.01823393468870747\n",
      "Gradient Descent(114/399): loss=0.4105224244656789, w0=0.0006189137439597092, w1=-0.018333384502313255\n",
      "Gradient Descent(115/399): loss=0.4101732141621823, w0=0.0006240076989668289, w1=-0.018432213610425772\n",
      "Gradient Descent(116/399): loss=0.4098268292585384, w0=0.0006290571849199003, w1=-0.018530428749382804\n",
      "Gradient Descent(117/399): loss=0.40948322197376413, w0=0.0006340621061700474, w1=-0.018628036561786812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(118/399): loss=0.4091423455673667, w0=0.0006390223801938251, w1=-0.01872504359806117\n",
      "Gradient Descent(119/399): loss=0.40880415431189976, w0=0.0006439379371912614, w1=-0.018821456317977346\n",
      "Gradient Descent(120/399): loss=0.4084686034663658, w0=0.0006488087196942324, w1=-0.01891728109215366\n",
      "Gradient Descent(121/399): loss=0.4081356492504342, w0=0.0006536346821849153, w1=-0.019012524203526157\n",
      "Gradient Descent(122/399): loss=0.407805248819451, w0=0.0006584157907240649, w1=-0.01910719184879216\n",
      "Gradient Descent(123/399): loss=0.40747736024020814, w0=0.0006631520225888719, w1=-0.01920129013982709\n",
      "Gradient Descent(124/399): loss=0.4071519424674529, w0=0.000667843365920163, w1=-0.019294825105075054\n",
      "Gradient Descent(125/399): loss=0.406828955321105, w0=0.0006724898193787106, w1=-0.019387802690913735\n",
      "Gradient Descent(126/399): loss=0.40650835946416525, w0=0.0006770913918104275, w1=-0.01948022876299412\n",
      "Gradient Descent(127/399): loss=0.4061901163812847, w0=0.0006816481019202239, w1=-0.01957210910755554\n",
      "Gradient Descent(128/399): loss=0.40587418835797834, w0=0.0006861599779543125, w1=-0.019663449432716543\n",
      "Gradient Descent(129/399): loss=0.40556053846045836, w0=0.0006906270573907537, w1=-0.019754255369742058\n",
      "Gradient Descent(130/399): loss=0.40524913051606737, w0=0.000695049386638034, w1=-0.01984453247428736\n",
      "Gradient Descent(131/399): loss=0.4049399290942901, w0=0.0006994270207414811, w1=-0.019934286227619242\n",
      "Gradient Descent(132/399): loss=0.40463289948832737, w0=0.00070376002309732, w1=-0.02002352203781489\n",
      "Gradient Descent(133/399): loss=0.4043280076972097, w0=0.000708048465174182, w1=-0.0201122452409389\n",
      "Gradient Descent(134/399): loss=0.4040252204084362, w0=0.0007122924262418821, w1=-0.020200461102198863\n",
      "Gradient Descent(135/399): loss=0.40372450498111906, w0=0.0007164919931072838, w1=-0.020288174817079913\n",
      "Gradient Descent(136/399): loss=0.40342582942961747, w0=0.0007206472598570786, w1=-0.020375391512458707\n",
      "Gradient Descent(137/399): loss=0.40312916240764574, w0=0.0007247583276073068, w1=-0.020462116247697212\n",
      "Gradient Descent(138/399): loss=0.4028344731928398, w0=0.0007288253042594541, w1=-0.020548354015716693\n",
      "Gradient Descent(139/399): loss=0.4025417316717653, w0=0.0007328483042629623, w1=-0.020634109744052304\n",
      "Gradient Descent(140/399): loss=0.4022509083253559, w0=0.0007368274483839942, w1=-0.020719388295888656\n",
      "Gradient Descent(141/399): loss=0.4019619742147651, w0=0.0007407628634802994, w1=-0.02080419447107674\n",
      "Gradient Descent(142/399): loss=0.40167490096762015, w0=0.0007446546822820294, w1=-0.020888533007132573\n",
      "Gradient Descent(143/399): loss=0.40138966076466337, w0=0.0007485030431783566, w1=-0.020972408580217886\n",
      "Gradient Descent(144/399): loss=0.4011062263267693, w0=0.0007523080900097524, w1=-0.02105582580610329\n",
      "Gradient Descent(145/399): loss=0.4008245709023267, w0=0.0007560699718657852, w1=-0.021138789241114155\n",
      "Gradient Descent(146/399): loss=0.40054466825497026, w0=0.0007597888428883024, w1=-0.021221303383059633\n",
      "Gradient Descent(147/399): loss=0.4002664926516547, w0=0.0007634648620798633, w1=-0.021303372672145087\n",
      "Gradient Descent(148/399): loss=0.39999001885105834, w0=0.0007670981931172928, w1=-0.02138500149186828\n",
      "Gradient Descent(149/399): loss=0.39971522209230637, w0=0.0007706890041702305, w1=-0.021466194169899627\n",
      "Gradient Descent(150/399): loss=0.3994420780840008, w0=0.0007742374677245507, w1=-0.02154695497894684\n",
      "Gradient Descent(151/399): loss=0.3991705629935542, w0=0.0007777437604105332, w1=-0.021627288137604228\n",
      "Gradient Descent(152/399): loss=0.39890065343681036, w0=0.0007812080628356687, w1=-0.021707197811186983\n",
      "Gradient Descent(153/399): loss=0.39863232646794644, w0=0.0007846305594219828, w1=-0.021786688112550728\n",
      "Gradient Descent(154/399): loss=0.3983655595696469, w0=0.000788011438247768, w1=-0.021865763102896608\n",
      "Gradient Descent(155/399): loss=0.39810033064354255, w0=0.0007913508908936132, w1=-0.02194442679256222\n",
      "Gradient Descent(156/399): loss=0.39783661800090414, w0=0.0007946491122926275, w1=-0.02202268314179862\n",
      "Gradient Descent(157/399): loss=0.3975744003535818, w0=0.0007979063005847503, w1=-0.02210053606153371\n",
      "Gradient Descent(158/399): loss=0.39731365680518815, w0=0.0008011226569750505, w1=-0.02217798941412224\n",
      "Gradient Descent(159/399): loss=0.39705436684250955, w0=0.0008042983855959138, w1=-0.022255047014082697\n",
      "Gradient Descent(160/399): loss=0.39679651032714647, w0=0.0008074336933730222, w1=-0.022331712628821332\n",
      "Gradient Descent(161/399): loss=0.39654006748736925, w0=0.0008105287898950323, w1=-0.022407989979343543\n",
      "Gradient Descent(162/399): loss=0.39628501891018825, w0=0.0008135838872868601, w1=-0.0224838827409529\n",
      "Gradient Descent(163/399): loss=0.39603134553362696, w0=0.000816599200086483, w1=-0.022559394543938006\n",
      "Gradient Descent(164/399): loss=0.3957790286391961, w0=0.0008195749451251721, w1=-0.02263452897424745\n",
      "Gradient Descent(165/399): loss=0.39552804984456047, w0=0.0008225113414110696, w1=-0.02270928957415308\n",
      "Gradient Descent(166/399): loss=0.39527839109639157, w0=0.0008254086100160268, w1=-0.02278367984290178\n",
      "Gradient Descent(167/399): loss=0.39503003466340475, w0=0.0008282669739656237, w1=-0.022857703237356027\n",
      "Gradient Descent(168/399): loss=0.3947829631295696, w0=0.0008310866581322904, w1=-0.02293136317262339\n",
      "Gradient Descent(169/399): loss=0.39453715938749423, w0=0.0008338678891314518, w1=-0.023004663022675217\n",
      "Gradient Descent(170/399): loss=0.394292606631973, w0=0.0008366108952206217, w1=-0.023077606120954667\n",
      "Gradient Descent(171/399): loss=0.39404928835369774, w0=0.0008393159062013727, w1=-0.02315019576097435\n",
      "Gradient Descent(172/399): loss=0.3938071883331222, w0=0.0008419831533241095, w1=-0.02322243519690373\n",
      "Gradient Descent(173/399): loss=0.3935662906344831, w0=0.0008446128691955758, w1=-0.023294327644146465\n",
      "Gradient Descent(174/399): loss=0.39332657959996264, w0=0.0008472052876890265, w1=-0.023365876279907938\n",
      "Gradient Descent(175/399): loss=0.3930880398439984, w0=0.0008497606438569994, w1=-0.023437084243753094\n",
      "Gradient Descent(176/399): loss=0.39285065624773086, w0=0.000852279173846619, w1=-0.02350795463815481\n",
      "Gradient Descent(177/399): loss=0.39261441395358565, w0=0.0008547611148173723, w1=-0.023578490529032964\n",
      "Gradient Descent(178/399): loss=0.3923792983599865, w0=0.0008572067048612913, w1=-0.02364869494628434\n",
      "Gradient Descent(179/399): loss=0.3921452951161962, w0=0.0008596161829254842, w1=-0.02371857088430362\n",
      "Gradient Descent(180/399): loss=0.3919123901172812, w0=0.0008619897887369547, w1=-0.02378812130249555\n",
      "Gradient Descent(181/399): loss=0.39168056949919516, w0=0.0008643277627296528, w1=-0.023857349125778467\n",
      "Gradient Descent(182/399): loss=0.39144981963398146, w0=0.0008666303459737003, w1=-0.023926257245079397\n",
      "Gradient Descent(183/399): loss=0.391220127125086, w0=0.0008688977801067369, w1=-0.023994848517820792\n",
      "Gradient Descent(184/399): loss=0.390991478802784, w0=0.0008711303072673324, w1=-0.02406312576839914\n",
      "Gradient Descent(185/399): loss=0.390763861719711, w0=0.0008733281700304132, w1=-0.024131091788655558\n",
      "Gradient Descent(186/399): loss=0.3905372631464999, w0=0.0008754916113446528, w1=-0.024198749338338515\n",
      "Gradient Descent(187/399): loss=0.39031167056751714, w0=0.0008776208744717753, w1=-0.02426610114555886\n",
      "Gradient Descent(188/399): loss=0.39008707167670054, w0=0.0008797162029277247, w1=-0.024333149907237256\n",
      "Gradient Descent(189/399): loss=0.38986345437349035, w0=0.0008817778404256509, w1=-0.024399898289544208\n",
      "Gradient Descent(190/399): loss=0.38964080675885493, w0=0.0008838060308206685, w1=-0.024466348928332764\n",
      "Gradient Descent(191/399): loss=0.3894191171314054, w0=0.000885801018056341, w1=-0.024532504429564072\n",
      "Gradient Descent(192/399): loss=0.38919837398360135, w0=0.0008877630461128478, w1=-0.024598367369725906\n",
      "Gradient Descent(193/399): loss=0.38897856599803915, w0=0.0008896923589567905, w1=-0.02466394029624429\n",
      "Gradient Descent(194/399): loss=0.38875968204382644, w0=0.0008915892004925967, w1=-0.024729225727888337\n",
      "Gradient Descent(195/399): loss=0.3885417111730355, w0=0.0008934538145154803, w1=-0.024794226155168454\n",
      "Gradient Descent(196/399): loss=0.3883246426172381, w0=0.0008952864446659185, w1=-0.024858944040727998\n",
      "Gradient Descent(197/399): loss=0.3881084657841154, w0=0.0008970873343856062, w1=-0.024923381819728533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(198/399): loss=0.38789317025414394, w0=0.0008988567268748504, w1=-0.02498754190022879\n",
      "Gradient Descent(199/399): loss=0.3876787457773537, w0=0.0009005948650513669, w1=-0.025051426663557447\n",
      "Gradient Descent(200/399): loss=0.387465182270159, w0=0.0009023019915104431, w1=-0.025115038464679846\n",
      "Gradient Descent(201/399): loss=0.387252469812256, w0=0.0009039783484864317, w1=-0.025178379632558742\n",
      "Gradient Descent(202/399): loss=0.38704059864358975, w0=0.0009056241778155418, w1=-0.025241452470509216\n",
      "Gradient Descent(203/399): loss=0.38682955916138584, w0=0.0009072397208998912, w1=-0.025304259256547838\n",
      "Gradient Descent(204/399): loss=0.38661934191724745, w0=0.0009088252186727902, w1=-0.02536680224373619\n",
      "Gradient Descent(205/399): loss=0.38640993761431147, w0=0.000910380911565222, w1=-0.025429083660518843\n",
      "Gradient Descent(206/399): loss=0.3862013371044703, w0=0.0009119070394734909, w1=-0.025491105711055926\n",
      "Gradient Descent(207/399): loss=0.3859935313856474, w0=0.0009134038417280057, w1=-0.025552870575550318\n",
      "Gradient Descent(208/399): loss=0.38578651159913546, w0=0.0009148715570631702, w1=-0.02561438041056962\n",
      "Gradient Descent(209/399): loss=0.3855802690269872, w0=0.00091631042358835, w1=-0.025675637349362967\n",
      "Gradient Descent(210/399): loss=0.3853747950894634, w0=0.0009177206787598896, w1=-0.02573664350217279\n",
      "Gradient Descent(211/399): loss=0.38517008134253206, w0=0.0009191025593541494, w1=-0.0257974009565416\n",
      "Gradient Descent(212/399): loss=0.38496611947542325, w0=0.0009204563014415379, w1=-0.025857911777613922\n",
      "Gradient Descent(213/399): loss=0.38476290130822993, w0=0.0009217821403615116, w1=-0.025918178008433386\n",
      "Gradient Descent(214/399): loss=0.3845604187895636, w0=0.0009230803106985168, w1=-0.025978201670235172\n",
      "Gradient Descent(215/399): loss=0.3843586639942541, w0=0.0009243510462588491, w1=-0.026037984762733793\n",
      "Gradient Descent(216/399): loss=0.3841576291210989, w0=0.0009255945800484054, w1=-0.026097529264406363\n",
      "Gradient Descent(217/399): loss=0.38395730649065773, w0=0.0009268111442513045, w1=-0.026156837132771383\n",
      "Gradient Descent(218/399): loss=0.38375768854309245, w0=0.0009280009702093537, w1=-0.026215910304663175\n",
      "Gradient Descent(219/399): loss=0.3835587678360504, w0=0.0009291642884023378, w1=-0.02627475069650199\n",
      "Gradient Descent(220/399): loss=0.3833605370425908, w0=0.0009303013284291091, w1=-0.02633336020455991\n",
      "Gradient Descent(221/399): loss=0.3831629889491527, w0=0.0009314123189894562, w1=-0.026391740705222582\n",
      "Gradient Descent(222/399): loss=0.3829661164535638, w0=0.000932497487866731, w1=-0.026449894055246885\n",
      "Gradient Descent(223/399): loss=0.38276991256309006, w0=0.0009335570619112125, w1=-0.026507822092014594\n",
      "Gradient Descent(224/399): loss=0.38257437039252185, w0=0.0009345912670241888, w1=-0.026565526633782106\n",
      "Gradient Descent(225/399): loss=0.38237948316230014, w0=0.000935600328142736, w1=-0.02662300947992629\n",
      "Gradient Descent(226/399): loss=0.3821852441966798, w0=0.000936584469225176, w1=-0.026680272411186548\n",
      "Gradient Descent(227/399): loss=0.3819916469219276, w0=0.0009375439132371941, w1=-0.02673731718990316\n",
      "Gradient Descent(228/399): loss=0.3817986848645566, w0=0.0009384788821385985, w1=-0.02679414556025192\n",
      "Gradient Descent(229/399): loss=0.3816063516495952, w0=0.0009393895968707039, w1=-0.026850759248475225\n",
      "Gradient Descent(230/399): loss=0.38141464099888944, w0=0.0009402762773443218, w1=-0.02690715996310958\n",
      "Gradient Descent(231/399): loss=0.3812235467294387, w0=0.0009411391424283401, w1=-0.02696334939520965\n",
      "Gradient Descent(232/399): loss=0.3810330627517634, w0=0.000941978409938877, w1=-0.02701932921856888\n",
      "Gradient Descent(233/399): loss=0.3808431830683048, w0=0.0009427942966289917, w1=-0.027075101089936774\n",
      "Gradient Descent(234/399): loss=0.3806539017718541, w0=0.0009435870181789366, w1=-0.027130666649232863\n",
      "Gradient Descent(235/399): loss=0.3804652130440137, w0=0.0009443567891869365, w1=-0.027186027519757433\n",
      "Gradient Descent(236/399): loss=0.38027711115368645, w0=0.0009451038231604788, w1=-0.027241185308399068\n",
      "Gradient Descent(237/399): loss=0.3800895904555944, w0=0.0009458283325081009, w1=-0.02729614160583907\n",
      "Gradient Descent(238/399): loss=0.37990264538882446, w0=0.0009465305285316606, w1=-0.027350897986752783\n",
      "Gradient Descent(239/399): loss=0.37971627047540407, w0=0.0009472106214190748, w1=-0.027405456010007916\n",
      "Gradient Descent(240/399): loss=0.37953046031890053, w0=0.0009478688202375149, w1=-0.027459817218859876\n",
      "Gradient Descent(241/399): loss=0.37934520960305007, w0=0.0009485053329270435, w1=-0.027513983141144192\n",
      "Gradient Descent(242/399): loss=0.37916051309041, w0=0.000949120366294681, w1=-0.027567955289466064\n",
      "Gradient Descent(243/399): loss=0.37897636562103654, w0=0.0009497141260088902, w1=-0.027621735161387086\n",
      "Gradient Descent(244/399): loss=0.3787927621111892, w0=0.0009502868165944642, w1=-0.02767532423960921\n",
      "Gradient Descent(245/399): loss=0.3786096975520566, w0=0.0009508386414278086, w1=-0.027728723992155965\n",
      "Gradient Descent(246/399): loss=0.3784271670085082, w0=0.0009513698027326049, w1=-0.027781935872551018\n",
      "Gradient Descent(247/399): loss=0.378245165617867, w0=0.0009518805015758437, w1=-0.02783496131999408\n",
      "Gradient Descent(248/399): loss=0.3780636885887069, w0=0.0009523709378642175, w1=-0.027887801759534238\n",
      "Gradient Descent(249/399): loss=0.37788273119967053, w0=0.0009528413103408619, w1=-0.02794045860224073\n",
      "Gradient Descent(250/399): loss=0.3777022887983097, w0=0.0009532918165824343, w1=-0.027992933245371242\n",
      "Gradient Descent(251/399): loss=0.37752235679994717, w0=0.0009537226529965212, w1=-0.028045227072537714\n",
      "Gradient Descent(252/399): loss=0.3773429306865578, w0=0.0009541340148193627, w1=-0.02809734145386975\n",
      "Gradient Descent(253/399): loss=0.3771640060056721, w0=0.0009545260961138854, w1=-0.028149277746175635\n",
      "Gradient Descent(254/399): loss=0.3769855783692977, w0=0.0009548990897680344, w1=-0.028201037293101045\n",
      "Gradient Descent(255/399): loss=0.3768076434528618, w0=0.000955253187493394, w1=-0.028252621425285413\n",
      "Gradient Descent(256/399): loss=0.3766301969941718, w0=0.0009555885798240893, w1=-0.028304031460516074\n",
      "Gradient Descent(257/399): loss=0.376453234792395, w0=0.0009559054561159604, w1=-0.028355268703880172\n",
      "Gradient Descent(258/399): loss=0.3762767527070568, w0=0.0009562040045459987, w1=-0.02840633444791439\n",
      "Gradient Descent(259/399): loss=0.37610074665705606, w0=0.0009564844121120388, w1=-0.028457229972752525\n",
      "Gradient Descent(260/399): loss=0.37592521261969897, w0=0.0009567468646326972, w1=-0.028507956546270953\n",
      "Gradient Descent(261/399): loss=0.3757501466297498, w0=0.00095699154674755, w1=-0.028558515424232028\n",
      "Gradient Descent(262/399): loss=0.37557554477849786, w0=0.0009572186419175423, w1=-0.02860890785042541\n",
      "Gradient Descent(263/399): loss=0.37540140321284243, w0=0.0009574283324256203, w1=-0.02865913505680743\n",
      "Gradient Descent(264/399): loss=0.37522771813439243, w0=0.0009576207993775818, w1=-0.02870919826363843\n",
      "Gradient Descent(265/399): loss=0.37505448579858286, w0=0.000957796222703134, w1=-0.0287590986796182\n",
      "Gradient Descent(266/399): loss=0.37488170251380615, w0=0.000957954781157156, w1=-0.028808837502019482\n",
      "Gradient Descent(267/399): loss=0.3747093646405597, w0=0.0009580966523211551, w1=-0.02885841591681961\n",
      "Gradient Descent(268/399): loss=0.37453746859060766, w0=0.0009582220126049139, w1=-0.028907835098830284\n",
      "Gradient Descent(269/399): loss=0.37436601082615695, w0=0.0009583310372483192, w1=-0.028957096211825545\n",
      "Gradient Descent(270/399): loss=0.3741949878590488, w0=0.0009584239003233681, w1=-0.029006200408667946\n",
      "Gradient Descent(271/399): loss=0.3740243962499641, w0=0.000958500774736344, w1=-0.02905514883143298\n",
      "Gradient Descent(272/399): loss=0.3738542326076414, w0=0.0009585618322301582, w1=-0.029103942611531774\n",
      "Gradient Descent(273/399): loss=0.37368449358810973, w0=0.0009586072433868491, w1=-0.02915258286983206\n",
      "Gradient Descent(274/399): loss=0.37351517589393435, w0=0.0009586371776302357, w1=-0.029201070716777515\n",
      "Gradient Descent(275/399): loss=0.37334627627347583, w0=0.0009586518032287184, w1=-0.029249407252505407\n",
      "Gradient Descent(276/399): loss=0.3731777915201603, w0=0.0009586512872982221, w1=-0.029297593566962654\n",
      "Gradient Descent(277/399): loss=0.37300971847176523, w0=0.0009586357958052776, w1=-0.029345630740020286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(278/399): loss=0.3728420540097143, w0=0.0009586054935702335, w1=-0.029393519841586314\n",
      "Gradient Descent(279/399): loss=0.3726747950583864, w0=0.0009585605442705974, w1=-0.02944126193171709\n",
      "Gradient Descent(280/399): loss=0.372507938584436, w0=0.0009585011104444984, w1=-0.02948885806072712\n",
      "Gradient Descent(281/399): loss=0.37234148159612424, w0=0.0009584273534942682, w1=-0.029536309269297414\n",
      "Gradient Descent(282/399): loss=0.37217542114266355, w0=0.000958339433690135, w1=-0.029583616588582332\n",
      "Gradient Descent(283/399): loss=0.37200975431356975, w0=0.0009582375101740272, w1=-0.029630781040315007\n",
      "Gradient Descent(284/399): loss=0.37184447823802985, w0=0.000958121740963481, w1=-0.029677803636911342\n",
      "Gradient Descent(285/399): loss=0.37167959008427653, w0=0.0009579922829556496, w1=-0.029724685381572608\n",
      "Gradient Descent(286/399): loss=0.3715150870589754, w0=0.0009578492919314079, w1=-0.029771427268386653\n",
      "Gradient Descent(287/399): loss=0.3713509664066218, w0=0.0009576929225595501, w1=-0.029818030282427764\n",
      "Gradient Descent(288/399): loss=0.371187225408948, w0=0.0009575233284010769, w1=-0.029864495399855195\n",
      "Gradient Descent(289/399): loss=0.3710238613843403, w0=0.0009573406619135662, w1=-0.029910823588010386\n",
      "Gradient Descent(290/399): loss=0.37086087168726667, w0=0.0009571450744556261, w1=-0.02995701580551287\n",
      "Gradient Descent(291/399): loss=0.37069825370771153, w0=0.0009569367162914259, w1=-0.03000307300235494\n",
      "Gradient Descent(292/399): loss=0.37053600487062405, w0=0.0009567157365953009, w1=-0.030048996119995022\n",
      "Gradient Descent(293/399): loss=0.3703741226353717, w0=0.0009564822834564285, w1=-0.03009478609144986\n",
      "Gradient Descent(294/399): loss=0.37021260449520516, w0=0.0009562365038835722, w1=-0.030140443841385454\n",
      "Gradient Descent(295/399): loss=0.37005144797673195, w0=0.0009559785438098905, w1=-0.030185970286206825\n",
      "Gradient Descent(296/399): loss=0.36989065063939797, w0=0.0009557085480978066, w1=-0.03023136633414659\n",
      "Gradient Descent(297/399): loss=0.36973021007497797, w0=0.0009554266605439378, w1=-0.030276632885352397\n",
      "Gradient Descent(298/399): loss=0.36957012390707566, w0=0.0009551330238840793, w1=-0.03032177083197319\n",
      "Gradient Descent(299/399): loss=0.36941038979063, w0=0.0009548277797982421, w1=-0.030366781058244394\n",
      "Gradient Descent(300/399): loss=0.36925100541143163, w0=0.0009545110689157405, w1=-0.030411664440571966\n",
      "Gradient Descent(301/399): loss=0.3690919684856455, w0=0.0009541830308203268, w1=-0.03045642184761536\n",
      "Gradient Descent(302/399): loss=0.3689332767593433, w0=0.0009538438040553714, w1=-0.030501054140369445\n",
      "Gradient Descent(303/399): loss=0.36877492800804257, w0=0.0009534935261290851, w1=-0.03054556217224534\n",
      "Gradient Descent(304/399): loss=0.368616920036252, w0=0.0009531323335197811, w1=-0.03058994678915024\n",
      "Gradient Descent(305/399): loss=0.3684592506770271, w0=0.0009527603616811749, w1=-0.030634208829566193\n",
      "Gradient Descent(306/399): loss=0.368301917791532, w0=0.0009523777450477193, w1=-0.030678349124627885\n",
      "Gradient Descent(307/399): loss=0.36814491926860615, w0=0.0009519846170399724, w1=-0.030722368498199443\n",
      "Gradient Descent(308/399): loss=0.36798825302434196, w0=0.0009515811100699965, w1=-0.030766267766950243\n",
      "Gradient Descent(309/399): loss=0.3678319170016665, w0=0.0009511673555467857, w1=-0.03081004774042977\n",
      "Gradient Descent(310/399): loss=0.36767590916993165, w0=0.0009507434838817205, w1=-0.030853709221141536\n",
      "Gradient Descent(311/399): loss=0.3675202275245095, w0=0.0009503096244940462, w1=-0.030897253004616047\n",
      "Gradient Descent(312/399): loss=0.3673648700863962, w0=0.0009498659058163753, w1=-0.030940679879482878\n",
      "Gradient Descent(313/399): loss=0.3672098349018196, w0=0.00094941245530021, w1=-0.03098399062754182\n",
      "Gradient Descent(314/399): loss=0.36705512004185636, w0=0.0009489493994214841, w1=-0.031027186023833156\n",
      "Gradient Descent(315/399): loss=0.36690072360205295, w0=0.0009484768636861219, w1=-0.03107026683670705\n",
      "Gradient Descent(316/399): loss=0.36674664370205395, w0=0.0009479949726356135, w1=-0.03111323382789207\n",
      "Gradient Descent(317/399): loss=0.3665928784852357, w0=0.0009475038498526029, w1=-0.031156087752562872\n",
      "Gradient Descent(318/399): loss=0.3664394261183459, w0=0.0009470036179664897, w1=-0.031198829359407033\n",
      "Gradient Descent(319/399): loss=0.36628628479115044, w0=0.0009464943986590407, w1=-0.03124145939069107\n",
      "Gradient Descent(320/399): loss=0.36613345271608344, w0=0.000945976312670011, w1=-0.03128397858232563\n",
      "Gradient Descent(321/399): loss=0.3659809281279057, w0=0.000945449479802774, w1=-0.03132638766392991\n",
      "Gradient Descent(322/399): loss=0.36582870928336536, w0=0.0009449140189299556, w1=-0.03136868735889523\n",
      "Gradient Descent(323/399): loss=0.36567679446086815, w0=0.0009443700479990758, w1=-0.03141087838444791\n",
      "Gradient Descent(324/399): loss=0.3655251819601486, w0=0.0009438176840381924, w1=-0.03145296145171129\n",
      "Gradient Descent(325/399): loss=0.36537387010195005, w0=0.0009432570431615482, w1=-0.03149493726576707\n",
      "Gradient Descent(326/399): loss=0.36522285722770764, w0=0.0009426882405752185, w1=-0.03153680652571588\n",
      "Gradient Descent(327/399): loss=0.3650721416992373, w0=0.0009421113905827594, w1=-0.03157856992473709\n",
      "Gradient Descent(328/399): loss=0.3649217218984295, w0=0.0009415266065908545, w1=-0.03162022815014794\n",
      "Gradient Descent(329/399): loss=0.3647715962269479, w0=0.0009409340011149591, w1=-0.03166178188346193\n",
      "Gradient Descent(330/399): loss=0.36462176310593286, w0=0.0009403336857849415, w1=-0.03170323180044651\n",
      "Gradient Descent(331/399): loss=0.36447222097570897, w0=0.0009397257713507196, w1=-0.031744578571180126\n",
      "Gradient Descent(332/399): loss=0.36432296829549915, w0=0.0009391103676878919, w1=-0.03178582286010849\n",
      "Gradient Descent(333/399): loss=0.36417400354314106, w0=0.0009384875838033624, w1=-0.031826965326100264\n",
      "Gradient Descent(334/399): loss=0.3640253252148091, w0=0.0009378575278409577, w1=-0.03186800662250206\n",
      "Gradient Descent(335/399): loss=0.3638769318247409, w0=0.000937220307087036, w1=-0.03190894739719276\n",
      "Gradient Descent(336/399): loss=0.3637288219049686, w0=0.0009365760279760868, w1=-0.03194978829263722\n",
      "Gradient Descent(337/399): loss=0.3635809940050525, w0=0.0009359247960963207, w1=-0.03199052994593934\n",
      "Gradient Descent(338/399): loss=0.363433446691821, w0=0.0009352667161952479, w1=-0.03203117298889449\n",
      "Gradient Descent(339/399): loss=0.36328617854911405, w0=0.0009346018921852456, w1=-0.032071718048041355\n",
      "Gradient Descent(340/399): loss=0.36313918817753016, w0=0.000933930427149112, w1=-0.032112165744713116\n",
      "Gradient Descent(341/399): loss=0.362992474194177, w0=0.0009332524233456083, w1=-0.0321525166950881\n",
      "Gradient Descent(342/399): loss=0.3628460352324283, w0=0.0009325679822149857, w1=-0.032192771510239776\n",
      "Gradient Descent(343/399): loss=0.36269986994168146, w0=0.0009318772043844982, w1=-0.03223293079618624\n",
      "Gradient Descent(344/399): loss=0.3625539769871209, w0=0.0009311801896739, w1=-0.03227299515393906\n",
      "Gradient Descent(345/399): loss=0.36240835504948565, w0=0.0009304770371009277, w1=-0.03231296517955158\n",
      "Gradient Descent(346/399): loss=0.3622630028248378, w0=0.0009297678448867642, w1=-0.032352841464166684\n",
      "Gradient Descent(347/399): loss=0.3621179190243381, w0=0.0009290527104614878, w1=-0.03239262459406399\n",
      "Gradient Descent(348/399): loss=0.3619731023740222, w0=0.0009283317304695013, w1=-0.03243231515070649\n",
      "Gradient Descent(349/399): loss=0.36182855161458305, w0=0.0009276050007749445, w1=-0.032471913710786726\n",
      "Gradient Descent(350/399): loss=0.3616842655011535, w0=0.0009268726164670874, w1=-0.03251142084627231\n",
      "Gradient Descent(351/399): loss=0.36154024280309566, w0=0.0009261346718657033, w1=-0.03255083712445106\n",
      "Gradient Descent(352/399): loss=0.36139648230379107, w0=0.0009253912605264238, w1=-0.03259016310797555\n",
      "Gradient Descent(353/399): loss=0.36125298280043544, w0=0.0009246424752460721, w1=-0.03262939935490714\n",
      "Gradient Descent(354/399): loss=0.36110974310383614, w0=0.0009238884080679762, w1=-0.03266854641875958\n",
      "Gradient Descent(355/399): loss=0.36096676203821315, w0=0.0009231291502872608, w1=-0.032707604848542036\n",
      "Gradient Descent(356/399): loss=0.3608240384410024, w0=0.000922364792456118, w1=-0.032746575188801696\n",
      "Gradient Descent(357/399): loss=0.3606815711626634, w0=0.0009215954243890561, w1=-0.03278545797966588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(358/399): loss=0.3605393590664885, w0=0.0009208211351681256, w1=-0.032824253756883644\n",
      "Gradient Descent(359/399): loss=0.36039740102841633, w0=0.0009200420131481228, w1=-0.03286296305186698\n",
      "Gradient Descent(360/399): loss=0.36025569593684714, w0=0.0009192581459617712, w1=-0.0329015863917315\n",
      "Gradient Descent(361/399): loss=0.36011424269246184, w0=0.0009184696205248779, w1=-0.032940124299336696\n",
      "Gradient Descent(362/399): loss=0.3599730402080432, w0=0.0009176765230414684, w1=-0.032978577293325755\n",
      "Gradient Descent(363/399): loss=0.3598320874083007, w0=0.0009168789390088959, w1=-0.033016945888164916\n",
      "Gradient Descent(364/399): loss=0.3596913832296963, w0=0.0009160769532229273, w1=-0.033055230594182416\n",
      "Gradient Descent(365/399): loss=0.35955092662027544, w0=0.0009152706497828044, w1=-0.033093431917606976\n",
      "Gradient Descent(366/399): loss=0.35941071653949874, w0=0.0009144601120962807, w1=-0.03313155036060589\n",
      "Gradient Descent(367/399): loss=0.3592707519580767, w0=0.0009136454228846327, w1=-0.033169586421322686\n",
      "Gradient Descent(368/399): loss=0.3591310318578076, w0=0.000912826664187647, w1=-0.03320754059391437\n",
      "Gradient Descent(369/399): loss=0.3589915552314171, w0=0.0009120039173685807, w1=-0.03324541336858827\n",
      "Gradient Descent(370/399): loss=0.3588523210824014, w0=0.0009111772631190978, w1=-0.033283205231638496\n",
      "Gradient Descent(371/399): loss=0.35871332842487097, w0=0.0009103467814641786, w1=-0.033320916665481946\n",
      "Gradient Descent(372/399): loss=0.3585745762833987, w0=0.0009095125517670034, w1=-0.033358548148694\n",
      "Gradient Descent(373/399): loss=0.3584360636928696, w0=0.0009086746527338112, w1=-0.033396100156043766\n",
      "Gradient Descent(374/399): loss=0.35829778969833226, w0=0.0009078331624187302, w1=-0.03343357315852896\n",
      "Gradient Descent(375/399): loss=0.3581597533548528, w0=0.0009069881582285837, w1=-0.03347096762341045\n",
      "Gradient Descent(376/399): loss=0.3580219537273726, w0=0.0009061397169276688, w1=-0.033508284014246374\n",
      "Gradient Descent(377/399): loss=0.35788438989056565, w0=0.0009052879146425081, w1=-0.03354552279092591\n",
      "Gradient Descent(378/399): loss=0.35774706092870046, w0=0.0009044328268665748, w1=-0.033582684409702704\n",
      "Gradient Descent(379/399): loss=0.357609965935502, w0=0.000903574528464992, w1=-0.033619769323227924\n",
      "Gradient Descent(380/399): loss=0.35747310401401766, w0=0.0009027130936792034, w1=-0.03365677798058297\n",
      "Gradient Descent(381/399): loss=0.35733647427648396, w0=0.0009018485961316176, w1=-0.033693710827311804\n",
      "Gradient Descent(382/399): loss=0.35720007584419644, w0=0.0009009811088302263, w1=-0.03373056830545301\n",
      "Gradient Descent(383/399): loss=0.35706390784737974, w0=0.0009001107041731936, w1=-0.03376735085357143\n",
      "Gradient Descent(384/399): loss=0.35692796942506144, w0=0.0008992374539534191, w1=-0.03380405890678953\n",
      "Gradient Descent(385/399): loss=0.35679225972494744, w0=0.0008983614293630738, w1=-0.033840692896818396\n",
      "Gradient Descent(386/399): loss=0.35665677790329836, w0=0.0008974827009981079, w1=-0.03387725325198843\n",
      "Gradient Descent(387/399): loss=0.35652152312480934, w0=0.0008966013388627317, w1=-0.03391374039727972\n",
      "Gradient Descent(388/399): loss=0.3563864945624895, w0=0.0008957174123738696, w1=-0.03395015475435206\n",
      "Gradient Descent(389/399): loss=0.35625169139754576, w0=0.0008948309903655853, w1=-0.03398649674157472\n",
      "Gradient Descent(390/399): loss=0.3561171128192665, w0=0.000893942141093481, w1=-0.034022766774055836\n",
      "Gradient Descent(391/399): loss=0.35598275802490814, w0=0.0008930509322390679, w1=-0.034058965263671546\n",
      "Gradient Descent(392/399): loss=0.35584862621958263, w0=0.0008921574309141104, w1=-0.0340950926190948\n",
      "Gradient Descent(393/399): loss=0.35571471661614723, w0=0.0008912617036649418, w1=-0.03413114924582387\n",
      "Gradient Descent(394/399): loss=0.35558102843509565, w0=0.0008903638164767533, w1=-0.034167135546210585\n",
      "Gradient Descent(395/399): loss=0.3554475609044512, w0=0.0008894638347778549, w1=-0.03420305191948826\n",
      "Gradient Descent(396/399): loss=0.35531431325966123, w0=0.0008885618234439093, w1=-0.034238898761799336\n",
      "Gradient Descent(397/399): loss=0.35518128474349303, w0=0.0008876578468021384, w1=-0.03427467646622274\n",
      "Gradient Descent(398/399): loss=0.35504847460593203, w0=0.0008867519686355024, w1=-0.03431038542280096\n",
      "Gradient Descent(399/399): loss=0.35491588210408087, w0=0.0008858442521868509, w1=-0.03434602601856687\n",
      "++++ gamma = 0.00193069772888\n",
      "ciaociaociao\n",
      "Gradient Descent(0/399): loss=0.4999999999999887, w0=-1.32971637681253e-06, w1=-0.0006921790679257304\n",
      "Gradient Descent(1/399): loss=0.4950784855981547, w0=-3.3999368549629226e-07, w1=-0.001364102101453097\n",
      "Gradient Descent(2/399): loss=0.4904627189325216, w0=2.750487952348317e-06, w1=-0.002016630983260504\n",
      "Gradient Descent(3/399): loss=0.4861270266978452, w0=7.739962651971102e-06, w1=-0.002650583050714501\n",
      "Gradient Descent(4/399): loss=0.4820482142407507, w0=1.4442244655716491e-05, w1=-0.003266733780820366\n",
      "Gradient Descent(5/399): loss=0.47820530097893144, w0=2.2685511380033788e-05, w1=-0.003865819286058249\n",
      "Gradient Descent(6/399): loss=0.47457928698600604, w0=3.231119179006695e-05, w1=-0.004448538636865886\n",
      "Gradient Descent(7/399): loss=0.47115294661033874, w0=4.317294962599386e-05, w1=-0.005015556024991112\n",
      "Gradient Descent(8/399): loss=0.46791064562465523, w0=5.513575218969709e-05, w1=-0.005567502780568956\n",
      "Gradient Descent(9/399): loss=0.4648381789223762, w0=6.80750164380367e-05, w1=-0.006104979254558591\n",
      "Gradient Descent(10/399): loss=0.4619226262074881, w0=8.187582503917469e-05, w1=-0.006628556577087541\n",
      "Gradient Descent(11/399): loss=0.4591522234842621, w0=9.643220584768712e-05, w1=-0.007138778301278549\n",
      "Gradient Descent(12/399): loss=0.45651624845458777, w0=0.00011164646895697103, w1=-0.007636161941265093\n",
      "Gradient Descent(13/399): loss=0.45400491818465233, w0=0.00012742859610615357, w1=-0.008121200412322614\n",
      "Gradient Descent(14/399): loss=0.4516092976177343, w0=0.00014369567776412014, w1=-0.008594363380343946\n",
      "Gradient Descent(15/399): loss=0.4493212176927255, w0=0.0001603713936947371, w1=-0.009056098527259816\n",
      "Gradient Descent(16/399): loss=0.4471332019842055, w0=0.00017738553323294872, w1=-0.0095068327384408\n",
      "Gradient Descent(17/399): loss=0.4450384009138493, w0=0.0001946735518782051, w1=-0.00994697321760856\n",
      "Gradient Descent(18/399): loss=0.44303053269831133, w0=0.00021217616114572462, w1=-0.010376908534325486\n",
      "Gradient Descent(19/399): loss=0.4411038302984326, w0=0.0002298389489127061, w1=-0.010797009608717415\n",
      "Gradient Descent(20/399): loss=0.43925299372106763, w0=0.00024761202776038896, w1=-0.011207630637709168\n",
      "Gradient Descent(21/399): loss=0.43747314710006185, w0=0.0002654497090478165, w1=-0.011609109966713\n",
      "Gradient Descent(22/399): loss=0.43575980004853304, w0=0.00028331020066277345, w1=-0.012001770910401808\n",
      "Gradient Descent(23/399): loss=0.4341088128320536, w0=0.00030115532658268207, w1=-0.012385922525918969\n",
      "Gradient Descent(24/399): loss=0.43251636496269064, w0=0.0003189502665458975, w1=-0.012761860341621863\n",
      "Gradient Descent(25/399): loss=0.43097892685815786, w0=0.00033666331428415156, w1=-0.013129867044223968\n",
      "Gradient Descent(26/399): loss=0.42949323424934127, w0=0.00035426565290187486, w1=-0.013490213126988608\n",
      "Gradient Descent(27/399): loss=0.42805626505390537, w0=0.0003717311461095429, w1=-0.013843157501433853\n",
      "Gradient Descent(28/399): loss=0.42666521846413696, w0=0.00038903614412758296, w1=-0.014188948074831002\n",
      "Gradient Descent(29/399): loss=0.42531749602414604, w0=0.0004061593031761031, w1=-0.014527822295616838\n",
      "Gradient Descent(30/399): loss=0.42401068449546964, w0=0.0004230814175549423, w1=-0.014860007668690969\n",
      "Gradient Descent(31/399): loss=0.42274254033135067, w0=0.0004397852633993358, w1=-0.015185722242432916\n",
      "Gradient Descent(32/399): loss=0.4215109755988594, w0=0.0004562554532697574, w1=-0.01550517506914781\n",
      "Gradient Descent(33/399): loss=0.4203140452048212, w0=0.00047247830080104106, w1=-0.015818566640533816\n",
      "Gradient Descent(34/399): loss=0.41914993529648803, w0=0.000488441694696414, w1=-0.016126089299657646\n",
      "Gradient Descent(35/399): loss=0.4180169527212472, w0=0.0005041349814072141, w1=-0.01642792763082599\n",
      "Gradient Descent(36/399): loss=0.4169135154415642, w0=0.0005195488558893655, w1=-0.01672425882864969\n",
      "Gradient Descent(37/399): loss=0.415838143812005, w0=0.0005346752598736484, w1=-0.01701525304751317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(38/399): loss=0.414789452634685, w0=0.0005495072871288389, w1=-0.017301073732583792\n",
      "Gradient Descent(39/399): loss=0.41376614391799527, w0=0.000564039095235311, w1=-0.017581877933423345\n",
      "Gradient Descent(40/399): loss=0.41276700027106755, w0=0.00057826582342202, w1=-0.01785781660119695\n",
      "Gradient Descent(41/399): loss=0.4117908788732423, w0=0.0005921835160522308, w1=-0.018129034870412224\n",
      "Gradient Descent(42/399): loss=0.4108367059639086, w0=0.00060578905137318, w1=-0.018395672326063737\n",
      "Gradient Descent(43/399): loss=0.40990347180355047, w0=0.0006190800751723245, w1=-0.018657863257003804\n",
      "Gradient Descent(44/399): loss=0.40899022606172947, w0=0.0006320549390081273, w1=-0.018915736896310563\n",
      "Gradient Descent(45/399): loss=0.4080960735921316, w0=0.0006447126427066744, w1=-0.019169417649377435\n",
      "Gradient Descent(46/399): loss=0.40722017055874904, w0=0.0006570527808369652, w1=-0.019419025310404493\n",
      "Gradient Descent(47/399): loss=0.406361720880802, w0=0.0006690754928976398, w1=-0.01966467526793152\n",
      "Gradient Descent(48/399): loss=0.40551997296718945, w0=0.0006807814169663328, w1=-0.01990647870001451\n",
      "Gradient Descent(49/399): loss=0.40469421671410605, w0=0.0006921716465798967, w1=-0.02014454275961181\n",
      "Gradient Descent(50/399): loss=0.40388378074202996, w0=0.000703247690629552, w1=-0.020378970750712842\n",
      "Gradient Descent(51/399): loss=0.40308802985059133, w0=0.0007140114360696706, w1=-0.020609862295711268\n",
      "Gradient Descent(52/399): loss=0.402306362671901, w0=0.0007244651132525051, w1=-0.020837313494495226\n",
      "Gradient Descent(53/399): loss=0.4015382095047887, w0=0.0007346112637138007, w1=-0.021061417075700088\n",
      "Gradient Descent(54/399): loss=0.4007830303140704, w0=0.0007444527102459624, w1=-0.02128226254054348\n",
      "Gradient Descent(55/399): loss=0.4000403128804818, w0=0.0007539925291063586, w1=-0.021499936299638363\n",
      "Gradient Descent(56/399): loss=0.39930957108826776, w0=0.0007632340242184889, w1=-0.021714521803157413\n",
      "Gradient Descent(57/399): loss=0.3985903433386467, w0=0.0007721807032331854, w1=-0.02192609966470082\n",
      "Gradient Descent(58/399): loss=0.397882191078471, w0=0.000780836255325815, w1=-0.02213474777919973\n",
      "Gradient Descent(59/399): loss=0.39718469743439555, w0=0.000789204530613636, w1=-0.022340541435168875\n",
      "Gradient Descent(60/399): loss=0.396497465943772, w0=0.0007972895210850994, w1=-0.022543553421604464\n",
      "Gradient Descent(61/399): loss=0.3958201193742849, w0=0.0008050953429400022, w1=-0.0227438541298068\n",
      "Gradient Descent(62/399): loss=0.3951522986250833, w0=0.0008126262202460361, w1=-0.022941511650391676\n",
      "Gradient Descent(63/399): loss=0.3944936617028163, w0=0.0008198864698234698, w1=-0.023136591865739877\n",
      "Gradient Descent(64/399): loss=0.3938438827665806, w0=0.0008268804872754797, w1=-0.023329158538120533\n",
      "Gradient Descent(65/399): loss=0.39320265123632026, w0=0.0008336127340870436, w1=-0.023519273393710908\n",
      "Gradient Descent(66/399): loss=0.39256967095970924, w0=0.0008400877257203435, w1=-0.02370699620272322\n",
      "Gradient Descent(67/399): loss=0.39194465943298606, w0=0.0008463100206393358, w1=-0.02389238485583746\n",
      "Gradient Descent(68/399): loss=0.39132734707160677, w0=0.0008522842102005379, w1=-0.0240754954371284\n",
      "Gradient Descent(69/399): loss=0.3907174765269411, w0=0.0008580149093511904, w1=-0.024256382293664794\n",
      "Gradient Descent(70/399): loss=0.3901148020455685, w0=0.000863506748079792, w1=-0.024435098101949132\n",
      "Gradient Descent(71/399): loss=0.3895190888680172, w0=0.0008687643635675929, w1=-0.024611693931357162\n",
      "Gradient Descent(72/399): loss=0.38893011266406663, w0=0.0008737923929929869, w1=-0.02478621930472801\n",
      "Gradient Descent(73/399): loss=0.3883476590019689, w0=0.0008785954669438792, w1=-0.024958722256247425\n",
      "Gradient Descent(74/399): loss=0.38777152284917027, w0=0.0008831782033960405, w1=-0.025129249386759263\n",
      "Gradient Descent(75/399): loss=0.38720150810231013, w0=0.0008875452022182004, w1=-0.025297845916632946\n",
      "Gradient Descent(76/399): loss=0.38663742714446303, w0=0.0008917010401672, w1=-0.025464555736307987\n",
      "Gradient Descent(77/399): loss=0.38607910042774435, w0=0.0008956502663389236, w1=-0.02562942145463016\n",
      "Gradient Descent(78/399): loss=0.38552635607956276, w0=0.000899397398042974, w1=-0.025792484445087867\n",
      "Gradient Descent(79/399): loss=0.3849790295309298, w0=0.0009029469170711583, w1=-0.025953784890051543\n",
      "Gradient Descent(80/399): loss=0.38443696316536646, w0=0.0009063032663318133, w1=-0.02611336182311351\n",
      "Gradient Descent(81/399): loss=0.38390000598706125, w0=0.0009094708468238427, w1=-0.0262712531696206\n",
      "Gradient Descent(82/399): loss=0.3833680133070324, w0=0.000912454014926056, w1=-0.026427495785487035\n",
      "Gradient Descent(83/399): loss=0.3828408464461485, w0=0.0009152570799790086, w1=-0.02658212549437047\n",
      "Gradient Descent(84/399): loss=0.38231837245394523, w0=0.0009178843021380522, w1=-0.02673517712328983\n",
      "Gradient Descent(85/399): loss=0.38180046384225186, w0=0.0009203398904777108, w1=-0.026886684536759433\n",
      "Gradient Descent(86/399): loss=0.38128699833271884, w0=0.0009226280013288182, w1=-0.02703668066951014\n",
      "Gradient Descent(87/399): loss=0.38077785861740376, w0=0.0009247527368310877, w1=-0.027185197557864447\n",
      "Gradient Descent(88/399): loss=0.38027293213162566, w0=0.0009267181436849383, w1=-0.027332266369829227\n",
      "Gradient Descent(89/399): loss=0.37977211083836604, w0=0.000928528212087483, w1=-0.027477917433966315\n",
      "Gradient Descent(90/399): loss=0.3792752910235355, w0=0.0009301868748385957, w1=-0.027622180267098242\n",
      "Gradient Descent(91/399): loss=0.3787823731014764, w0=0.0009316980066039197, w1=-0.027765083600903382\n",
      "Gradient Descent(92/399): loss=0.37829326143011516, w0=0.000933065423322564, w1=-0.027906655407452057\n",
      "Gradient Descent(93/399): loss=0.37780786413521733, w0=0.000934292881748065, w1=-0.028046922923732544\n",
      "Gradient Descent(94/399): loss=0.3773260929432315, w0=0.0009353840791119603, w1=-0.028185912675213396\n",
      "Gradient Descent(95/399): loss=0.37684786302224976, w0=0.0009363426529000538, w1=-0.028323650498486213\n",
      "Gradient Descent(96/399): loss=0.37637309283063614, w0=0.0009371721807321194, w1=-0.028460161563030724\n",
      "Gradient Descent(97/399): loss=0.37590170397290334, w0=0.0009378761803364317, w1=-0.028595470392141983\n",
      "Gradient Descent(98/399): loss=0.37543362106245337, w0=0.0009384581096111032, w1=-0.028729600883057448\n",
      "Gradient Descent(99/399): loss=0.37496877159081077, w0=0.0009389213667647585, w1=-0.028862576326319857\n",
      "Gradient Descent(100/399): loss=0.37450708580300607, w0=0.0009392692905295992, w1=-0.028994419424410063\n",
      "Gradient Descent(101/399): loss=0.3740484965787896, w0=0.0009395051604403932, w1=-0.02912515230968217\n",
      "Gradient Descent(102/399): loss=0.37359293931937065, w0=0.0009396321971733778, w1=-0.02925479656163191\n",
      "Gradient Descent(103/399): loss=0.3731403518393969, w0=0.0009396535629394901, w1=-0.02938337322352748\n",
      "Gradient Descent(104/399): loss=0.37269067426391156, w0=0.0009395723619267291, w1=-0.029510902818430763\n",
      "Gradient Descent(105/399): loss=0.37224384893002704, w0=0.0009393916407868306, w1=-0.02963740536463544\n",
      "Gradient Descent(106/399): loss=0.3717998202930884, w0=0.0009391143891617756, w1=-0.029762900390547166\n",
      "Gradient Descent(107/399): loss=0.37135853483709463, w0=0.0009387435402459782, w1=-0.029887406949029888\n",
      "Gradient Descent(108/399): loss=0.37091994098917075, w0=0.0009382819713803013, w1=-0.030010943631241026\n",
      "Gradient Descent(109/399): loss=0.3704839890378908, w0=0.0009377325046743271, w1=-0.030133528579977347\n",
      "Gradient Descent(110/399): loss=0.3700506310552647, w0=0.0009370979076535763, w1=-0.030255179502552163\n",
      "Gradient Descent(111/399): loss=0.36961982082220757, w0=0.0009363808939286103, w1=-0.03037591368322356\n",
      "Gradient Descent(112/399): loss=0.36919151375732784, w0=0.0009355841238831848, w1=-0.030495747995192408\n",
      "Gradient Descent(113/399): loss=0.3687656668488739, w0=0.0009347102053788343, w1=-0.030614698912188053\n",
      "Gradient Descent(114/399): loss=0.3683422385896866, w0=0.0009337616944734661, w1=-0.030732782519658646\n",
      "Gradient Descent(115/399): loss=0.3679211889150175, w0=0.0009327410961517301, w1=-0.03085001452558238\n",
      "Gradient Descent(116/399): loss=0.36750247914307843, w0=0.0009316508650651035, w1=-0.030966410270915085\n",
      "Gradient Descent(117/399): loss=0.36708607191819087, w0=0.0009304934062797914, w1=-0.031081984739688876\n",
      "Gradient Descent(118/399): loss=0.36667193115641883, w0=0.0009292710760306942, w1=-0.031196752568775955\n",
      "Gradient Descent(119/399): loss=0.36626002199356433, w0=0.0009279861824798357, w1=-0.03131072805733092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(120/399): loss=0.36585031073542273, w0=0.0009266409864777743, w1=-0.031423925175924385\n",
      "Gradient Descent(121/399): loss=0.3654427648101869, w0=0.0009252377023266419, w1=-0.03153635757538008\n",
      "Gradient Descent(122/399): loss=0.36503735272290894, w0=0.0009237784985435718, w1=-0.03164803859532709\n",
      "Gradient Descent(123/399): loss=0.3646340440119197, w0=0.0009222654986233777, w1=-0.03175898127247833\n",
      "Gradient Descent(124/399): loss=0.36423280920712137, w0=0.0009207007817994491, w1=-0.03186919834864582\n",
      "Gradient Descent(125/399): loss=0.36383361979006523, w0=0.0009190863838019154, w1=-0.03197870227850294\n",
      "Gradient Descent(126/399): loss=0.36343644815573767, w0=0.0009174242976122212, w1=-0.0320875052371033\n",
      "Gradient Descent(127/399): loss=0.36304126757597355, w0=0.000915716474213329, w1=-0.032195619127165426\n",
      "Gradient Descent(128/399): loss=0.36264805216442697, w0=0.0009139648233348434, w1=-0.0323030555861322\n",
      "Gradient Descent(129/399): loss=0.3622567768430296, w0=0.0009121712141924171, w1=-0.03240982599301336\n",
      "Gradient Descent(130/399): loss=0.36186741730987027, w0=0.0009103374762208614, w1=-0.03251594147501917\n",
      "Gradient Descent(131/399): loss=0.3614799500084317, w0=0.0009084653998004462, w1=-0.03262141291399301\n",
      "Gradient Descent(132/399): loss=0.36109435209812446, w0=0.0009065567369759254, w1=-0.03272625095265022\n",
      "Gradient Descent(133/399): loss=0.3607106014260649, w0=0.0009046132021678744, w1=-0.03283046600063026\n",
      "Gradient Descent(134/399): loss=0.3603286765000354, w0=0.0009026364728759781, w1=-0.03293406824036894\n",
      "Gradient Descent(135/399): loss=0.3599485564625819, w0=0.0009006281903739432, w1=-0.033037067632797144\n",
      "Gradient Descent(136/399): loss=0.3595702210661941, w0=0.000898589960395755, w1=-0.03313947392287236\n",
      "Gradient Descent(137/399): loss=0.359193650649524, w0=0.000896523353813035, w1=-0.033241296644948744\n",
      "Gradient Descent(138/399): loss=0.35881882611459637, w0=0.0008944299073032859, w1=-0.03334254512799159\n",
      "Gradient Descent(139/399): loss=0.3584457289049683, w0=0.0008923111240088472, w1=-0.03344322850064149\n",
      "Gradient Descent(140/399): loss=0.3580743409847965, w0=0.0008901684741864115, w1=-0.0335433556961335\n",
      "Gradient Descent(141/399): loss=0.3577046448187743, w0=0.0008880033958469774, w1=-0.033642935457076216\n",
      "Gradient Descent(142/399): loss=0.35733662335289773, w0=0.0008858172953861421, w1=-0.03374197634009565\n",
      "Gradient Descent(143/399): loss=0.3569702599960307, w0=0.0008836115482046576, w1=-0.03384048672034842\n",
      "Gradient Descent(144/399): loss=0.3566055386022269, w0=0.000881387499319197, w1=-0.03393847479590867\n",
      "Gradient Descent(145/399): loss=0.35624244345378464, w0=0.0008791464639632937, w1=-0.03403594859203297\n",
      "Gradient Descent(146/399): loss=0.3558809592449972, w0=0.0008768897281784386, w1=-0.03413291596530729\n",
      "Gradient Descent(147/399): loss=0.35552107106656994, w0=0.0008746185493953312, w1=-0.034229384607679846\n",
      "Gradient Descent(148/399): loss=0.35516276439067795, w0=0.0008723341570052986, w1=-0.03432536205038363\n",
      "Gradient Descent(149/399): loss=0.3548060250566367, w0=0.0008700377529219102, w1=-0.034420855667752236\n",
      "Gradient Descent(150/399): loss=0.35445083925715626, w0=0.0008677305121328239, w1=-0.034515872680932334\n",
      "Gradient Descent(151/399): loss=0.3540971935251575, w0=0.0008654135832419173, w1=-0.03461042016149628\n",
      "Gradient Descent(152/399): loss=0.3537450747211275, w0=0.0008630880890017605, w1=-0.03470450503495783\n",
      "Gradient Descent(153/399): loss=0.3533944700209861, w0=0.0008607551268365016, w1=-0.034798134084194285\n",
      "Gradient Descent(154/399): loss=0.3530453669044492, w0=0.0008584157693552415, w1=-0.03489131395277773\n",
      "Gradient Descent(155/399): loss=0.3526977531438606, w0=0.0008560710648559816, w1=-0.0349840511482185\n",
      "Gradient Descent(156/399): loss=0.35235161679347915, w0=0.0008537220378202379, w1=-0.035076352045123355\n",
      "Gradient Descent(157/399): loss=0.3520069461791967, w0=0.000851369689398416, w1=-0.03516822288827116\n",
      "Gradient Descent(158/399): loss=0.35166372988867256, w0=0.0008490149978860508, w1=-0.03525966979560851\n",
      "Gradient Descent(159/399): loss=0.3513219567618654, w0=0.0008466589191910181, w1=-0.035350698761167816\n",
      "Gradient Descent(160/399): loss=0.35098161588194543, w0=0.0008443023872918277, w1=-0.03544131565791009\n",
      "Gradient Descent(161/399): loss=0.3506426965665723, w0=0.0008419463146871141, w1=-0.03553152624049481\n",
      "Gradient Descent(162/399): loss=0.3503051883595227, w0=0.0008395915928364415, w1=-0.035621336147979\n",
      "Gradient Descent(163/399): loss=0.34996908102265334, w0=0.0008372390925925435, w1=-0.03571075090644757\n",
      "Gradient Descent(164/399): loss=0.34963436452818447, w0=0.0008348896646251206, w1=-0.035799775931577064\n",
      "Gradient Descent(165/399): loss=0.34930102905129223, w0=0.0008325441398363185, w1=-0.03588841653113466\n",
      "Gradient Descent(166/399): loss=0.34896906496299557, w0=0.0008302033297680143, w1=-0.03597667790741439\n",
      "Gradient Descent(167/399): loss=0.3486384628233268, w0=0.000827868027001037, w1=-0.03606456515961233\n",
      "Gradient Descent(168/399): loss=0.34830921337477183, w0=0.0008255390055464491, w1=-0.0361520832861426\n",
      "Gradient Descent(169/399): loss=0.3479813075359721, w0=0.0008232170212290202, w1=-0.03623923718689579\n",
      "Gradient Descent(170/399): loss=0.3476547363956743, w0=0.0008209028120630185, w1=-0.036326031665441544\n",
      "Gradient Descent(171/399): loss=0.34732949120692, w0=0.0008185970986204511, w1=-0.03641247143117674\n",
      "Gradient Descent(172/399): loss=0.3470055633814641, w0=0.0008163005843918809, w1=-0.036498561101421\n",
      "Gradient Descent(173/399): loss=0.3466829444844123, w0=0.0008140139561399489, w1=-0.0365843052034608\n",
      "Gradient Descent(174/399): loss=0.34636162622906963, w0=0.0008117378842457307, w1=-0.03666970817654377\n",
      "Gradient Descent(175/399): loss=0.34604160047199195, w0=0.0008094730230480526, w1=-0.03675477437382447\n",
      "Gradient Descent(176/399): loss=0.3457228592082287, w0=0.0008072200111758977, w1=-0.03683950806426304\n",
      "Gradient Descent(177/399): loss=0.34540539456675445, w0=0.0008049794718740231, w1=-0.03692391343447797\n",
      "Gradient Descent(178/399): loss=0.3450891988060753, w0=0.0008027520133219179, w1=-0.03700799459055425\n",
      "Gradient Descent(179/399): loss=0.34477426431000946, w0=0.0008005382289462228, w1=-0.03709175555980814\n",
      "Gradient Descent(180/399): loss=0.34446058358362885, w0=0.0007983386977267351, w1=-0.03717520029250975\n",
      "Gradient Descent(181/399): loss=0.34414814924936016, w0=0.0007961539844961196, w1=-0.037258332663564424\n",
      "Gradient Descent(182/399): loss=0.34383695404323406, w0=0.0007939846402334471, w1=-0.037341156474154244\n",
      "Gradient Descent(183/399): loss=0.34352699081128324, w0=0.0007918312023516778, w1=-0.037423675453340556\n",
      "Gradient Descent(184/399): loss=0.3432182525060741, w0=0.000789694194979208, w1=-0.037505893259628595\n",
      "Gradient Descent(185/399): loss=0.3429107321833759, w0=0.0007875741292355931, w1=-0.03758781348249522\n",
      "Gradient Descent(186/399): loss=0.34260442299895555, w0=0.000785471503501566, w1=-0.037669439643880746\n",
      "Gradient Descent(187/399): loss=0.34229931820549486, w0=0.000783386803683458, w1=-0.037750775199645745\n",
      "Gradient Descent(188/399): loss=0.3419954111496269, w0=0.0007813205034721373, w1=-0.03783182354099379\n",
      "Gradient Descent(189/399): loss=0.34169269526908363, w0=0.000779273064596573, w1=-0.03791258799586102\n",
      "Gradient Descent(190/399): loss=0.34139116408995307, w0=0.0007772449370721309, w1=-0.037993071830273346\n",
      "Gradient Descent(191/399): loss=0.3410908112240394, w0=0.0007752365594437097, w1=-0.03807327824967214\n",
      "Gradient Descent(192/399): loss=0.34079163036632354, w0=0.00077324835902382, w1=-0.038153210400209234\n",
      "Gradient Descent(193/399): loss=0.3404936152925203, w0=0.0007712807521257099, w1=-0.03823287137001202\n",
      "Gradient Descent(194/399): loss=0.3401967598567246, w0=0.0007693341442916362, w1=-0.03831226419041936\n",
      "Gradient Descent(195/399): loss=0.3399010579891492, w0=0.0007674089305163837, w1=-0.03839139183718907\n",
      "Gradient Descent(196/399): loss=0.3396065036939461, w0=0.0007655054954661259, w1=-0.03847025723167774\n",
      "Gradient Descent(197/399): loss=0.3393130910471077, w0=0.0007636242136927271, w1=-0.038548863241993477\n",
      "Gradient Descent(198/399): loss=0.33902081419444835, w0=0.0007617654498435763, w1=-0.03862721268412239\n",
      "Gradient Descent(199/399): loss=0.33872966734965937, w0=0.0007599295588670478, w1=-0.03870530832302932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(200/399): loss=0.3384396447924367, w0=0.0007581168862136783, w1=-0.03878315287373359\n",
      "Gradient Descent(201/399): loss=0.3381507408666773, w0=0.0007563277680331487, w1=-0.038860749002360265\n",
      "Gradient Descent(202/399): loss=0.3378629499787426, w0=0.0007545625313671601, w1=-0.03893809932716769\n",
      "Gradient Descent(203/399): loss=0.3375762665957844, w0=0.0007528214943382872, w1=-0.03901520641955169\n",
      "Gradient Descent(204/399): loss=0.33729068524413347, w0=0.000751104966334895, w1=-0.039092072805027155\n",
      "Gradient Descent(205/399): loss=0.3370062005077463, w0=0.0007494132481922006, w1=-0.03916870096418753\n",
      "Gradient Descent(206/399): loss=0.3367228070267081, w0=0.000747746632369561, w1=-0.03924509333364268\n",
      "Gradient Descent(207/399): loss=0.33644049949579, w0=0.000746105403124066, w1=-0.03932125230693575\n",
      "Gradient Descent(208/399): loss=0.3361592726630592, w0=0.0007444898366805148, w1=-0.03939718023543947\n",
      "Gradient Descent(209/399): loss=0.3358791213285387, w0=0.0007429002013978513, w1=-0.03947287942923241\n",
      "Gradient Descent(210/399): loss=0.3356000403429141, w0=0.0007413367579321336, w1=-0.03954835215795566\n",
      "Gradient Descent(211/399): loss=0.33532202460628835, w0=0.0007397997593961106, w1=-0.039623600651650444\n",
      "Gradient Descent(212/399): loss=0.3350450690669806, w0=0.000738289451515478, w1=-0.03969862710157709\n",
      "Gradient Descent(213/399): loss=0.334769168720366, w0=0.0007368060727818827, w1=-0.039773433661015774\n",
      "Gradient Descent(214/399): loss=0.3344943186077583, w0=0.0007353498546027459, w1=-0.03984802244604957\n",
      "Gradient Descent(215/399): loss=0.33422051381533185, w0=0.000733921021447971, w1=-0.03992239553633009\n",
      "Gradient Descent(216/399): loss=0.3339477494730801, w0=0.000732519790993604, w1=-0.03999655497582625\n",
      "Gradient Descent(217/399): loss=0.33367602075381186, w0=0.0007311463742625089, w1=-0.04007050277355648\n",
      "Gradient Descent(218/399): loss=0.33340532287218133, w0=0.0007298009757621237, w1=-0.040144240904304795\n",
      "Gradient Descent(219/399): loss=0.3331356510837538, w0=0.0007284837936193572, w1=-0.04021777130932115\n",
      "Gradient Descent(220/399): loss=0.33286700068410036, w0=0.0007271950197126889, w1=-0.04029109589700633\n",
      "Gradient Descent(221/399): loss=0.3325993670079276, w0=0.0007259348398015298, w1=-0.04036421654358187\n",
      "Gradient Descent(222/399): loss=0.332332745428234, w0=0.0007247034336529043, w1=-0.04043713509374527\n",
      "Gradient Descent(223/399): loss=0.3320671313554964, w0=0.0007235009751655084, w1=-0.04050985336131089\n",
      "Gradient Descent(224/399): loss=0.33180252023688495, w0=0.0007223276324912009, w1=-0.040582373129836834\n",
      "Gradient Descent(225/399): loss=0.3315389075555025, w0=0.0007211835681539823, w1=-0.040654696153238135\n",
      "Gradient Descent(226/399): loss=0.3312762888296516, w0=0.0007200689391665144, w1=-0.0407268241563866\n",
      "Gradient Descent(227/399): loss=0.3310146596121244, w0=0.0007189838971442333, w1=-0.0407987588356976\n",
      "Gradient Descent(228/399): loss=0.3307540154895178, w0=0.0007179285884171071, w1=-0.040870501859704096\n",
      "Gradient Descent(229/399): loss=0.3304943520815705, w0=0.0007169031541390877, w1=-0.040942054869618216\n",
      "Gradient Descent(230/399): loss=0.3302356650405215, w0=0.0007159077303953075, w1=-0.04101341947988065\n",
      "Gradient Descent(231/399): loss=0.3299779500504896, w0=0.0007149424483070658, w1=-0.041084597278698155\n",
      "Gradient Descent(232/399): loss=0.32972120282687556, w0=0.0007140074341346562, w1=-0.04115558982856942\n",
      "Gradient Descent(233/399): loss=0.3294654191157799, w0=0.0007131028093780767, w1=-0.04122639866679962\n",
      "Gradient Descent(234/399): loss=0.32921059469344266, w0=0.000712228690875671, w1=-0.0412970253060038\n",
      "Gradient Descent(235/399): loss=0.32895672536569903, w0=0.0007113851909007427, w1=-0.041367471234599495\n",
      "Gradient Descent(236/399): loss=0.32870380696745416, w0=0.0007105724172561863, w1=-0.04143773791728867\n",
      "Gradient Descent(237/399): loss=0.3284518353621729, w0=0.0007097904733671775, w1=-0.04150782679552937\n",
      "Gradient Descent(238/399): loss=0.3282008064413878, w0=0.0007090394583719628, w1=-0.04157773928799719\n",
      "Gradient Descent(239/399): loss=0.32795071612421905, w0=0.0007083194672107915, w1=-0.0416474767910369\n",
      "Gradient Descent(240/399): loss=0.327701560356915, w0=0.0007076305907130255, w1=-0.04171704067910433\n",
      "Gradient Descent(241/399): loss=0.32745333511240016, w0=0.000706972915682469, w1=-0.0417864323051989\n",
      "Gradient Descent(242/399): loss=0.3272060363898428, w0=0.0007063465249809546, w1=-0.04185565300128679\n",
      "Gradient Descent(243/399): loss=0.3269596602142326, w0=0.0007057514976102221, w1=-0.041924704078715166\n",
      "Gradient Descent(244/399): loss=0.3267142026359724, w0=0.0007051879087921281, w1=-0.04199358682861755\n",
      "Gradient Descent(245/399): loss=0.3264696597304824, w0=0.000704655830047219, w1=-0.04206230252231053\n",
      "Gradient Descent(246/399): loss=0.32622602759781444, w0=0.0007041553292717052, w1=-0.04213085241168206\n",
      "Gradient Descent(247/399): loss=0.325983302362281, w0=0.0007036864708128676, w1=-0.042199237729571494\n",
      "Gradient Descent(248/399): loss=0.3257414801720926, w0=0.0007032493155429311, w1=-0.04226745969014156\n",
      "Gradient Descent(249/399): loss=0.32550055719900595, w0=0.0007028439209314375, w1=-0.04233551948924245\n",
      "Gradient Descent(250/399): loss=0.325260529637984, w0=0.0007024703411161491, w1=-0.042403418304768146\n",
      "Gradient Descent(251/399): loss=0.3250213937068647, w0=0.0007021286269725129, w1=-0.042471157297005305\n",
      "Gradient Descent(252/399): loss=0.3247831456460394, w0=0.0007018188261817188, w1=-0.042538737608974676\n",
      "Gradient Descent(253/399): loss=0.32454578171814047, w0=0.0007015409832973789, w1=-0.04260616036676536\n",
      "Gradient Descent(254/399): loss=0.3243092982077376, w0=0.0007012951398108568, w1=-0.04267342667986198\n",
      "Gradient Descent(255/399): loss=0.3240736914210438, w0=0.0007010813342152775, w1=-0.04274053764146501\n",
      "Gradient Descent(256/399): loss=0.32383895768562704, w0=0.0007008996020682442, w1=-0.042807494328804305\n",
      "Gradient Descent(257/399): loss=0.3236050933501335, w0=0.0007007499760532885, w1=-0.042874297803446065\n",
      "Gradient Descent(258/399): loss=0.3233720947840144, w0=0.0007006324860400835, w1=-0.04294094911159336\n",
      "Gradient Descent(259/399): loss=0.3231399583772639, w0=0.0007005471591434413, w1=-0.04300744928438032\n",
      "Gradient Descent(260/399): loss=0.3229086805401613, w0=0.0007004940197811254, w1=-0.04307379933816019\n",
      "Gradient Descent(261/399): loss=0.32267825770302183, w0=0.000700473089730499, w1=-0.043140000274787316\n",
      "Gradient Descent(262/399): loss=0.3224486863159534, w0=0.0007004843881840356, w1=-0.043206053081893264\n",
      "Gradient Descent(263/399): loss=0.32221996284861953, w0=0.0007005279318037154, w1=-0.043271958733157145\n",
      "Gradient Descent(264/399): loss=0.3219920837900091, w0=0.00070060373477433, w1=-0.04333771818857031\n",
      "Gradient Descent(265/399): loss=0.321765045648212, w0=0.0007007118088557195, w1=-0.043403332394695515\n",
      "Gradient Descent(266/399): loss=0.321538844950199, w0=0.0007008521634339638, w1=-0.04346880228492069\n",
      "Gradient Descent(267/399): loss=0.3213134782416096, w0=0.0007010248055715492, w1=-0.043534128779707436\n",
      "Gradient Descent(268/399): loss=0.32108894208654415, w0=0.0007012297400565328, w1=-0.04359931278683432\n",
      "Gradient Descent(269/399): loss=0.3208652330673591, w0=0.0007014669694507253, w1=-0.04366435520163516\n",
      "Gradient Descent(270/399): loss=0.32064234778447187, w0=0.0007017364941369122, w1=-0.043729256907232325\n",
      "Gradient Descent(271/399): loss=0.32042028285616575, w0=0.0007020383123651337, w1=-0.04379401877476522\n",
      "Gradient Descent(272/399): loss=0.3201990349184022, w0=0.0007023724202980434, w1=-0.043858641663614056\n",
      "Gradient Descent(273/399): loss=0.3199786006246376, w0=0.0007027388120553637, w1=-0.04392312642161894\n",
      "Gradient Descent(274/399): loss=0.31975897664564296, w0=0.0007031374797574583, w1=-0.043987473885294476\n",
      "Gradient Descent(275/399): loss=0.31954015966932836, w0=0.0007035684135680388, w1=-0.04405168488003993\n",
      "Gradient Descent(276/399): loss=0.31932214640057277, w0=0.0007040316017360237, w1=-0.04411576022034504\n",
      "Gradient Descent(277/399): loss=0.319104933561056, w0=0.0007045270306365679, w1=-0.0441797007099916\n",
      "Gradient Descent(278/399): loss=0.31888851788909484, w0=0.000705054684811279, w1=-0.04424350714225092\n",
      "Gradient Descent(279/399): loss=0.3186728961394846, w0=0.0007056145470076381, w1=-0.044307180300077145\n",
      "Gradient Descent(280/399): loss=0.31845806508334046, w0=0.0007062065982176408, w1=-0.0443707209562967\n",
      "Gradient Descent(281/399): loss=0.31824402150794767, w0=0.0007068308177156751, w1=-0.04443412987379377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(282/399): loss=0.3180307622166088, w0=0.0007074871830956514, w1=-0.04449740780569204\n",
      "Gradient Descent(283/399): loss=0.3178182840284998, w0=0.0007081756703074003, w1=-0.044560555495532644\n",
      "Gradient Descent(284/399): loss=0.31760658377852563, w0=0.0007088962536923536, w1=-0.04462357367744855\n",
      "Gradient Descent(285/399): loss=0.3173956583171801, w0=0.0007096489060185223, w1=-0.044686463076335295\n",
      "Gradient Descent(286/399): loss=0.3171855045104089, w0=0.0007104335985147871, w1=-0.044749224408018325\n",
      "Gradient Descent(287/399): loss=0.31697611923947455, w0=0.0007112503009045152, w1=-0.044811858379416834\n",
      "Gradient Descent(288/399): loss=0.3167674994008254, w0=0.0007120989814385166, w1=-0.04487436568870434\n",
      "Gradient Descent(289/399): loss=0.31655964190596586, w0=0.0007129796069273538, w1=-0.04493674702546595\n",
      "Gradient Descent(290/399): loss=0.31635254368133, w0=0.0007138921427730193, w1=-0.04499900307085241\n",
      "Gradient Descent(291/399): loss=0.3161462016681572, w0=0.0007148365529999912, w1=-0.0450611344977311\n",
      "Gradient Descent(292/399): loss=0.3159406128223717, w0=0.000715812800285683, w1=-0.04512314197083389\n",
      "Gradient Descent(293/399): loss=0.31573577411446097, w0=0.0007168208459902968, w1=-0.045185026146902015\n",
      "Gradient Descent(294/399): loss=0.31553168252936054, w0=0.0007178606501860934, w1=-0.04524678767482806\n",
      "Gradient Descent(295/399): loss=0.3153283350663382, w0=0.0007189321716860927, w1=-0.04530842719579501\n",
      "Gradient Descent(296/399): loss=0.31512572873888156, w0=0.0007200353680722124, w1=-0.045369945343412564\n",
      "Gradient Descent(297/399): loss=0.314923860574587, w0=0.0007211701957228606, w1=-0.045431342743850606\n",
      "Gradient Descent(298/399): loss=0.3147227276150507, w0=0.0007223366098399896, w1=-0.04549262001597008\n",
      "Gradient Descent(299/399): loss=0.31452232691576254, w0=0.0007235345644756244, w1=-0.04555377777145115\n",
      "Gradient Descent(300/399): loss=0.3143226555460006, w0=0.0007247640125578762, w1=-0.045614816614918854\n",
      "Gradient Descent(301/399): loss=0.3141237105887277, w0=0.0007260249059164494, w1=-0.04567573714406618\n",
      "Gradient Descent(302/399): loss=0.3139254891404908, w0=0.000727317195307655, w1=-0.045736539949774675\n",
      "Gradient Descent(303/399): loss=0.31372798831132076, w0=0.0007286408304389388, w1=-0.04579722561623268\n",
      "Gradient Descent(304/399): loss=0.31353120522463485, w0=0.0007299957599929341, w1=-0.04585779472105114\n",
      "Gradient Descent(305/399): loss=0.31333513701713916, w0=0.0007313819316510502, w1=-0.04591824783537716\n",
      "Gradient Descent(306/399): loss=0.31313978083873584, w0=0.0007327992921166025, w1=-0.0459785855240052\n",
      "Gradient Descent(307/399): loss=0.31294513385242756, w0=0.0007342477871374988, w1=-0.04603880834548618\n",
      "Gradient Descent(308/399): loss=0.3127511932342273, w0=0.0007357273615284844, w1=-0.04609891685223429\n",
      "Gradient Descent(309/399): loss=0.31255795617306714, w0=0.0007372379591929607, w1=-0.04615891159063173\n",
      "Gradient Descent(310/399): loss=0.31236541987070937, w0=0.0007387795231443817, w1=-0.04621879310113138\n",
      "Gradient Descent(311/399): loss=0.31217358154165975, w0=0.0007403519955272391, w1=-0.0462785619183574\n",
      "Gradient Descent(312/399): loss=0.3119824384130793, w0=0.000741955317637643, w1=-0.046338218571203835\n",
      "Gradient Descent(313/399): loss=0.31179198772470157, w0=0.0007435894299435075, w1=-0.04639776358293129\n",
      "Gradient Descent(314/399): loss=0.3116022267287469, w0=0.0007452542721043477, w1=-0.04645719747126166\n",
      "Gradient Descent(315/399): loss=0.3114131526898414, w0=0.0007469497829906974, w1=-0.04651652074847104\n",
      "Gradient Descent(316/399): loss=0.31122476288493406, w0=0.0007486759007031529, w1=-0.04657573392148073\n",
      "Gradient Descent(317/399): loss=0.31103705460321807, w0=0.0007504325625910533, w1=-0.0466348374919465\n",
      "Gradient Descent(318/399): loss=0.31085002514605076, w0=0.0007522197052708015, w1=-0.046693831956346084\n",
      "Gradient Descent(319/399): loss=0.3106636718268757, w0=0.0007540372646438352, w1=-0.04675271780606493\n",
      "Gradient Descent(320/399): loss=0.3104779919711461, w0=0.0007558851759142537, w1=-0.0468114955274803\n",
      "Gradient Descent(321/399): loss=0.3102929829162483, w0=0.0007577633736061079, w1=-0.04687016560204369\n",
      "Gradient Descent(322/399): loss=0.31010864201142774, w0=0.0007596717915803597, w1=-0.046928728506361685\n",
      "Gradient Descent(323/399): loss=0.3099249666177142, w0=0.0007616103630515178, w1=-0.04698718471227514\n",
      "Gradient Descent(324/399): loss=0.309741954107849, w0=0.000763579020603956, w1=-0.047045534686936924\n",
      "Gradient Descent(325/399): loss=0.30955960186621384, w0=0.0007655776962079202, w1=-0.04710377889288805\n",
      "Gradient Descent(326/399): loss=0.3093779072887582, w0=0.0007676063212352306, w1=-0.04716191778813239\n",
      "Gradient Descent(327/399): loss=0.30919686778293054, w0=0.0007696648264746844, w1=-0.04721995182620991\n",
      "Gradient Descent(328/399): loss=0.3090164807676076, w0=0.0007717531421471659, w1=-0.04727788145626847\n",
      "Gradient Descent(329/399): loss=0.30883674367302705, w0=0.000773871197920469, w1=-0.04733570712313424\n",
      "Gradient Descent(330/399): loss=0.3086576539407194, w0=0.0007760189229238376, w1=-0.04739342926738076\n",
      "Gradient Descent(331/399): loss=0.30847920902344095, w0=0.0007781962457622298, w1=-0.04745104832539666\n",
      "Gradient Descent(332/399): loss=0.3083014063851073, w0=0.000780403094530312, w1=-0.04750856472945206\n",
      "Gradient Descent(333/399): loss=0.3081242435007289, w0=0.0007826393968261862, w1=-0.04756597890776369\n",
      "Gradient Descent(334/399): loss=0.30794771785634606, w0=0.000784905079764858, w1=-0.047623291284558784\n",
      "Gradient Descent(335/399): loss=0.3077718269489649, w0=0.0007872000699914486, w1=-0.047680502280137683\n",
      "Gradient Descent(336/399): loss=0.30759656828649456, w0=0.0007895242936941566, w1=-0.047737612310935304\n",
      "Gradient Descent(337/399): loss=0.30742193938768436, w0=0.0007918776766169744, w1=-0.047794621789581375\n",
      "Gradient Descent(338/399): loss=0.3072479377820625, w0=0.0007942601440721643, w1=-0.04785153112495955\n",
      "Gradient Descent(339/399): loss=0.3070745610098744, w0=0.0007966716209524975, w1=-0.047908340722265394\n",
      "Gradient Descent(340/399): loss=0.3069018066220227, w0=0.0007991120317432625, w1=-0.04796505098306321\n",
      "Gradient Descent(341/399): loss=0.306729672180008, w0=0.000801581300534046, w1=-0.048021662305341856\n",
      "Gradient Descent(342/399): loss=0.30655815525586816, w0=0.0008040793510302922, w1=-0.04807817508356944\n",
      "Gradient Descent(343/399): loss=0.30638725343212114, w0=0.0008066061065646418, w1=-0.04813458970874699\n",
      "Gradient Descent(344/399): loss=0.3062169643017063, w0=0.0008091614901080593, w1=-0.048190906568461145\n",
      "Gradient Descent(345/399): loss=0.30604728546792737, w0=0.0008117454242807478, w1=-0.048247126046935776\n",
      "Gradient Descent(346/399): loss=0.3058782145443956, w0=0.0008143578313628603, w1=-0.048303248525082686\n",
      "Gradient Descent(347/399): loss=0.30570974915497356, w0=0.0008169986333050071, w1=-0.04835927438055132\n",
      "Gradient Descent(348/399): loss=0.3055418869337191, w0=0.0008196677517385657, w1=-0.04841520398777758\n",
      "Gradient Descent(349/399): loss=0.3053746255248309, w0=0.0008223651079857971, w1=-0.04847103771803164\n",
      "Gradient Descent(350/399): loss=0.305207962582593, w0=0.0008250906230697703, w1=-0.048526775939464965\n",
      "Gradient Descent(351/399): loss=0.3050418957713211, w0=0.0008278442177241001, w1=-0.04858241901715635\n",
      "Gradient Descent(352/399): loss=0.30487642276530963, w0=0.0008306258124025025, w1=-0.04863796731315716\n",
      "Gradient Descent(353/399): loss=0.3047115412487773, w0=0.0008334353272881685, w1=-0.048693421186535704\n",
      "Gradient Descent(354/399): loss=0.304547248915816, w0=0.000836272682302963, w1=-0.04874878099342079\n",
      "Gradient Descent(355/399): loss=0.30438354347033697, w0=0.0008391377971164497, w1=-0.04880404708704442\n",
      "Gradient Descent(356/399): loss=0.3042204226260208, w0=0.0008420305911547456, w1=-0.048859219817783774\n",
      "Gradient Descent(357/399): loss=0.30405788410626555, w0=0.0008449509836092108, w1=-0.04891429953320232\n",
      "Gradient Descent(358/399): loss=0.3038959256441357, w0=0.0008478988934449724, w1=-0.04896928657809024\n",
      "Gradient Descent(359/399): loss=0.3037345449823122, w0=0.0008508742394092899, w1=-0.04902418129450408\n",
      "Gradient Descent(360/399): loss=0.30357373987304276, w0=0.000853876940039762, w1=-0.049078984021805626\n",
      "Gradient Descent(361/399): loss=0.3034135080780922, w0=0.0008569069136723802, w1=-0.049133695096700125\n",
      "Gradient Descent(362/399): loss=0.3032538473686931, w0=0.0008599640784494293, w1=-0.04918831485327379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(363/399): loss=0.3030947555254984, w0=0.0008630483523272411, w1=-0.04924284362303056\n",
      "Gradient Descent(364/399): loss=0.30293623033853156, w0=0.0008661596530838006, w1=-0.04929728173492826\n",
      "Gradient Descent(365/399): loss=0.3027782696071403, w0=0.0008692978983262103, w1=-0.04935162951541407\n",
      "Gradient Descent(366/399): loss=0.3026208711399481, w0=0.0008724630054980133, w1=-0.049405887288459294\n",
      "Gradient Descent(367/399): loss=0.30246403275480815, w0=0.0008756548918863788, w1=-0.0494600553755936\n",
      "Gradient Descent(368/399): loss=0.3023077522787555, w0=0.0008788734746291529, w1=-0.04951413409593854\n",
      "Gradient Descent(369/399): loss=0.3021520275479624, w0=0.0008821186707217758, w1=-0.04956812376624052\n",
      "Gradient Descent(370/399): loss=0.30199685640769053, w0=0.0008853903970240696, w1=-0.04962202470090313\n",
      "Gradient Descent(371/399): loss=0.30184223671224764, w0=0.0008886885702668972, w1=-0.049675837212018924\n",
      "Gradient Descent(372/399): loss=0.3016881663249404, w0=0.0008920131070586975, w1=-0.04972956160940061\n",
      "Gradient Descent(373/399): loss=0.30153464311803085, w0=0.0008953639238918955, w1=-0.049783198200611674\n",
      "Gradient Descent(374/399): loss=0.3013816649726914, w0=0.0008987409371491932, w1=-0.04983674729099646\n",
      "Gradient Descent(375/399): loss=0.301229229778961, w0=0.0009021440631097408, w1=-0.0498902091837097\n",
      "Gradient Descent(376/399): loss=0.3010773354357008, w0=0.0009055732179551922, w1=-0.04994358417974554\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-3951966e466c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0;34m[\u001b[0m\u001b[0mbest_deg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma_best\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_max\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mcross_validation_GD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_single_jet_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx_single_jet_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_fold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgammas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mtx_single_jet_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx_single_jet_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen_init_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx_single_jet_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtx_single_jet_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_deg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-5bc3a3ad1721>\u001b[0m in \u001b[0;36mcross_validation_GD\u001b[0;34m(y, tx, k_fold, max_iters, degs, lambdas)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0minitial_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_tx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mleast_squares_GD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_tx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msingle_gamma_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfct\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mse'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0;31m# Compute the predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/matteociprian/Documents/GitHub/LMO_ML/project1/A_final/lib/implementations.py\u001b[0m in \u001b[0;36mleast_squares_GD\u001b[0;34m(y, tx, initial_w, max_iters, gamma, fct)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcosts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mgradLw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcosts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgradLw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m# store w and loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/matteociprian/Documents/GitHub/LMO_ML/project1/A_final/lib/costs.py\u001b[0m in \u001b[0;36mcompute_gradient\u001b[0;34m(y, tx, w, fct)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfct\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'mse'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mfct\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'mae'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "OUT_FOLDER = 'output/'\n",
    "name = 'Gradient_descent.csv'\n",
    "degs=range(4,8)\n",
    "gammas=np.logspace(-4,-1,8);\n",
    "k_fold=5\n",
    "max_iters=400\n",
    "\n",
    "\n",
    "mask_jets_train = split_jets_mask(tx_train)\n",
    "mask_jets_test = split_jets_mask(tx_test)\n",
    "len_mask = len(mask_jets_train)\n",
    "\n",
    "y_preds_train = np.zeros(len(y_train))\n",
    "y_preds_test = np.zeros(tx_test.shape[0])\n",
    "\n",
    "\n",
    "\n",
    "for mask_jet_id in range(len_mask):\n",
    "    print('**** Starting Jet ', mask_jet_id, '*****')\n",
    "    tx_single_jet_train = tx_train[mask_jets_train[mask_jet_id]]\n",
    "    tx_single_jet_test =  tx_test[mask_jets_test[mask_jet_id]]\n",
    "    y_single_jet_train =   y_train[mask_jets_train[mask_jet_id]]\n",
    "    \n",
    "    \n",
    "    \n",
    "    [best_deg, gamma_best, acc_max] =cross_validation_GD(y_single_jet_train, tx_single_jet_train, k_fold, max_iters, degs,gammas)\n",
    "    tx_single_jet_train, tx_single_jet_test,len_init_data = prepare_data(tx_single_jet_train,tx_single_jet_test, best_deg)\n",
    "    \n",
    "    initial_w = np.zeros(tx_single_jet_train.shape[1]);\n",
    "    w, loss = func_GD (y=y_single_jet_train, tx=tx_single_jet_train, test_set=tx_single_jet_test, max_iters=max_iters,gamma=best_gamma, initial_w=initial_w);\n",
    "    \n",
    "    y_pred_train = predict_labels(w, tx_single_jet_train)\n",
    "    y_pred_test = predict_labels(w, tx_single_jet_test)\n",
    "    y_preds_train[mask_jets_train[mask_jet_id]] = y_pred_train\n",
    "    y_preds_test[mask_jets_test[mask_jet_id]] = y_pred_test\n",
    "    \n",
    "    right_train = np.sum(y_single_jet_train == y_pred_train)/len(y_single_jet_train )*100\n",
    "    \n",
    "    print('**** Jet ', mask_jet_id, '*****accuracy jet',right_train)\n",
    "    \n",
    "\n",
    "create_csv_submission(ids_test, y_preds_test, OUT_FOLDER+name)\n",
    "right_train_total = np.sum(y_train == y_preds_train)/len(y_train)*100\n",
    "\n",
    "print('>>>>>>>> Accuracy TOTAL ',right_train_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "deg=5;\n",
    "lambda_=1e-5\n",
    "mask_jets_train = split_jets_mask(tx_train)\n",
    "mask_jets_test = split_jets_mask(tx_test)\n",
    "len_mask = len(mask_jets_train)\n",
    "for mask_jet_id in range(len_mask):\n",
    "    print('**** Starting Jet ', mask_jet_id, '*****')\n",
    "    tx_single_jet_train = tx_train[mask_jets_train[mask_jet_id]]\n",
    "    tx_single_jet_test =  tx_test[mask_jets_test[mask_jet_id]]\n",
    "    y_single_jet_train =   y_train[mask_jets_train[mask_jet_id]]\n",
    "    tx_single_jet_train, tx_single_jet_test, len_init = prepare_data(tx_single_jet_train, tx_single_jet_test, deg);\n",
    "    \n",
    "    [w,loss]=func_ridge_regression (y=y_single_jet_train, tx=tx_single_jet_train, test_set=tx_single_jet_test, lambda_=lambda_);    \n",
    "    \n",
    "    y_pred_train = predict_labels(w, tx_single_jet_train)\n",
    "    y_pred_test = predict_labels(w, tx_single_jet_test)\n",
    "    y_preds_train[mask_jets_train[mask_jet_id]] = y_pred_train\n",
    "    y_preds_test[mask_jets_test[mask_jet_id]] = y_pred_test\n",
    "    \n",
    "    right_train = np.sum(y_single_jet_train == y_pred_train)/len(y_single_jet_train )*100\n",
    "    \n",
    "    print('**** Jet ', mask_jet_id, '*****accuracy jet',right_train)\n",
    "    \n",
    "\n",
    "    \n",
    "right_train_total = np.sum(y_train == y_preds_train)/len(y_train )*100\n",
    "print('>>>>>>>> Accuracy TOTAL ',right_train_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##  SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def func_SGD (y, tx, test_set, max_iters, gamma, initial_w, batch_size):\n",
    "    name = 'Stochastic_Gradient_descent'\n",
    "    \n",
    "    w,loss = least_squares_SGD(y, tx, initial_w, max_iters,\\\n",
    "                               gamma, batch_size)\n",
    "    \n",
    "    y_pred = predict_labels(w, test_set)\n",
    "    create_csv_submission(ids_test, y_pred, OUT_FOLDER+name)\n",
    "    \n",
    "    plt.plot(w, 'go')\n",
    "    plt.title('SGD: weights with loss:  '+str(loss),fontsize=15, fontweight='bold');\n",
    "    plt.show()\n",
    "    \n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "deg=5;\n",
    "lambda_=1e-5\n",
    "mask_jets_train = split_jets_mask(tx_train)\n",
    "mask_jets_test = split_jets_mask(tx_test)\n",
    "len_mask = len(mask_jets_train)\n",
    "for mask_jet_id in range(len_mask):\n",
    "    print('**** Starting Jet ', mask_jet_id, '*****')\n",
    "    tx_single_jet_train = tx_train[mask_jets_train[mask_jet_id]]\n",
    "    tx_single_jet_test =  tx_test[mask_jets_test[mask_jet_id]]\n",
    "    y_single_jet_train =   y_train[mask_jets_train[mask_jet_id]]\n",
    "    tx_single_jet_train, tx_single_jet_test, len_init = prepare_data(tx_single_jet_train, tx_single_jet_test, deg);\n",
    "    \n",
    "    initial_w = np.zeros(tx_single_jet_train.shape[1]);\n",
    "    w, loss = func_SGD (y=y_single_jet_train, tx=tx_single_jet_train, test_set=tx_single_jet_test, max_iters=500,gamma=0.00005, initial_w=initial_w,\\\n",
    "                   batch_size=1);    \n",
    "   \n",
    "    y_pred_train = predict_labels(w, tx_single_jet_train)\n",
    "    y_pred_test = predict_labels(w, tx_single_jet_test)\n",
    "    y_preds_train[mask_jets_train[mask_jet_id]] = y_pred_train\n",
    "    y_preds_test[mask_jets_test[mask_jet_id]] = y_pred_test\n",
    "    \n",
    "    right_train = np.sum(y_single_jet_train == y_pred_train)/len(y_single_jet_train )*100\n",
    "    \n",
    "    print('**** Jet ', mask_jet_id, '*****accuracy jet',right_train)\n",
    "    \n",
    "\n",
    "    \n",
    "right_train_total = np.sum(y_train == y_preds_train)/len(y_train )*100\n",
    "print('>>>>>>>> Accuracy TOTAL ',right_train_total)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "def cross_validation_logistic_regression(y,tx, k_fold, max_iters, gammas):\n",
    "    \n",
    "    accuracies_test=np.zeros(len(gammas))\n",
    "    accuracies_train=np.zeros(len(gammas))\n",
    "    acc_max=0;\n",
    "    gamma_best=0;\n",
    "    \n",
    "    for i, single_gamma in enumerate(gammas):\n",
    "        \n",
    "        seed=1;\n",
    "\n",
    "        # get k'th subgroup in test, others in train\n",
    "        k_indices = build_k_indices(y, k_fold, seed)\n",
    "        accuracy_train = np.zeros(k_fold)\n",
    "        accuracy_test = np.zeros(k_fold)\n",
    "\n",
    "\n",
    "        for k in range(k_fold):\n",
    "            #print('----- FOLD', k, '-----')\n",
    "            k_index = k_indices[k]\n",
    "            test_y = y[k_index]\n",
    "            test_tx = tx[k_index,:]\n",
    "            mask = np.ones(len(y), dtype=bool) # set all elements to True\n",
    "            mask[k_index] = False              # set test elements to False\n",
    "            train_tx = tx[mask,:]              # select only True elements (ie train elements)\n",
    "            train_y = y[mask]\n",
    "            initial_w = np.zeros(train_tx.shape[1]);\n",
    "            weights,loss = logistic_regression(train_y, train_tx, initial_w, max_iters, single_gamma)\n",
    "            # Compute the predictions\n",
    "            y_pred_train = predict_labels(weights, train_tx)\n",
    "            y_pred_test = predict_labels(weights, test_tx);\n",
    "            accuracy_train[k] = np.sum(y_pred_train == train_y)/len(train_y)\n",
    "            accuracy_test[k] = np.sum(y_pred_test == test_y)/len(test_y)\n",
    "            \n",
    "        accuracies_test[i]=np.mean(accuracy_test);\n",
    "        accuracies_train[i]=np.mean(accuracy_train);\n",
    "        \n",
    "        print('GAMMA:', single_gamma, '---','ACCURANCY TEST:',accuracies_test[i], '---','ACCURANCY TRAIN:',accuracies_train[i])\n",
    "        if (accuracies_test[i]>acc_max):\n",
    "                gamma_best=gammas[i];\n",
    "                acc_max=accuracies_test[i];\n",
    "    \n",
    "    return [gamma_best,acc_max]\n",
    "               \n",
    "        # Compute accuracy of the predictions\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "\n",
    "\n",
    "def cross_validation_log(y, tx, k_fold, max_iters, degs,gammas):            \n",
    "    seed=1;\n",
    "    \n",
    "    # get k'th subgroup in test, others in train\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    accuracy_train = np.zeros([k_fold, len(degs),len(gammas)])\n",
    "    accuracy_test = np.zeros([k_fold, len(degs),len(gammas)])\n",
    "\n",
    "    acc_max=0;\n",
    "    deg_best=0;\n",
    "    gammas_best=0;\n",
    "               \n",
    "    for k in range(k_fold):\n",
    "        print('----- FOLD', k, '-----')\n",
    "        k_index = k_indices[k]\n",
    "        test_y = y[k_index]\n",
    "        test_tx = tx[k_index,:]\n",
    "        mask = np.ones(len(y), dtype=bool) # set all elements to True\n",
    "        mask[k_index] = False              # set test elements to False\n",
    "        train_tx = tx[mask,:]              # select only True elements (ie train elements)\n",
    "        train_y = y[mask]\n",
    "\n",
    "        len_init_data = 0\n",
    "\n",
    "        for i, single_deg in enumerate(degs):\n",
    "            print('++++ Deg =', single_deg)\n",
    "            for j , single_gamma_ in  enumerate(gammas):\n",
    "                print('++++ gamma =', single_gamma_)\n",
    "                \n",
    "                if i==0 and j==0:\n",
    "                    train_tx, test_tx, len_init_data = prepare_data(train_tx, test_tx, single_deg);\n",
    "                else:\n",
    "                    shape_tx=train_tx.shape[1];\n",
    "                    print('ciaociaociao')\n",
    "                    train_tx = add_powers(train_tx, single_deg, 0, len_init_data, features='x', current_max_deg=single_deg-1)\n",
    "                    test_tx = add_powers(test_tx, single_deg, 0, len_init_data, features='x', current_max_deg=single_deg-1)\n",
    "                    train_tx[:,shape_tx:]=standardize(train_tx[:,shape_tx:])[0]\n",
    "                initial_w = np.zeros(train_tx.shape[1]);\n",
    "                \n",
    "                weights,loss = logistic_regression(train_y,train_tx, initial_w, max_iters, single_gamma_);\n",
    "\n",
    "                # Compute the predictions\n",
    "                y_pred_train = predict_labels(weights, train_tx);\n",
    "                y_pred_test = predict_labels(weights, test_tx);\n",
    "                accuracy_train[k, i,j] = np.sum(y_pred_train == train_y)/len(train_y)\n",
    "                accuracy_test[k, i,j] = np.sum(y_pred_test == test_y)/len(test_y)\n",
    "        \n",
    "      \n",
    "                \n",
    "    accuracies_test=np.mean(accuracy_test, axis=0); # mean on the k's\n",
    "    accuracies_train=np.mean(accuracy_train, axis=0);\n",
    "\n",
    " \n",
    "    for i, single_deg in enumerate(degs):\n",
    "         for j , single_gamma_ in  enumerate(gammas):\n",
    "                if (accuracies_test[i,j]>acc_max):\n",
    "                    deg_best=degs[i];\n",
    "                    gammas_best=gammas[j];\n",
    "                    acc_mac=accuracies_test[i,j];\n",
    "    \n",
    "    \n",
    "                    \n",
    "        \n",
    "                \n",
    "    return [deg_best, gamma_best, acc_max];\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def func_logistic (y, tx, test_set, max_iters, gamma, initial_w):\n",
    "    name = 'Logistic regression'\n",
    "    \n",
    "    #w,loss = logistic_regression(y, tx, initial_w, max_iters, gamma)\n",
    "    w,loss = logistic_regression(y, tx, initial_w, max_iters, gamma)\n",
    "    \n",
    "    loss=loss/len(y);\n",
    "    y_pred = predict_labels(w, test_set)\n",
    "    create_csv_submission(ids_test, y_pred, OUT_FOLDER+name)  \n",
    "    \n",
    "    plt.plot(w, 'go')\n",
    "    plt.title('logistic reg: weights with loss:  '+str(loss),fontsize=15, fontweight='bold');\n",
    "    plt.show()\n",
    "    \n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Starting Jet  0 *****\n",
      "----- FOLD 0 -----\n",
      "++++ Deg = 2\n",
      "++++ gamma = 1e-05\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-ca3ae6ddd4b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0;34m[\u001b[0m\u001b[0mbest_gamma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macc_max\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcross_validation_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_single_jet_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx_single_jet_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_fold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgammas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'BEST GAMMA:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_gamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'---'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ACCURANCY:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macc_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-69ddeacb0533>\u001b[0m in \u001b[0;36mcross_validation_log\u001b[0;34m(y, tx, k_fold, max_iters, degs, gammas)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                     \u001b[0mtrain_tx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_tx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen_init_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_tx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_tx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msingle_deg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                     \u001b[0mshape_tx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_tx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/matteociprian/Documents/GitHub/LMO_ML/project1/A_final/preprocessing_functions.py\u001b[0m in \u001b[0;36mprepare_data\u001b[0;34m(train_tx, test_tx, deg)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;31m#print('Cross products')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m     \u001b[0mtrain_tx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_all_cross_prod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_tx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m     \u001b[0mtest_tx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_all_cross_prod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_tx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/matteociprian/Documents/GitHub/LMO_ML/project1/A_final/preprocessing_functions.py\u001b[0m in \u001b[0;36madd_all_cross_prod\u001b[0;34m(tx)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m                 \u001b[0mtx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_cross_prod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/matteociprian/Documents/GitHub/LMO_ML/project1/A_final/preprocessing_functions.py\u001b[0m in \u001b[0;36madd_cross_prod\u001b[0;34m(tx, i, j)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0madd_cross_prod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0madd_all_cross_prod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gammas=np.linspace(0.00001,0.9,10);\n",
    "k_fold=3;\n",
    "max_iters=1000;\n",
    "degs=[2,3];\n",
    "\n",
    "\n",
    "mask_jets_train = split_jets_mask(tx_train)\n",
    "mask_jets_test = split_jets_mask(tx_test)\n",
    "len_mask = len(mask_jets_train)\n",
    "for mask_jet_id in range(len_mask):\n",
    "    print('**** Starting Jet ', mask_jet_id, '*****')\n",
    "    tx_single_jet_train = tx_train[mask_jets_train[mask_jet_id]]\n",
    "    tx_single_jet_test =  tx_test[mask_jets_test[mask_jet_id]]\n",
    "    y_single_jet_train =   y_train[mask_jets_train[mask_jet_id]]\n",
    "    tx_single_jet_train, tx_single_jet_test, len_init = prepare_data(tx_single_jet_train, tx_single_jet_test, deg);\n",
    "    \n",
    "    \n",
    "    \n",
    "    [best_gamma,acc_max]=cross_validation_log(y_single_jet_train, tx_single_jet_train, k_fold, max_iters, degs, gammas)\n",
    "    \n",
    "    print('BEST GAMMA:', best_gamma, '---','ACCURANCY:',acc_max)\n",
    "\n",
    "    initial_w = np.zeros(tx_single_jet_train.shape[1]);\n",
    "    w, loss = func_logistic (y=y_single_jet_train, tx=tx_single_jet_train, test_set=tx_single_jet_test, max_iters=1000,\\\n",
    "                         gamma=best_gamma, initial_w=initial_w);\n",
    "    \n",
    "    y_pred_train = predict_labels(w, tx_single_jet_train)\n",
    "    y_pred_test = predict_labels(w, tx_single_jet_test)\n",
    "    y_preds_train[mask_jets_train[mask_jet_id]] = y_pred_train\n",
    "    y_preds_test[mask_jets_test[mask_jet_id]] = y_pred_test\n",
    "    \n",
    "    right_train = np.sum(y_single_jet_train == y_pred_train)/len(y_single_jet_train )*100\n",
    "    \n",
    "    print('**** Jet ', mask_jet_id, '*****accuracy jet',right_train)\n",
    "    \n",
    "\n",
    "    \n",
    "right_train_total = np.sum(y_train == y_preds_train)/len(y_train )*100\n",
    "print('>>>>>>>> Accuracy TOTAL ',right_train_total)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOGISTIC REGULARIZED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "def cross_validation_logistic_regression_reg(y,tx, k_fold, max_iters, gammas,lambdas):\n",
    "    \n",
    "    acc_max=0;\n",
    "    accuracies_test=np.zeros([len(gammas),len(lambdas)]);\n",
    "    \n",
    "    accuracies_train=np.zeros([len(gammas),len(lambdas)]);\n",
    "    \n",
    "    \n",
    "    for i, single_gamma in enumerate(gammas):\n",
    "        for j, single_lambda in enumerate(lambdas):\n",
    "            seed=1;\n",
    "        \n",
    "            # get k'th subgroup in test, others in train\n",
    "            k_indices = build_k_indices(y, k_fold, seed)\n",
    "            accuracy_train = np.zeros(k_fold)\n",
    "            accuracy_test = np.zeros(k_fold)\n",
    "\n",
    "\n",
    "            for k in range(k_fold):\n",
    "                #print('----- FOLD', k, '-----')\n",
    "                k_index = k_indices[k]\n",
    "                test_y = y[k_index]\n",
    "                test_tx = tx[k_index,:]\n",
    "\n",
    "                mask = np.ones(len(y), dtype=bool) # set all elements to True\n",
    "                mask[k_index] = False              # set test elements to False\n",
    "                train_tx = tx[mask,:]              # select only True elements (ie train elements)\n",
    "                train_y = y[mask]\n",
    "                initial_w = np.zeros(train_tx.shape[1]);\n",
    "                weights,loss = reg_logistic_regression(train_y, train_tx,single_lambda, initial_w, max_iters, single_gamma)\n",
    "                # Compute the predictions\n",
    "                y_pred_train = predict_labels(weights, train_tx)\n",
    "                y_pred_test = predict_labels(weights, test_tx)\n",
    "                predictions=True;\n",
    "                accuracy_train[k] = np.sum(y_pred_train == train_y)/len(train_y)\n",
    "                accuracy_test[k] = np.sum(y_pred_test == test_y)/len(test_y)\n",
    "                \n",
    "                \n",
    "            accuracies_test[i,j]= np.mean(accuracy_test);\n",
    "            accuracies_train[i,j]= np.mean(accuracy_train);\n",
    "            print('GAMMA', single_gamma, '---','LAMBDA', single_lambda, '---ACCURANCY TEST:',accuracies_test[i,j],'---ACCURANCY TRAIN:',accuracies_train[i,j])\n",
    "            \n",
    "            if (accuracies_test[i,j]>acc_max):\n",
    "                gamma_best=gammas[i];\n",
    "                lambda_best=lambdas[j];\n",
    "                acc_max=accuracies_test[i,j];\n",
    "            \n",
    "            \n",
    "    return [gamma_best,lambda_best,acc_max]\n",
    "     \n",
    "    \n",
    "    \n",
    "            # Compute accuracy of the predictions\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def func_logistic_reg (y, tx, lambda_, test_set, max_iters, gamma, initial_w):\n",
    "    name = 'Logistic regression regularized'\n",
    "    \n",
    "    #w,loss = logistic_regression(y, tx, initial_w, max_iters, gamma)\n",
    "    w,loss = reg_logistic_regression(y, tx, lambda_, initial_w, max_iters, gamma)\n",
    "    \n",
    "    loss=loss/len(y);\n",
    "    y_pred = predict_labels(w, test_set)\n",
    "    create_csv_submission(ids_test, y_pred, OUT_FOLDER+name)\n",
    "    \n",
    "    plt.plot(w, 'go')\n",
    "    plt.title('logistic reg: weights with the normalized log-like:  '+str(loss),fontsize=15, fontweight='bold');\n",
    "    plt.show()\n",
    "    \n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test cross_validation\n",
    "\n",
    "gammas=np.linspace(0.000001,0.0001,2);\n",
    "lambdas=np.linspace(0.001,0.3,2);\n",
    "k_fold=6;\n",
    "max_iters=50;\n",
    "\n",
    "deg=2;\n",
    "\n",
    "mask_jets_train = split_jets_mask(tx_train)\n",
    "mask_jets_test = split_jets_mask(tx_test)\n",
    "len_mask = len(mask_jets_train)\n",
    "for mask_jet_id in range(len_mask):\n",
    "    print('**** Starting Jet ', mask_jet_id, '*****')\n",
    "    tx_single_jet_train = tx_train[mask_jets_train[mask_jet_id]]\n",
    "    tx_single_jet_test =  tx_test[mask_jets_test[mask_jet_id]]\n",
    "    y_single_jet_train =   y_train[mask_jets_train[mask_jet_id]]\n",
    "    tx_single_jet_train, tx_single_jet_test, len_init\n",
    "    = prepare_data(tx_single_jet_train, tx_single_jet_test, deg);\n",
    "    \n",
    "    [gamma_best,lambda_best,acc_max]= cross_validation_logistic_regression_reg(y_single_jet_train, tx_single_jet_train, k_fold, max_iters, gammas,lambdas)\n",
    "    print('BEST GAMMA', gamma_best, '---','BEST LAMBDA', lambda_best, '---ACCURANCY:',acc_max)\n",
    "    \n",
    "    initial_w = np.zeros(tx_single_jet_train.shape[1]);\n",
    "    w, loss = func_logistic_reg (y=y_single_jet_train, tx=tx_single_jet_train, lambda_=lambda_best, test_set=tx_single_jet_test , max_iters=max_iters,\\\n",
    "                         gamma=gamma_best, initial_w=initial_w);\n",
    "\n",
    "\n",
    "    \n",
    "    y_pred_train = predict_labels(w, tx_single_jet_train)\n",
    "    y_pred_test = predict_labels(w, tx_single_jet_test)\n",
    "    y_preds_train[mask_jets_train[mask_jet_id]] = y_pred_train\n",
    "    y_preds_test[mask_jets_test[mask_jet_id]] = y_pred_test\n",
    "    \n",
    "    right_train = np.sum(y_single_jet_train == y_pred_train)/len(y_single_jet_train )*100\n",
    "    \n",
    "    print('**** Jet ', mask_jet_id, '*****accuracy jet',right_train)\n",
    "    \n",
    "\n",
    "    \n",
    "right_train_total = np.sum(y_train == y_preds_train)/len(y_train )*100\n",
    "print('>>>>>>>> Accuracy TOTAL ',right_train_total)\n",
    "\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
