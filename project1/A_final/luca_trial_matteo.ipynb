{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Project 1\n",
    "\n",
    "    We begin by loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from preprocessing_functions import *\n",
    "%matplotlib inline \n",
    "import numpy as np   # generic stuff\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#### REMOVE THIS LINE BEFORE SUBMISSION\n",
    "import pandas as pd\n",
    "#######################################################################\n",
    "\n",
    "from lib.proj1_helpers import * #the helper provided for the project\n",
    "from lib.costs import *\n",
    "\n",
    "# choose which implementations you would like\n",
    "from lib.implementations import *\n",
    "#from implementations import * #our implementations of the functions done by us\n",
    "\n",
    "\n",
    "import datetime\n",
    "from helpers import * #helpers of exo 2\n",
    "# Useful starting lines\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_FOLDER = '../data/' # get rid of the ..\n",
    "\n",
    "y_train, tx_train, ids_train = load_csv_data(DATA_FOLDER+'train.csv',sub_sample=False)\n",
    "\n",
    "y_test, tx_test, ids_test = load_csv_data(DATA_FOLDER+'test.csv',sub_sample=False)\n",
    "AAA = 0;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note that here we are only considering a sub_sample as the \"True\" value indicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 30)\n",
      "(568238, 30)\n",
      "(250000,)\n"
     ]
    }
   ],
   "source": [
    "print(tx_train.shape)\n",
    "print(tx_test.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>138.470</td>\n",
       "      <td>51.655</td>\n",
       "      <td>97.827</td>\n",
       "      <td>27.980</td>\n",
       "      <td>0.91</td>\n",
       "      <td>124.711</td>\n",
       "      <td>2.666</td>\n",
       "      <td>3.064</td>\n",
       "      <td>41.928</td>\n",
       "      <td>197.760</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.277</td>\n",
       "      <td>258.733</td>\n",
       "      <td>2.0</td>\n",
       "      <td>67.435</td>\n",
       "      <td>2.150</td>\n",
       "      <td>0.444</td>\n",
       "      <td>46.062</td>\n",
       "      <td>1.24</td>\n",
       "      <td>-2.475</td>\n",
       "      <td>113.497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>160.937</td>\n",
       "      <td>68.768</td>\n",
       "      <td>103.235</td>\n",
       "      <td>48.146</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.473</td>\n",
       "      <td>2.078</td>\n",
       "      <td>125.157</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.916</td>\n",
       "      <td>164.546</td>\n",
       "      <td>1.0</td>\n",
       "      <td>46.226</td>\n",
       "      <td>0.725</td>\n",
       "      <td>1.158</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>46.226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-999.000</td>\n",
       "      <td>162.172</td>\n",
       "      <td>125.953</td>\n",
       "      <td>35.635</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.148</td>\n",
       "      <td>9.336</td>\n",
       "      <td>197.814</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.186</td>\n",
       "      <td>260.414</td>\n",
       "      <td>1.0</td>\n",
       "      <td>44.251</td>\n",
       "      <td>2.053</td>\n",
       "      <td>-2.028</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>44.251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>143.905</td>\n",
       "      <td>81.417</td>\n",
       "      <td>80.943</td>\n",
       "      <td>0.414</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.310</td>\n",
       "      <td>0.414</td>\n",
       "      <td>75.968</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060</td>\n",
       "      <td>86.062</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>175.864</td>\n",
       "      <td>16.915</td>\n",
       "      <td>134.805</td>\n",
       "      <td>16.405</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.891</td>\n",
       "      <td>16.405</td>\n",
       "      <td>57.983</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.871</td>\n",
       "      <td>53.131</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0        1        2       3       4        5        6      7       8   \\\n",
       "0  138.470   51.655   97.827  27.980    0.91  124.711    2.666  3.064  41.928   \n",
       "1  160.937   68.768  103.235  48.146 -999.00 -999.000 -999.000  3.473   2.078   \n",
       "2 -999.000  162.172  125.953  35.635 -999.00 -999.000 -999.000  3.148   9.336   \n",
       "3  143.905   81.417   80.943   0.414 -999.00 -999.000 -999.000  3.310   0.414   \n",
       "4  175.864   16.915  134.805  16.405 -999.00 -999.000 -999.000  3.891  16.405   \n",
       "\n",
       "        9    ...        20       21   22       23       24       25       26  \\\n",
       "0  197.760   ...    -0.277  258.733  2.0   67.435    2.150    0.444   46.062   \n",
       "1  125.157   ...    -1.916  164.546  1.0   46.226    0.725    1.158 -999.000   \n",
       "2  197.814   ...    -2.186  260.414  1.0   44.251    2.053   -2.028 -999.000   \n",
       "3   75.968   ...     0.060   86.062  0.0 -999.000 -999.000 -999.000 -999.000   \n",
       "4   57.983   ...    -0.871   53.131  0.0 -999.000 -999.000 -999.000 -999.000   \n",
       "\n",
       "       27       28       29  \n",
       "0    1.24   -2.475  113.497  \n",
       "1 -999.00 -999.000   46.226  \n",
       "2 -999.00 -999.000   44.251  \n",
       "3 -999.00 -999.000    0.000  \n",
       "4 -999.00 -999.000    0.000  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# illegal: \n",
    "pd.DataFrame(tx_train).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for methods: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OUT_FOLDER = 'output/'\n",
    "\n",
    "def func_least_squares (y, tx, test_set, fct='mse'):\n",
    "    #name = 'least_squares'\n",
    "    w,loss = least_squares(y,tx,fct)\n",
    "    #y_pred = predict_labels(w, test_set)\n",
    "    #create_csv_submission(ids_test, y_pred, OUT_FOLDER+name)\n",
    "    \n",
    "    plt.plot(w, 'go')\n",
    "    plt.title('least squares weights for loss:  '+str(loss),fontsize=15, fontweight='bold');\n",
    "    plt.show()\n",
    "    \n",
    "    return w, loss\n",
    "\n",
    "def func_GD (y, tx, test_set, max_iters, gamma, initial_w):\n",
    "    w,loss = least_squares_GD(y, tx, initial_w, max_iters, gamma,fct='mse');\n",
    "    #y_pred = predict_labels(w, test_set)\n",
    "    #create_csv_submission(ids_test, y_pred, OUT_FOLDER+name)\n",
    "    \n",
    "    plt.plot(w, 'go')\n",
    "    plt.title('GD: weights with loss:  '+str(loss),fontsize=15, fontweight='bold');\n",
    "    plt.show()\n",
    "    \n",
    "    return w, loss\n",
    "\n",
    "########################  RIDGE REGRESSION #######################################################\n",
    "def func_ridge_regression (y, tx, test_set, lambda_):\n",
    "    name = 'Ridge_regression'\n",
    "    \n",
    "    w,loss = ridge_regression(y, tx, lambda_=lambda_, fct='mse');\n",
    "    \n",
    "    y_pred = predict_labels(w, test_set)\n",
    "    create_csv_submission(ids_test, y_pred, OUT_FOLDER+name)\n",
    "    \n",
    "    plt.plot(w, 'go')\n",
    "    plt.title('Ridge regression: weights ;loss:  '+str(loss),fontsize=15, fontweight='bold');\n",
    "    plt.show()\n",
    "    \n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LEAST-SQUARES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#todo: cross_validation su degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"Build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "def cross_validation_least_square(y, tx, k_fold, degs, seed):\n",
    "    \"\"\"Finds the best degree for Least Squares method with cross validation.\"\"\"\n",
    "    # Get k'th subgroup in test, others in train\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    \n",
    "    # Initialisation of the matrices of accuracies for all the degrees and different folds\n",
    "    accuracy_train = np.zeros([k_fold, len(degs)])\n",
    "    accuracy_test = np.zeros([k_fold, len(degs)])\n",
    "\n",
    "    # Loop over the different folds\n",
    "    for k in range(k_fold):\n",
    "        print('----- FOLD', k, '-----')\n",
    "        k_index = k_indices[k]\n",
    "        test_y = y[k_index]\n",
    "        test_tx = tx[k_index,:]\n",
    "        mask = np.ones(len(y), dtype=bool) # set all elements to True\n",
    "        mask[k_index] = False              # set test elements to False\n",
    "        train_tx = tx[mask,:]              # select only True elements (ie train elements)\n",
    "        train_y = y[mask]\n",
    "\n",
    "        len_init_data = 0\n",
    "\n",
    "        # Loop over the different degrees to try\n",
    "        for i, single_deg in enumerate(degs):\n",
    "            print('++++ Deg =', single_deg)\n",
    "            if i==0:\n",
    "                # Preprocess the data (cleaning and adding features)\n",
    "                train_tx, test_tx, len_init_data = prepare_data(train_tx, test_tx, single_deg);\n",
    "            else:\n",
    "                nb_cols_tx = train_tx.shape[1]\n",
    "                # Add the supplementary powers of the features with respect to the previous iteration and standardize\n",
    "                train_tx = add_powers(train_tx, single_deg, 0, len_init_data, features='x', current_max_deg=single_deg-1)\n",
    "                test_tx = add_powers(test_tx, single_deg, 0, len_init_data, features='x', current_max_deg=single_deg-1)\n",
    "                train_tx[:,nb_cols_tx:] = standardize(train_tx[:,nb_cols_tx:])[0]\n",
    "                test_tx[:,nb_cols_tx:] = standardize(test_tx[:,nb_cols_tx:])[0]\n",
    "                unique_cols = keep_unique_cols(train_tx)\n",
    "                train_tx = train_tx[:,unique_cols]\n",
    "                test_tx = test_tx[:,unique_cols]\n",
    "                \n",
    "            # Compute the weights with LS\n",
    "            weights, loss = least_squares(train_y, train_tx, fct='mse');\n",
    "\n",
    "            # Compute the predictions\n",
    "            y_pred_train = predict_labels(weights, train_tx)\n",
    "            y_pred_test = predict_labels(weights, test_tx);\n",
    "            accuracy_train[k, i] = np.sum(y_pred_train == train_y)/len(train_y)\n",
    "            accuracy_test[k, i] = np.sum(y_pred_test == test_y)/len(test_y)\n",
    "\n",
    "    accuracies_test = np.mean(accuracy_test, axis=0); # mean on the k's\n",
    "    accuracies_train = np.mean(accuracy_train, axis=0);\n",
    "\n",
    "    max_index, acc_max = max(enumerate(accuracies_test), key=operator.itemgetter(1))\n",
    "    print(accuracy_test)\n",
    "    print(accuracies_test)\n",
    "    print(k_fold)\n",
    "    print(len(degs))\n",
    "    print(max_index)\n",
    "    print(acc_max)\n",
    "    deg_best = degs[max_index]\n",
    "    \n",
    "    return deg_best, acc_max;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Starting Jet  0 *****\n",
      "----- FOLD 0 -----\n",
      "++++ Deg = 2\n",
      "++++ Deg = 3\n",
      "++++ Deg = 4\n",
      "++++ Deg = 5\n",
      "++++ Deg = 6\n",
      "++++ Deg = 7\n",
      "++++ Deg = 8\n",
      "++++ Deg = 9\n",
      "++++ Deg = 10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-d63810797048>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0my_single_jet_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask_jets_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask_jet_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mbest_deg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validation_least_square\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_single_jet_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx_single_jet_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_fold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Best degree = '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_deg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-678a387f66f5>\u001b[0m in \u001b[0;36mcross_validation_least_square\u001b[0;34m(y, tx, k_fold, degs, seed)\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mtrain_tx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnb_cols_tx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstandardize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_tx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnb_cols_tx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0mtest_tx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnb_cols_tx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstandardize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_tx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnb_cols_tx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                 \u001b[0munique_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeep_unique_cols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_tx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m                 \u001b[0mtrain_tx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_tx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0munique_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0mtest_tx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_tx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0munique_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/matteociprian/Documents/GitHub/LMO_ML/project1/A_final/preprocessing_functions.py\u001b[0m in \u001b[0;36mkeep_unique_cols\u001b[0;34m(tx)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0merase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mid_loop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m                     \u001b[0merase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                     \u001b[0mequal_to\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "OUT_FOLDER = 'output/'\n",
    "name = 'least_squares.csv'\n",
    "\n",
    "degs=range(2,14)\n",
    "k_fold=5\n",
    "seed=1;\n",
    "\n",
    "mask_jets_train = split_jets_mask(tx_train)\n",
    "mask_jets_test = split_jets_mask(tx_test)\n",
    "len_mask = len(mask_jets_train)\n",
    "y_preds_train = np.zeros(len(y_train))\n",
    "y_preds_test = np.zeros(tx_test.shape[0])\n",
    "\n",
    "\n",
    "for mask_jet_id in range(len_mask):\n",
    "    print('**** Starting Jet ', mask_jet_id, '*****')\n",
    "    tx_single_jet_train = tx_train[mask_jets_train[mask_jet_id]]\n",
    "    tx_single_jet_test = tx_test[mask_jets_test[mask_jet_id]]\n",
    "    y_single_jet_train = y_train[mask_jets_train[mask_jet_id]]\n",
    "    \n",
    "    best_deg, best_acc = cross_validation_least_square(y_single_jet_train, tx_single_jet_train, k_fold, degs,seed)\n",
    "    print('Best degree = ', best_deg)\n",
    "    \n",
    "    tx_single_jet_train, tx_single_jet_test,len_init_data = prepare_data(tx_single_jet_train,tx_single_jet_test, best_deg)\n",
    "    w, loss = func_least_squares(y=y_single_jet_train, tx=tx_single_jet_train, test_set=tx_single_jet_test, fct='mse')\n",
    "    \n",
    "    y_pred_train = predict_labels(w, tx_single_jet_train)\n",
    "    y_pred_test = predict_labels(w, tx_single_jet_test)\n",
    "    y_preds_train[mask_jets_train[mask_jet_id]] = y_pred_train\n",
    "    y_preds_test[mask_jets_test[mask_jet_id]] = y_pred_test\n",
    "    \n",
    "    right_train = np.sum(y_single_jet_train == y_pred_train)/len(y_single_jet_train)*100\n",
    "    \n",
    "    print('**** Jet ', mask_jet_id, '***** Accuracy jet', right_train)\n",
    "    \n",
    "\n",
    "create_csv_submission(ids_test, y_preds_test, OUT_FOLDER+name)\n",
    "right_train_total = np.sum(y_train == y_preds_train)/len(y_train)*100\n",
    "print('>>>>>>>> Accuracy TOTAL ',right_train_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_preds_test[y_preds_test==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_preds_test[y_preds_test==-1])+len(y_preds_test[y_preds_test==1])==tx_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds_test[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cross validation su gamma e degree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "\n",
    "\n",
    "def cross_validation_GD(y, tx, k_fold, max_iters, degs,lambdas):            \n",
    "    seed=1;\n",
    "    \n",
    "    # get k'th subgroup in test, others in train\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    accuracy_train = np.zeros([k_fold, len(degs),len(gammas)])\n",
    "    accuracy_test = np.zeros([k_fold, len(degs),len(gammas)])\n",
    "\n",
    "    acc_max=0;\n",
    "    deg_best=0;\n",
    "    gammas_best=0;\n",
    "               \n",
    "    for k in range(k_fold):\n",
    "        print('----- FOLD', k, '-----')\n",
    "        k_index = k_indices[k]\n",
    "        test_y = y[k_index]\n",
    "        test_tx = tx[k_index,:]\n",
    "        mask = np.ones(len(y), dtype=bool) # set all elements to True\n",
    "        mask[k_index] = False              # set test elements to False\n",
    "        train_tx = tx[mask,:]              # select only True elements (ie train elements)\n",
    "        train_y = y[mask]\n",
    "\n",
    "        len_init_data = 0\n",
    "\n",
    "        for i, single_deg in enumerate(degs):\n",
    "            print('++++ Deg =', single_deg)\n",
    "            for j , single_gamma_ in  enumerate(gammas):\n",
    "                print('++++ gamma =', single_gamma_)\n",
    "                \n",
    "                if i==0:\n",
    "                    train_tx, test_tx, len_init_data = prepare_data(train_tx, test_tx, single_deg);\n",
    "                else:\n",
    "                    shape_tx=train_tx.shape[1];\n",
    "                    train_tx = add_powers(train_tx, single_deg, 0, len_init_data, features='x', current_max_deg=single_deg-1)\n",
    "                    test_tx = add_powers(test_tx, single_deg, 0, len_init_data, features='x', current_max_deg=single_deg-1)\n",
    "                    train_tx[:,shape_tx:]=standardize(train_tx[:,shape_tx:])[0]\n",
    "                initial_w = np.zeros(train_tx.shape[1]);\n",
    "                \n",
    "                weights,loss = least_squares_GD(train_y,train_tx, initial_w, max_iters, single_gamma_,fct='mse');\n",
    "\n",
    "                # Compute the predictions\n",
    "                y_pred_train = predict_labels(weights, train_tx);\n",
    "                y_pred_test = predict_labels(weights, test_tx);\n",
    "                accuracy_train[k, i,j] = np.sum(y_pred_train == train_y)/len(train_y)\n",
    "                accuracy_test[k, i,j] = np.sum(y_pred_test == test_y)/len(test_y)\n",
    "        \n",
    "      \n",
    "                \n",
    "    accuracies_test=np.mean(accuracy_test, axis=0); # mean on the k's\n",
    "    accuracies_train=np.mean(accuracy_train, axis=0);\n",
    "\n",
    " \n",
    "    for i, single_deg in enumerate(degs):\n",
    "         for j , single_gamma_ in  enumerate(gammas):\n",
    "                if (accuracies_test[i,j]>acc_max):\n",
    "                    deg_best=degs[i];\n",
    "                    gammas_best=gammas[j];\n",
    "                    acc_mac=accuracies_test[i,j];\n",
    "    \n",
    "    \n",
    "                    \n",
    "        \n",
    "                \n",
    "    return [deg_best, gamma_best, acc_max];\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1]\n",
      " [2 2 2]]\n",
      "(2, 2)\n",
      "[[ 1 -1 -1]\n",
      " [ 2  1  1]]\n"
     ]
    }
   ],
   "source": [
    "cubic=np.array([[1,1,1],[2,2,2]])\n",
    "print(cubic)\n",
    "print (cubic[:,1:].shape)\n",
    "\n",
    "cubic[:,1:]=standardize(cubic[:,1:])[0];\n",
    "print(cubic)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Starting Jet  0 *****\n",
      "----- FOLD 0 -----\n",
      "++++ Deg = 4\n",
      "++++ gamma = 0.0001\n",
      "Gradient Descent(0/199): loss=0.4999999999999887, w0=-6.887232304259569e-08, w1=-3.5851239558151825e-05\n",
      "Gradient Descent(1/199): loss=0.4997886023020471, w0=-1.3279692118023493e-07, w1=-7.165426932699993e-05\n",
      "Gradient Descent(2/199): loss=0.49957776994619885, w0=-1.9180268329462098e-07, w1=-0.00010740917970274563\n",
      "Gradient Descent(3/199): loss=0.49936750087119885, w0=-2.459183816411012e-07, w1=-0.00014311606088238587\n",
      "Gradient Descent(4/199): loss=0.49915779302441243, w0=-2.951726722743844e-07, w1=-0.000178775002864219\n",
      "Gradient Descent(5/199): loss=0.498948644361784, w0=-3.395940954727797e-07, w1=-0.00021438609544834942\n",
      "Gradient Descent(6/199): loss=0.49874005284779666, w0=-3.792110761609197e-07, w1=-0.0002499494282371909\n",
      "Gradient Descent(7/199): loss=0.4985320164554317, w0=-4.1405192433098656e-07, w1=-0.00028546509063596823\n",
      "Gradient Descent(8/199): loss=0.4983245331661278, w0=-4.4414483546245015e-07, w1=-0.0003209331718532171\n",
      "Gradient Descent(9/199): loss=0.4981176009697414, w0=-4.6951789094031177e-07, w1=-0.00035635376090128356\n",
      "Gradient Descent(10/199): loss=0.4979112178645054, w0=-4.901990584718737e-07, w1=-0.000391726946596821\n",
      "Gradient Descent(11/199): loss=0.49770538185699026, w0=-5.062161925020312e-07, w1=-0.00042705281756128617\n",
      "Gradient Descent(12/199): loss=0.49750009096206405, w0=-5.175970346270864e-07, w1=-0.0004623314622214339\n",
      "Gradient Descent(13/199): loss=0.4972953432028535, w0=-5.24369214007105e-07, w1=-0.0004975629688098099\n",
      "Gradient Descent(14/199): loss=0.4970911366107044, w0=-5.265602477768134e-07, w1=-0.000532747425365243\n",
      "Gradient Descent(15/199): loss=0.49688746922514226, w0=-5.241975414550211e-07, w1=-0.0005678849197333352\n",
      "Gradient Descent(16/199): loss=0.49668433909383425, w0=-5.173083893526141e-07, w1=-0.0006029755395669503\n",
      "Gradient Descent(17/199): loss=0.4964817442725501, w0=-5.059199749790999e-07, w1=-0.0006380193723267019\n",
      "Gradient Descent(18/199): loss=0.4962796828251246, w0=-4.90059371447695e-07, w1=-0.0006730165052814394\n",
      "Gradient Descent(19/199): loss=0.4960781528234176, w0=-4.6975354187899173e-07, w1=-0.0007079670255087326\n",
      "Gradient Descent(20/199): loss=0.49587715234727864, w0=-4.4502933980319824e-07, w1=-0.0007428710198953553\n",
      "Gradient Descent(21/199): loss=0.4956766794845071, w0=-4.1591350956093465e-07, w1=-0.0007777285751377666\n",
      "Gradient Descent(22/199): loss=0.4954767323308153, w0=-3.8243268670263026e-07, w1=-0.0008125397777425925\n",
      "Gradient Descent(23/199): loss=0.4952773089897917, w0=-3.4461339838648637e-07, w1=-0.0008473047140271043\n",
      "Gradient Descent(24/199): loss=0.4950784075728629, w0=-3.024820637750462e-07, w1=-0.000882023470119697\n",
      "Gradient Descent(25/199): loss=0.4948800261992581, w0=-2.5606499443034053e-07, w1=-0.0009166961319603652\n",
      "Gradient Descent(26/199): loss=0.4946821629959701, w0=-2.053883947076479e-07, w1=-0.0009513227853011792\n",
      "Gradient Descent(27/199): loss=0.49448481609772077, w0=-1.5047836214785388e-07, w1=-0.0009859035157067575\n",
      "Gradient Descent(28/199): loss=0.49428798364692395, w0=-9.136088786841646e-08, w1=-0.0010204384085547404\n",
      "Gradient Descent(29/199): loss=0.49409166379364944, w0=-2.806185695295112e-08, w1=-0.0010549275490362602\n",
      "Gradient Descent(30/199): loss=0.49389585469558656, w0=3.93929511605685e-08, w1=-0.0010893710221564117\n",
      "Gradient Descent(31/199): loss=0.49370055451800926, w0=1.1097786229298799e-07, w1=-0.0011237689127347197\n",
      "Gradient Descent(32/199): loss=0.4935057614337398, w0=1.8666730713851785e-07, w1=-0.0011581213054056069\n",
      "Gradient Descent(33/199): loss=0.4933114736231136, w0=2.664358208806434e-07, w1=-0.001192428284618859\n",
      "Gradient Descent(34/199): loss=0.49311768927394417, w0=3.5025804280940357e-07, w1=-0.0012266899346400897\n",
      "Gradient Descent(35/199): loss=0.49292440658148806, w0=4.3810871594001577e-07, w1=-0.0012609063395512036\n",
      "Gradient Descent(36/199): loss=0.4927316237484097, w0=5.299626866328737e-07, w1=-0.0012950775832508575\n",
      "Gradient Descent(37/199): loss=0.49253933898474794, w0=6.25794904214862e-07, w1=-0.0013292037494549215\n",
      "Gradient Descent(38/199): loss=0.49234755050787926, w0=7.255804206020422e-07, w1=-0.0013632849216969375\n",
      "Gradient Descent(39/199): loss=0.49215625654248607, w0=8.292943899236579e-07, w1=-0.0013973211833285778\n",
      "Gradient Descent(40/199): loss=0.4919654553205213, w0=9.369120681474894e-07, w1=-0.0014313126175201007\n",
      "Gradient Descent(41/199): loss=0.49177514508117437, w0=1.048408812706515e-06, w1=-0.0014652593072608069\n",
      "Gradient Descent(42/199): loss=0.4915853240708382, w0=1.16376008212692e-06, w1=-0.0014991613353594923\n",
      "Gradient Descent(43/199): loss=0.49139599054307503, w0=1.282941435657409e-06, w1=-0.0015330187844449013\n",
      "Gradient Descent(44/199): loss=0.49120714275858346, w0=1.4059285328998346e-06, w1=-0.0015668317369661783\n",
      "Gradient Descent(45/199): loss=0.491018778985165, w0=1.532697133441154e-06, w1=-0.0016006002751933171\n",
      "Gradient Descent(46/199): loss=0.49083089749769127, w0=1.6632230964866587e-06, w1=-0.0016343244812176106\n",
      "Gradient Descent(47/199): loss=0.49064349657807066, w0=1.7974823804945325e-06, w1=-0.0016680044369520974\n",
      "Gradient Descent(48/199): loss=0.49045657451521646, w0=1.935451042811696e-06, w1=-0.0017016402241320089\n",
      "Gradient Descent(49/199): loss=0.4902701296050139, w0=2.077105239310945e-06, w1=-0.0017352319243152138\n",
      "Gradient Descent(50/199): loss=0.490084160150288, w0=2.2224212240293683e-06, w1=-0.0017687796188826622\n",
      "Gradient Descent(51/199): loss=0.4898986644607707, w0=2.3713753488080608e-06, w1=-0.0018022833890388276\n",
      "Gradient Descent(52/199): loss=0.4897136408530704, w0=2.5239440629331013e-06, w1=-0.0018357433158121487\n",
      "Gradient Descent(53/199): loss=0.4895290876506383, w0=2.680103912777819e-06, w1=-0.001869159480055469\n",
      "Gradient Descent(54/199): loss=0.48934500318373847, w0=2.8398315414463155e-06, w1=-0.0019025319624464754\n",
      "Gradient Descent(55/199): loss=0.4891613857894153, w0=3.00310368841826e-06, w1=-0.0019358608434881363\n",
      "Gradient Descent(56/199): loss=0.4889782338114624, w0=3.169897189194943e-06, w1=-0.0019691462035091372\n",
      "Gradient Descent(57/199): loss=0.48879554560039185, w0=3.340188974946584e-06, w1=-0.002002388122664316\n",
      "Gradient Descent(58/199): loss=0.48861331951340237, w0=3.513956072160888e-06, w1=-0.0020355866809350976\n",
      "Gradient Descent(59/199): loss=0.48843155391434956, w0=3.691175602292862e-06, w1=-0.002068741958129924\n",
      "Gradient Descent(60/199): loss=0.48825024717371457, w0=3.8718247814158575e-06, w1=-0.002101854033884689\n",
      "Gradient Descent(61/199): loss=0.4880693976685735, w0=4.05588091987386e-06, w1=-0.002134922987663166\n",
      "Gradient Descent(62/199): loss=0.4878890037825678, w0=4.243321421935014e-06, w1=-0.0021679488987574372\n",
      "Gradient Descent(63/199): loss=0.48770906390587343, w0=4.434123785446379e-06, w1=-0.002200931846288322\n",
      "Gradient Descent(64/199): loss=0.4875295764351707, w0=4.628265601489895e-06, w1=-0.0022338719092058028\n",
      "Gradient Descent(65/199): loss=0.4873505397736154, w0=4.825724554039599e-06, w1=-0.002266769166289451\n",
      "Gradient Descent(66/199): loss=0.4871719523308075, w0=5.026478419620024e-06, w1=-0.0022996236961488506\n",
      "Gradient Descent(67/199): loss=0.48699381252276386, w0=5.23050506696583e-06, w1=-0.002332435577224021\n",
      "Gradient Descent(68/199): loss=0.48681611877188613, w0=5.437782456682656e-06, w1=-0.0023652048877858393\n",
      "Gradient Descent(69/199): loss=0.48663886950693386, w0=5.648288640909134e-06, w1=-0.0023979317059364593\n",
      "Gradient Descent(70/199): loss=0.4864620631629938, w0=5.862001762980157e-06, w1=-0.002430616109609733\n",
      "Gradient Descent(71/199): loss=0.486285698181452, w0=6.0789000570912916e-06, w1=-0.002463258176571628\n",
      "Gradient Descent(72/199): loss=0.48610977300996455, w0=6.298961847964414e-06, w1=-0.002495857984420642\n",
      "Gradient Descent(73/199): loss=0.48593428610242895, w0=6.522165550514532e-06, w1=-0.0025284156105882228\n",
      "Gradient Descent(74/199): loss=0.48575923591895565, w0=6.748489669517769e-06, w1=-0.00256093113233918\n",
      "Gradient Descent(75/199): loss=0.4855846209258402, w0=6.9779127992805425e-06, w1=-0.002593404626772099\n",
      "Gradient Descent(76/199): loss=0.48541043959553376, w0=7.210413623309925e-06, w1=-0.002625836170819754\n",
      "Gradient Descent(77/199): loss=0.48523669040661666, w0=7.445970913985151e-06, w1=-0.002658225841249518\n",
      "Gradient Descent(78/199): loss=0.4850633718437695, w0=7.68456353223032e-06, w1=-0.0026905737146637743\n",
      "Gradient Descent(79/199): loss=0.48489048239774546, w0=7.92617042718823e-06, w1=-0.0027228798675003234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(80/199): loss=0.4847180205653432, w0=8.170770635895399e-06, w1=-0.0027551443760327927\n",
      "Gradient Descent(81/199): loss=0.48454598484937883, w0=8.41834328295822e-06, w1=-0.002787367316371041\n",
      "Gradient Descent(82/199): loss=0.48437437375865905, w0=8.66886758023028e-06, w1=-0.0028195487644615655\n",
      "Gradient Descent(83/199): loss=0.4842031858079534, w0=8.92232282649081e-06, w1=-0.0028516887960879055\n",
      "Gradient Descent(84/199): loss=0.4840324195179678, w0=9.1786884071243e-06, w1=-0.002883787486871045\n",
      "Gradient Descent(85/199): loss=0.4838620734153176, w0=9.437943793801222e-06, w1=-0.0029158449122698147\n",
      "Gradient Descent(86/199): loss=0.48369214603250016, w0=9.700068544159913e-06, w1=-0.0029478611475812944\n",
      "Gradient Descent(87/199): loss=0.4835226359078691, w0=9.965042301489575e-06, w1=-0.0029798362679412105\n",
      "Gradient Descent(88/199): loss=0.48335354158560695, w0=1.0232844794414407e-05, w1=-0.0030117703483243364\n",
      "Gradient Descent(89/199): loss=0.48318486161569973, w0=1.050345583657885e-05, w1=-0.0030436634635448885\n",
      "Gradient Descent(90/199): loss=0.4830165945539099, w0=1.077685532633397e-05, w1=-0.0030755156882569237\n",
      "Gradient Descent(91/199): loss=0.48284873896175096, w0=1.1053023246424927e-05, w1=-0.003107327096954734\n",
      "Gradient Descent(92/199): loss=0.48268129340646093, w0=1.1331939663679575e-05, w1=-0.003139097763973241\n",
      "Gradient Descent(93/199): loss=0.4825142564609769, w0=1.1613584728698168e-05, w1=-0.0031708277634883886\n",
      "Gradient Descent(94/199): loss=0.48234762670390907, w0=1.1897938675544162e-05, w1=-0.003202517169517535\n",
      "Gradient Descent(95/199): loss=0.4821814027195155, w0=1.2184981821436109e-05, w1=-0.0032341660559198453\n",
      "Gradient Descent(96/199): loss=0.48201558309767656, w0=1.2474694566440657e-05, w1=-0.0032657744963966767\n",
      "Gradient Descent(97/199): loss=0.4818501664338697, w0=1.2767057393166652e-05, w1=-0.0032973425644919727\n",
      "Gradient Descent(98/199): loss=0.481685151329144, w0=1.3062050866460295e-05, w1=-0.0033288703335926464\n",
      "Gradient Descent(99/199): loss=0.48152053639009595, w0=1.3359655633101407e-05, w1=-0.0033603578769289696\n",
      "Gradient Descent(100/199): loss=0.4813563202288432, w0=1.365985242150077e-05, w1=-0.003391805267574957\n",
      "Gradient Descent(101/199): loss=0.4811925014630012, w0=1.3962622041398545e-05, w1=-0.0034232125784487504\n",
      "Gradient Descent(102/199): loss=0.4810290787156574, w0=1.4267945383563745e-05, w1=-0.0034545798823130028\n",
      "Gradient Descent(103/199): loss=0.48086605061534826, w0=1.4575803419494799e-05, w1=-0.00348590725177526\n",
      "Gradient Descent(104/199): loss=0.48070341579603215, w0=1.4886177201121167e-05, w1=-0.003517194759288341\n",
      "Gradient Descent(105/199): loss=0.48054117289706877, w0=1.5199047860506024e-05, w1=-0.00354844247715072\n",
      "Gradient Descent(106/199): loss=0.4803793205631913, w0=1.5514396609549992e-05, w1=-0.0035796504775069044\n",
      "Gradient Descent(107/199): loss=0.48021785744448514, w0=1.5832204739695928e-05, w1=-0.0036108188323478118\n",
      "Gradient Descent(108/199): loss=0.48005678219636283, w0=1.6152453621634773e-05, w1=-0.0036419476135111477\n",
      "Gradient Descent(109/199): loss=0.4798960934795401, w0=1.647512470501242e-05, w1=-0.003673036892681782\n",
      "Gradient Descent(110/199): loss=0.479735789960013, w0=1.680019951813766e-05, w1=-0.0037040867413921223\n",
      "Gradient Descent(111/199): loss=0.4795758703090329, w0=1.7127659667691136e-05, w1=-0.003735097231022489\n",
      "Gradient Descent(112/199): loss=0.4794163332030855, w0=1.7457486838435348e-05, w1=-0.0037660684328014874\n",
      "Gradient Descent(113/199): loss=0.4792571773238648, w0=1.7789662792925688e-05, w1=-0.0037970004178063785\n",
      "Gradient Descent(114/199): loss=0.4790984013582521, w0=1.8124169371222493e-05, w1=-0.0038278932569634516\n",
      "Gradient Descent(115/199): loss=0.4789400039982916, w0=1.8460988490604128e-05, w1=-0.0038587470210483922\n",
      "Gradient Descent(116/199): loss=0.47878198394116817, w0=1.8800102145281104e-05, w1=-0.0038895617806866513\n",
      "Gradient Descent(117/199): loss=0.47862433988918407, w0=1.914149240611118e-05, w1=-0.003920337606353813\n",
      "Gradient Descent(118/199): loss=0.4784670705497365, w0=1.9485141420315515e-05, w1=-0.00395107456837596\n",
      "Gradient Descent(119/199): loss=0.4783101746352949, w0=1.98310314111958e-05, w1=-0.00398177273693004\n",
      "Gradient Descent(120/199): loss=0.4781536508633785, w0=2.0179144677852405e-05, w1=-0.004012432182044232\n",
      "Gradient Descent(121/199): loss=0.4779974979565341, w0=2.0529463594903554e-05, w1=-0.004043052973598305\n",
      "Gradient Descent(122/199): loss=0.4778417146423134, w0=2.0881970612205452e-05, w1=-0.004073635181323985\n",
      "Gradient Descent(123/199): loss=0.4776862996532515, w0=2.1236648254573442e-05, w1=-0.004104178874805315\n",
      "Gradient Descent(124/199): loss=0.4775312517268442, w0=2.159347912150415e-05, w1=-0.004134684123479013\n"
     ]
    }
   ],
   "source": [
    "\n",
    "OUT_FOLDER = 'output/'\n",
    "name = 'Gradient_descent.csv'\n",
    "degs=range(4,8)\n",
    "gammas=np.logspace(-4,-1,8);\n",
    "k_fold=5\n",
    "max_iters=200\n",
    "\n",
    "\n",
    "mask_jets_train = split_jets_mask(tx_train)\n",
    "mask_jets_test = split_jets_mask(tx_test)\n",
    "len_mask = len(mask_jets_train)\n",
    "\n",
    "y_preds_train = np.zeros(len(y_train))\n",
    "y_preds_test = np.zeros(tx_test.shape[0])\n",
    "\n",
    "\n",
    "\n",
    "for mask_jet_id in range(len_mask):\n",
    "    print('**** Starting Jet ', mask_jet_id, '*****')\n",
    "    tx_single_jet_train = tx_train[mask_jets_train[mask_jet_id]]\n",
    "    tx_single_jet_test =  tx_test[mask_jets_test[mask_jet_id]]\n",
    "    y_single_jet_train =   y_train[mask_jets_train[mask_jet_id]]\n",
    "    \n",
    "    \n",
    "    \n",
    "    [best_deg, gamma_best, acc_max] =cross_validation_GD(y_single_jet_train, tx_single_jet_train, k_fold, max_iters, degs,gammas)\n",
    "    tx_single_jet_train, tx_single_jet_test,len_init_data = prepare_data(tx_single_jet_train,tx_single_jet_test, best_deg)\n",
    "    \n",
    "    initial_w = np.zeros(tx_single_jet_train.shape[1]);\n",
    "    w, loss = func_GD (y=y_single_jet_train, tx=tx_single_jet_train, test_set=tx_single_jet_test, max_iters=max_iters,gamma=best_gamma, initial_w=initial_w);\n",
    "    \n",
    "    y_pred_train = predict_labels(w, tx_single_jet_train)\n",
    "    y_pred_test = predict_labels(w, tx_single_jet_test)\n",
    "    y_preds_train[mask_jets_train[mask_jet_id]] = y_pred_train\n",
    "    y_preds_test[mask_jets_test[mask_jet_id]] = y_pred_test\n",
    "    \n",
    "    right_train = np.sum(y_single_jet_train == y_pred_train)/len(y_single_jet_train )*100\n",
    "    \n",
    "    print('**** Jet ', mask_jet_id, '*****accuracy jet',right_train)\n",
    "    \n",
    "\n",
    "create_csv_submission(ids_test, y_preds_test, OUT_FOLDER+name)\n",
    "right_train_total = np.sum(y_train == y_preds_train)/len(y_train)*100\n",
    "\n",
    "print('>>>>>>>> Accuracy TOTAL ',right_train_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "deg=5;\n",
    "lambda_=1e-5\n",
    "mask_jets_train = split_jets_mask(tx_train)\n",
    "mask_jets_test = split_jets_mask(tx_test)\n",
    "len_mask = len(mask_jets_train)\n",
    "for mask_jet_id in range(len_mask):\n",
    "    print('**** Starting Jet ', mask_jet_id, '*****')\n",
    "    tx_single_jet_train = tx_train[mask_jets_train[mask_jet_id]]\n",
    "    tx_single_jet_test =  tx_test[mask_jets_test[mask_jet_id]]\n",
    "    y_single_jet_train =   y_train[mask_jets_train[mask_jet_id]]\n",
    "    tx_single_jet_train, tx_single_jet_test, len_init = prepare_data(tx_single_jet_train, tx_single_jet_test, deg);\n",
    "    \n",
    "    [w,loss]=func_ridge_regression (y=y_single_jet_train, tx=tx_single_jet_train, test_set=tx_single_jet_test, lambda_=lambda_);    \n",
    "    \n",
    "    y_pred_train = predict_labels(w, tx_single_jet_train)\n",
    "    y_pred_test = predict_labels(w, tx_single_jet_test)\n",
    "    y_preds_train[mask_jets_train[mask_jet_id]] = y_pred_train\n",
    "    y_preds_test[mask_jets_test[mask_jet_id]] = y_pred_test\n",
    "    \n",
    "    right_train = np.sum(y_single_jet_train == y_pred_train)/len(y_single_jet_train )*100\n",
    "    \n",
    "    print('**** Jet ', mask_jet_id, '*****accuracy jet',right_train)\n",
    "    \n",
    "\n",
    "    \n",
    "right_train_total = np.sum(y_train == y_preds_train)/len(y_train )*100\n",
    "print('>>>>>>>> Accuracy TOTAL ',right_train_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##  SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def func_SGD (y, tx, test_set, max_iters, gamma, initial_w, batch_size):\n",
    "    name = 'Stochastic_Gradient_descent'\n",
    "    \n",
    "    w,loss = least_squares_SGD(y, tx, initial_w, max_iters,\\\n",
    "                               gamma, batch_size)\n",
    "    \n",
    "    y_pred = predict_labels(w, test_set)\n",
    "    create_csv_submission(ids_test, y_pred, OUT_FOLDER+name)\n",
    "    \n",
    "    plt.plot(w, 'go')\n",
    "    plt.title('SGD: weights with loss:  '+str(loss),fontsize=15, fontweight='bold');\n",
    "    plt.show()\n",
    "    \n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "deg=5;\n",
    "lambda_=1e-5\n",
    "mask_jets_train = split_jets_mask(tx_train)\n",
    "mask_jets_test = split_jets_mask(tx_test)\n",
    "len_mask = len(mask_jets_train)\n",
    "for mask_jet_id in range(len_mask):\n",
    "    print('**** Starting Jet ', mask_jet_id, '*****')\n",
    "    tx_single_jet_train = tx_train[mask_jets_train[mask_jet_id]]\n",
    "    tx_single_jet_test =  tx_test[mask_jets_test[mask_jet_id]]\n",
    "    y_single_jet_train =   y_train[mask_jets_train[mask_jet_id]]\n",
    "    tx_single_jet_train, tx_single_jet_test, len_init = prepare_data(tx_single_jet_train, tx_single_jet_test, deg);\n",
    "    \n",
    "    initial_w = np.zeros(tx_single_jet_train.shape[1]);\n",
    "    w, loss = func_SGD (y=y_single_jet_train, tx=tx_single_jet_train, test_set=tx_single_jet_test, max_iters=500,gamma=0.00005, initial_w=initial_w,\\\n",
    "                   batch_size=1);    \n",
    "   \n",
    "    y_pred_train = predict_labels(w, tx_single_jet_train)\n",
    "    y_pred_test = predict_labels(w, tx_single_jet_test)\n",
    "    y_preds_train[mask_jets_train[mask_jet_id]] = y_pred_train\n",
    "    y_preds_test[mask_jets_test[mask_jet_id]] = y_pred_test\n",
    "    \n",
    "    right_train = np.sum(y_single_jet_train == y_pred_train)/len(y_single_jet_train )*100\n",
    "    \n",
    "    print('**** Jet ', mask_jet_id, '*****accuracy jet',right_train)\n",
    "    \n",
    "\n",
    "    \n",
    "right_train_total = np.sum(y_train == y_preds_train)/len(y_train )*100\n",
    "print('>>>>>>>> Accuracy TOTAL ',right_train_total)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "def cross_validation_logistic_regression(y,tx, k_fold, max_iters, gammas):\n",
    "    \n",
    "    accuracies_test=np.zeros(len(gammas))\n",
    "    accuracies_train=np.zeros(len(gammas))\n",
    "    acc_max=0;\n",
    "    gamma_best=0;\n",
    "    \n",
    "    for i, single_gamma in enumerate(gammas):\n",
    "        \n",
    "        seed=1;\n",
    "\n",
    "        # get k'th subgroup in test, others in train\n",
    "        k_indices = build_k_indices(y, k_fold, seed)\n",
    "        accuracy_train = np.zeros(k_fold)\n",
    "        accuracy_test = np.zeros(k_fold)\n",
    "\n",
    "\n",
    "        for k in range(k_fold):\n",
    "            #print('----- FOLD', k, '-----')\n",
    "            k_index = k_indices[k]\n",
    "            test_y = y[k_index]\n",
    "            test_tx = tx[k_index,:]\n",
    "            mask = np.ones(len(y), dtype=bool) # set all elements to True\n",
    "            mask[k_index] = False              # set test elements to False\n",
    "            train_tx = tx[mask,:]              # select only True elements (ie train elements)\n",
    "            train_y = y[mask]\n",
    "            initial_w = np.zeros(train_tx.shape[1]);\n",
    "            weights,loss = logistic_regression(train_y, train_tx, initial_w, max_iters, single_gamma)\n",
    "            # Compute the predictions\n",
    "            y_pred_train = predict_labels(weights, train_tx)\n",
    "            y_pred_test = predict_labels(weights, test_tx);\n",
    "            accuracy_train[k] = np.sum(y_pred_train == train_y)/len(train_y)\n",
    "            accuracy_test[k] = np.sum(y_pred_test == test_y)/len(test_y)\n",
    "            \n",
    "        accuracies_test[i]=np.mean(accuracy_test);\n",
    "        accuracies_train[i]=np.mean(accuracy_train);\n",
    "        \n",
    "        print('GAMMA:', single_gamma, '---','ACCURANCY TEST:',accuracies_test[i], '---','ACCURANCY TRAIN:',accuracies_train[i])\n",
    "        if (accuracies_test[i]>acc_max):\n",
    "                gamma_best=gammas[i];\n",
    "                acc_max=accuracies_test[i];\n",
    "    \n",
    "    return [gamma_best,acc_max]\n",
    "               \n",
    "        # Compute accuracy of the predictions\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def func_logistic (y, tx, test_set, max_iters, gamma, initial_w):\n",
    "    name = 'Logistic regression'\n",
    "    \n",
    "    #w,loss = logistic_regression(y, tx, initial_w, max_iters, gamma)\n",
    "    w,loss = logistic_regression(y, tx, initial_w, max_iters, gamma)\n",
    "    \n",
    "    loss=loss/len(y);\n",
    "    y_pred = predict_labels(w, test_set)\n",
    "    create_csv_submission(ids_test, y_pred, OUT_FOLDER+name)  \n",
    "    \n",
    "    plt.plot(w, 'go')\n",
    "    plt.title('logistic reg: weights with loss:  '+str(loss),fontsize=15, fontweight='bold');\n",
    "    plt.show()\n",
    "    \n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gammas=np.linspace(0.00001,0.9,10);\n",
    "k_fold=3;\n",
    "max_iters=1000;\n",
    "deg=5;\n",
    "\n",
    "\n",
    "mask_jets_train = split_jets_mask(tx_train)\n",
    "mask_jets_test = split_jets_mask(tx_test)\n",
    "len_mask = len(mask_jets_train)\n",
    "for mask_jet_id in range(len_mask):\n",
    "    print('**** Starting Jet ', mask_jet_id, '*****')\n",
    "    tx_single_jet_train = tx_train[mask_jets_train[mask_jet_id]]\n",
    "    tx_single_jet_test =  tx_test[mask_jets_test[mask_jet_id]]\n",
    "    y_single_jet_train =   y_train[mask_jets_train[mask_jet_id]]\n",
    "    tx_single_jet_train, tx_single_jet_test, len_init = prepare_data(tx_single_jet_train, tx_single_jet_test, deg);\n",
    "    \n",
    "    \n",
    "    \n",
    "    [best_gamma,acc_max]=cross_validation_logistic_regression(y_single_jet_train, tx_single_jet_train, k_fold, max_iters, gammas)\n",
    "    \n",
    "    print('BEST GAMMA:', best_gamma, '---','ACCURANCY:',acc_max)\n",
    "\n",
    "    initial_w = np.zeros(tx_single_jet_train.shape[1]);\n",
    "    w, loss = func_logistic (y=y_single_jet_train, tx=tx_single_jet_train, test_set=tx_single_jet_test, max_iters=1000,\\\n",
    "                         gamma=best_gamma, initial_w=initial_w);\n",
    "    \n",
    "    y_pred_train = predict_labels(w, tx_single_jet_train)\n",
    "    y_pred_test = predict_labels(w, tx_single_jet_test)\n",
    "    y_preds_train[mask_jets_train[mask_jet_id]] = y_pred_train\n",
    "    y_preds_test[mask_jets_test[mask_jet_id]] = y_pred_test\n",
    "    \n",
    "    right_train = np.sum(y_single_jet_train == y_pred_train)/len(y_single_jet_train )*100\n",
    "    \n",
    "    print('**** Jet ', mask_jet_id, '*****accuracy jet',right_train)\n",
    "    \n",
    "\n",
    "    \n",
    "right_train_total = np.sum(y_train == y_preds_train)/len(y_train )*100\n",
    "print('>>>>>>>> Accuracy TOTAL ',right_train_total)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOGISTIC REGULARIZED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "def cross_validation_logistic_regression_reg(y,tx, k_fold, max_iters, gammas,lambdas):\n",
    "    \n",
    "    acc_max=0;\n",
    "    accuracies_test=np.zeros([len(gammas),len(lambdas)]);\n",
    "    \n",
    "    accuracies_train=np.zeros([len(gammas),len(lambdas)]);\n",
    "    \n",
    "    \n",
    "    for i, single_gamma in enumerate(gammas):\n",
    "        for j, single_lambda in enumerate(lambdas):\n",
    "            seed=1;\n",
    "        \n",
    "            # get k'th subgroup in test, others in train\n",
    "            k_indices = build_k_indices(y, k_fold, seed)\n",
    "            accuracy_train = np.zeros(k_fold)\n",
    "            accuracy_test = np.zeros(k_fold)\n",
    "\n",
    "\n",
    "            for k in range(k_fold):\n",
    "                #print('----- FOLD', k, '-----')\n",
    "                k_index = k_indices[k]\n",
    "                test_y = y[k_index]\n",
    "                test_tx = tx[k_index,:]\n",
    "\n",
    "                mask = np.ones(len(y), dtype=bool) # set all elements to True\n",
    "                mask[k_index] = False              # set test elements to False\n",
    "                train_tx = tx[mask,:]              # select only True elements (ie train elements)\n",
    "                train_y = y[mask]\n",
    "                initial_w = np.zeros(train_tx.shape[1]);\n",
    "                weights,loss = reg_logistic_regression(train_y, train_tx,single_lambda, initial_w, max_iters, single_gamma)\n",
    "                # Compute the predictions\n",
    "                y_pred_train = predict_labels(weights, train_tx)\n",
    "                y_pred_test = predict_labels(weights, test_tx)\n",
    "                predictions=True;\n",
    "                accuracy_train[k] = np.sum(y_pred_train == train_y)/len(train_y)\n",
    "                accuracy_test[k] = np.sum(y_pred_test == test_y)/len(test_y)\n",
    "                \n",
    "                \n",
    "            accuracies_test[i,j]= np.mean(accuracy_test);\n",
    "            accuracies_train[i,j]= np.mean(accuracy_train);\n",
    "            print('GAMMA', single_gamma, '---','LAMBDA', single_lambda, '---ACCURANCY TEST:',accuracies_test[i,j],'---ACCURANCY TRAIN:',accuracies_train[i,j])\n",
    "            \n",
    "            if (accuracies_test[i,j]>acc_max):\n",
    "                gamma_best=gammas[i];\n",
    "                lambda_best=lambdas[j];\n",
    "                acc_max=accuracies_test[i,j];\n",
    "            \n",
    "            \n",
    "    return [gamma_best,lambda_best,acc_max]\n",
    "     \n",
    "    \n",
    "    \n",
    "            # Compute accuracy of the predictions\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def func_logistic_reg (y, tx, lambda_, test_set, max_iters, gamma, initial_w):\n",
    "    name = 'Logistic regression regularized'\n",
    "    \n",
    "    #w,loss = logistic_regression(y, tx, initial_w, max_iters, gamma)\n",
    "    w,loss = reg_logistic_regression(y, tx, lambda_, initial_w, max_iters, gamma)\n",
    "    \n",
    "    loss=loss/len(y);\n",
    "    y_pred = predict_labels(w, test_set)\n",
    "    create_csv_submission(ids_test, y_pred, OUT_FOLDER+name)\n",
    "    \n",
    "    plt.plot(w, 'go')\n",
    "    plt.title('logistic reg: weights with the normalized log-like:  '+str(loss),fontsize=15, fontweight='bold');\n",
    "    plt.show()\n",
    "    \n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test cross_validation\n",
    "\n",
    "gammas=np.linspace(0.000001,0.0001,2);\n",
    "lambdas=np.linspace(0.001,0.3,2);\n",
    "k_fold=6;\n",
    "max_iters=50;\n",
    "\n",
    "deg=2;\n",
    "\n",
    "mask_jets_train = split_jets_mask(tx_train)\n",
    "mask_jets_test = split_jets_mask(tx_test)\n",
    "len_mask = len(mask_jets_train)\n",
    "for mask_jet_id in range(len_mask):\n",
    "    print('**** Starting Jet ', mask_jet_id, '*****')\n",
    "    tx_single_jet_train = tx_train[mask_jets_train[mask_jet_id]]\n",
    "    tx_single_jet_test =  tx_test[mask_jets_test[mask_jet_id]]\n",
    "    y_single_jet_train =   y_train[mask_jets_train[mask_jet_id]]\n",
    "    tx_single_jet_train, tx_single_jet_test, len_init\n",
    "    = prepare_data(tx_single_jet_train, tx_single_jet_test, deg);\n",
    "    \n",
    "    [gamma_best,lambda_best,acc_max]= cross_validation_logistic_regression_reg(y_single_jet_train, tx_single_jet_train, k_fold, max_iters, gammas,lambdas)\n",
    "    print('BEST GAMMA', gamma_best, '---','BEST LAMBDA', lambda_best, '---ACCURANCY:',acc_max)\n",
    "    \n",
    "    initial_w = np.zeros(tx_single_jet_train.shape[1]);\n",
    "    w, loss = func_logistic_reg (y=y_single_jet_train, tx=tx_single_jet_train, lambda_=lambda_best, test_set=tx_single_jet_test , max_iters=max_iters,\\\n",
    "                         gamma=gamma_best, initial_w=initial_w);\n",
    "\n",
    "\n",
    "    \n",
    "    y_pred_train = predict_labels(w, tx_single_jet_train)\n",
    "    y_pred_test = predict_labels(w, tx_single_jet_test)\n",
    "    y_preds_train[mask_jets_train[mask_jet_id]] = y_pred_train\n",
    "    y_preds_test[mask_jets_test[mask_jet_id]] = y_pred_test\n",
    "    \n",
    "    right_train = np.sum(y_single_jet_train == y_pred_train)/len(y_single_jet_train )*100\n",
    "    \n",
    "    print('**** Jet ', mask_jet_id, '*****accuracy jet',right_train)\n",
    "    \n",
    "\n",
    "    \n",
    "right_train_total = np.sum(y_train == y_preds_train)/len(y_train )*100\n",
    "print('>>>>>>>> Accuracy TOTAL ',right_train_total)\n",
    "\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
