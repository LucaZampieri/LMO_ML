{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Project 1\n",
    "\n",
    "    We begin by loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from preprocessing_functions import *\n",
    "%matplotlib inline \n",
    "import numpy as np   # generic stuff\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#### REMOVE THIS LINE BEFORE SUBMISSION\n",
    "import pandas as pd\n",
    "#######################################################################\n",
    "\n",
    "from lib.proj1_helpers import * #the helper provided for the project\n",
    "from lib.costs import *\n",
    "\n",
    "# choose which implementations you would like\n",
    "from lib.implementations import *\n",
    "#from implementations import * #our implementations of the functions done by us\n",
    "\n",
    "\n",
    "import datetime\n",
    "from helpers import * #helpers of exo 2\n",
    "# Useful starting lines\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_FOLDER = '../data/' # get rid of the ..\n",
    "\n",
    "y_train, tx_train, ids_train = load_csv_data(DATA_FOLDER+'train.csv',sub_sample=False)\n",
    "\n",
    "y_test, tx_test, ids_test = load_csv_data(DATA_FOLDER+'test.csv',sub_sample=False)\n",
    "AAA = 0;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note that here we are only considering a sub_sample as the \"True\" value indicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 30)\n",
      "(568238, 30)\n",
      "(250000,)\n"
     ]
    }
   ],
   "source": [
    "print(tx_train.shape)\n",
    "print(tx_test.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>138.470</td>\n",
       "      <td>51.655</td>\n",
       "      <td>97.827</td>\n",
       "      <td>27.980</td>\n",
       "      <td>0.91</td>\n",
       "      <td>124.711</td>\n",
       "      <td>2.666</td>\n",
       "      <td>3.064</td>\n",
       "      <td>41.928</td>\n",
       "      <td>197.760</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.277</td>\n",
       "      <td>258.733</td>\n",
       "      <td>2.0</td>\n",
       "      <td>67.435</td>\n",
       "      <td>2.150</td>\n",
       "      <td>0.444</td>\n",
       "      <td>46.062</td>\n",
       "      <td>1.24</td>\n",
       "      <td>-2.475</td>\n",
       "      <td>113.497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>160.937</td>\n",
       "      <td>68.768</td>\n",
       "      <td>103.235</td>\n",
       "      <td>48.146</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.473</td>\n",
       "      <td>2.078</td>\n",
       "      <td>125.157</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.916</td>\n",
       "      <td>164.546</td>\n",
       "      <td>1.0</td>\n",
       "      <td>46.226</td>\n",
       "      <td>0.725</td>\n",
       "      <td>1.158</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>46.226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-999.000</td>\n",
       "      <td>162.172</td>\n",
       "      <td>125.953</td>\n",
       "      <td>35.635</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.148</td>\n",
       "      <td>9.336</td>\n",
       "      <td>197.814</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.186</td>\n",
       "      <td>260.414</td>\n",
       "      <td>1.0</td>\n",
       "      <td>44.251</td>\n",
       "      <td>2.053</td>\n",
       "      <td>-2.028</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>44.251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>143.905</td>\n",
       "      <td>81.417</td>\n",
       "      <td>80.943</td>\n",
       "      <td>0.414</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.310</td>\n",
       "      <td>0.414</td>\n",
       "      <td>75.968</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060</td>\n",
       "      <td>86.062</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>175.864</td>\n",
       "      <td>16.915</td>\n",
       "      <td>134.805</td>\n",
       "      <td>16.405</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.891</td>\n",
       "      <td>16.405</td>\n",
       "      <td>57.983</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.871</td>\n",
       "      <td>53.131</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0        1        2       3       4        5        6      7       8   \\\n",
       "0  138.470   51.655   97.827  27.980    0.91  124.711    2.666  3.064  41.928   \n",
       "1  160.937   68.768  103.235  48.146 -999.00 -999.000 -999.000  3.473   2.078   \n",
       "2 -999.000  162.172  125.953  35.635 -999.00 -999.000 -999.000  3.148   9.336   \n",
       "3  143.905   81.417   80.943   0.414 -999.00 -999.000 -999.000  3.310   0.414   \n",
       "4  175.864   16.915  134.805  16.405 -999.00 -999.000 -999.000  3.891  16.405   \n",
       "\n",
       "        9    ...        20       21   22       23       24       25       26  \\\n",
       "0  197.760   ...    -0.277  258.733  2.0   67.435    2.150    0.444   46.062   \n",
       "1  125.157   ...    -1.916  164.546  1.0   46.226    0.725    1.158 -999.000   \n",
       "2  197.814   ...    -2.186  260.414  1.0   44.251    2.053   -2.028 -999.000   \n",
       "3   75.968   ...     0.060   86.062  0.0 -999.000 -999.000 -999.000 -999.000   \n",
       "4   57.983   ...    -0.871   53.131  0.0 -999.000 -999.000 -999.000 -999.000   \n",
       "\n",
       "       27       28       29  \n",
       "0    1.24   -2.475  113.497  \n",
       "1 -999.00 -999.000   46.226  \n",
       "2 -999.00 -999.000   44.251  \n",
       "3 -999.00 -999.000    0.000  \n",
       "4 -999.00 -999.000    0.000  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# illegal: \n",
    "pd.DataFrame(tx_train).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for methods: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OUT_FOLDER = 'output/'\n",
    "\n",
    "def func_least_squares (y, tx, test_set, fct='mse'):\n",
    "    #name = 'least_squares'\n",
    "    w,loss = least_squares(y,tx,fct)\n",
    "    #y_pred = predict_labels(w, test_set)\n",
    "    #create_csv_submission(ids_test, y_pred, OUT_FOLDER+name)\n",
    "    \n",
    "    plt.plot(w, 'go')\n",
    "    plt.title('least squares weights for loss:  '+str(loss),fontsize=15, fontweight='bold');\n",
    "    plt.show()\n",
    "    \n",
    "    return w, loss\n",
    "\n",
    "def func_GD (y, tx, test_set, max_iters, gamma, initial_w):\n",
    "    w,loss = least_squares_GD(y, tx, initial_w, max_iters, gamma,fct='mse');\n",
    "    #y_pred = predict_labels(w, test_set)\n",
    "    #create_csv_submission(ids_test, y_pred, OUT_FOLDER+name)\n",
    "    \n",
    "    plt.plot(w, 'go')\n",
    "    plt.title('GD: weights with loss:  '+str(loss),fontsize=15, fontweight='bold');\n",
    "    plt.show()\n",
    "    \n",
    "    return w, loss\n",
    "\n",
    "########################  RIDGE REGRESSION #######################################################\n",
    "def func_ridge_regression (y, tx, test_set, lambda_):\n",
    "    name = 'Ridge_regression'\n",
    "    \n",
    "    w,loss = ridge_regression(y, tx, lambda_=lambda_, fct='mse');\n",
    "    \n",
    "    y_pred = predict_labels(w, test_set)\n",
    "    create_csv_submission(ids_test, y_pred, OUT_FOLDER+name)\n",
    "    \n",
    "    plt.plot(w, 'go')\n",
    "    plt.title('Ridge regression: weights ;loss:  '+str(loss),fontsize=15, fontweight='bold');\n",
    "    plt.show()\n",
    "    \n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LEAST-SQUARES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#todo: cross_validation su degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"Build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "def cross_validation_least_square(y, tx, k_fold, degs, seed):\n",
    "    \"\"\"Finds the best degree for Least Squares method with cross validation.\"\"\"\n",
    "    # Get k'th subgroup in test, others in train\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    \n",
    "    # Initialisation of the matrices of accuracies for all the degrees and different folds\n",
    "    accuracy_train = np.zeros([k_fold, len(degs)])\n",
    "    accuracy_test = np.zeros([k_fold, len(degs)])\n",
    "\n",
    "    # Loop over the different folds\n",
    "    for k in range(k_fold):\n",
    "        print('----- FOLD', k, '-----')\n",
    "        k_index = k_indices[k]\n",
    "        test_y = y[k_index]\n",
    "        test_tx = tx[k_index,:]\n",
    "        mask = np.ones(len(y), dtype=bool) # set all elements to True\n",
    "        mask[k_index] = False              # set test elements to False\n",
    "        train_tx = tx[mask,:]              # select only True elements (ie train elements)\n",
    "        train_y = y[mask]\n",
    "\n",
    "        len_init_data = 0\n",
    "\n",
    "        # Loop over the different degrees to try\n",
    "        for i, single_deg in enumerate(degs):\n",
    "            print('++++ Deg =', single_deg)\n",
    "            if i==0:\n",
    "                # Preprocess the data (cleaning and adding features)\n",
    "                train_tx, test_tx, len_init_data = prepare_data(train_tx, test_tx, single_deg);\n",
    "            else:\n",
    "                nb_cols_tx = train_tx.shape[1]\n",
    "                # Add the supplementary powers of the features with respect to the previous iteration and standardize\n",
    "                train_tx = add_powers(train_tx, single_deg, 0, len_init_data, features='x', current_max_deg=single_deg-1)\n",
    "                test_tx = add_powers(test_tx, single_deg, 0, len_init_data, features='x', current_max_deg=single_deg-1)\n",
    "                train_tx[:,nb_cols_tx:] = standardize(train_tx[:,nb_cols_tx:])[0]\n",
    "                test_tx[:,nb_cols_tx:] = standardize(test_tx[:,nb_cols_tx:])[0]\n",
    "                unique_cols = keep_unique_cols(train_tx)\n",
    "                train_tx = train_tx[:,unique_cols]\n",
    "                test_tx = test_tx[:,unique_cols]\n",
    "                \n",
    "            # Compute the weights with LS\n",
    "            weights, loss = least_squares(train_y, train_tx, fct='mse');\n",
    "\n",
    "            # Compute the predictions\n",
    "            y_pred_train = predict_labels(weights, train_tx)\n",
    "            y_pred_test = predict_labels(weights, test_tx);\n",
    "            accuracy_train[k, i] = np.sum(y_pred_train == train_y)/len(train_y)\n",
    "            accuracy_test[k, i] = np.sum(y_pred_test == test_y)/len(test_y)\n",
    "\n",
    "    accuracies_test = np.mean(accuracy_test, axis=0); # mean on the k's\n",
    "    accuracies_train = np.mean(accuracy_train, axis=0);\n",
    "\n",
    "    max_index, acc_max = max(enumerate(accuracies_test), key=operator.itemgetter(1))\n",
    "    print(accuracy_test)\n",
    "    print(accuracies_test)\n",
    "    print(k_fold)\n",
    "    print(len(degs))\n",
    "    print(max_index)\n",
    "    print(acc_max)\n",
    "    deg_best = degs[max_index]\n",
    "    \n",
    "    return deg_best, acc_max;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Starting Jet  0 *****\n",
      "----- FOLD 0 -----\n",
      "++++ Deg = 2\n",
      "++++ Deg = 3\n",
      "++++ Deg = 4\n",
      "++++ Deg = 5\n",
      "++++ Deg = 6\n",
      "++++ Deg = 7\n",
      "++++ Deg = 8\n",
      "++++ Deg = 9\n",
      "++++ Deg = 10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-d63810797048>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0my_single_jet_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask_jets_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask_jet_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mbest_deg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validation_least_square\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_single_jet_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx_single_jet_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_fold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Best degree = '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_deg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-678a387f66f5>\u001b[0m in \u001b[0;36mcross_validation_least_square\u001b[0;34m(y, tx, k_fold, degs, seed)\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mtrain_tx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnb_cols_tx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstandardize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_tx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnb_cols_tx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0mtest_tx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnb_cols_tx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstandardize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_tx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnb_cols_tx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                 \u001b[0munique_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeep_unique_cols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_tx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m                 \u001b[0mtrain_tx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_tx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0munique_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0mtest_tx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_tx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0munique_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/matteociprian/Documents/GitHub/LMO_ML/project1/A_final/preprocessing_functions.py\u001b[0m in \u001b[0;36mkeep_unique_cols\u001b[0;34m(tx)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0merase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mid_loop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m                     \u001b[0merase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                     \u001b[0mequal_to\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "OUT_FOLDER = 'output/'\n",
    "name = 'least_squares.csv'\n",
    "\n",
    "degs=range(2,14)\n",
    "k_fold=5\n",
    "seed=1;\n",
    "\n",
    "mask_jets_train = split_jets_mask(tx_train)\n",
    "mask_jets_test = split_jets_mask(tx_test)\n",
    "len_mask = len(mask_jets_train)\n",
    "y_preds_train = np.zeros(len(y_train))\n",
    "y_preds_test = np.zeros(tx_test.shape[0])\n",
    "\n",
    "\n",
    "for mask_jet_id in range(len_mask):\n",
    "    print('**** Starting Jet ', mask_jet_id, '*****')\n",
    "    tx_single_jet_train = tx_train[mask_jets_train[mask_jet_id]]\n",
    "    tx_single_jet_test = tx_test[mask_jets_test[mask_jet_id]]\n",
    "    y_single_jet_train = y_train[mask_jets_train[mask_jet_id]]\n",
    "    \n",
    "    best_deg, best_acc = cross_validation_least_square(y_single_jet_train, tx_single_jet_train, k_fold, degs,seed)\n",
    "    print('Best degree = ', best_deg)\n",
    "    \n",
    "    tx_single_jet_train, tx_single_jet_test,len_init_data = prepare_data(tx_single_jet_train,tx_single_jet_test, best_deg)\n",
    "    w, loss = func_least_squares(y=y_single_jet_train, tx=tx_single_jet_train, test_set=tx_single_jet_test, fct='mse')\n",
    "    \n",
    "    y_pred_train = predict_labels(w, tx_single_jet_train)\n",
    "    y_pred_test = predict_labels(w, tx_single_jet_test)\n",
    "    y_preds_train[mask_jets_train[mask_jet_id]] = y_pred_train\n",
    "    y_preds_test[mask_jets_test[mask_jet_id]] = y_pred_test\n",
    "    \n",
    "    right_train = np.sum(y_single_jet_train == y_pred_train)/len(y_single_jet_train)*100\n",
    "    \n",
    "    print('**** Jet ', mask_jet_id, '***** Accuracy jet', right_train)\n",
    "    \n",
    "\n",
    "create_csv_submission(ids_test, y_preds_test, OUT_FOLDER+name)\n",
    "right_train_total = np.sum(y_train == y_preds_train)/len(y_train)*100\n",
    "print('>>>>>>>> Accuracy TOTAL ',right_train_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(y_preds_test[y_preds_test==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(y_preds_test[y_preds_test==-1])+len(y_preds_test[y_preds_test==1])==tx_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_preds_test[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cross validation su gamma e degree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "\n",
    "\n",
    "def cross_validation_GD(y, tx, k_fold, max_iters, degs,lambdas):            \n",
    "    seed=1;\n",
    "    \n",
    "    # get k'th subgroup in test, others in train\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    accuracy_train = np.zeros([k_fold, len(degs),len(gammas)])\n",
    "    accuracy_test = np.zeros([k_fold, len(degs),len(gammas)])\n",
    "\n",
    "    acc_max=0;\n",
    "    deg_best=0;\n",
    "    gammas_best=0;\n",
    "               \n",
    "    for k in range(k_fold):\n",
    "        print('----- FOLD', k, '-----')\n",
    "        k_index = k_indices[k]\n",
    "        test_y = y[k_index]\n",
    "        test_tx = tx[k_index,:]\n",
    "        mask = np.ones(len(y), dtype=bool) # set all elements to True\n",
    "        mask[k_index] = False              # set test elements to False\n",
    "        train_tx = tx[mask,:]              # select only True elements (ie train elements)\n",
    "        train_y = y[mask]\n",
    "\n",
    "        len_init_data = 0\n",
    "\n",
    "        for i, single_deg in enumerate(degs):\n",
    "            print('++++ Deg =', single_deg)\n",
    "            for j , single_gamma_ in  enumerate(gammas):\n",
    "                print('++++ gamma =', single_gamma_)\n",
    "                \n",
    "                if i==0 and j==0:\n",
    "                    train_tx, test_tx, len_init_data = prepare_data(train_tx, test_tx, single_deg);\n",
    "                else:\n",
    "                    shape_tx=train_tx.shape[1];\n",
    "                    print('ciaociaociao')\n",
    "                    train_tx = add_powers(train_tx, single_deg, 0, len_init_data, features='x', current_max_deg=single_deg-1)\n",
    "                    test_tx = add_powers(test_tx, single_deg, 0, len_init_data, features='x', current_max_deg=single_deg-1)\n",
    "                    train_tx[:,shape_tx:]=standardize(train_tx[:,shape_tx:])[0]\n",
    "                initial_w = np.zeros(train_tx.shape[1]);\n",
    "                \n",
    "                weights,loss = least_squares_GD(train_y,train_tx, initial_w, max_iters, single_gamma_,fct='mse');\n",
    "\n",
    "                # Compute the predictions\n",
    "                y_pred_train = predict_labels(weights, train_tx);\n",
    "                y_pred_test = predict_labels(weights, test_tx);\n",
    "                accuracy_train[k, i,j] = np.sum(y_pred_train == train_y)/len(train_y)\n",
    "                accuracy_test[k, i,j] = np.sum(y_pred_test == test_y)/len(test_y)\n",
    "        \n",
    "      \n",
    "                \n",
    "    accuracies_test=np.mean(accuracy_test, axis=0); # mean on the k's\n",
    "    accuracies_train=np.mean(accuracy_train, axis=0);\n",
    "\n",
    " \n",
    "    for i, single_deg in enumerate(degs):\n",
    "         for j , single_gamma_ in  enumerate(gammas):\n",
    "                if (accuracies_test[i,j]>acc_max):\n",
    "                    deg_best=degs[i];\n",
    "                    gammas_best=gammas[j];\n",
    "                    acc_mac=accuracies_test[i,j];\n",
    "    \n",
    "    \n",
    "                    \n",
    "        \n",
    "                \n",
    "    return [deg_best, gamma_best, acc_max];\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1]\n",
      " [2 2 2]]\n",
      "(2, 2)\n",
      "[[ 1 -1 -1]\n",
      " [ 2  1  1]]\n"
     ]
    }
   ],
   "source": [
    "cubic=np.array([[1,1,1],[2,2,2]])\n",
    "print(cubic)\n",
    "print (cubic[:,1:].shape)\n",
    "\n",
    "cubic[:,1:]=standardize(cubic[:,1:])[0];\n",
    "print(cubic)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Starting Jet  0 *****\n",
      "----- FOLD 0 -----\n",
      "++++ Deg = 4\n",
      "++++ gamma = 0.0001\n",
      "Gradient Descent(0/399): loss=0.4999999999999887, w0=-6.887232304259569e-08, w1=-3.5851239558151825e-05\n",
      "Gradient Descent(1/399): loss=0.4997886023020471, w0=-1.3279692118023493e-07, w1=-7.165426932699993e-05\n",
      "Gradient Descent(2/399): loss=0.49957776994619885, w0=-1.9180268329462098e-07, w1=-0.00010740917970274563\n",
      "Gradient Descent(3/399): loss=0.49936750087119885, w0=-2.459183816411012e-07, w1=-0.00014311606088238587\n",
      "Gradient Descent(4/399): loss=0.49915779302441243, w0=-2.951726722743844e-07, w1=-0.000178775002864219\n",
      "Gradient Descent(5/399): loss=0.498948644361784, w0=-3.395940954727797e-07, w1=-0.00021438609544834942\n",
      "Gradient Descent(6/399): loss=0.49874005284779666, w0=-3.792110761609197e-07, w1=-0.0002499494282371909\n",
      "Gradient Descent(7/399): loss=0.4985320164554317, w0=-4.1405192433098656e-07, w1=-0.00028546509063596823\n",
      "Gradient Descent(8/399): loss=0.4983245331661278, w0=-4.4414483546245015e-07, w1=-0.0003209331718532171\n",
      "Gradient Descent(9/399): loss=0.4981176009697414, w0=-4.6951789094031177e-07, w1=-0.00035635376090128356\n",
      "Gradient Descent(10/399): loss=0.4979112178645054, w0=-4.901990584718737e-07, w1=-0.000391726946596821\n",
      "Gradient Descent(11/399): loss=0.49770538185699026, w0=-5.062161925020312e-07, w1=-0.00042705281756128617\n",
      "Gradient Descent(12/399): loss=0.49750009096206405, w0=-5.175970346270864e-07, w1=-0.0004623314622214339\n",
      "Gradient Descent(13/399): loss=0.4972953432028535, w0=-5.24369214007105e-07, w1=-0.0004975629688098099\n",
      "Gradient Descent(14/399): loss=0.4970911366107044, w0=-5.265602477768134e-07, w1=-0.000532747425365243\n",
      "Gradient Descent(15/399): loss=0.49688746922514226, w0=-5.241975414550211e-07, w1=-0.0005678849197333352\n",
      "Gradient Descent(16/399): loss=0.49668433909383425, w0=-5.173083893526141e-07, w1=-0.0006029755395669503\n",
      "Gradient Descent(17/399): loss=0.4964817442725501, w0=-5.059199749790999e-07, w1=-0.0006380193723267019\n",
      "Gradient Descent(18/399): loss=0.4962796828251246, w0=-4.90059371447695e-07, w1=-0.0006730165052814394\n",
      "Gradient Descent(19/399): loss=0.4960781528234176, w0=-4.6975354187899173e-07, w1=-0.0007079670255087326\n",
      "Gradient Descent(20/399): loss=0.49587715234727864, w0=-4.4502933980319824e-07, w1=-0.0007428710198953553\n",
      "Gradient Descent(21/399): loss=0.4956766794845071, w0=-4.1591350956093465e-07, w1=-0.0007777285751377666\n",
      "Gradient Descent(22/399): loss=0.4954767323308153, w0=-3.8243268670263026e-07, w1=-0.0008125397777425925\n",
      "Gradient Descent(23/399): loss=0.4952773089897917, w0=-3.4461339838648637e-07, w1=-0.0008473047140271043\n",
      "Gradient Descent(24/399): loss=0.4950784075728629, w0=-3.024820637750462e-07, w1=-0.000882023470119697\n",
      "Gradient Descent(25/399): loss=0.4948800261992581, w0=-2.5606499443034053e-07, w1=-0.0009166961319603652\n",
      "Gradient Descent(26/399): loss=0.4946821629959701, w0=-2.053883947076479e-07, w1=-0.0009513227853011792\n",
      "Gradient Descent(27/399): loss=0.49448481609772077, w0=-1.5047836214785388e-07, w1=-0.0009859035157067575\n",
      "Gradient Descent(28/399): loss=0.49428798364692395, w0=-9.136088786841646e-08, w1=-0.0010204384085547404\n",
      "Gradient Descent(29/399): loss=0.49409166379364944, w0=-2.806185695295112e-08, w1=-0.0010549275490362602\n",
      "Gradient Descent(30/399): loss=0.49389585469558656, w0=3.93929511605685e-08, w1=-0.0010893710221564117\n",
      "Gradient Descent(31/399): loss=0.49370055451800926, w0=1.1097786229298799e-07, w1=-0.0011237689127347197\n",
      "Gradient Descent(32/399): loss=0.4935057614337398, w0=1.8666730713851785e-07, w1=-0.0011581213054056069\n",
      "Gradient Descent(33/399): loss=0.4933114736231136, w0=2.664358208806434e-07, w1=-0.001192428284618859\n",
      "Gradient Descent(34/399): loss=0.49311768927394417, w0=3.5025804280940357e-07, w1=-0.0012266899346400897\n",
      "Gradient Descent(35/399): loss=0.49292440658148806, w0=4.3810871594001577e-07, w1=-0.0012609063395512036\n",
      "Gradient Descent(36/399): loss=0.4927316237484097, w0=5.299626866328737e-07, w1=-0.0012950775832508575\n",
      "Gradient Descent(37/399): loss=0.49253933898474794, w0=6.25794904214862e-07, w1=-0.0013292037494549215\n",
      "Gradient Descent(38/399): loss=0.49234755050787926, w0=7.255804206020422e-07, w1=-0.0013632849216969375\n",
      "Gradient Descent(39/399): loss=0.49215625654248607, w0=8.292943899236579e-07, w1=-0.0013973211833285778\n",
      "Gradient Descent(40/399): loss=0.4919654553205213, w0=9.369120681474894e-07, w1=-0.0014313126175201007\n",
      "Gradient Descent(41/399): loss=0.49177514508117437, w0=1.048408812706515e-06, w1=-0.0014652593072608069\n",
      "Gradient Descent(42/399): loss=0.4915853240708382, w0=1.16376008212692e-06, w1=-0.0014991613353594923\n",
      "Gradient Descent(43/399): loss=0.49139599054307503, w0=1.282941435657409e-06, w1=-0.0015330187844449013\n",
      "Gradient Descent(44/399): loss=0.49120714275858346, w0=1.4059285328998346e-06, w1=-0.0015668317369661783\n",
      "Gradient Descent(45/399): loss=0.491018778985165, w0=1.532697133441154e-06, w1=-0.0016006002751933171\n",
      "Gradient Descent(46/399): loss=0.49083089749769127, w0=1.6632230964866587e-06, w1=-0.0016343244812176106\n",
      "Gradient Descent(47/399): loss=0.49064349657807066, w0=1.7974823804945325e-06, w1=-0.0016680044369520974\n",
      "Gradient Descent(48/399): loss=0.49045657451521646, w0=1.935451042811696e-06, w1=-0.0017016402241320089\n",
      "Gradient Descent(49/399): loss=0.4902701296050139, w0=2.077105239310945e-06, w1=-0.0017352319243152138\n",
      "Gradient Descent(50/399): loss=0.490084160150288, w0=2.2224212240293683e-06, w1=-0.0017687796188826622\n",
      "Gradient Descent(51/399): loss=0.4898986644607707, w0=2.3713753488080608e-06, w1=-0.0018022833890388276\n",
      "Gradient Descent(52/399): loss=0.4897136408530704, w0=2.5239440629331013e-06, w1=-0.0018357433158121487\n",
      "Gradient Descent(53/399): loss=0.4895290876506383, w0=2.680103912777819e-06, w1=-0.001869159480055469\n",
      "Gradient Descent(54/399): loss=0.48934500318373847, w0=2.8398315414463155e-06, w1=-0.0019025319624464754\n",
      "Gradient Descent(55/399): loss=0.4891613857894153, w0=3.00310368841826e-06, w1=-0.0019358608434881363\n",
      "Gradient Descent(56/399): loss=0.4889782338114624, w0=3.169897189194943e-06, w1=-0.0019691462035091372\n",
      "Gradient Descent(57/399): loss=0.48879554560039185, w0=3.340188974946584e-06, w1=-0.002002388122664316\n",
      "Gradient Descent(58/399): loss=0.48861331951340237, w0=3.513956072160888e-06, w1=-0.0020355866809350976\n",
      "Gradient Descent(59/399): loss=0.48843155391434956, w0=3.691175602292862e-06, w1=-0.002068741958129924\n",
      "Gradient Descent(60/399): loss=0.48825024717371457, w0=3.8718247814158575e-06, w1=-0.002101854033884689\n",
      "Gradient Descent(61/399): loss=0.4880693976685735, w0=4.05588091987386e-06, w1=-0.002134922987663166\n",
      "Gradient Descent(62/399): loss=0.4878890037825678, w0=4.243321421935014e-06, w1=-0.0021679488987574372\n",
      "Gradient Descent(63/399): loss=0.48770906390587343, w0=4.434123785446379e-06, w1=-0.002200931846288322\n",
      "Gradient Descent(64/399): loss=0.4875295764351707, w0=4.628265601489895e-06, w1=-0.0022338719092058028\n",
      "Gradient Descent(65/399): loss=0.4873505397736154, w0=4.825724554039599e-06, w1=-0.002266769166289451\n",
      "Gradient Descent(66/399): loss=0.4871719523308075, w0=5.026478419620024e-06, w1=-0.0022996236961488506\n",
      "Gradient Descent(67/399): loss=0.48699381252276386, w0=5.23050506696583e-06, w1=-0.002332435577224021\n",
      "Gradient Descent(68/399): loss=0.48681611877188613, w0=5.437782456682656e-06, w1=-0.0023652048877858393\n",
      "Gradient Descent(69/399): loss=0.48663886950693386, w0=5.648288640909134e-06, w1=-0.0023979317059364593\n",
      "Gradient Descent(70/399): loss=0.4864620631629938, w0=5.862001762980157e-06, w1=-0.002430616109609733\n",
      "Gradient Descent(71/399): loss=0.486285698181452, w0=6.0789000570912916e-06, w1=-0.002463258176571628\n",
      "Gradient Descent(72/399): loss=0.48610977300996455, w0=6.298961847964414e-06, w1=-0.002495857984420642\n",
      "Gradient Descent(73/399): loss=0.48593428610242895, w0=6.522165550514532e-06, w1=-0.0025284156105882228\n",
      "Gradient Descent(74/399): loss=0.48575923591895565, w0=6.748489669517769e-06, w1=-0.00256093113233918\n",
      "Gradient Descent(75/399): loss=0.4855846209258402, w0=6.9779127992805425e-06, w1=-0.002593404626772099\n",
      "Gradient Descent(76/399): loss=0.48541043959553376, w0=7.210413623309925e-06, w1=-0.002625836170819754\n",
      "Gradient Descent(77/399): loss=0.48523669040661666, w0=7.445970913985151e-06, w1=-0.002658225841249518\n",
      "Gradient Descent(78/399): loss=0.4850633718437695, w0=7.68456353223032e-06, w1=-0.0026905737146637743\n",
      "Gradient Descent(79/399): loss=0.48489048239774546, w0=7.92617042718823e-06, w1=-0.0027228798675003234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(80/399): loss=0.4847180205653432, w0=8.170770635895399e-06, w1=-0.0027551443760327927\n",
      "Gradient Descent(81/399): loss=0.48454598484937883, w0=8.41834328295822e-06, w1=-0.002787367316371041\n",
      "Gradient Descent(82/399): loss=0.48437437375865905, w0=8.66886758023028e-06, w1=-0.0028195487644615655\n",
      "Gradient Descent(83/399): loss=0.4842031858079534, w0=8.92232282649081e-06, w1=-0.0028516887960879055\n",
      "Gradient Descent(84/399): loss=0.4840324195179678, w0=9.1786884071243e-06, w1=-0.002883787486871045\n",
      "Gradient Descent(85/399): loss=0.4838620734153176, w0=9.437943793801222e-06, w1=-0.0029158449122698147\n",
      "Gradient Descent(86/399): loss=0.48369214603250016, w0=9.700068544159913e-06, w1=-0.0029478611475812944\n",
      "Gradient Descent(87/399): loss=0.4835226359078691, w0=9.965042301489575e-06, w1=-0.0029798362679412105\n",
      "Gradient Descent(88/399): loss=0.48335354158560695, w0=1.0232844794414407e-05, w1=-0.0030117703483243364\n",
      "Gradient Descent(89/399): loss=0.48318486161569973, w0=1.050345583657885e-05, w1=-0.0030436634635448885\n",
      "Gradient Descent(90/399): loss=0.4830165945539099, w0=1.077685532633397e-05, w1=-0.0030755156882569237\n",
      "Gradient Descent(91/399): loss=0.48284873896175096, w0=1.1053023246424927e-05, w1=-0.003107327096954734\n",
      "Gradient Descent(92/399): loss=0.48268129340646093, w0=1.1331939663679575e-05, w1=-0.003139097763973241\n",
      "Gradient Descent(93/399): loss=0.4825142564609769, w0=1.1613584728698168e-05, w1=-0.0031708277634883886\n",
      "Gradient Descent(94/399): loss=0.48234762670390907, w0=1.1897938675544162e-05, w1=-0.003202517169517535\n",
      "Gradient Descent(95/399): loss=0.4821814027195155, w0=1.2184981821436109e-05, w1=-0.0032341660559198453\n",
      "Gradient Descent(96/399): loss=0.48201558309767656, w0=1.2474694566440657e-05, w1=-0.0032657744963966767\n",
      "Gradient Descent(97/399): loss=0.4818501664338697, w0=1.2767057393166652e-05, w1=-0.0032973425644919727\n",
      "Gradient Descent(98/399): loss=0.481685151329144, w0=1.3062050866460295e-05, w1=-0.0033288703335926464\n",
      "Gradient Descent(99/399): loss=0.48152053639009595, w0=1.3359655633101407e-05, w1=-0.0033603578769289696\n",
      "Gradient Descent(100/399): loss=0.4813563202288432, w0=1.365985242150077e-05, w1=-0.003391805267574957\n",
      "Gradient Descent(101/399): loss=0.4811925014630012, w0=1.3962622041398545e-05, w1=-0.0034232125784487504\n",
      "Gradient Descent(102/399): loss=0.4810290787156574, w0=1.4267945383563745e-05, w1=-0.0034545798823130028\n",
      "Gradient Descent(103/399): loss=0.48086605061534826, w0=1.4575803419494799e-05, w1=-0.00348590725177526\n",
      "Gradient Descent(104/399): loss=0.48070341579603215, w0=1.4886177201121167e-05, w1=-0.003517194759288341\n",
      "Gradient Descent(105/399): loss=0.48054117289706877, w0=1.5199047860506024e-05, w1=-0.00354844247715072\n",
      "Gradient Descent(106/399): loss=0.4803793205631913, w0=1.5514396609549992e-05, w1=-0.0035796504775069044\n",
      "Gradient Descent(107/399): loss=0.48021785744448514, w0=1.5832204739695928e-05, w1=-0.0036108188323478118\n",
      "Gradient Descent(108/399): loss=0.48005678219636283, w0=1.6152453621634773e-05, w1=-0.0036419476135111477\n",
      "Gradient Descent(109/399): loss=0.4798960934795401, w0=1.647512470501242e-05, w1=-0.003673036892681782\n",
      "Gradient Descent(110/399): loss=0.479735789960013, w0=1.680019951813766e-05, w1=-0.0037040867413921223\n",
      "Gradient Descent(111/399): loss=0.4795758703090329, w0=1.7127659667691136e-05, w1=-0.003735097231022489\n",
      "Gradient Descent(112/399): loss=0.4794163332030855, w0=1.7457486838435348e-05, w1=-0.0037660684328014874\n",
      "Gradient Descent(113/399): loss=0.4792571773238648, w0=1.7789662792925688e-05, w1=-0.0037970004178063785\n",
      "Gradient Descent(114/399): loss=0.4790984013582521, w0=1.8124169371222493e-05, w1=-0.0038278932569634516\n",
      "Gradient Descent(115/399): loss=0.4789400039982916, w0=1.8460988490604128e-05, w1=-0.0038587470210483922\n",
      "Gradient Descent(116/399): loss=0.47878198394116817, w0=1.8800102145281104e-05, w1=-0.0038895617806866513\n",
      "Gradient Descent(117/399): loss=0.47862433988918407, w0=1.914149240611118e-05, w1=-0.003920337606353813\n",
      "Gradient Descent(118/399): loss=0.4784670705497365, w0=1.9485141420315515e-05, w1=-0.00395107456837596\n",
      "Gradient Descent(119/399): loss=0.4783101746352949, w0=1.98310314111958e-05, w1=-0.00398177273693004\n",
      "Gradient Descent(120/399): loss=0.4781536508633785, w0=2.0179144677852405e-05, w1=-0.004012432182044232\n",
      "Gradient Descent(121/399): loss=0.4779974979565341, w0=2.0529463594903554e-05, w1=-0.004043052973598305\n",
      "Gradient Descent(122/399): loss=0.4778417146423134, w0=2.0881970612205452e-05, w1=-0.004073635181323985\n",
      "Gradient Descent(123/399): loss=0.4776862996532515, w0=2.1236648254573442e-05, w1=-0.004104178874805315\n",
      "Gradient Descent(124/399): loss=0.4775312517268442, w0=2.159347912150415e-05, w1=-0.004134684123479013\n",
      "Gradient Descent(125/399): loss=0.47737656960552644, w0=2.195244588689861e-05, w1=-0.004165150996634837\n",
      "Gradient Descent(126/399): loss=0.47722225203665003, w0=2.2313531298786366e-05, w1=-0.004195579563415936\n",
      "Gradient Descent(127/399): loss=0.4770682977724634, w0=2.2676718179050587e-05, w1=-0.004225969892819215\n",
      "Gradient Descent(128/399): loss=0.4769147055700877, w0=2.3041989423154127e-05, w1=-0.004256322053695686\n",
      "Gradient Descent(129/399): loss=0.4767614741914973, w0=2.3409327999866575e-05, w1=-0.004286636114750826\n",
      "Gradient Descent(130/399): loss=0.47660860240349734, w0=2.3778716950992283e-05, w1=-0.004316912144544931\n",
      "Gradient Descent(131/399): loss=0.4764560889777034, w0=2.415013939109933e-05, w1=-0.004347150211493469\n",
      "Gradient Descent(132/399): loss=0.4763039326905186, w0=2.452357850724951e-05, w1=-0.004377350383867434\n",
      "Gradient Descent(133/399): loss=0.47615213232311493, w0=2.489901755872921e-05, w1=-0.004407512729793696\n",
      "Gradient Descent(134/399): loss=0.47600068666141027, w0=2.5276439876781304e-05, w1=-0.004437637317255351\n",
      "Gradient Descent(135/399): loss=0.47584959449604836, w0=2.5655828864337965e-05, w1=-0.004467724214092072\n",
      "Gradient Descent(136/399): loss=0.47569885462237826, w0=2.6037167995754452e-05, w1=-0.0044977734880004595\n",
      "Gradient Descent(137/399): loss=0.4755484658404335, w0=2.642044081654383e-05, w1=-0.0045277852065343845\n",
      "Gradient Descent(138/399): loss=0.47539842695491147, w0=2.680563094311263e-05, w1=-0.004557759437105339\n",
      "Gradient Descent(139/399): loss=0.4752487367751528, w0=2.7192722062497474e-05, w1=-0.0045876962469827785\n",
      "Gradient Descent(140/399): loss=0.4750993941151216, w0=2.7581697932102603e-05, w1=-0.004617595703294472\n",
      "Gradient Descent(141/399): loss=0.47495039779338477, w0=2.7972542379438382e-05, w1=-0.004647457873026839\n",
      "Gradient Descent(142/399): loss=0.47480174663309205, w0=2.836523930186069e-05, w1=-0.004677282823025299\n",
      "Gradient Descent(143/399): loss=0.47465343946195576, w0=2.8759772666311277e-05, w1=-0.004707070619994608\n",
      "Gradient Descent(144/399): loss=0.47450547511223146, w0=2.9156126509059017e-05, w1=-0.004736821330499203\n",
      "Gradient Descent(145/399): loss=0.47435785242069745, w0=2.9554284935442108e-05, w1=-0.004766535020963542\n",
      "Gradient Descent(146/399): loss=0.4742105702286353, w0=2.995423211961117e-05, w1=-0.00479621175767244\n",
      "Gradient Descent(147/399): loss=0.47406362738181007, w0=3.0355952304273253e-05, w1=-0.004825851606771412\n",
      "Gradient Descent(148/399): loss=0.4739170227304518, w0=3.07594298004368e-05, w1=-0.004855454634267003\n",
      "Gradient Descent(149/399): loss=0.47377075512923394, w0=3.116464898715745e-05, w1=-0.004885020906027134\n",
      "Gradient Descent(150/399): loss=0.47362482343725654, w0=3.157159431128481e-05, w1=-0.004914550487781429\n",
      "Gradient Descent(151/399): loss=0.4734792265180249, w0=3.198025028721009e-05, w1=-0.004944043445121554\n",
      "Gradient Descent(152/399): loss=0.47333396323943133, w0=3.239060149661464e-05, w1=-0.004973499843501548\n",
      "Gradient Descent(153/399): loss=0.4731890324737368, w0=3.280263258821943e-05, w1=-0.00500291974823816\n",
      "Gradient Descent(154/399): loss=0.4730444330975504, w0=3.3216328277535345e-05, w1=-0.005032303224511173\n",
      "Gradient Descent(155/399): loss=0.47290016399181145, w0=3.363167334661442e-05, w1=-0.005061650337363743\n",
      "Gradient Descent(156/399): loss=0.47275622404177114, w0=3.4048652643801954e-05, w1=-0.005090961151702724\n",
      "Gradient Descent(157/399): loss=0.4726126121369726, w0=3.446725108348951e-05, w1=-0.005120235732298998\n",
      "Gradient Descent(158/399): loss=0.47246932717123313, w0=3.488745364586878e-05, w1=-0.005149474143787805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(159/399): loss=0.4723263680426262, w0=3.530924537668634e-05, w1=-0.0051786764506690655\n",
      "Gradient Descent(160/399): loss=0.4721837336534616, w0=3.573261138699927e-05, w1=-0.005207842717307714\n",
      "Gradient Descent(161/399): loss=0.47204142291026857, w0=3.615753685293166e-05, w1=-0.005236973007934016\n",
      "Gradient Descent(162/399): loss=0.47189943472377655, w0=3.658400701543196e-05, w1=-0.0052660673866439\n",
      "Gradient Descent(163/399): loss=0.4717577680088971, w0=3.7012007180031225e-05, w1=-0.005295125917399276\n",
      "Gradient Descent(164/399): loss=0.4716164216847075, w0=3.744152271660219e-05, w1=-0.00532414866402836\n",
      "Gradient Descent(165/399): loss=0.47147539467443006, w0=3.7872539059119234e-05, w1=-0.005353135690225997\n",
      "Gradient Descent(166/399): loss=0.47133468590541683, w0=3.830504170541917e-05, w1=-0.0053820870595539805\n",
      "Gradient Descent(167/399): loss=0.47119429430912974, w0=3.8739016216962895e-05, w1=-0.005411002835441371\n",
      "Gradient Descent(168/399): loss=0.47105421882112436, w0=3.917444821859794e-05, w1=-0.0054398830811848205\n",
      "Gradient Descent(169/399): loss=0.47091445838103185, w0=3.961132339832178e-05, w1=-0.005468727859948883\n",
      "Gradient Descent(170/399): loss=0.47077501193254107, w0=4.004962750704603e-05, w1=-0.005497537234766341\n",
      "Gradient Descent(171/399): loss=0.4706358784233816, w0=4.048934635836153e-05, w1=-0.005526311268538513\n",
      "Gradient Descent(172/399): loss=0.47049705680530624, w0=4.093046582830417e-05, w1=-0.005555050024035577\n",
      "Gradient Descent(173/399): loss=0.4703585460340737, w0=4.137297185512162e-05, w1=-0.00558375356389688\n",
      "Gradient Descent(174/399): loss=0.47022034506943144, w0=4.181685043904089e-05, w1=-0.005612421950631254\n",
      "Gradient Descent(175/399): loss=0.47008245287509853, w0=4.2262087642036663e-05, w1=-0.005641055246617329\n",
      "Gradient Descent(176/399): loss=0.4699448684187493, w0=4.2708669587600533e-05, w1=-0.005669653514103846\n",
      "Gradient Descent(177/399): loss=0.4698075906719943, w0=4.315658246051101e-05, w1=-0.005698216815209966\n",
      "Gradient Descent(178/399): loss=0.4696706186103671, w0=4.360581250660434e-05, w1=-0.005726745211925586\n",
      "Gradient Descent(179/399): loss=0.4695339512133035, w0=4.405634603254622e-05, w1=-0.00575523876611164\n",
      "Gradient Descent(180/399): loss=0.46939758746412785, w0=4.4508169405604184e-05, w1=-0.005783697539500415\n",
      "Gradient Descent(181/399): loss=0.4692615263500347, w0=4.496126905342098e-05, w1=-0.005812121593695857\n",
      "Gradient Descent(182/399): loss=0.4691257668620734, w0=4.5415631463788605e-05, w1=-0.005840510990173878\n",
      "Gradient Descent(183/399): loss=0.468990307995131, w0=4.587124318442322e-05, w1=-0.005868865790282661\n",
      "Gradient Descent(184/399): loss=0.4688551487479157, w0=4.632809082274087e-05, w1=-0.0058971860552429695\n",
      "Gradient Descent(185/399): loss=0.46872028812294125, w0=4.678616104563395e-05, w1=-0.005925471846148446\n",
      "Gradient Descent(186/399): loss=0.46858572512651064, w0=4.7245440579248546e-05, w1=-0.0059537232239659215\n",
      "Gradient Descent(187/399): loss=0.46845145876869915, w0=4.7705916208762495e-05, w1=-0.005981940249535716\n",
      "Gradient Descent(188/399): loss=0.4683174880633397, w0=4.8167574778164295e-05, w1=-0.00601012298357194\n",
      "Gradient Descent(189/399): loss=0.4681838120280053, w0=4.863040319003277e-05, w1=-0.006038271486662797\n",
      "Gradient Descent(190/399): loss=0.4680504296839943, w0=4.909438840531755e-05, w1=-0.006066385819270887\n",
      "Gradient Descent(191/399): loss=0.467917340056314, w0=4.955951744312029e-05, w1=-0.0060944660417334975\n",
      "Gradient Descent(192/399): loss=0.4677845421736657, w0=5.002577738047671e-05, w1=-0.006122512214262913\n",
      "Gradient Descent(193/399): loss=0.4676520350684278, w0=5.0493155352139416e-05, w1=-0.006150524396946705\n",
      "Gradient Descent(194/399): loss=0.4675198177766411, w0=5.0961638550361434e-05, w1=-0.006178502649748033\n",
      "Gradient Descent(195/399): loss=0.46738788933799336, w0=5.143121422468062e-05, w1=-0.006206447032505942\n",
      "Gradient Descent(196/399): loss=0.46725624879580296, w0=5.190186968170474e-05, w1=-0.006234357604935654\n",
      "Gradient Descent(197/399): loss=0.46712489519700484, w0=5.2373592284897385e-05, w1=-0.006262234426628867\n",
      "Gradient Descent(198/399): loss=0.4669938275921339, w0=5.28463694543646e-05, w1=-0.006290077557054047\n",
      "Gradient Descent(199/399): loss=0.46686304503531084, w0=5.3320188666642325e-05, w1=-0.006317887055556721\n",
      "Gradient Descent(200/399): loss=0.4667325465842258, w0=5.3795037454484564e-05, w1=-0.006345662981359771\n",
      "Gradient Descent(201/399): loss=0.4666023313001254, w0=5.42709034066523e-05, w1=-0.006373405393563725\n",
      "Gradient Descent(202/399): loss=0.4664723982477954, w0=5.4747774167703206e-05, w1=-0.006401114351147047\n",
      "Gradient Descent(203/399): loss=0.46634274649554663, w0=5.522563743778206e-05, w1=-0.006428789912966427\n",
      "Gradient Descent(204/399): loss=0.4662133751152008, w0=5.570448097241192e-05, w1=-0.006456432137757073\n",
      "Gradient Descent(205/399): loss=0.46608428318207507, w0=5.618429258228611e-05, w1=-0.006484041084132995\n",
      "Gradient Descent(206/399): loss=0.46595546977496755, w0=5.6665060133060824e-05, w1=-0.006511616810587298\n",
      "Gradient Descent(207/399): loss=0.4658269339761424, w0=5.7146771545148576e-05, w1=-0.006539159375492465\n",
      "Gradient Descent(208/399): loss=0.4656986748713152, w0=5.7629414793512356e-05, w1=-0.006566668837100645\n",
      "Gradient Descent(209/399): loss=0.4655706915496392, w0=5.811297790746051e-05, w1=-0.006594145253543937\n",
      "Gradient Descent(210/399): loss=0.4654429831036898, w0=5.8597448970442355e-05, w1=-0.0066215886828346745\n",
      "Gradient Descent(211/399): loss=0.4653155486294513, w0=5.908281611984454e-05, w1=-0.006648999182865713\n",
      "Gradient Descent(212/399): loss=0.46518838722630135, w0=5.956906754678811e-05, w1=-0.006676376811410708\n",
      "Gradient Descent(213/399): loss=0.46506149799699836, w0=6.005619149592632e-05, w1=-0.006703721626124402\n",
      "Gradient Descent(214/399): loss=0.46493488004766537, w0=6.054417626524317e-05, w1=-0.0067310336845429015\n",
      "Gradient Descent(215/399): loss=0.4648085324877776, w0=6.103301020585263e-05, w1=-0.0067583130440839615\n",
      "Gradient Descent(216/399): loss=0.4646824544301478, w0=6.152268172179863e-05, w1=-0.006785559762047264\n",
      "Gradient Descent(217/399): loss=0.464556644990912, w0=6.201317926985568e-05, w1=-0.006812773895614697\n",
      "Gradient Descent(218/399): loss=0.4644311032895161, w0=6.250449135933035e-05, w1=-0.006839955501850634\n",
      "Gradient Descent(219/399): loss=0.46430582844870155, w0=6.29966065518633e-05, w1=-0.00686710463770221\n",
      "Gradient Descent(220/399): loss=0.4641808195944925, w0=6.348951346123214e-05, w1=-0.006894221359999602\n",
      "Gradient Descent(221/399): loss=0.46405607585618047, w0=6.398320075315486e-05, w1=-0.006921305725456301\n",
      "Gradient Descent(222/399): loss=0.4639315963663133, w0=6.447765714509416e-05, w1=-0.00694835779066939\n",
      "Gradient Descent(223/399): loss=0.46380738026067836, w0=6.497287140606226e-05, w1=-0.006975377612119818\n",
      "Gradient Descent(224/399): loss=0.46368342667829154, w0=6.546883235642659e-05, w1=-0.007002365246172676\n",
      "Gradient Descent(225/399): loss=0.46355973476138307, w0=6.596552886771604e-05, w1=-0.007029320749077468\n",
      "Gradient Descent(226/399): loss=0.4634363036553837, w0=6.646294986242801e-05, w1=-0.007056244176968383\n",
      "Gradient Descent(227/399): loss=0.46331313250891193, w0=6.696108431383606e-05, w1=-0.0070831355858645695\n",
      "Gradient Descent(228/399): loss=0.46319022047376046, w0=6.745992124579832e-05, w1=-0.007109995031670405\n",
      "Gradient Descent(229/399): loss=0.463067566704883, w0=6.795944973256654e-05, w1=-0.007136822570175766\n",
      "Gradient Descent(230/399): loss=0.46294517036038163, w0=6.845965889859582e-05, w1=-0.0071636182570563\n",
      "Gradient Descent(231/399): loss=0.4628230306014929, w0=6.896053791835507e-05, w1=-0.00719038214787369\n",
      "Gradient Descent(232/399): loss=0.4627011465925753, w0=6.946207601613809e-05, w1=-0.007217114298075928\n",
      "Gradient Descent(233/399): loss=0.462579517501097, w0=6.996426246587539e-05, w1=-0.007243814762997578\n",
      "Gradient Descent(234/399): loss=0.4624581424976212, w0=7.046708659094657e-05, w1=-0.007270483597860045\n",
      "Gradient Descent(235/399): loss=0.4623370207557953, w0=7.097053776399353e-05, w1=-0.007297120857771841\n",
      "Gradient Descent(236/399): loss=0.46221615145233647, w0=7.147460540673418e-05, w1=-0.007323726597728852\n",
      "Gradient Descent(237/399): loss=0.46209553376702023, w0=7.1979278989777e-05, w1=-0.007350300872614597\n",
      "Gradient Descent(238/399): loss=0.461975166882667, w0=7.248454803243606e-05, w1=-0.007376843737200497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(239/399): loss=0.46185504998512983, w0=7.29904021025469e-05, w1=-0.007403355246146138\n",
      "Gradient Descent(240/399): loss=0.46173518226328225, w0=7.349683081628289e-05, w1=-0.00742983545399953\n",
      "Gradient Descent(241/399): loss=0.46161556290900446, w0=7.40038238379724e-05, w1=-0.007456284415197373\n",
      "Gradient Descent(242/399): loss=0.4614961911171728, w0=7.45113708799165e-05, w1=-0.007482702184065315\n",
      "Gradient Descent(243/399): loss=0.46137706608564577, w0=7.501946170220744e-05, w1=-0.007509088814818214\n",
      "Gradient Descent(244/399): loss=0.4612581870152529, w0=7.552808611254762e-05, w1=-0.007535444361560397\n",
      "Gradient Descent(245/399): loss=0.4611395531097817, w0=7.603723396606937e-05, w1=-0.007561768878285919\n",
      "Gradient Descent(246/399): loss=0.46102116357596634, w0=7.654689516515527e-05, w1=-0.007588062418878821\n",
      "Gradient Descent(247/399): loss=0.46090301762347413, w0=7.705705965925915e-05, w1=-0.0076143250371133895\n",
      "Gradient Descent(248/399): loss=0.46078511446489534, w0=7.756771744472772e-05, w1=-0.007640556786654411\n",
      "Gradient Descent(249/399): loss=0.46066745331572956, w0=7.807885856462286e-05, w1=-0.007666757721057429\n",
      "Gradient Descent(250/399): loss=0.4605500333943743, w0=7.859047310854453e-05, w1=-0.007692927893769\n",
      "Gradient Descent(251/399): loss=0.4604328539221139, w0=7.910255121245429e-05, w1=-0.0077190673581269496\n",
      "Gradient Descent(252/399): loss=0.4603159141231058, w0=7.96150830584995e-05, w1=-0.007745176167360622\n",
      "Gradient Descent(253/399): loss=0.4601992132243712, w0=8.012805887483811e-05, w1=-0.00777125437459114\n",
      "Gradient Descent(254/399): loss=0.4600827504557812, w0=8.064146893546414e-05, w1=-0.007797302032831652\n",
      "Gradient Descent(255/399): loss=0.4599665250500464, w0=8.115530356003367e-05, w1=-0.007823319194987589\n",
      "Gradient Descent(256/399): loss=0.45985053624270456, w0=8.166955311369159e-05, w1=-0.007849305913856911\n",
      "Gradient Descent(257/399): loss=0.45973478327210937, w0=8.218420800689885e-05, w1=-0.007875262242130364\n",
      "Gradient Descent(258/399): loss=0.4596192653794192, w0=8.269925869526042e-05, w1=-0.007901188232391722\n",
      "Gradient Descent(259/399): loss=0.45950398180858465, w0=8.321469567935381e-05, w1=-0.007927083937118048\n",
      "Gradient Descent(260/399): loss=0.45938893180633866, w0=8.373050950455821e-05, w1=-0.007952949408679928\n",
      "Gradient Descent(261/399): loss=0.4592741146221836, w0=8.42466907608843e-05, w1=-0.007978784699341733\n",
      "Gradient Descent(262/399): loss=0.4591595295083812, w0=8.476323008280459e-05, w1=-0.008004589861261858\n",
      "Gradient Descent(263/399): loss=0.4590451757199403, w0=8.528011814908441e-05, w1=-0.008030364946492972\n",
      "Gradient Descent(264/399): loss=0.45893105251460675, w0=8.57973456826135e-05, w1=-0.008056110006982264\n",
      "Gradient Descent(265/399): loss=0.45881715915285043, w0=8.631490345023818e-05, w1=-0.008081825094571686\n",
      "Gradient Descent(266/399): loss=0.45870349489785717, w0=8.683278226259422e-05, w1=-0.008107510260998204\n",
      "Gradient Descent(267/399): loss=0.45859005901551425, w0=8.735097297394013e-05, w1=-0.008133165557894034\n",
      "Gradient Descent(268/399): loss=0.4584768507744017, w0=8.786946648199124e-05, w1=-0.008158791036786892\n",
      "Gradient Descent(269/399): loss=0.4583638694457809, w0=8.838825372775423e-05, w1=-0.008184386749100232\n",
      "Gradient Descent(270/399): loss=0.45825111430358323, w0=8.890732569536232e-05, w1=-0.008209952746153494\n",
      "Gradient Descent(271/399): loss=0.45813858462439916, w0=8.942667341191107e-05, w1=-0.008235489079162342\n",
      "Gradient Descent(272/399): loss=0.4580262796874688, w0=8.994628794729469e-05, w1=-0.008260995799238903\n",
      "Gradient Descent(273/399): loss=0.45791419877466866, w0=9.046616041404305e-05, w1=-0.00828647295739201\n",
      "Gradient Descent(274/399): loss=0.4578023411705038, w0=9.098628196715912e-05, w1=-0.008311920604527446\n",
      "Gradient Descent(275/399): loss=0.45769070616209495, w0=9.150664380395718e-05, w1=-0.008337338791448172\n",
      "Gradient Descent(276/399): loss=0.4575792930391692, w0=9.202723716390144e-05, w1=-0.008362727568854577\n",
      "Gradient Descent(277/399): loss=0.45746810109404845, w0=9.254805332844535e-05, w1=-0.008388086987344707\n",
      "Gradient Descent(278/399): loss=0.45735712962164043, w0=9.306908362087144e-05, w1=-0.008413417097414508\n",
      "Gradient Descent(279/399): loss=0.45724637791942624, w0=9.359031940613177e-05, w1=-0.008438717949458061\n",
      "Gradient Descent(280/399): loss=0.4571358452874519, w0=9.411175209068886e-05, w1=-0.008463989593767814\n",
      "Gradient Descent(281/399): loss=0.45702553102831645, w0=9.463337312235731e-05, w1=-0.008489232080534825\n",
      "Gradient Descent(282/399): loss=0.4569154344471623, w0=9.515517399014592e-05, w1=-0.008514445459848988\n",
      "Gradient Descent(283/399): loss=0.45680555485166524, w0=9.567714622410039e-05, w1=-0.008539629781699275\n",
      "Gradient Descent(284/399): loss=0.45669589155202406, w0=9.619928139514658e-05, w1=-0.008564785095973963\n",
      "Gradient Descent(285/399): loss=0.4565864438609495, w0=9.672157111493437e-05, w1=-0.00858991145246087\n",
      "Gradient Descent(286/399): loss=0.4564772110936557, w0=9.7244007035682e-05, w1=-0.008615008900847586\n",
      "Gradient Descent(287/399): loss=0.4563681925678488, w0=9.776658085002106e-05, w1=-0.008640077490721708\n",
      "Gradient Descent(288/399): loss=0.4562593876037175, w0=9.828928429084196e-05, w1=-0.008665117271571066\n",
      "Gradient Descent(289/399): loss=0.4561507955239229, w0=9.881210913114e-05, w1=-0.008690128292783955\n",
      "Gradient Descent(290/399): loss=0.45604241565358866, w0=9.9335047183862e-05, w1=-0.008715110603649366\n",
      "Gradient Descent(291/399): loss=0.455934247320291, w0=9.985809030175343e-05, w1=-0.008740064253357216\n",
      "Gradient Descent(292/399): loss=0.4558262898540491, w0=0.00010038123037720615, w1=-0.00876498929099857\n",
      "Gradient Descent(293/399): loss=0.4557185425873148, w0=0.00010090445934210665, w1=-0.008789885765565879\n",
      "Gradient Descent(294/399): loss=0.45561100485496375, w0=0.00010142776916768486, w1=-0.0088147537259532\n",
      "Gradient Descent(295/399): loss=0.455503675994284, w0=0.00010195115186436346, w1=-0.008839593220956427\n",
      "Gradient Descent(296/399): loss=0.45539655534496826, w0=0.00010247459948160785, w1=-0.00886440429927351\n",
      "Gradient Descent(297/399): loss=0.45528964224910284, w0=0.00010299810410777648, w1=-0.00888918700950469\n",
      "Gradient Descent(298/399): loss=0.45518293605115945, w0=0.00010352165786997186, w1=-0.008913941400152716\n",
      "Gradient Descent(299/399): loss=0.455076436097984, w0=0.00010404525293389208, w1=-0.008938667519623078\n",
      "Gradient Descent(300/399): loss=0.45497014173878814, w0=0.00010456888150368278, w1=-0.00896336541622422\n",
      "Gradient Descent(301/399): loss=0.45486405232513977, w0=0.00010509253582178977, w1=-0.008988035138167769\n",
      "Gradient Descent(302/399): loss=0.4547581672109532, w0=0.00010561620816881209, w1=-0.009012676733568762\n",
      "Gradient Descent(303/399): loss=0.4546524857524801, w0=0.00010613989086335564, w1=-0.00903729025044586\n",
      "Gradient Descent(304/399): loss=0.4545470073083003, w0=0.00010666357626188733, w1=-0.009061875736721573\n",
      "Gradient Descent(305/399): loss=0.45444173123931164, w0=0.00010718725675858975, w1=-0.009086433240222479\n",
      "Gradient Descent(306/399): loss=0.45433665690872227, w0=0.00010771092478521634, w1=-0.009110962808679448\n",
      "Gradient Descent(307/399): loss=0.4542317836820397, w0=0.00010823457281094716, w1=-0.009135464489727858\n",
      "Gradient Descent(308/399): loss=0.45412711092706304, w0=0.00010875819334224505, w1=-0.009159938330907816\n",
      "Gradient Descent(309/399): loss=0.45402263801387266, w0=0.00010928177892271245, w1=-0.009184384379664375\n",
      "Gradient Descent(310/399): loss=0.45391836431482224, w0=0.0001098053221329486, w1=-0.009208802683347754\n",
      "Gradient Descent(311/399): loss=0.4538142892045293, w0=0.00011032881559040734, w1=-0.009233193289213553\n",
      "Gradient Descent(312/399): loss=0.4537104120598656, w0=0.00011085225194925542, w1=-0.009257556244422973\n",
      "Gradient Descent(313/399): loss=0.45360673225994913, w0=0.00011137562390023129, w1=-0.009281891596043028\n",
      "Gradient Descent(314/399): loss=0.4535032491861346, w0=0.00011189892417050438, w1=-0.009306199391046764\n",
      "Gradient Descent(315/399): loss=0.4533999622220047, w0=0.00011242214552353494, w1=-0.009330479676313473\n",
      "Gradient Descent(316/399): loss=0.45329687075336134, w0=0.00011294528075893434, w1=-0.009354732498628908\n",
      "Gradient Descent(317/399): loss=0.4531939741682165, w0=0.00011346832271232587, w1=-0.009378957904685495\n",
      "Gradient Descent(318/399): loss=0.4530912718567841, w0=0.00011399126425520612, w1=-0.009403155941082549\n",
      "Gradient Descent(319/399): loss=0.4529887632114707, w0=0.00011451409829480674, w1=-0.009427326654326485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(320/399): loss=0.45288644762686703, w0=0.00011503681777395676, w1=-0.009451470090831032\n",
      "Gradient Descent(321/399): loss=0.45278432449973915, w0=0.00011555941567094543, w1=-0.009475586296917443\n",
      "Gradient Descent(322/399): loss=0.4526823932290204, w0=0.00011608188499938549, w1=-0.009499675318814706\n",
      "Gradient Descent(323/399): loss=0.4525806532158022, w0=0.000116604218808077, w1=-0.009523737202659758\n",
      "Gradient Descent(324/399): loss=0.4524791038633258, w0=0.0001171264101808716, w1=-0.00954777199449769\n",
      "Gradient Descent(325/399): loss=0.4523777445769738, w0=0.00011764845223653729, w1=-0.009571779740281961\n",
      "Gradient Descent(326/399): loss=0.4522765747642611, w0=0.0001181703381286237, w1=-0.009595760485874604\n",
      "Gradient Descent(327/399): loss=0.4521755938348282, w0=0.0001186920610453278, w1=-0.009619714277046436\n",
      "Gradient Descent(328/399): loss=0.4520748012004302, w0=0.0001192136142093602, w1=-0.009643641159477266\n",
      "Gradient Descent(329/399): loss=0.45197419627493096, w0=0.00011973499087781177, w1=-0.0096675411787561\n",
      "Gradient Descent(330/399): loss=0.4518737784742931, w0=0.00012025618434202092, w1=-0.009691414380381352\n",
      "Gradient Descent(331/399): loss=0.4517735472165708, w0=0.0001207771879274412, w1=-0.009715260809761046\n",
      "Gradient Descent(332/399): loss=0.4516735019219006, w0=0.00012129799499350947, w1=-0.009739080512213024\n",
      "Gradient Descent(333/399): loss=0.45157364201249395, w0=0.00012181859893351452, w1=-0.009762873532965153\n",
      "Gradient Descent(334/399): loss=0.45147396691262864, w0=0.0001223389931744662, w1=-0.009786639917155523\n",
      "Gradient Descent(335/399): loss=0.45137447604864134, w0=0.00012285917117696487, w1=-0.009810379709832659\n",
      "Gradient Descent(336/399): loss=0.45127516884891816, w0=0.00012337912643507157, w1=-0.00983409295595572\n",
      "Gradient Descent(337/399): loss=0.45117604474388795, w0=0.00012389885247617846, w1=-0.009857779700394705\n",
      "Gradient Descent(338/399): loss=0.45107710316601396, w0=0.00012441834286087974, w1=-0.00988143998793065\n",
      "Gradient Descent(339/399): loss=0.45097834354978483, w0=0.00012493759118284323, w1=-0.009905073863255835\n",
      "Gradient Descent(340/399): loss=0.45087976533170854, w0=0.00012545659106868206, w1=-0.009928681370973988\n",
      "Gradient Descent(341/399): loss=0.4507813679503021, w0=0.00012597533617782723, w1=-0.009952262555600477\n",
      "Gradient Descent(342/399): loss=0.45068315084608584, w0=0.00012649382020240028, w1=-0.00997581746156252\n",
      "Gradient Descent(343/399): loss=0.4505851134615748, w0=0.00012701203686708666, w1=-0.00999934613319938\n",
      "Gradient Descent(344/399): loss=0.4504872552412707, w0=0.00012752997992900942, w1=-0.01002284861476256\n",
      "Gradient Descent(345/399): loss=0.4503895756316535, w0=0.00012804764317760336, w1=-0.01004632495041601\n",
      "Gradient Descent(346/399): loss=0.4502920740811754, w0=0.0001285650204344898, w1=-0.010069775184236325\n",
      "Gradient Descent(347/399): loss=0.4501947500402517, w0=0.0001290821055533515, w1=-0.010093199360212938\n",
      "Gradient Descent(348/399): loss=0.4500976029612537, w0=0.00012959889241980835, w1=-0.010116597522248317\n",
      "Gradient Descent(349/399): loss=0.45000063229850057, w0=0.00013011537495129333, w1=-0.010139969714158167\n",
      "Gradient Descent(350/399): loss=0.4499038375082531, w0=0.00013063154709692885, w1=-0.010163315979671622\n",
      "Gradient Descent(351/399): loss=0.4498072180487039, w0=0.00013114740283740378, w1=-0.010186636362431445\n",
      "Gradient Descent(352/399): loss=0.44971077337997173, w0=0.00013166293618485067, w1=-0.010209930905994216\n",
      "Gradient Descent(353/399): loss=0.44961450296409333, w0=0.00013217814118272357, w1=-0.010233199653830535\n",
      "Gradient Descent(354/399): loss=0.44951840626501594, w0=0.00013269301190567623, w1=-0.010256442649325213\n",
      "Gradient Descent(355/399): loss=0.44942248274858965, w0=0.00013320754245944077, w1=-0.010279659935777462\n",
      "Gradient Descent(356/399): loss=0.44932673188256045, w0=0.00013372172698070676, w1=-0.010302851556401095\n",
      "Gradient Descent(357/399): loss=0.4492311531365624, w0=0.0001342355596370007, w1=-0.010326017554324714\n",
      "Gradient Descent(358/399): loss=0.4491357459821108, w0=0.0001347490346265661, w1=-0.010349157972591904\n",
      "Gradient Descent(359/399): loss=0.44904050989259425, w0=0.00013526214617824372, w1=-0.010372272854161426\n",
      "Gradient Descent(360/399): loss=0.4489454443432675, w0=0.0001357748885513525, w1=-0.010395362241907404\n",
      "Gradient Descent(361/399): loss=0.4488505488112451, w0=0.00013628725603557075, w1=-0.010418426178619522\n",
      "Gradient Descent(362/399): loss=0.44875582277549286, w0=0.0001367992429508179, w1=-0.010441464707003209\n",
      "Gradient Descent(363/399): loss=0.4486612657168212, w0=0.0001373108436471366, w1=-0.01046447786967983\n",
      "Gradient Descent(364/399): loss=0.44856687711787807, w0=0.0001378220525045752, w1=-0.010487465709186879\n",
      "Gradient Descent(365/399): loss=0.44847265646314244, w0=0.0001383328639330707, w1=-0.010510428267978163\n",
      "Gradient Descent(366/399): loss=0.44837860323891554, w0=0.00013884327237233225, w1=-0.010533365588423991\n",
      "Gradient Descent(367/399): loss=0.4482847169333156, w0=0.00013935327229172486, w1=-0.010556277712811367\n",
      "Gradient Descent(368/399): loss=0.4481909970362695, w0=0.00013986285819015366, w1=-0.010579164683344168\n",
      "Gradient Descent(369/399): loss=0.4480974430395066, w0=0.00014037202459594848, w1=-0.010602026542143343\n",
      "Gradient Descent(370/399): loss=0.4480040544365513, w0=0.00014088076606674896, w1=-0.010624863331247089\n",
      "Gradient Descent(371/399): loss=0.4479108307227161, w0=0.00014138907718939004, w1=-0.010647675092611041\n",
      "Gradient Descent(372/399): loss=0.44781777139509554, w0=0.0001418969525797878, w1=-0.010670461868108456\n",
      "Gradient Descent(373/399): loss=0.4477248759525573, w0=0.00014240438688282583, w1=-0.010693223699530402\n",
      "Gradient Descent(374/399): loss=0.4476321438957376, w0=0.0001429113747722418, w1=-0.01071596062858594\n",
      "Gradient Descent(375/399): loss=0.4475395747270334, w0=0.00014341791095051474, w1=-0.010738672696902304\n",
      "Gradient Descent(376/399): loss=0.4474471679505953, w0=0.00014392399014875247, w1=-0.010761359946025093\n",
      "Gradient Descent(377/399): loss=0.44735492307232105, w0=0.00014442960712657957, w1=-0.010784022417418444\n",
      "Gradient Descent(378/399): loss=0.44726283959984914, w0=0.0001449347566720257, w1=-0.010806660152465225\n",
      "Gradient Descent(379/399): loss=0.44717091704255163, w0=0.00014543943360141432, w1=-0.01082927319246721\n",
      "Gradient Descent(380/399): loss=0.4470791549115272, w0=0.00014594363275925192, w1=-0.010851861578645261\n",
      "Gradient Descent(381/399): loss=0.4469875527195957, w0=0.00014644734901811746, w1=-0.010874425352139516\n",
      "Gradient Descent(382/399): loss=0.4468961099812904, w0=0.00014695057727855237, w1=-0.010896964554009559\n",
      "Gradient Descent(383/399): loss=0.44680482621285167, w0=0.0001474533124689509, w1=-0.01091947922523461\n",
      "Gradient Descent(384/399): loss=0.4467137009322205, w0=0.0001479555495454508, w1=-0.010941969406713698\n",
      "Gradient Descent(385/399): loss=0.4466227336590323, w0=0.00014845728349182453, w1=-0.010964435139265847\n",
      "Gradient Descent(386/399): loss=0.4465319239146091, w0=0.00014895850931937076, w1=-0.010986876463630247\n",
      "Gradient Descent(387/399): loss=0.4464412712219554, w0=0.00014945922206680622, w1=-0.01100929342046644\n",
      "Gradient Descent(388/399): loss=0.44635077510574894, w0=0.00014995941680015813, w1=-0.01103168605035449\n",
      "Gradient Descent(389/399): loss=0.4462604350923367, w0=0.0001504590886126568, w1=-0.011054054393795172\n",
      "Gradient Descent(390/399): loss=0.4461702507097266, w0=0.00015095823262462876, w1=-0.011076398491210136\n",
      "Gradient Descent(391/399): loss=0.44608022148758275, w0=0.00015145684398339026, w1=-0.011098718382942092\n",
      "Gradient Descent(392/399): loss=0.44599034695721784, w0=0.00015195491786314106, w1=-0.011121014109254984\n",
      "Gradient Descent(393/399): loss=0.4459006266515873, w0=0.00015245244946485872, w1=-0.011143285710334164\n",
      "Gradient Descent(394/399): loss=0.4458110601052832, w0=0.00015294943401619325, w1=-0.011165533226286573\n",
      "Gradient Descent(395/399): loss=0.4457216468545278, w0=0.0001534458667713621, w1=-0.011187756697140906\n",
      "Gradient Descent(396/399): loss=0.4456323864371671, w0=0.00015394174301104545, w1=-0.011209956162847796\n",
      "Gradient Descent(397/399): loss=0.44554327839266494, w0=0.00015443705804228216, w1=-0.011232131663279984\n",
      "Gradient Descent(398/399): loss=0.4454543222620968, w0=0.00015493180719836576, w1=-0.011254283238232491\n",
      "Gradient Descent(399/399): loss=0.44536551758814347, w0=0.00015542598583874104, w1=-0.011276410927422795\n",
      "++++ gamma = 0.000268269579528\n",
      "ciaociaociao\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/399): loss=0.4999999999999887, w0=-1.8476349143751833e-07, w1=-9.617796961822002e-05\n",
      "Gradient Descent(1/399): loss=0.4994190213321271, w0=-3.31830276863767e-07, w1=-0.00019200435456225397\n",
      "Gradient Descent(2/399): loss=0.4988422477155444, w0=-4.417656812402293e-07, w1=-0.00028748094664720345\n",
      "Gradient Descent(3/399): loss=0.4982696374956127, w0=-5.15128927360894e-07, w1=-0.00038260952692916275\n",
      "Gradient Descent(4/399): loss=0.4977011494914685, w0=-5.524731964632497e-07, w1=-0.00047739186577970864\n",
      "Gradient Descent(5/399): loss=0.4971367429899027, w0=-5.543456882474625e-07, w1=-0.0005718297229598037\n",
      "Gradient Descent(6/399): loss=0.49657637773932795, w0=-5.212876803095354e-07, w1=-0.0006659248476931219\n",
      "Gradient Descent(7/399): loss=0.49602001394383466, w0=-4.5383458699416463e-07, w1=-0.0007596789787387992\n",
      "Gradient Descent(8/399): loss=0.4954676122573338, w0=-3.525160176729379e-07, w1=-0.0008530938444636123\n",
      "Gradient Descent(9/399): loss=0.49491913377778624, w0=-2.1785583445354215e-07, w1=-0.0009461711629135948\n",
      "Gradient Descent(10/399): loss=0.4943745400415152, w0=-5.037220932545146e-08, w1=-0.001038912641885092\n",
      "Gradient Descent(11/399): loss=0.4938337930176016, w0=1.4942231925234884e-07, w1=-0.0011313199789952616\n",
      "Gradient Descent(12/399): loss=0.49329685510236104, w0=3.810207903161071e-07, w1=-0.001223394861752021\n",
      "Gradient Descent(13/399): loss=0.49276368911390267, w0=6.439217649092568e-07, w1=-0.0013151389676234523\n",
      "Gradient Descent(14/399): loss=0.49223425828676276, w0=9.376292711360916e-07, w1=-0.0014065539641066657\n",
      "Gradient Descent(15/399): loss=0.49170852626662076, w0=1.2616527497519704e-06, w1=-0.0014976415087961234\n",
      "Gradient Descent(16/399): loss=0.4911864571050872, w0=1.6155070002847704e-06, w1=-0.0015884032494514354\n",
      "Gradient Descent(17/399): loss=0.49066801525457016, w0=1.9987121276824143e-06, w1=-0.0016788408240646262\n",
      "Gradient Descent(18/399): loss=0.4901531655632133, w0=2.4107934894813195e-06, w1=-0.001768955860926879\n",
      "Gradient Descent(19/399): loss=0.4896418732699084, w0=2.8512816434907397e-06, w1=-0.0018587499786947618\n",
      "Gradient Descent(20/399): loss=0.48913410399937685, w0=3.3197122959878695e-06, w1=-0.0019482247864559394\n",
      "Gradient Descent(21/399): loss=0.4886298237573248, w0=3.815626250418815e-06, w1=-0.0020373818837943767\n",
      "Gradient Descent(22/399): loss=0.4881289989256645, w0=4.338569356600464e-06, w1=-0.0021262228608550362\n",
      "Gradient Descent(23/399): loss=0.4876315962578055, w0=4.888092460418297e-06, w1=-0.0022147492984080754\n",
      "Gradient Descent(24/399): loss=0.48713758287401243, w0=5.463751354015451e-06, w1=-0.0023029627679125465\n",
      "Gradient Descent(25/399): loss=0.4866469262568287, w0=6.065106726468077e-06, w1=-0.002390864831579605\n",
      "Gradient Descent(26/399): loss=0.48615959424656496, w0=6.6917241149423655e-06, w1=-0.002478457042435231\n",
      "Gradient Descent(27/399): loss=0.48567555503685195, w0=7.343173856328468e-06, w1=-0.0025657409443824654\n",
      "Gradient Descent(28/399): loss=0.4851947771702556, w0=8.019031039346702e-06, w1=-0.002652718072263165\n",
      "Gradient Descent(29/399): loss=0.4847172295339538, w0=8.718875457121398e-06, w1=-0.002739389951919286\n",
      "Gradient Descent(30/399): loss=0.4842428813554749, w0=9.442291560217855e-06, w1=-0.0028257581002536923\n",
      "Gradient Descent(31/399): loss=0.48377170219849586, w0=1.0188868410137872e-05, w1=-0.0029118240252904975\n",
      "Gradient Descent(32/399): loss=0.4833036619586986, w0=1.0958199633269379e-05, w1=-0.002997589226234943\n",
      "Gradient Descent(33/399): loss=0.48283873085968587, w0=1.1749883375285744e-05, w1=-0.0030830551935328173\n",
      "Gradient Descent(34/399): loss=0.48237687944895175, w0=1.2563522255990334e-05, w1=-0.003168223408929419\n",
      "Gradient Descent(35/399): loss=0.4819180785939112, w0=1.3398723324602064e-05, w1=-0.0032530953455280668\n",
      "Gradient Descent(36/399): loss=0.4814622994779836, w0=1.4255098015477538e-05, w1=-0.003337672467848164\n",
      "Gradient Descent(37/399): loss=0.48100951359673016, w0=1.5132262104265609e-05, w1=-0.0034219562318828166\n",
      "Gradient Descent(38/399): loss=0.480559692754046, w0=1.6029835664490062e-05, w1=-0.0035059480851560108\n",
      "Gradient Descent(39/399): loss=0.48011280905840426, w0=1.694744302455631e-05, w1=-0.003589649466779355\n",
      "Gradient Descent(40/399): loss=0.47966883491915346, w0=1.7884712725177904e-05, w1=-0.0036730618075083896\n",
      "Gradient Descent(41/399): loss=0.47922774304286364, w0=1.8841277477218826e-05, w1=-0.003756186529798464\n",
      "Gradient Descent(42/399): loss=0.4787895064297251, w0=1.9816774119947453e-05, w1=-0.0038390250478601913\n",
      "Gradient Descent(43/399): loss=0.4783540983699953, w0=2.081084357969822e-05, w1=-0.003921578767714478\n",
      "Gradient Descent(44/399): loss=0.4779214924404961, w0=2.1823130828936986e-05, w1=-0.004003849087247139\n",
      "Gradient Descent(45/399): loss=0.4774916625011564, w0=2.285328484572614e-05, w1=-0.004085837396263091\n",
      "Gradient Descent(46/399): loss=0.47706458269160495, w0=2.3900958573585635e-05, w1=-0.004167545076540144\n",
      "Gradient Descent(47/399): loss=0.47664022742780804, w0=2.496580888174593e-05, w1=-0.004248973501882374\n",
      "Gradient Descent(48/399): loss=0.4762185713987528, w0=2.604749652578921e-05, w1=-0.004330124038173099\n",
      "Gradient Descent(49/399): loss=0.47579958956317764, w0=2.7145686108674986e-05, w1=-0.004410998043427449\n",
      "Gradient Descent(50/399): loss=0.4753832571463441, w0=2.8260046042146292e-05, w1=-0.004491596867844543\n",
      "Gradient Descent(51/399): loss=0.4749695496368554, w0=2.939024850851289e-05, w1=-0.004571921853859265\n",
      "Gradient Descent(52/399): loss=0.47455844278351583, w0=3.0535969422807686e-05, w1=-0.004651974336193654\n",
      "Gradient Descent(53/399): loss=0.47414991259223455, w0=3.169688839531279e-05, w1=-0.004731755641907904\n",
      "Gradient Descent(54/399): loss=0.47374393532296893, w0=3.2872688694451585e-05, w1=-0.00481126709045098\n",
      "Gradient Descent(55/399): loss=0.4733404874867108, w0=3.406305721004329e-05, w1=-0.004890509993710853\n",
      "Gradient Descent(56/399): loss=0.4729395458425129, w0=3.526768441691636e-05, w1=-0.004969485656064351\n",
      "Gradient Descent(57/399): loss=0.4725410873945547, w0=3.6486264338877396e-05, w1=-0.005048195374426649\n",
      "Gradient Descent(58/399): loss=0.4721450893892471, w0=3.77184945130319e-05, w1=-0.005126640438300368\n",
      "Gradient Descent(59/399): loss=0.47175152931237807, w0=3.8964075954453686e-05, w1=-0.005204822129824324\n",
      "Gradient Descent(60/399): loss=0.47136038488629345, w0=4.022271312119928e-05, w1=-0.005282741723821894\n",
      "Gradient Descent(61/399): loss=0.4709716340671169, w0=4.1494113879664196e-05, w1=-0.005360400487849034\n",
      "Gradient Descent(62/399): loss=0.4705852550420071, w0=4.277798947027754e-05, w1=-0.005437799682241929\n",
      "Gradient Descent(63/399): loss=0.4702012262264509, w0=4.407405447353182e-05, w1=-0.005514940560164286\n",
      "Gradient Descent(64/399): loss=0.4698195262615912, w0=4.538202677634459e-05, w1=-0.005591824367654278\n",
      "Gradient Descent(65/399): loss=0.46944013401159246, w0=4.670162753874872e-05, w1=-0.005668452343671137\n",
      "Gradient Descent(66/399): loss=0.46906302856103926, w0=4.8032581160908104e-05, w1=-0.00574482572014139\n",
      "Gradient Descent(67/399): loss=0.4686881892123697, w0=4.9374615250455616e-05, w1=-0.005820945722004766\n",
      "Gradient Descent(68/399): loss=0.4683155954833428, w0=5.072746059015016e-05, w1=-0.005896813567259745\n",
      "Gradient Descent(69/399): loss=0.4679452271045379, w0=5.209085110584977e-05, w1=-0.005972430467008778\n",
      "Gradient Descent(70/399): loss=0.46757706401688953, w0=5.346452383479761e-05, w1=-0.006047797625503167\n",
      "Gradient Descent(71/399): loss=0.4672110863692512, w0=5.4848218894217836e-05, w1=-0.006122916240187612\n",
      "Gradient Descent(72/399): loss=0.4668472745159942, w0=5.624167945021835e-05, w1=-0.006197787501744427\n",
      "Gradient Descent(73/399): loss=0.466485609014634, w0=5.764465168699736e-05, w1=-0.0062724125941374265\n",
      "Gradient Descent(74/399): loss=0.4661260706234923, w0=5.9056884776350924e-05, w1=-0.006346792694655491\n",
      "Gradient Descent(75/399): loss=0.46576864029938525, w0=6.047813084747842e-05, w1=-0.006420928973955803\n",
      "Gradient Descent(76/399): loss=0.46541329919534297, w0=6.190814495708308e-05, w1=-0.006494822596106772\n",
      "Gradient Descent(77/399): loss=0.4650600286583605, w0=6.334668505976476e-05, w1=-0.006568474718630632\n",
      "Gradient Descent(78/399): loss=0.4647088102271747, w0=6.4793511978702e-05, w1=-0.0066418864925457295\n",
      "Gradient Descent(79/399): loss=0.46435962563007216, w0=6.624838937662069e-05, w1=-0.0067150590624085035\n",
      "Gradient Descent(80/399): loss=0.46401245678272474, w0=6.771108372704639e-05, w1=-0.006787993566355148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(81/399): loss=0.46366728578605176, w0=6.918136428583773e-05, w1=-0.0068606911361429705\n",
      "Gradient Descent(82/399): loss=0.4633240949241114, w0=7.065900306299791e-05, w1=-0.006933152897191449\n",
      "Gradient Descent(83/399): loss=0.4629828666620184, w0=7.21437747947619e-05, w1=-0.007005379968622984\n",
      "Gradient Descent(84/399): loss=0.4626435836438874, w0=7.36354569159563e-05, w1=-0.0070773734633033485\n",
      "Gradient Descent(85/399): loss=0.46230622869080507, w0=7.513382953262957e-05, w1=-0.0071491344878818475\n",
      "Gradient Descent(86/399): loss=0.4619707847988251, w0=7.663867539494965e-05, w1=-0.007220664142831179\n",
      "Gradient Descent(87/399): loss=0.4616372351369902, w0=7.814977987036665e-05, w1=-0.007291963522487002\n",
      "Gradient Descent(88/399): loss=0.46130556304537995, w0=7.966693091703784e-05, w1=-0.007363033715087216\n",
      "Gradient Descent(89/399): loss=0.46097575203318186, w0=8.118991905751249e-05, w1=-0.007433875802810953\n",
      "Gradient Descent(90/399): loss=0.4606477857767879, w0=8.2718537352674e-05, w1=-0.007504490861817286\n",
      "Gradient Descent(91/399): loss=0.46032164811791454, w0=8.425258137593678e-05, w1=-0.007574879962283653\n",
      "Gradient Descent(92/399): loss=0.45999732306174673, w0=8.579184918769541e-05, w1=-0.007645044168443998\n",
      "Gradient Descent(93/399): loss=0.45967479477510537, w0=8.733614131002375e-05, w1=-0.007714984538626641\n",
      "Gradient Descent(94/399): loss=0.4593540475846376, w0=8.888526070162129e-05, w1=-0.007784702125291866\n",
      "Gradient Descent(95/399): loss=0.45903506597502985, w0=9.043901273300469e-05, w1=-0.007854197975069237\n",
      "Gradient Descent(96/399): loss=0.45871783458724325, w0=9.199720516194176e-05, w1=-0.007923473128794644\n",
      "Gradient Descent(97/399): loss=0.4584023382167715, w0=9.355964810912583e-05, w1=-0.007992528621547076\n",
      "Gradient Descent(98/399): loss=0.4580885618119195, w0=9.512615403408789e-05, w1=-0.008061365482685138\n",
      "Gradient Descent(99/399): loss=0.4577764904721052, w0=9.669653771134451e-05, w1=-0.008129984735883283\n",
      "Gradient Descent(100/399): loss=0.4574661094461806, w0=9.827061620677891e-05, w1=-0.008198387399167804\n",
      "Gradient Descent(101/399): loss=0.45715740413077416, w0=9.984820885425312e-05, w1=-0.008266574484952554\n",
      "Gradient Descent(102/399): loss=0.45685036006865565, w0=0.00010142913723244894, w1=-0.008334547000074402\n",
      "Gradient Descent(103/399): loss=0.45654496294711827, w0=0.00010301322514193543, w1=-0.008402305945828446\n",
      "Gradient Descent(104/399): loss=0.4562411985963825, w0=0.00010460029858246078, w1=-0.008469852318002962\n",
      "Gradient Descent(105/399): loss=0.45593905298802034, w0=0.0001061901857304663, w1=-0.00853718710691411\n",
      "Gradient Descent(106/399): loss=0.45563851223339646, w0=0.00010778271691682044, w1=-0.008604311297440373\n",
      "Gradient Descent(107/399): loss=0.45533956258213126, w0=0.00010937772460477076, w1=-0.00867122586905677\n",
      "Gradient Descent(108/399): loss=0.455042190420581, w0=0.00011097504336811147, w1=-0.008737931795868811\n",
      "Gradient Descent(109/399): loss=0.4547463822703375, w0=0.00011257450986956475, w1=-0.008804430046646198\n",
      "Gradient Descent(110/399): loss=0.4544521247867454, w0=0.0001141759628393736, w1=-0.008870721584856309\n",
      "Gradient Descent(111/399): loss=0.4541594047574383, w0=0.00011577924305410419, w1=-0.008936807368697423\n",
      "Gradient Descent(112/399): loss=0.4538682091008933, w0=0.00011738419331565569, w1=-0.009002688351131705\n",
      "Gradient Descent(113/399): loss=0.4535785248650003, w0=0.00011899065843047552, w1=-0.009068365479917974\n",
      "Gradient Descent(114/399): loss=0.45329033922565265, w0=0.00012059848518897806, w1=-0.00913383969764421\n",
      "Gradient Descent(115/399): loss=0.45300363948535227, w0=0.00012220752234516491, w1=-0.009199111941759854\n",
      "Gradient Descent(116/399): loss=0.45271841307183164, w0=0.00012381762059644454, w1=-0.00926418314460786\n",
      "Gradient Descent(117/399): loss=0.4524346475366945, w0=0.00012542863256364954, w1=-0.009329054233456525\n",
      "Gradient Descent(118/399): loss=0.4521523305540712, w0=0.00012704041277124958, w1=-0.009393726130531094\n",
      "Gradient Descent(119/399): loss=0.4518714499192905, w0=0.0001286528176277581, w1=-0.009458199753045137\n",
      "Gradient Descent(120/399): loss=0.4515919935475672, w0=0.00013026570540633077, w1=-0.009522476013231695\n",
      "Gradient Descent(121/399): loss=0.4513139494727073, w0=0.00013187893622555402, w1=-0.009586555818374224\n",
      "Gradient Descent(122/399): loss=0.45103730584582624, w0=0.00013349237203042174, w1=-0.009650440070837301\n",
      "Gradient Descent(123/399): loss=0.4507620509340845, w0=0.00013510587657349822, w1=-0.009714129668097118\n",
      "Gradient Descent(124/399): loss=0.4504881731194361, w0=0.00013671931539626565, w1=-0.009777625502771763\n",
      "Gradient Descent(125/399): loss=0.4502156608973956, w0=0.00013833255581065427, w1=-0.009840928462651288\n",
      "Gradient Descent(126/399): loss=0.44994450287581467, w0=0.00013994546688075353, w1=-0.009904039430727549\n",
      "Gradient Descent(127/399): loss=0.4496746877736801, w0=0.00014155791940470233, w1=-0.009966959285223858\n",
      "Gradient Descent(128/399): loss=0.4494062044199179, w0=0.00014316978589675675, w1=-0.010029688899624405\n",
      "Gradient Descent(129/399): loss=0.4491390417522195, w0=0.00014478094056953353, w1=-0.010092229142703487\n",
      "Gradient Descent(130/399): loss=0.44887318881587635, w0=0.00014639125931642736, w1=-0.010154580878554524\n",
      "Gradient Descent(131/399): loss=0.44860863476263063, w0=0.00014800061969420068, w1=-0.010216744966618866\n",
      "Gradient Descent(132/399): loss=0.4483453688495397, w0=0.000149608900905744, w1=-0.010278722261714413\n",
      "Gradient Descent(133/399): loss=0.4480833804378521, w0=0.00015121598378300518, w1=-0.01034051361406401\n",
      "Gradient Descent(134/399): loss=0.44782265899190055, w0=0.00015282175077008613, w1=-0.01040211986932367\n",
      "Gradient Descent(135/399): loss=0.4475631940780024, w0=0.00015442608590650504, w1=-0.010463541868610574\n",
      "Gradient Descent(136/399): loss=0.447304975363379, w0=0.00015602887481062282, w1=-0.01052478044853089\n",
      "Gradient Descent(137/399): loss=0.44704799261508277, w0=0.00015763000466323206, w1=-0.010585836441207397\n",
      "Gradient Descent(138/399): loss=0.44679223569894055, w0=0.0001592293641913068, w1=-0.010646710674306899\n",
      "Gradient Descent(139/399): loss=0.44653769457850667, w0=0.00016082684365191163, w1=-0.01070740397106747\n",
      "Gradient Descent(140/399): loss=0.44628435931402965, w0=0.0001624223348162688, w1=-0.010767917150325499\n",
      "Gradient Descent(141/399): loss=0.446032220061431, w0=0.00016401573095398126, w1=-0.010828251026542534\n",
      "Gradient Descent(142/399): loss=0.4457812670712946, w0=0.00016560692681741074, w1=-0.010888406409831962\n",
      "Gradient Descent(143/399): loss=0.44553149068786957, w0=0.00016719581862620892, w1=-0.010948384105985485\n",
      "Gradient Descent(144/399): loss=0.4452828813480835, w0=0.00016878230405200037, w1=-0.011008184916499417\n",
      "Gradient Descent(145/399): loss=0.4450354295805681, w0=0.0001703662822032158, w1=-0.011067809638600802\n",
      "Gradient Descent(146/399): loss=0.444789126004694, w0=0.00017194765361007406, w1=-0.011127259065273348\n",
      "Gradient Descent(147/399): loss=0.4445439613296205, w0=0.0001735263202097116, w1=-0.011186533985283175\n",
      "Gradient Descent(148/399): loss=0.444299926353352, w0=0.00017510218533145773, w1=-0.011245635183204395\n",
      "Gradient Descent(149/399): loss=0.444057011961809, w0=0.00017667515368225458, w1=-0.011304563439444507\n",
      "Gradient Descent(150/399): loss=0.44381520912790684, w0=0.00017824513133222003, w1=-0.011363319530269618\n",
      "Gradient Descent(151/399): loss=0.44357450891064687, w0=0.00017981202570035233, w1=-0.011421904227829493\n",
      "Gradient Descent(152/399): loss=0.4433349024542177, w0=0.00018137574554037526, w1=-0.011480318300182426\n",
      "Gradient Descent(153/399): loss=0.4430963809871067, w0=0.00018293620092672206, w1=-0.01153856251131994\n",
      "Gradient Descent(154/399): loss=0.4428589358212206, w0=0.00018449330324065712, w1=-0.011596637621191326\n",
      "Gradient Descent(155/399): loss=0.442622558351018, w0=0.00018604696515653384, w1=-0.011654544385727997\n",
      "Gradient Descent(156/399): loss=0.4423872400526512, w0=0.00018759710062818763, w1=-0.011712283556867695\n",
      "Gradient Descent(157/399): loss=0.4421529724831159, w0=0.00018914362487546245, w1=-0.011769855882578504\n",
      "Gradient Descent(158/399): loss=0.44191974727941397, w0=0.00019068645437086965, w1=-0.011827262106882726\n",
      "Gradient Descent(159/399): loss=0.4416875561577224, w0=0.00019222550682637804, w1=-0.011884502969880575\n",
      "Gradient Descent(160/399): loss=0.4414563909125748, w0=0.00019376070118033365, w1=-0.011941579207773714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(161/399): loss=0.44122624341604894, w0=0.00019529195758450802, w1=-0.01199849155288863\n",
      "Gradient Descent(162/399): loss=0.4409971056169667, w0=0.0001968191973912739, w1=-0.012055240733699854\n",
      "Gradient Descent(163/399): loss=0.44076896954010014, w0=0.0001983423431409068, w1=-0.012111827474853011\n",
      "Gradient Descent(164/399): loss=0.4405418272853884, w0=0.0001998613185490115, w1=-0.012168252497187724\n",
      "Gradient Descent(165/399): loss=0.44031567102716396, w0=0.00020137604849407225, w1=-0.012224516517760355\n",
      "Gradient Descent(166/399): loss=0.44009049301338454, w0=0.0002028864590051251, w1=-0.012280620249866592\n",
      "Gradient Descent(167/399): loss=0.43986628556487795, w0=0.00020439247724955185, w1=-0.012336564403063882\n",
      "Gradient Descent(168/399): loss=0.439643041074591, w0=0.00020589403152099364, w1=-0.012392349683193714\n",
      "Gradient Descent(169/399): loss=0.439420752006851, w0=0.00020739105122738368, w1=-0.012447976792403745\n",
      "Gradient Descent(170/399): loss=0.43919941089663184, w0=0.00020888346687909773, w1=-0.012503446429169781\n",
      "Gradient Descent(171/399): loss=0.43897901034883086, w0=0.00021037121007722094, w1=-0.012558759288317609\n",
      "Gradient Descent(172/399): loss=0.43875954303755327, w0=0.00021185421350193028, w1=-0.012613916061044669\n",
      "Gradient Descent(173/399): loss=0.4385410017054032, w0=0.00021333241090099134, w1=-0.012668917434941598\n",
      "Gradient Descent(174/399): loss=0.4383233791627843, w0=0.00021480573707836827, w1=-0.012723764094013615\n",
      "Gradient Descent(175/399): loss=0.4381066682872071, w0=0.00021627412788294585, w1=-0.012778456718701762\n",
      "Gradient Descent(176/399): loss=0.4378908620226051, w0=0.00021773752019736258, w1=-0.012832995985904008\n",
      "Gradient Descent(177/399): loss=0.43767595337865683, w0=0.00021919585192695378, w1=-0.012887382568996205\n",
      "Gradient Descent(178/399): loss=0.4374619354301179, w0=0.0002206490619888035, w1=-0.012941617137852905\n",
      "Gradient Descent(179/399): loss=0.4372488013161565, w0=0.00022209709030090427, w1=-0.012995700358868033\n",
      "Gradient Descent(180/399): loss=0.43703654423970134, w0=0.0002235398777714237, w1=-0.013049632894975427\n",
      "Gradient Descent(181/399): loss=0.43682515746679307, w0=0.00022497736628807668, w1=-0.013103415405669236\n",
      "Gradient Descent(182/399): loss=0.43661463432594294, w0=0.00022640949870760243, w1=-0.013157048547024177\n",
      "Gradient Descent(183/399): loss=0.4364049682075009, w0=0.00022783621884534523, w1=-0.013210532971715665\n",
      "Gradient Descent(184/399): loss=0.43619615256302935, w0=0.00022925747146493772, w1=-0.0132638693290398\n",
      "Gradient Descent(185/399): loss=0.43598818090468267, w0=0.00023067320226808607, w1=-0.013317058264933224\n",
      "Gradient Descent(186/399): loss=0.4357810468045949, w0=0.00023208335788445567, w1=-0.013370100421992848\n",
      "Gradient Descent(187/399): loss=0.43557474389427453, w0=0.00023348788586165668, w1=-0.013422996439495434\n",
      "Gradient Descent(188/399): loss=0.4353692658640048, w0=0.00023488673465532837, w1=-0.013475746953417063\n",
      "Gradient Descent(189/399): loss=0.4351646064622503, w0=0.00023627985361932105, w1=-0.013528352596452459\n",
      "Gradient Descent(190/399): loss=0.43496075949507285, w0=0.00023766719299597516, w1=-0.013580813998034198\n",
      "Gradient Descent(191/399): loss=0.4347577188255481, w0=0.00023904870390649595, w1=-0.013633131784351769\n",
      "Gradient Descent(192/399): loss=0.4345554783731962, w0=0.0002404243383414234, w1=-0.013685306578370535\n",
      "Gradient Descent(193/399): loss=0.43435403211341134, w0=0.00024179404915119604, w1=-0.013737338999850537\n",
      "Gradient Descent(194/399): loss=0.43415337407690197, w0=0.00024315779003680798, w1=-0.013789229665365202\n",
      "Gradient Descent(195/399): loss=0.433953498349136, w0=0.00024451551554055816, w1=-0.013840979188319905\n",
      "Gradient Descent(196/399): loss=0.4337543990697912, w0=0.00024586718103689097, w1=-0.013892588178970416\n",
      "Gradient Descent(197/399): loss=0.43355607043221256, w0=0.0002472127427233273, w1=-0.013944057244441234\n",
      "Gradient Descent(198/399): loss=0.43335850668287507, w0=0.0002485521576114852, w1=-0.013995386988743778\n",
      "Gradient Descent(199/399): loss=0.4331617021208527, w0=0.0002498853835181891, w1=-0.01404657801279448\n",
      "Gradient Descent(200/399): loss=0.4329656510972917, w0=0.0002512123790566672, w1=-0.014097630914432741\n",
      "Gradient Descent(201/399): loss=0.4327703480148922, w0=0.0002525331036278356, w1=-0.014148546288438781\n",
      "Gradient Descent(202/399): loss=0.4325757873273937, w0=0.00025384751741166844, w1=-0.014199324726551363\n",
      "Gradient Descent(203/399): loss=0.4323819635390646, w0=0.00025515558135865394, w1=-0.014249966817485405\n",
      "Gradient Descent(204/399): loss=0.43218887120420085, w0=0.0002564572571813343, w1=-0.014300473146949471\n",
      "Gradient Descent(205/399): loss=0.4319965049266275, w0=0.0002577525073459295, w1=-0.014350844297663149\n",
      "Gradient Descent(206/399): loss=0.43180485935920543, w0=0.0002590412950640441, w1=-0.014401080849374323\n",
      "Gradient Descent(207/399): loss=0.4316139292033453, w0=0.00026032358428445585, w1=-0.014451183378876312\n",
      "Gradient Descent(208/399): loss=0.4314237092085249, w0=0.00026159933968498576, w1=-0.01450115246002492\n",
      "Gradient Descent(209/399): loss=0.43123419417181175, w0=0.00026286852666444847, w1=-0.014550988663755357\n",
      "Gradient Descent(210/399): loss=0.43104537893739214, w0=0.00026413111133468244, w1=-0.014600692558099053\n",
      "Gradient Descent(211/399): loss=0.43085725839610445, w0=0.0002653870605126589, w1=-0.014650264708200366\n",
      "Gradient Descent(212/399): loss=0.4306698274849766, w0=0.0002666363417126692, w1=-0.014699705676333179\n",
      "Gradient Descent(213/399): loss=0.4304830811867697, w0=0.0002678789231385891, w1=-0.014749016021917388\n",
      "Gradient Descent(214/399): loss=0.4302970145295263, w0=0.0002691147736762204, w1=-0.014798196301535279\n",
      "Gradient Descent(215/399): loss=0.43011162258612273, w0=0.00027034386288570764, w1=-0.014847247068947803\n",
      "Gradient Descent(216/399): loss=0.42992690047382714, w0=0.0002715661609940306, w1=-0.014896168875110746\n",
      "Gradient Descent(217/399): loss=0.42974284335386126, w0=0.0002727816388875709, w1=-0.014944962268190786\n",
      "Gradient Descent(218/399): loss=0.4295594464309681, w0=0.00027399026810475237, w1=-0.014993627793581456\n",
      "Gradient Descent(219/399): loss=0.42937670495298197, w0=0.0002751920208287545, w1=-0.015042165993918992\n",
      "Gradient Descent(220/399): loss=0.42919461421040644, w0=0.0002763868698802981, w1=-0.015090577409098086\n",
      "Gradient Descent(221/399): loss=0.4290131695359938, w0=0.0002775747887105024, w1=-0.015138862576287538\n",
      "Gradient Descent(222/399): loss=0.42883236630432947, w0=0.00027875575139381343, w1=-0.015187022029945799\n",
      "Gradient Descent(223/399): loss=0.42865219993142323, w0=0.0002799297326210021, w1=-0.015235056301836419\n",
      "Gradient Descent(224/399): loss=0.4284726658743002, w0=0.0002810967076922322, w1=-0.015282965921043393\n",
      "Gradient Descent(225/399): loss=0.42829375963060146, w0=0.0002822566525101969, w1=-0.01533075141398641\n",
      "Gradient Descent(226/399): loss=0.42811547673818423, w0=0.000283409543573324, w1=-0.015378413304435996\n",
      "Gradient Descent(227/399): loss=0.4279378127747292, w0=0.0002845553579690477, w1=-0.015425952113528573\n",
      "Gradient Descent(228/399): loss=0.4277607633573509, w0=0.0002856940733671484, w1=-0.01547336835978141\n",
      "Gradient Descent(229/399): loss=0.4275843241422117, w0=0.00028682566801315786, w1=-0.015520662559107482\n",
      "Gradient Descent(230/399): loss=0.4274084908241417, w0=0.00028795012072183, w1=-0.015567835224830223\n",
      "Gradient Descent(231/399): loss=0.42723325913625954, w0=0.0002890674108706774, w1=-0.015614886867698206\n",
      "Gradient Descent(232/399): loss=0.42705862484960044, w0=0.0002901775183935709, w1=-0.01566181799589971\n",
      "Gradient Descent(233/399): loss=0.4268845837727461, w0=0.00029128042377440385, w1=-0.015708629115077203\n",
      "Gradient Descent(234/399): loss=0.4267111317514589, w0=0.0002923761080408191, w1=-0.015755320728341717\n",
      "Gradient Descent(235/399): loss=0.42653826466832023, w0=0.00029346455275799824, w1=-0.015801893336287152\n",
      "Gradient Descent(236/399): loss=0.4263659784423721, w0=0.00029454574002251295, w1=-0.015848347437004483\n",
      "Gradient Descent(237/399): loss=0.4261942690287629, w0=0.0002956196524562374, w1=-0.015894683526095868\n",
      "Gradient Descent(238/399): loss=0.4260231324183975, w0=0.0002966862732003216, w1=-0.01594090209668866\n",
      "Gradient Descent(239/399): loss=0.4258525646375883, w0=0.00029774558590922416, w1=-0.015987003639449354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(240/399): loss=0.4256825617477141, w0=0.00029879757474480546, w1=-0.01603298864259742\n",
      "Gradient Descent(241/399): loss=0.4255131198448783, w0=0.00029984222437047894, w1=-0.01607885759191907\n",
      "Gradient Descent(242/399): loss=0.42534423505957386, w0=0.000300879519945421, w1=-0.01612461097078091\n",
      "Gradient Descent(243/399): loss=0.42517590355634927, w0=0.00030190944711883875, w1=-0.016170249260143533\n",
      "Gradient Descent(244/399): loss=0.4250081215334804, w0=0.00030293199202429473, w1=-0.016215772938575006\n",
      "Gradient Descent(245/399): loss=0.4248408852226437, w0=0.0003039471412740885, w1=-0.016261182482264287\n",
      "Gradient Descent(246/399): loss=0.42467419088859326, w0=0.00030495488195369423, w1=-0.01630647836503454\n",
      "Gradient Descent(247/399): loss=0.4245080348288428, w0=0.00030595520161625376, w1=-0.01635166105835638\n",
      "Gradient Descent(248/399): loss=0.42434241337334766, w0=0.00030694808827712484, w1=-0.016396731031361023\n",
      "Gradient Descent(249/399): loss=0.4241773228841941, w0=0.00030793353040848377, w1=-0.016441688750853365\n",
      "Gradient Descent(250/399): loss=0.424012759755288, w0=0.000308911516933982, w1=-0.01648653468132497\n",
      "Gradient Descent(251/399): loss=0.4238487204120499, w0=0.0003098820372234559, w1=-0.016531269284966968\n",
      "Gradient Descent(252/399): loss=0.4236852013111111, w0=0.00031084508108769013, w1=-0.016575893021682896\n",
      "Gradient Descent(253/399): loss=0.4235221989400132, w0=0.0003118006387732325, w1=-0.01662040634910143\n",
      "Gradient Descent(254/399): loss=0.42335970981691196, w0=0.0003127487009572614, w1=-0.016664809722589057\n",
      "Gradient Descent(255/399): loss=0.4231977304902822, w0=0.0003136892587425045, w1=-0.01670910359526266\n",
      "Gradient Descent(256/399): loss=0.4230362575386288, w0=0.0003146223036522078, w1=-0.016753288418002023\n",
      "Gradient Descent(257/399): loss=0.422875287570196, w0=0.00031554782762515597, w1=-0.016797364639462253\n",
      "Gradient Descent(258/399): loss=0.42271481722268484, w0=0.0003164658230107418, w1=-0.016841332706086138\n",
      "Gradient Descent(259/399): loss=0.422554843162969, w0=0.00031737628256408554, w1=-0.016885193062116412\n",
      "Gradient Descent(260/399): loss=0.4223953620868176, w0=0.00031827919944120284, w1=-0.016928946149607958\n",
      "Gradient Descent(261/399): loss=0.42223637071861675, w0=0.0003191745671942215, w1=-0.01697259240843992\n",
      "Gradient Descent(262/399): loss=0.4220778658110982, w0=0.00032006237976664605, w1=-0.017016132276327745\n",
      "Gradient Descent(263/399): loss=0.42191984414506567, w0=0.00032094263148866963, w1=-0.017059566188835158\n",
      "Gradient Descent(264/399): loss=0.4217623025291297, w0=0.0003218153170725334, w1=-0.017102894579386043\n",
      "Gradient Descent(265/399): loss=0.42160523779944103, w0=0.0003226804316079323, w1=-0.017146117879276276\n",
      "Gradient Descent(266/399): loss=0.4214486468194277, w0=0.0003235379705574667, w1=-0.01718923651768546\n",
      "Gradient Descent(267/399): loss=0.4212925264795356, w0=0.00032438792975214044, w1=-0.017232250921688598\n",
      "Gradient Descent(268/399): loss=0.42113687369697095, w0=0.00032523030538690346, w1=-0.0172751615162677\n",
      "Gradient Descent(269/399): loss=0.4209816854154462, w0=0.0003260650940162396, w1=-0.017317968724323295\n",
      "Gradient Descent(270/399): loss=0.4208269586049269, w0=0.00032689229254979885, w1=-0.017360672966685902\n",
      "Gradient Descent(271/399): loss=0.42067269026138304, w0=0.00032771189824807326, w1=-0.017403274662127405\n",
      "Gradient Descent(272/399): loss=0.42051887740654115, w0=0.0003285239087181167, w1=-0.01744577422737237\n",
      "Gradient Descent(273/399): loss=0.4203655170876412, w0=0.0003293283219093075, w1=-0.017488172077109294\n",
      "Gradient Descent(274/399): loss=0.4202126063771928, w0=0.000330125136109154, w1=-0.017530468624001764\n",
      "Gradient Descent(275/399): loss=0.420060142372737, w0=0.00033091434993914214, w1=-0.01757266427869958\n",
      "Gradient Descent(276/399): loss=0.4199081221966079, w0=0.000331695962350625, w1=-0.017614759449849773\n",
      "Gradient Descent(277/399): loss=0.4197565429956991, w0=0.00033246997262075396, w1=-0.017656754544107596\n",
      "Gradient Descent(278/399): loss=0.4196054019412303, w0=0.00033323638034845086, w1=-0.017698649966147402\n",
      "Gradient Descent(279/399): loss=0.4194546962285176, w0=0.00033399518545042055, w1=-0.017740446118673488\n",
      "Gradient Descent(280/399): loss=0.4193044230767453, w0=0.0003347463881572041, w1=-0.017782143402430863\n",
      "Gradient Descent(281/399): loss=0.4191545797287415, w0=0.0003354899890092716, w1=-0.017823742216215948\n",
      "Gradient Descent(282/399): loss=0.4190051634507532, w0=0.0003362259888531545, w1=-0.0178652429568872\n",
      "Gradient Descent(283/399): loss=0.4188561715322272, w0=0.00033695438883761747, w1=-0.017906646019375704\n",
      "Gradient Descent(284/399): loss=0.4187076012855895, w0=0.0003376751904098681, w1=-0.017947951796695647\n",
      "Gradient Descent(285/399): loss=0.4185594500460301, w0=0.0003383883953118059, w1=-0.017989160679954787\n",
      "Gradient Descent(286/399): loss=0.41841171517128817, w0=0.0003390940055763087, w1=-0.01803027305836481\n",
      "Gradient Descent(287/399): loss=0.41826439404143945, w0=0.0003397920235235567, w1=-0.018071289319251647\n",
      "Gradient Descent(288/399): loss=0.4181174840586865, w0=0.00034048245175739416, w1=-0.018112209848065733\n",
      "Gradient Descent(289/399): loss=0.4179709826471507, w0=0.0003411652931617274, w1=-0.01815303502839218\n",
      "Gradient Descent(290/399): loss=0.41782488725266576, w0=0.00034184055089695993, w1=-0.018193765241960907\n",
      "Gradient Descent(291/399): loss=0.4176791953425749, w0=0.0003425082283964631, w1=-0.018234400868656707\n",
      "Gradient Descent(292/399): loss=0.417533904405528, w0=0.00034316832936308317, w1=-0.01827494228652924\n",
      "Gradient Descent(293/399): loss=0.41738901195128186, w0=0.0003438208577656835, w1=-0.018315389871802985\n",
      "Gradient Descent(294/399): loss=0.4172445155105028, w0=0.000344465817835722, w1=-0.018355743998887106\n",
      "Gradient Descent(295/399): loss=0.4171004126345704, w0=0.00034510321406386324, w1=-0.018396005040385285\n",
      "Gradient Descent(296/399): loss=0.4169567008953845, w0=0.0003457330511966252, w1=-0.018436173367105475\n",
      "Gradient Descent(297/399): loss=0.4168133778851712, w0=0.00034635533423305997, w1=-0.0184762493480696\n",
      "Gradient Descent(298/399): loss=0.41667044121629526, w0=0.0003469700684214683, w1=-0.018516233350523204\n",
      "Gradient Descent(299/399): loss=0.4165278885210694, w0=0.0003475772592561476, w1=-0.018556125739945025\n",
      "Gradient Descent(300/399): loss=0.41638571745157044, w0=0.0003481769124741731, w1=-0.018595926880056533\n",
      "Gradient Descent(301/399): loss=0.41624392567945223, w0=0.00034876903405221176, w1=-0.01863563713283139\n",
      "Gradient Descent(302/399): loss=0.416102510895766, w0=0.00034935363020336866, w1=-0.01867525685850486\n",
      "Gradient Descent(303/399): loss=0.4159614708107767, w0=0.00034993070737406566, w1=-0.018714786415583162\n",
      "Gradient Descent(304/399): loss=0.415820803153785, w0=0.00035050027224095185, w1=-0.01875422616085278\n",
      "Gradient Descent(305/399): loss=0.415680505672951, w0=0.00035106233170784557, w1=-0.018793576449389695\n",
      "Gradient Descent(306/399): loss=0.4155405761351173, w0=0.00035161689290270757, w1=-0.01883283763456856\n",
      "Gradient Descent(307/399): loss=0.41540101232563575, w0=0.0003521639631746454, w1=-0.018872010068071852\n",
      "Gradient Descent(308/399): loss=0.4152618120481956, w0=0.00035270355009094816, w1=-0.018911094099898933\n",
      "Gradient Descent(309/399): loss=0.41512297312465285, w0=0.00035323566143415157, w1=-0.01895009007837508\n",
      "Gradient Descent(310/399): loss=0.41498449339486165, w0=0.0003537603051991334, w1=-0.018988998350160442\n",
      "Gradient Descent(311/399): loss=0.41484637071650693, w0=0.00035427748959023837, w1=-0.01902781926025896\n",
      "Gradient Descent(312/399): loss=0.4147086029649398, w0=0.00035478722301843273, w1=-0.019066553152027235\n",
      "Gradient Descent(313/399): loss=0.41457118803301296, w0=0.0003552895140984877, w1=-0.019105200367183307\n",
      "Gradient Descent(314/399): loss=0.4144341238309193, w0=0.0003557843716461921, w1=-0.01914376124581544\n",
      "Gradient Descent(315/399): loss=0.4142974082860307, w0=0.0003562718046755937, w1=-0.019182236126390802\n",
      "Gradient Descent(316/399): loss=0.41416103934273957, w0=0.0003567518223962684, w1=-0.01922062534576413\n",
      "Gradient Descent(317/399): loss=0.41402501496230104, w0=0.0003572244342106181, w1=-0.019258929239186318\n",
      "Gradient Descent(318/399): loss=0.4138893331226775, w0=0.00035768964971119577, w1=-0.019297148140312963\n",
      "Gradient Descent(319/399): loss=0.4137539918183839, w0=0.0003581474786780587, w1=-0.019335282381212866\n",
      "Gradient Descent(320/399): loss=0.41361898906033556, w0=0.0003585979310761482, w1=-0.019373332292376474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(321/399): loss=0.41348432287569614, w0=0.00035904101705269714, w1=-0.019411298202724275\n",
      "Gradient Descent(322/399): loss=0.4133499913077281, w0=0.0003594767469346635, w1=-0.019449180439615146\n",
      "Gradient Descent(323/399): loss=0.413215992415644, w0=0.0003599051312261909, w1=-0.019486979328854643\n",
      "Gradient Descent(324/399): loss=0.4130823242744608, w0=0.00036032618060609486, w1=-0.019524695194703248\n",
      "Gradient Descent(325/399): loss=0.41294898497485305, w0=0.0003607399059253756, w1=-0.01956232835988457\n",
      "Gradient Descent(326/399): loss=0.41281597262300934, w0=0.00036114631820475607, w1=-0.019599879145593493\n",
      "Gradient Descent(327/399): loss=0.4126832853404896, w0=0.00036154542863224554, w1=-0.01963734787150427\n",
      "Gradient Descent(328/399): loss=0.4125509212640841, w0=0.0003619372485607286, w1=-0.019674734855778586\n",
      "Gradient Descent(329/399): loss=0.4124188785456734, w0=0.0003623217895055788, w1=-0.019712040415073545\n",
      "Gradient Descent(330/399): loss=0.41228715535208965, w0=0.00036269906314229724, w1=-0.01974926486454965\n",
      "Gradient Descent(331/399): loss=0.4121557498649797, w0=0.0003630690813041757, w1=-0.019786408517878693\n",
      "Gradient Descent(332/399): loss=0.41202466028066964, w0=0.0003634318559799836, w1=-0.019823471687251636\n",
      "Gradient Descent(333/399): loss=0.4118938848100297, w0=0.00036378739931167955, w1=-0.019860454683386413\n",
      "Gradient Descent(334/399): loss=0.4117634216783408, w0=0.00036413572359214613, w1=-0.019897357815535713\n",
      "Gradient Descent(335/399): loss=0.41163326912516385, w0=0.00036447684126294854, w1=-0.019934181391494697\n",
      "Gradient Descent(336/399): loss=0.41150342540420765, w0=0.0003648107649121165, w1=-0.019970925717608685\n",
      "Gradient Descent(337/399): loss=0.41137388878320147, w0=0.000365137507271949, w1=-0.020007591098780787\n",
      "Gradient Descent(338/399): loss=0.41124465754376527, w0=0.00036545708121684257, w1=-0.020044177838479494\n",
      "Gradient Descent(339/399): loss=0.41111572998128315, w0=0.00036576949976114105, w1=-0.020080686238746228\n",
      "Gradient Descent(340/399): loss=0.4109871044047787, w0=0.000366074776057009, w1=-0.020117116600202833\n",
      "Gradient Descent(341/399): loss=0.41085877913678953, w0=0.0003663729233923264, w1=-0.020153469222059038\n",
      "Gradient Descent(342/399): loss=0.4107307525132451, w0=0.00036666395518860604, w1=-0.02018974440211988\n",
      "Gradient Descent(343/399): loss=0.4106030228833437, w0=0.00036694788499893195, w1=-0.020225942436793064\n",
      "Gradient Descent(344/399): loss=0.4104755886094324, w0=0.00036722472650591996, w1=-0.020262063621096282\n",
      "Gradient Descent(345/399): loss=0.4103484480668873, w0=0.0003674944935196994, w1=-0.020298108248664522\n",
      "Gradient Descent(346/399): loss=0.4102215996439949, w0=0.00036775719997591584, w1=-0.020334076611757287\n",
      "Gradient Descent(347/399): loss=0.41009504174183475, w0=0.000368012859933755, w1=-0.020369969001265805\n",
      "Gradient Descent(348/399): loss=0.4099687727741633, w0=0.0003682614875739871, w1=-0.020405785706720192\n",
      "Gradient Descent(349/399): loss=0.4098427911672994, w0=0.00036850309719703206, w1=-0.02044152701629656\n",
      "Gradient Descent(350/399): loss=0.40971709536000894, w0=0.0003687377032210447, w1=-0.020477193216824088\n",
      "Gradient Descent(351/399): loss=0.40959168380339384, w0=0.0003689653201800203, w1=-0.02051278459379207\n",
      "Gradient Descent(352/399): loss=0.4094665549607786, w0=0.00036918596272192, w1=-0.020548301431356896\n",
      "Gradient Descent(353/399): loss=0.4093417073076008, w0=0.0003693996456068159, w1=-0.020583744012349005\n",
      "Gradient Descent(354/399): loss=0.4092171393313011, w0=0.0003696063837050559, w1=-0.020619112618279808\n",
      "Gradient Descent(355/399): loss=0.4090928495312142, w0=0.00036980619199544773, w1=-0.020654407529348544\n",
      "Gradient Descent(356/399): loss=0.40896883641846205, w0=0.0003699990855634619, w1=-0.020689629024449124\n",
      "Gradient Descent(357/399): loss=0.40884509851584705, w0=0.00037018507959945423, w1=-0.02072477738117692\n",
      "Gradient Descent(358/399): loss=0.40872163435774667, w0=0.00037036418939690683, w1=-0.020759852875835517\n",
      "Gradient Descent(359/399): loss=0.40859844249000854, w0=0.00037053643035068746, w1=-0.02079485578344343\n",
      "Gradient Descent(360/399): loss=0.40847552146984795, w0=0.0003707018179553278, w1=-0.020829786377740772\n",
      "Gradient Descent(361/399): loss=0.4083528698657446, w0=0.00037086036780331963, w1=-0.020864644931195903\n",
      "Gradient Descent(362/399): loss=0.4082304862573413, w0=0.0003710120955834293, w1=-0.020899431715012025\n",
      "Gradient Descent(363/399): loss=0.40810836923534377, w0=0.0003711570170790298, w1=-0.020934146999133735\n",
      "Gradient Descent(364/399): loss=0.4079865174014211, w0=0.000371295148166451, w1=-0.020968791052253553\n",
      "Gradient Descent(365/399): loss=0.4078649293681067, w0=0.00037142650481334694, w1=-0.02100336414181841\n",
      "Gradient Descent(366/399): loss=0.4077436037587013, w0=0.00037155110307708085, w1=-0.021037866534036102\n",
      "Gradient Descent(367/399): loss=0.4076225392071754, w0=0.00037166895910312734, w1=-0.02107229849388168\n",
      "Gradient Descent(368/399): loss=0.4075017343580747, w0=0.0003717800891234914, w1=-0.021106660285103843\n",
      "Gradient Descent(369/399): loss=0.40738118786642447, w0=0.0003718845094551446, w1=-0.021140952170231275\n",
      "Gradient Descent(370/399): loss=0.40726089839763535, w0=0.0003719822364984776, w1=-0.021175174410578937\n",
      "Gradient Descent(371/399): loss=0.4071408646274115, w0=0.0003720732867357699, w1=-0.021209327266254345\n",
      "Gradient Descent(372/399): loss=0.4070210852416569, w0=0.0003721576767296748, w1=-0.021243410996163784\n",
      "Gradient Descent(373/399): loss=0.40690155893638535, w0=0.00037223542312172196, w1=-0.021277425858018513\n",
      "Gradient Descent(374/399): loss=0.4067822844176295, w0=0.00037230654263083505, w1=-0.02131137210834092\n",
      "Gradient Descent(375/399): loss=0.40666326040135137, w0=0.0003723710520518657, w1=-0.021345250002470655\n",
      "Gradient Descent(376/399): loss=0.4065444856133531, w0=0.00037242896825414316, w1=-0.021379059794570696\n",
      "Gradient Descent(377/399): loss=0.40642595878919097, w0=0.00037248030818003933, w1=-0.02141280173763343\n",
      "Gradient Descent(378/399): loss=0.4063076786740857, w0=0.0003725250888435498, w1=-0.021446476083486647\n",
      "Gradient Descent(379/399): loss=0.40618964402283897, w0=0.00037256332732888943, w1=-0.02148008308279955\n",
      "Gradient Descent(380/399): loss=0.40607185359974635, w0=0.00037259504078910384, w1=-0.02151362298508868\n",
      "Gradient Descent(381/399): loss=0.4059543061785128, w0=0.00037262024644469527, w1=-0.02154709603872386\n",
      "Gradient Descent(382/399): loss=0.4058370005421699, w0=0.0003726389615822637, w1=-0.021580502490934052\n",
      "Gradient Descent(383/399): loss=0.40571993548299234, w0=0.0003726512035531624, w1=-0.021613842587813226\n",
      "Gradient Descent(384/399): loss=0.4056031098024156, w0=0.0003726569897721683, w1=-0.021647116574326174\n",
      "Gradient Descent(385/399): loss=0.4054865223109552, w0=0.00037265633771616674, w1=-0.021680324694314287\n",
      "Gradient Descent(386/399): loss=0.4053701718281252, w0=0.00037264926492285006, w1=-0.021713467190501313\n",
      "Gradient Descent(387/399): loss=0.4052540571823592, w0=0.0003726357889894311, w1=-0.021746544304499068\n",
      "Gradient Descent(388/399): loss=0.40513817721093104, w0=0.0003726159275713701, w1=-0.021779556276813133\n",
      "Gradient Descent(389/399): loss=0.40502253075987615, w0=0.0003725896983811157, w1=-0.021812503346848505\n",
      "Gradient Descent(390/399): loss=0.4049071166839143, w0=0.0003725571191868595, w1=-0.021845385752915208\n",
      "Gradient Descent(391/399): loss=0.40479193384637296, w0=0.00037251820781130464, w1=-0.0218782037322339\n",
      "Gradient Descent(392/399): loss=0.40467698111911055, w0=0.00037247298213044717, w1=-0.021910957520941424\n",
      "Gradient Descent(393/399): loss=0.40456225738244156, w0=0.0003724214600723715, w1=-0.021943647354096332\n",
      "Gradient Descent(394/399): loss=0.40444776152506157, w0=0.00037236365961605835, w1=-0.02197627346568439\n",
      "Gradient Descent(395/399): loss=0.40433349244397376, w0=0.0003722995987902063, w1=-0.02200883608862404\n",
      "Gradient Descent(396/399): loss=0.40421944904441487, w0=0.00037222929567206606, w1=-0.02204133545477183\n",
      "Gradient Descent(397/399): loss=0.4041056302397832, w0=0.0003721527683862875, w1=-0.02207377179492783\n",
      "Gradient Descent(398/399): loss=0.40399203495156705, w0=0.0003720700351037795, w1=-0.022106145338840994\n",
      "Gradient Descent(399/399): loss=0.4038786621092719, w0=0.0003719811140405823, w1=-0.02213845631521451\n",
      "++++ gamma = 0.000719685673001\n",
      "ciaociaociao\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/399): loss=0.4999999999999887, w0=-4.956642416006324e-07, w1=-0.00025801623469334025\n",
      "Gradient Descent(1/399): loss=0.4984074787246081, w0=-7.049990571385746e-07, w1=-0.0005134688741594068\n",
      "Gradient Descent(2/399): loss=0.49684616036082363, w0=-6.390997723981293e-07, w1=-0.0007663934648813193\n",
      "Gradient Descent(3/399): loss=0.4953152081988466, w0=-3.087418877945349e-07, w1=-0.0010168249711848443\n",
      "Gradient Descent(4/399): loss=0.49381381130984725, w0=2.7561029174925095e-07, w1=-0.0012647977862361211\n",
      "Gradient Descent(5/399): loss=0.4923411836481514, w0=1.1037952591443092e-06, w1=-0.0015103457428028025\n",
      "Gradient Descent(6/399): loss=0.4908965631888764, w0=2.165946133374344e-06, w1=-0.0017535021237842648\n",
      "Gradient Descent(7/399): loss=0.4894792110994442, w0=3.452482707482458e-06, w1=-0.0019942996725163743\n",
      "Gradient Descent(8/399): loss=0.4880884109434803, w0=4.954103710327425e-06, w1=-0.002232770602856182\n",
      "Gradient Descent(9/399): loss=0.48672346791567506, w0=6.66177927629495e-06, w1=-0.0024689466090517457\n",
      "Gradient Descent(10/399): loss=0.48538370810625237, w0=8.566743617309667e-06, w1=-0.0027028588754021976\n",
      "Gradient Descent(11/399): loss=0.4840684777937607, w0=1.0660487891648776e-05, w1=-0.0029345380857129823\n",
      "Gradient Descent(12/399): loss=0.4827771427649551, w0=1.2934753264210093e-05, w1=-0.003164014432551121\n",
      "Gradient Descent(13/399): loss=0.48150908766060396, w0=1.5381524153033538e-05, w1=-0.003391317626305211\n",
      "Gradient Descent(14/399): loss=0.48026371534610635, w0=1.7993021657018747e-05, w1=-0.0036164769040547243\n",
      "Gradient Descent(15/399): loss=0.47904044630585907, w0=2.07616971599203e-05, w1=-0.003839521038253121\n",
      "Gradient Descent(16/399): loss=0.4778387180603635, w0=2.368022610583681e-05, w1=-0.004060478345229097\n",
      "Gradient Descent(17/399): loss=0.4766579846051081, w0=2.6741501941542126e-05, w1=-0.004279376693510246\n",
      "Gradient Descent(18/399): loss=0.47549771587030876, w0=2.9938630221134414e-05, w1=-0.004496243511973263\n",
      "Gradient Descent(19/399): loss=0.4743573972006335, w0=3.326492286860288e-05, w1=-0.004711105797824727\n",
      "Gradient Descent(20/399): loss=0.4732365288540733, w0=3.671389259403303e-05, w1=-0.004923990124416416\n",
      "Gradient Descent(21/399): loss=0.4721346255191654, w0=4.0279247459288756e-05, w1=-0.005134922648898971\n",
      "Gradient Descent(22/399): loss=0.47105121584981224, w0=4.3954885589123396e-05, w1=-0.005343929119717663\n",
      "Gradient Descent(23/399): loss=0.4699858420169658, w0=4.773489002378331e-05, w1=-0.005551034883953907\n",
      "Gradient Descent(24/399): loss=0.46893805927649146, w0=5.161352370927547e-05, w1=-0.005756264894516086\n",
      "Gradient Descent(25/399): loss=0.4679074355525492, w0=5.558522462157504e-05, w1=-0.005959643717183133\n",
      "Gradient Descent(26/399): loss=0.46689355103586355, w0=5.9644601021151684e-05, w1=-0.0061611955375042785\n",
      "Gradient Descent(27/399): loss=0.46589599779628044, w0=6.378642683429206e-05, w1=-0.006360944167558237\n",
      "Gradient Descent(28/399): loss=0.4649143794090368, w0=6.800563715779247e-05, w1=-0.006558913052575057\n",
      "Gradient Descent(29/399): loss=0.4639483105941953, w0=7.229732388368985e-05, w1=-0.0067551252774237696\n",
      "Gradient Descent(30/399): loss=0.46299741686872076, w0=7.665673144079013e-05, w1=-0.006949603572968886\n",
      "Gradient Descent(31/399): loss=0.46206133421069584, w0=8.10792526498417e-05, w1=-0.00714237032229873\n",
      "Gradient Descent(32/399): loss=0.46113970873519994, w0=8.556042468928831e-05, w1=-0.007333447566828505\n",
      "Gradient Descent(33/399): loss=0.46023219638139223, w0=9.009592516861913e-05, w1=-0.0075228570122809445\n",
      "Gradient Descent(34/399): loss=0.4593384626103645, w0=9.468156830641574e-05, w1=-0.007710620034547288\n",
      "Gradient Descent(35/399): loss=0.45845818211334194, w0=9.931330121027454e-05, w1=-0.007896757685431306\n",
      "Gradient Descent(36/399): loss=0.45759103852983585, w0=0.00010398720025586084, w1=-0.008081290698278976\n",
      "Gradient Descent(37/399): loss=0.4567367241753636, w0=0.00010869946756242518, w1=-0.008264239493496396\n",
      "Gradient Descent(38/399): loss=0.45589493977837087, w0=0.0001134464275621859, w1=-0.008445624183958434\n",
      "Gradient Descent(39/399): loss=0.4550653942260064, w0=0.00011822452366105264, w1=-0.008625464580310544\n",
      "Gradient Descent(40/399): loss=0.45424780431841216, w0=0.0001230303149882341, w1=-0.008803780196166134\n",
      "Gradient Descent(41/399): loss=0.4534418945312106, w0=0.00012786047323234105, w1=-0.008980590253201824\n",
      "Gradient Descent(42/399): loss=0.45264739678588123, w0=0.00013271177956166022, w1=-0.009155913686152838\n",
      "Gradient Descent(43/399): loss=0.4518640502277289, w0=0.00013758112162633832, w1=-0.009329769147710752\n",
      "Gradient Descent(44/399): loss=0.45109160101116785, w0=0.000142465490640277, w1=-0.009502175013325754\n",
      "Gradient Descent(45/399): loss=0.4503298020920457, w0=0.00014736197854059958, w1=-0.009673149385915528\n",
      "Gradient Descent(46/399): loss=0.4495784130267522, w0=0.0001522677752226084, w1=-0.0098427101004828\n",
      "Gradient Descent(47/399): loss=0.44883719977786185, w0=0.00015718016584820893, w1=-0.010010874728643562\n",
      "Gradient Descent(48/399): loss=0.4481059345260755, w0=0.0001620965282258311, w1=-0.010177660583067924\n",
      "Gradient Descent(49/399): loss=0.4473843954882303, w0=0.00016701433025993238, w1=-0.010343084721835512\n",
      "Gradient Descent(50/399): loss=0.44667236674116007, w0=0.0001719311274682192, w1=-0.010507163952707264\n",
      "Gradient Descent(51/399): loss=0.4459696380511957, w0=0.00017684456056477402, w1=-0.010669914837315437\n",
      "Gradient Descent(52/399): loss=0.4452760047091046, w0=0.00018175235310732444, w1=-0.010831353695273622\n",
      "Gradient Descent(53/399): loss=0.4445912673702766, w0=0.00018665230920693882, w1=-0.010991496608208468\n",
      "Gradient Descent(54/399): loss=0.44391523189996873, w0=0.00019154231129847965, w1=-0.011150359423714831\n",
      "Gradient Descent(55/399): loss=0.44324770922343204, w0=0.00019642031797019062, w1=-0.011307957759235985\n",
      "Gradient Descent(56/399): loss=0.4425885151807495, w0=0.00020128436185083844, w1=-0.011464307005870507\n",
      "Gradient Descent(57/399): loss=0.44193747038621883, w0=0.00020613254755287236, w1=-0.011619422332107403\n",
      "Gradient Descent(58/399): loss=0.4412944000921255, w0=0.00021096304967010643, w1=-0.011773318687491026\n",
      "Gradient Descent(59/399): loss=0.4406591340567516, w0=0.00021577411082847022, w1=-0.011926010806217259\n",
      "Gradient Descent(60/399): loss=0.44003150641647404, w0=0.00022056403978841304, w1=-0.012077513210662458\n",
      "Gradient Descent(61/399): loss=0.4394113555618155, w0=0.00022533120959758473, w1=-0.012227840214846547\n",
      "Gradient Descent(62/399): loss=0.4387985240173086, w0=0.0002300740557924539, w1=-0.012377005927831693\n",
      "Gradient Descent(63/399): loss=0.43819285832504984, w0=0.0002347910746475604, w1=-0.012525024257057897\n",
      "Gradient Descent(64/399): loss=0.4375942089318096, w0=0.0002394808214711345, w1=-0.01267190891161684\n",
      "Gradient Descent(65/399): loss=0.4370024300795895, w0=0.0002441419089458485, w1=-0.0128176734054653\n",
      "Gradient Descent(66/399): loss=0.4364173796995001, w0=0.00024877300551350175, w1=-0.012962331060579367\n",
      "Gradient Descent(67/399): loss=0.4358389193088585, w0=0.0002533728338024702, w1=-0.013105895010050737\n",
      "Gradient Descent(68/399): loss=0.43526691391139016, w0=0.00025794016909678514, w1=-0.013248378201126272\n",
      "Gradient Descent(69/399): loss=0.4347012319004371, w0=0.00026247383784573544, w1=-0.013389793398192008\n",
      "Gradient Descent(70/399): loss=0.4341417449650731, w0=0.0002669727162129175, w1=-0.01353015318570277\n",
      "Gradient Descent(71/399): loss=0.4335883279990267, w0=0.0002714357286636866, w1=-0.013669469971058527\n",
      "Gradient Descent(72/399): loss=0.4330408590123255, w0=0.000275861846589991, w1=-0.013807755987428567\n",
      "Gradient Descent(73/399): loss=0.43249921904556754, w0=0.0002802500869715982, w1=-0.013945023296524596\n",
      "Gradient Descent(74/399): loss=0.43196329208673995, w0=0.0002845995110727494, w1=-0.014081283791323784\n",
      "Gradient Descent(75/399): loss=0.43143296499049527, w0=0.0002889092231733028, w1=-0.014216549198742812\n",
      "Gradient Descent(76/399): loss=0.4309081273998162, w0=0.0002931783693334548, w1=-0.014350831082263893\n",
      "Gradient Descent(77/399): loss=0.43038867166998135, w0=0.0002974061361911487, w1=-0.014484140844513781\n",
      "Gradient Descent(78/399): loss=0.4298744927947665, w0=0.0003015917497913081, w1=-0.014616489729796688\n",
      "Gradient Descent(79/399): loss=0.42936548833480603, w0=0.0003057344744460527, w1=-0.014747888826582085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(80/399): loss=0.4288615583480468, w0=0.0003098336116250784, w1=-0.014878349069948262\n",
      "Gradient Descent(81/399): loss=0.4283626053222288, w0=0.0003138884988754055, w1=-0.015007881243982571\n",
      "Gradient Descent(82/399): loss=0.4278685341093285, w0=0.0003178985087697189, w1=-0.015136495984139216\n",
      "Gradient Descent(83/399): loss=0.4273792518619014, w0=0.00032186304788254686, w1=-0.015264203779555431\n",
      "Gradient Descent(84/399): loss=0.4268946679712682, w0=0.0003257815557935437, w1=-0.0153910149753269\n",
      "Gradient Descent(85/399): loss=0.42641469400748266, w0=0.0003296535041171622, w1=-0.015516939774743225\n",
      "Gradient Descent(86/399): loss=0.425939243661029, w0=0.00033347839555802027, w1=-0.01564198824148423\n",
      "Gradient Descent(87/399): loss=0.42546823268619255, w0=0.00033725576299128567, w1=-0.01576617030177791\n",
      "Gradient Descent(88/399): loss=0.42500157884605505, w0=0.0003409851685674196, w1=-0.015889495746520748\n",
      "Gradient Descent(89/399): loss=0.42453920185906047, w0=0.00034466620284063907, w1=-0.016011974233361172\n",
      "Gradient Descent(90/399): loss=0.4240810233471081, w0=0.00034829848392047366, w1=-0.01613361528874687\n",
      "Gradient Descent(91/399): loss=0.4236269667851207, w0=0.00035188165664581006, w1=-0.01625442830993669\n",
      "Gradient Descent(92/399): loss=0.4231769574520465, w0=0.0003554153917808335, w1=-0.016374422566977773\n",
      "Gradient Descent(93/399): loss=0.4227309223832513, w0=0.00035889938523229065, w1=-0.016493607204648694\n",
      "Gradient Descent(94/399): loss=0.4222887903242544, w0=0.0003623333572875152, w1=-0.016611991244369156\n",
      "Gradient Descent(95/399): loss=0.4218504916857746, w0=0.0003657170518726703, w1=-0.016729583586076992\n",
      "Gradient Descent(96/399): loss=0.4214159585000387, w0=0.0003690502358306787, w1=-0.016846393010073048\n",
      "Gradient Descent(97/399): loss=0.4209851243783212, w0=0.0003723326982183238, w1=-0.016962428178834605\n",
      "Gradient Descent(98/399): loss=0.4205579244696751, w0=0.0003755642496220195, w1=-0.017077697638797934\n",
      "Gradient Descent(99/399): loss=0.42013429542081815, w0=0.0003787447214917607, w1=-0.01719220982211057\n",
      "Gradient Descent(100/399): loss=0.41971417533714017, w0=0.0003818739654927768, w1=-0.017305973048353917\n",
      "Gradient Descent(101/399): loss=0.4192975037447991, w0=0.00038495185287442703, w1=-0.017418995526236717\n",
      "Gradient Descent(102/399): loss=0.4188842215538721, w0=0.0003879782738558853, w1=-0.01753128535525999\n",
      "Gradient Descent(103/399): loss=0.4184742710225318, w0=0.00039095313702817617, w1=-0.017642850527353916\n",
      "Gradient Descent(104/399): loss=0.41806759572221547, w0=0.00039387636877213445, w1=-0.01775369892848729\n",
      "Gradient Descent(105/399): loss=0.4176641405037598, w0=0.00039674791269187306, w1=-0.01786383834024998\n",
      "Gradient Descent(106/399): loss=0.4172638514644713, w0=0.00039956772906335345, w1=-0.017973276441408965\n",
      "Gradient Descent(107/399): loss=0.4168666759161067, w0=0.0004023357942976653, w1=-0.018082020809438406\n",
      "Gradient Descent(108/399): loss=0.41647256235373387, w0=0.000405052100418631, w1=-0.018190078922024285\n",
      "Gradient Descent(109/399): loss=0.416081460425451, w0=0.0004077166545543623, w1=-0.01829745815854405\n",
      "Gradient Descent(110/399): loss=0.4156933209029362, w0=0.0004103294784424045, w1=-0.018404165801521752\n",
      "Gradient Descent(111/399): loss=0.415308095652804, w0=0.00041289060794811515, w1=-0.01851020903805916\n",
      "Gradient Descent(112/399): loss=0.4149257376087472, w0=0.00041540009259593213, w1=-0.018615594961243234\n",
      "Gradient Descent(113/399): loss=0.4145462007444378, w0=0.00041785799511319534, w1=-0.01872033057153047\n",
      "Gradient Descent(114/399): loss=0.41416944004717, w0=0.00042026439098619606, w1=-0.0188244227781085\n",
      "Gradient Descent(115/399): loss=0.4137954114922165, w0=0.00042261936802813526, w1=-0.018927878400235403\n",
      "Gradient Descent(116/399): loss=0.41342407201788745, w0=0.00042492302595868183, w1=-0.019030704168557084\n",
      "Gradient Descent(117/399): loss=0.41305537950126203, w0=0.00042717547599482875, w1=-0.019132906726403203\n",
      "Gradient Descent(118/399): loss=0.4126892927345796, w0=0.00042937684045275397, w1=-0.019234492631061972\n",
      "Gradient Descent(119/399): loss=0.4123257714022699, w0=0.0004315272523604002, w1=-0.019335468355034274\n",
      "Gradient Descent(120/399): loss=0.41196477605860277, w0=0.0004336268550804954, w1=-0.019435840287267428\n",
      "Gradient Descent(121/399): loss=0.4116062681059425, w0=0.0004356758019437426, w1=-0.019535614734368996\n",
      "Gradient Descent(122/399): loss=0.4112502097735854, w0=0.0004376742558919162, w1=-0.019634797921801013\n",
      "Gradient Descent(123/399): loss=0.41089656409716935, w0=0.0004396223891306068, w1=-0.01973339599505496\n",
      "Gradient Descent(124/399): loss=0.4105452948986343, w0=0.000441520382791365, w1=-0.019831415020807854\n",
      "Gradient Descent(125/399): loss=0.41019636676672033, w0=0.000443368426603001, w1=-0.01992886098805979\n",
      "Gradient Descent(126/399): loss=0.4098497450379889, w0=0.00044516671857180166, w1=-0.02002573980925326\n",
      "Gradient Descent(127/399): loss=0.4095053957783498, w0=0.0004469154646704354, w1=-0.020122057321374592\n",
      "Gradient Descent(128/399): loss=0.40916328576508343, w0=0.00044861487853531927, w1=-0.02021781928703783\n",
      "Gradient Descent(129/399): loss=0.4088233824693399, w0=0.0004502651811722292, w1=-0.02031303139555134\n",
      "Gradient Descent(130/399): loss=0.4084856540391051, w0=0.0004518666006699408, w1=-0.020407699263967502\n",
      "Gradient Descent(131/399): loss=0.40815006928262004, w0=0.0004534193719216919, w1=-0.020501828438115734\n",
      "Gradient Descent(132/399): loss=0.40781659765223943, w0=0.00045492373635426595, w1=-0.02059542439361919\n",
      "Gradient Descent(133/399): loss=0.4074852092287173, w0=0.00045637994166449825, w1=-0.0206884925368954\n",
      "Gradient Descent(134/399): loss=0.40715587470591036, w0=0.0004577882415630134, w1=-0.020781038206141116\n",
      "Gradient Descent(135/399): loss=0.406828565375884, w0=0.00045914889552500765, w1=-0.020873066672301703\n",
      "Gradient Descent(136/399): loss=0.40650325311441127, w0=0.00046046216854789366, w1=-0.020964583140025275\n",
      "Gradient Descent(137/399): loss=0.4061799103668555, w0=0.00046172833091563046, w1=-0.021055592748601897\n",
      "Gradient Descent(138/399): loss=0.4058585101344223, w0=0.0004629476579695667, w1=-0.021146100572888096\n",
      "Gradient Descent(139/399): loss=0.4055390259607742, w0=0.000464120429885628, w1=-0.021236111624216913\n",
      "Gradient Descent(140/399): loss=0.4052214319189955, w0=0.0004652469314576858, w1=-0.02132563085129382\n",
      "Gradient Descent(141/399): loss=0.4049057025988999, w0=0.00046632745188694724, w1=-0.021414663141078642\n",
      "Gradient Descent(142/399): loss=0.4045918130946686, w0=0.00046736228457721187, w1=-0.02150321331965385\n",
      "Gradient Descent(143/399): loss=0.4042797389928122, w0=0.00046835172693584246, w1=-0.02159128615307933\n",
      "Gradient Descent(144/399): loss=0.4039694563604469, w0=0.000469296080180304, w1=-0.02167888634823399\n",
      "Gradient Descent(145/399): loss=0.4036609417338754, w0=0.00047019564915012625, w1=-0.021766018553644323\n",
      "Gradient Descent(146/399): loss=0.40335417210746527, w0=0.00047105074212415096, w1=-0.021852687360300223\n",
      "Gradient Descent(147/399): loss=0.4030491249228155, w0=0.00047186167064292635, w1=-0.021938897302458245\n",
      "Gradient Descent(148/399): loss=0.4027457780582057, w0=0.00047262874933611757, w1=-0.022024652858432515\n",
      "Gradient Descent(149/399): loss=0.40244410981831646, w0=0.0004733522957548028, w1=-0.022109958451373524\n",
      "Gradient Descent(150/399): loss=0.4021440989242171, w0=0.00047403263020852956, w1=-0.022194818450035\n",
      "Gradient Descent(151/399): loss=0.40184572450361156, w0=0.0004746700756070088, w1=-0.02227923716952905\n",
      "Gradient Descent(152/399): loss=0.4015489660813352, w0=0.0004752649573063264, w1=-0.022363218872069806\n",
      "Gradient Descent(153/399): loss=0.40125380357009777, w0=0.00047581760295955714, w1=-0.022446767767705714\n",
      "Gradient Descent(154/399): loss=0.400960217261463, w0=0.00047632834237166594, w1=-0.02252988801504072\n",
      "Gradient Descent(155/399): loss=0.4006681878170603, w0=0.0004767975073585876, w1=-0.022612583721944492\n",
      "Gradient Descent(156/399): loss=0.4003776962600222, w0=0.00047722543161037636, w1=-0.022694858946251895\n",
      "Gradient Descent(157/399): loss=0.40008872396664, w0=0.0004776124505583206, w1=-0.022776717696451867\n",
      "Gradient Descent(158/399): loss=0.39980125265823496, w0=0.0004779589012459208, w1=-0.02285816393236591\n",
      "Gradient Descent(159/399): loss=0.39951526439323315, w0=0.00047826512220363086, w1=-0.02293920156581634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(160/399): loss=0.39923074155944704, w0=0.0004785314533272664, w1=-0.023019834461284482\n",
      "Gradient Descent(161/399): loss=0.39894766686654914, w0=0.0004787582357599845, w1=-0.023100066436558972\n",
      "Gradient Descent(162/399): loss=0.39866602333874, w0=0.00047894581177774397, w1=-0.023179901263374336\n",
      "Gradient Descent(163/399): loss=0.3983857943076006, w0=0.00047909452467815574, w1=-0.02325934266803999\n",
      "Gradient Descent(164/399): loss=0.3981069634051272, w0=0.000479204718672636, w1=-0.023338394332059854\n",
      "Gradient Descent(165/399): loss=0.3978295145569398, w0=0.00047927673878177723, w1=-0.023417059892742686\n",
      "Gradient Descent(166/399): loss=0.39755343197566684, w0=0.00047931093073385396, w1=-0.02349534294380334\n",
      "Gradient Descent(167/399): loss=0.39727870015449074, w0=0.00047930764086638226, w1=-0.023573247035955064\n",
      "Gradient Descent(168/399): loss=0.39700530386086097, w0=0.0004792672160306543, w1=-0.02365077567749298\n",
      "Gradient Descent(169/399): loss=0.39673322813036316, w0=0.0004791900034991714, w1=-0.02372793233486893\n",
      "Gradient Descent(170/399): loss=0.3964624582607418, w0=0.0004790763508759, w1=-0.02380472043325779\n",
      "Gradient Descent(171/399): loss=0.39619297980607304, w0=0.00047892660600927885, w1=-0.023881143357115383\n",
      "Gradient Descent(172/399): loss=0.39592477857108466, w0=0.00047874111690790494, w1=-0.023957204450728204\n",
      "Gradient Descent(173/399): loss=0.3956578406056157, w0=0.0004785202316588301, w1=-0.024032907018754976\n",
      "Gradient Descent(174/399): loss=0.39539215219921553, w0=0.0004782642983484001, w1=-0.024108254326760264\n",
      "Gradient Descent(175/399): loss=0.39512769987587826, w0=0.00047797366498557065, w1=-0.02418324960174022\n",
      "Gradient Descent(176/399): loss=0.3948644703889061, w0=0.00047764867942763644, w1=-0.024257896032640627\n",
      "Gradient Descent(177/399): loss=0.39460245071590166, w0=0.00047728968930831035, w1=-0.024332196770867322\n",
      "Gradient Descent(178/399): loss=0.394341628053884, w0=0.0004768970419680922, w1=-0.024406154930789157\n",
      "Gradient Descent(179/399): loss=0.39408198981452564, w0=0.0004764710843868676, w1=-0.024479773590233605\n",
      "Gradient Descent(180/399): loss=0.3938235236195068, w0=0.00047601216311867926, w1=-0.024553055790975113\n",
      "Gradient Descent(181/399): loss=0.3935662172959843, w0=0.00047552062422861395, w1=-0.02462600453921635\n",
      "Gradient Descent(182/399): loss=0.3933100588721731, w0=0.00047499681323175086, w1=-0.024698622806062435\n",
      "Gradient Descent(183/399): loss=0.3930550365730343, w0=0.00047444107503411703, w1=-0.024770913527988284\n",
      "Gradient Descent(184/399): loss=0.39280113881607087, w0=0.00047385375387559833, w1=-0.024842879607299136\n",
      "Gradient Descent(185/399): loss=0.39254835420722545, w0=0.00047323519327475467, w1=-0.024914523912584436\n",
      "Gradient Descent(186/399): loss=0.3922966715368781, w0=0.00047258573597549005, w1=-0.024985849279165123\n",
      "Gradient Descent(187/399): loss=0.3920460797759431, w0=0.00047190572389552913, w1=-0.025056858509534447\n",
      "Gradient Descent(188/399): loss=0.39179656807205976, w0=0.0004711954980766532, w1=-0.02512755437379244\n",
      "Gradient Descent(189/399): loss=0.39154812574587494, w0=0.00047045539863664945, w1=-0.025197939610074094\n",
      "Gradient Descent(190/399): loss=0.39130074228741807, w0=0.000469685764722929, w1=-0.025268016924971368\n",
      "Gradient Descent(191/399): loss=0.39105440735256214, w0=0.0004688869344677699, w1=-0.02533778899394915\n",
      "Gradient Descent(192/399): loss=0.39080911075957214, w0=0.0004680592449451429, w1=-0.025407258461755216\n",
      "Gradient Descent(193/399): loss=0.39056484248573364, w0=0.00046720303212907785, w1=-0.025476427942824325\n",
      "Gradient Descent(194/399): loss=0.39032159266406635, w0=0.000466318630853531, w1=-0.025545300021676497\n",
      "Gradient Descent(195/399): loss=0.3900793515801138, w0=0.0004654063747737135, w1=-0.025613877253309617\n",
      "Gradient Descent(196/399): loss=0.3898381096688118, w0=0.0004644665963288424, w1=-0.025682162163586403\n",
      "Gradient Descent(197/399): loss=0.38959785751143167, w0=0.0004634996267062775, w1=-0.02575015724961587\n",
      "Gradient Descent(198/399): loss=0.3893585858325948, w0=0.00046250579580700655, w1=-0.02581786498012933\n",
      "Gradient Descent(199/399): loss=0.38912028549736144, w0=0.000461485432212444, w1=-0.025885287795851065\n",
      "Gradient Descent(200/399): loss=0.3888829475083866, w0=0.00046043886315250815, w1=-0.02595242810986369\n",
      "Gradient Descent(201/399): loss=0.3886465630031444, w0=0.00045936641447494325, w1=-0.02601928830796837\n",
      "Gradient Descent(202/399): loss=0.38841112325121674, w0=0.00045826841061585296, w1=-0.02608587074903988\n",
      "Gradient Descent(203/399): loss=0.38817661965164973, w0=0.00045714517457141375, w1=-0.02615217776537666\n",
      "Gradient Descent(204/399): loss=0.38794304373036836, w0=0.0004559970278707364, w1=-0.02621821166304592\n",
      "Gradient Descent(205/399): loss=0.38771038713765504, w0=0.0004548242905498452, w1=-0.026283974722223824\n",
      "Gradient Descent(206/399): loss=0.38747864164568624, w0=0.0004536272811267449, w1=-0.02634946919753093\n",
      "Gradient Descent(207/399): loss=0.3872477991461267, w0=0.0004524063165775468, w1=-0.02641469731836284\n",
      "Gradient Descent(208/399): loss=0.38701785164778113, w0=0.00045116171231362524, w1=-0.026479661289216232\n",
      "Gradient Descent(209/399): loss=0.3867887912742998, w0=0.00044989378215977655, w1=-0.026544363290010285\n",
      "Gradient Descent(210/399): loss=0.3865606102619385, w0=0.0004486028383333546, w1=-0.02660880547640358\n",
      "Gradient Descent(211/399): loss=0.3863333009573685, w0=0.0004472891914243558, w1=-0.026672989980106573\n",
      "Gradient Descent(212/399): loss=0.3861068558155414, w0=0.0004459531503764282, w1=-0.026736918909189666\n",
      "Gradient Descent(213/399): loss=0.3858812673975989, w0=0.00044459502246877944, w1=-0.026800594348386962\n",
      "Gradient Descent(214/399): loss=0.38565652836883413, w0=0.00044321511329896017, w1=-0.026864018359395787\n",
      "Gradient Descent(215/399): loss=0.3854326314966987, w0=0.0004418137267664981, w1=-0.026927192981172007\n",
      "Gradient Descent(216/399): loss=0.38520956964885555, w0=0.00044039116505736, w1=-0.026990120230221227\n",
      "Gradient Descent(217/399): loss=0.3849873357912771, w0=0.0004389477286292193, w1=-0.027052802100885932\n",
      "Gradient Descent(218/399): loss=0.3847659229863859, w0=0.0004374837161975069, w1=-0.027115240565628626\n",
      "Gradient Descent(219/399): loss=0.38454532439124023, w0=0.000435999424722224, w1=-0.02717743757531103\n",
      "Gradient Descent(220/399): loss=0.3843255332557569, w0=0.00043449514939549594, w1=-0.027239395059469398\n",
      "Gradient Descent(221/399): loss=0.38410654292097846, w0=0.00043297118362984683, w1=-0.027301114926586007\n",
      "Gradient Descent(222/399): loss=0.38388834681737816, w0=0.00043142781904717455, w1=-0.027362599064356893\n",
      "Gradient Descent(223/399): loss=0.3836709384632025, w0=0.00042986534546840744, w1=-0.02742384933995586\n",
      "Gradient Descent(224/399): loss=0.3834543114628528, w0=0.0004282840509038234, w1=-0.02748486760029484\n",
      "Gradient Descent(225/399): loss=0.38323845950530155, w0=0.0004266842215440128, w1=-0.027545655672280674\n",
      "Gradient Descent(226/399): loss=0.3830233763625458, w0=0.0004250661417514677, w1=-0.02760621536306831\n",
      "Gradient Descent(227/399): loss=0.3828090558880945, w0=0.00042343009405277927, w1=-0.027666548460310535\n",
      "Gradient Descent(228/399): loss=0.38259549201548954, w0=0.00042177635913142686, w1=-0.02772665673240425\n",
      "Gradient Descent(229/399): loss=0.38238267875685994, w0=0.0004201052158211418, w1=-0.02778654192873336\n",
      "Gradient Descent(230/399): loss=0.38217061020150833, w0=0.0004184169410998295, w1=-0.027846205779908324\n",
      "Gradient Descent(231/399): loss=0.381959280514529, w0=0.0004167118100840342, w1=-0.027905649998002414\n",
      "Gradient Descent(232/399): loss=0.38174868393545536, w0=0.0004149900960239312, w1=-0.02796487627678472\n",
      "Gradient Descent(233/399): loss=0.3815388147769385, w0=0.00041325207029883096, w1=-0.028023886291949972\n",
      "Gradient Descent(234/399): loss=0.3813296674234543, w0=0.0004114980024131804, w1=-0.028082681701345213\n",
      "Gradient Descent(235/399): loss=0.38112123633004, w0=0.00040972815999304777, w1=-0.02814126414519336\n",
      "Gradient Descent(236/399): loss=0.38091351602105505, w0=0.0004079428087830762, w1=-0.028199635246313713\n",
      "Gradient Descent(237/399): loss=0.38070650108897364, w0=0.0004061422126438932, w1=-0.028257796610339465\n",
      "Gradient Descent(238/399): loss=0.38050018619320025, w0=0.0004043266335499615, w1=-0.028315749825932216\n",
      "Gradient Descent(239/399): loss=0.38029456605891165, w0=0.0004024963315878604, w1=-0.028373496464993594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(240/399): loss=0.38008963547592445, w0=0.0004006515649549823, w1=-0.028431038082873974\n",
      "Gradient Descent(241/399): loss=0.37988538929758736, w0=0.0003987925899586348, w1=-0.02848837621857836\n",
      "Gradient Descent(242/399): loss=0.37968182243969567, w0=0.00039691966101553416, w1=-0.028545512394969476\n",
      "Gradient Descent(243/399): loss=0.37947892987943066, w0=0.00039503303065167985, w1=-0.028602448118968095\n",
      "Gradient Descent(244/399): loss=0.37927670665432167, w0=0.00039313294950259783, w1=-0.028659184881750645\n",
      "Gradient Descent(245/399): loss=0.3790751478612283, w0=0.00039121966631394233, w1=-0.02871572415894415\n",
      "Gradient Descent(246/399): loss=0.37887424865534736, w0=0.0003892934279424443, w1=-0.028772067410818523\n",
      "Gradient Descent(247/399): loss=0.3786740042492381, w0=0.0003873544793571968, w1=-0.028828216082476252\n",
      "Gradient Descent(248/399): loss=0.3784744099118694, w0=0.0003854030636412662, w1=-0.028884171604039537\n",
      "Gradient Descent(249/399): loss=0.3782754609676875, w0=0.0003834394219936199, w1=-0.028939935390834892\n",
      "Gradient Descent(250/399): loss=0.3780771527957011, w0=0.0003814637937313605, w1=-0.028995508843575254\n",
      "Gradient Descent(251/399): loss=0.37787948082858924, w0=0.0003794764162922562, w1=-0.02905089334853965\n",
      "Gradient Descent(252/399): loss=0.37768244055182376, w0=0.00037747752523755937, w1=-0.029106090277750428\n",
      "Gradient Descent(253/399): loss=0.37748602750281346, w0=0.0003754673542551028, w1=-0.029161100989148123\n",
      "Gradient Descent(254/399): loss=0.37729023727006306, w0=0.00037344613516266593, w1=-0.02921592682676396\n",
      "Gradient Descent(255/399): loss=0.3770950654923525, w0=0.0003714140979116017, w1=-0.029270569120890037\n",
      "Gradient Descent(256/399): loss=0.3769005078579311, w0=0.000369371470590716, w1=-0.02932502918824725\n",
      "Gradient Descent(257/399): loss=0.3767065601037284, w0=0.00036731847943039084, w1=-0.029379308332150935\n",
      "Gradient Descent(258/399): loss=0.3765132180145824, w0=0.00036525534880694447, w1=-0.029433407842674317\n",
      "Gradient Descent(259/399): loss=0.37632047742248215, w0=0.0003631823012472191, w1=-0.029487328996809763\n",
      "Gradient Descent(260/399): loss=0.37612833420582725, w0=0.0003610995574333899, w1=-0.029541073058627878\n",
      "Gradient Descent(261/399): loss=0.3759367842887003, w0=0.0003590073362079871, w1=-0.029594641279434484\n",
      "Gradient Descent(262/399): loss=0.3757458236401564, w0=0.00035690585457912423, w1=-0.029648034897925495\n",
      "Gradient Descent(263/399): loss=0.37555544827352544, w0=0.00035479532772592537, w1=-0.029701255140339736\n",
      "Gradient Descent(264/399): loss=0.37536565424572854, w0=0.0003526759690041446, w1=-0.029754303220609728\n",
      "Gradient Descent(265/399): loss=0.3751764376566082, w0=0.000350547989951971, w1=-0.02980718034051046\n",
      "Gradient Descent(266/399): loss=0.3749877946482735, w0=0.0003484116002960123, w1=-0.029859887689806183\n",
      "Gradient Descent(267/399): loss=0.37479972140445483, w0=0.0003462670079574516, w1=-0.02991242644639527\n",
      "Gradient Descent(268/399): loss=0.3746122141498749, w0=0.0003441144190583696, w1=-0.029964797776453136\n",
      "Gradient Descent(269/399): loss=0.37442526914963015, w0=0.0003419540379282282, w1=-0.030017002834573266\n",
      "Gradient Descent(270/399): loss=0.37423888270858474, w0=0.00033978606711050754, w1=-0.030069042763906383\n",
      "Gradient Descent(271/399): loss=0.37405305117077625, w0=0.0003376107073694923, w1=-0.030120918696297773\n",
      "Gradient Descent(272/399): loss=0.37386777091883433, w0=0.00033542815769720087, w1=-0.030172631752422793\n",
      "Gradient Descent(273/399): loss=0.3736830383734089, w0=0.000333238615320452, w1=-0.03022418304192058\n",
      "Gradient Descent(274/399): loss=0.3734988499926094, w0=0.000331042275708064, w1=-0.03027557366352601\n",
      "Gradient Descent(275/399): loss=0.3733152022714571, w0=0.00032883933257818075, w1=-0.030326804705199904\n",
      "Gradient Descent(276/399): loss=0.373132091741345, w0=0.00032662997790572, w1=-0.030377877244257535\n",
      "Gradient Descent(277/399): loss=0.3729495149695113, w0=0.00032441440192993863, w1=-0.030428792347495424\n",
      "Gradient Descent(278/399): loss=0.3727674685585186, w0=0.0003221927931621104, w1=-0.030479551071316485\n",
      "Gradient Descent(279/399): loss=0.3725859491457483, w0=0.0003199653383933111, w1=-0.030530154461853507\n",
      "Gradient Descent(280/399): loss=0.37240495340289975, w0=0.0003177322227023073, w1=-0.030580603555091037\n",
      "Gradient Descent(281/399): loss=0.37222447803550246, w0=0.0003154936294635431, w1=-0.030630899376985647\n",
      "Gradient Descent(282/399): loss=0.37204451978243525, w0=0.000313249740355222, w1=-0.030681042943584636\n",
      "Gradient Descent(283/399): loss=0.37186507541545455, w0=0.0003110007353674781, w1=-0.030731035261143163\n",
      "Gradient Descent(284/399): loss=0.37168614173873393, w0=0.00030874679281063424, w1=-0.03078087732623987\n",
      "Gradient Descent(285/399): loss=0.3715077155884088, w0=0.00030648808932354145, w1=-0.03083057012589097\n",
      "Gradient Descent(286/399): loss=0.37132979383213116, w0=0.00030422479988199703, w1=-0.030880114637662857\n",
      "Gradient Descent(287/399): loss=0.3711523733686343, w0=0.0003019570978072368, w1=-0.03092951182978325\n",
      "Gradient Descent(288/399): loss=0.3709754511273014, w0=0.00029968515477449823, w1=-0.030978762661250877\n",
      "Gradient Descent(289/399): loss=0.3707990240677461, w0=0.00029740914082165045, w1=-0.031027868081943747\n",
      "Gradient Descent(290/399): loss=0.37062308917939835, w0=0.00029512922435788816, w1=-0.031076829032725985\n",
      "Gradient Descent(291/399): loss=0.37044764348109793, w0=0.00029284557217248557, w1=-0.031125646445553302\n",
      "Gradient Descent(292/399): loss=0.37027268402069735, w0=0.00029055834944360725, w1=-0.031174321243577094\n",
      "Gradient Descent(293/399): loss=0.37009820787466896, w0=0.00028826771974717265, w1=-0.031222854341247174\n",
      "Gradient Descent(294/399): loss=0.36992421214772164, w0=0.0002859738450657714, w1=-0.03127124664441318\n",
      "Gradient Descent(295/399): loss=0.36975069397242194, w0=0.00028367688579762564, w1=-0.031319499050424665\n",
      "Gradient Descent(296/399): loss=0.3695776505088244, w0=0.00028137700076559725, w1=-0.031367612448229915\n",
      "Gradient Descent(297/399): loss=0.36940507894410796, w0=0.00027907434722623646, w1=-0.03141558771847344\n",
      "Gradient Descent(298/399): loss=0.369232976492216, w0=0.0002767690808788693, w1=-0.03146342573359225\n",
      "Gradient Descent(299/399): loss=0.36906134039350663, w0=0.00027446135587472133, w1=-0.031511127357910874\n",
      "Gradient Descent(300/399): loss=0.3688901679144071, w0=0.0002721513248260742, w1=-0.031558693447735126\n",
      "Gradient Descent(301/399): loss=0.36871945634707426, w0=0.00026983913881545356, w1=-0.031606124851444706\n",
      "Gradient Descent(302/399): loss=0.3685492030090611, w0=0.00026752494740484505, w1=-0.03165342240958459\n",
      "Gradient Descent(303/399): loss=0.36837940524298907, w0=0.0002652088986449359, w1=-0.03170058695495523\n",
      "Gradient Descent(304/399): loss=0.3682100604162267, w0=0.00026289113908438, w1=-0.0317476193127016\n",
      "Gradient Descent(305/399): loss=0.36804116592057234, w0=0.0002605718137790844, w1=-0.03179452030040111\n",
      "Gradient Descent(306/399): loss=0.36787271917194386, w0=0.00025825106630151433, w1=-0.03184129072815041\n",
      "Gradient Descent(307/399): loss=0.36770471761007295, w0=0.0002559290387500147, w1=-0.03188793139865099\n",
      "Gradient Descent(308/399): loss=0.36753715869820314, w0=0.0002536058717581466, w1=-0.03193444310729381\n",
      "Gradient Descent(309/399): loss=0.36737003992279677, w0=0.0002512817045040356, w1=-0.03198082664224277\n",
      "Gradient Descent(310/399): loss=0.3672033587932422, w0=0.0002489566747197311, w1=-0.03202708278451709\n",
      "Gradient Descent(311/399): loss=0.36703711284156937, w0=0.0002466309187005737, w1=-0.03207321230807272\n",
      "Gradient Descent(312/399): loss=0.36687129962216897, w0=0.0002443045713145691, w1=-0.032119215979882626\n",
      "Gradient Descent(313/399): loss=0.36670591671151576, w0=0.00024197776601176692, w1=-0.03216509456001612\n",
      "Gradient Descent(314/399): loss=0.36654096170789685, w0=0.00023965063483364228, w1=-0.03221084880171713\n",
      "Gradient Descent(315/399): loss=0.3663764322311456, w0=0.00023732330842247845, w1=-0.03225647945148148\n",
      "Gradient Descent(316/399): loss=0.36621232592237657, w0=0.00023499591603074916, w1=-0.032301987249133225\n",
      "Gradient Descent(317/399): loss=0.36604864044373037, w0=0.0002326685855304984, w1=-0.03234737292789996\n",
      "Gradient Descent(318/399): loss=0.36588537347811567, w0=0.00023034144342271674, w1=-0.032392637214487194\n",
      "Gradient Descent(319/399): loss=0.36572252272896216, w0=0.00022801461484671197, w1=-0.03243778082915183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(320/399): loss=0.36556008591997236, w0=0.00022568822358947323, w1=-0.03248280448577462\n",
      "Gradient Descent(321/399): loss=0.36539806079488046, w0=0.0002233623920950264, w1=-0.03252770889193176\n",
      "Gradient Descent(322/399): loss=0.3652364451172131, w0=0.00022103724147378014, w1=-0.032572494748965584\n",
      "Gradient Descent(323/399): loss=0.3650752366700557, w0=0.0002187128915118606, w1=-0.03261716275205433\n",
      "Gradient Descent(324/399): loss=0.36491443325582174, w0=0.00021638946068043357, w1=-0.03266171359028104\n",
      "Gradient Descent(325/399): loss=0.3647540326960243, w0=0.00021406706614501318, w1=-0.03270614794670163\n",
      "Gradient Descent(326/399): loss=0.3645940328310536, w0=0.00021174582377475533, w1=-0.032750466498412005\n",
      "Gradient Descent(327/399): loss=0.36443443151995736, w0=0.000209425848151735, w1=-0.032794669916614415\n",
      "Gradient Descent(328/399): loss=0.3642752266402219, w0=0.00020710725258020612, w1=-0.03283875886668295\n",
      "Gradient Descent(329/399): loss=0.36411641608756096, w0=0.00020479014909584295, w1=-0.0328827340082282\n",
      "Gradient Descent(330/399): loss=0.3639579977757051, w0=0.0002024746484749617, w1=-0.0329265959951611\n",
      "Gradient Descent(331/399): loss=0.3637999696361942, w0=0.0002001608602437216, w1=-0.03297034547575601\n",
      "Gradient Descent(332/399): loss=0.36364232961817466, w0=0.00019784889268730397, w1=-0.033013983092712924\n",
      "Gradient Descent(333/399): loss=0.363485075688198, w0=0.00019553885285906872, w1=-0.033057509483219\n",
      "Gradient Descent(334/399): loss=0.3633282058300247, w0=0.00019323084658968713, w1=-0.03310092527900924\n",
      "Gradient Descent(335/399): loss=0.36317171804442805, w0=0.00019092497849624963, w1=-0.03314423110642646\n",
      "Gradient Descent(336/399): loss=0.36301561034900354, w0=0.00018862135199134849, w1=-0.03318742758648047\n",
      "Gradient Descent(337/399): loss=0.36285988077798004, w0=0.0001863200692921336, w1=-0.03323051533490653\n",
      "Gradient Descent(338/399): loss=0.36270452738203374, w0=0.0001840212314293412, w1=-0.033273494962223094\n",
      "Gradient Descent(339/399): loss=0.36254954822810465, w0=0.00018172493825629438, w1=-0.03331636707378883\n",
      "Gradient Descent(340/399): loss=0.3623949413992168, w0=0.00017943128845787466, w1=-0.03335913226985889\n",
      "Gradient Descent(341/399): loss=0.3622407049943004, w0=0.00017714037955946383, w1=-0.03340179114564053\n",
      "Gradient Descent(342/399): loss=0.3620868371280157, w0=0.00017485230793585552, w1=-0.033444344291348\n",
      "Gradient Descent(343/399): loss=0.3619333359305818, w0=0.00017256716882013546, w1=-0.033486792292256796\n",
      "Gradient Descent(344/399): loss=0.36178019954760543, w0=0.00017028505631252992, w1=-0.033529135728757174\n",
      "Gradient Descent(345/399): loss=0.3616274261399135, w0=0.00016800606338922177, w1=-0.03357137517640707\n",
      "Gradient Descent(346/399): loss=0.36147501388338804, w0=0.00016573028191113335, w1=-0.03361351120598431\n",
      "Gradient Descent(347/399): loss=0.36132296096880345, w0=0.00016345780263267556, w1=-0.033655544383538204\n",
      "Gradient Descent(348/399): loss=0.3611712656016657, w0=0.00016118871521046267, w1=-0.03369747527044049\n",
      "Gradient Descent(349/399): loss=0.3610199260020538, w0=0.0001589231082119922, w1=-0.033739304423435645\n",
      "Gradient Descent(350/399): loss=0.3608689404044644, w0=0.00015666106912428932, w1=-0.033781032394690595\n",
      "Gradient Descent(351/399): loss=0.3607183070576575, w0=0.00015440268436251543, w1=-0.033822659731843785\n",
      "Gradient Descent(352/399): loss=0.36056802422450507, w0=0.00015214803927854004, w1=-0.033864186978053656\n",
      "Gradient Descent(353/399): loss=0.36041809018184057, w0=0.0001498972181694758, w1=-0.03390561467204654\n",
      "Gradient Descent(354/399): loss=0.36026850322031345, w0=0.00014765030428617618, w1=-0.033946943348163915\n",
      "Gradient Descent(355/399): loss=0.36011926164424146, w0=0.00014540737984169508, w1=-0.03398817353640917\n",
      "Gradient Descent(356/399): loss=0.3599703637714686, w0=0.0001431685260197084, w1=-0.034029305762493696\n",
      "Gradient Descent(357/399): loss=0.35982180793322355, w0=0.00014093382298289658, w1=-0.03407034054788248\n",
      "Gradient Descent(358/399): loss=0.35967359247397934, w0=0.00013870334988128841, w1=-0.034111278409839095\n",
      "Gradient Descent(359/399): loss=0.359525715751317, w0=0.00013647718486056512, w1=-0.03415211986147019\n",
      "Gradient Descent(360/399): loss=0.3593781761357886, w0=0.0001342554050703247, w1=-0.034192865411769385\n",
      "Gradient Descent(361/399): loss=0.3592309720107835, w0=0.00013203808667230614, w1=-0.034233515565660656\n",
      "Gradient Descent(362/399): loss=0.3590841017723979, w0=0.0001298253048485731, w1=-0.0342740708240412\n",
      "Gradient Descent(363/399): loss=0.35893756382930175, w0=0.0001276171338096568, w1=-0.03431453168382373\n",
      "Gradient Descent(364/399): loss=0.3587913566026125, w0=0.000125413646802658, w1=-0.034354898637978325\n",
      "Gradient Descent(365/399): loss=0.3586454785257666, w0=0.0001232149161193073, w1=-0.03439517217557371\n",
      "Gradient Descent(366/399): loss=0.3584999280443955, w0=0.00012102101310398413, w1=-0.03443535278181806\n",
      "Gradient Descent(367/399): loss=0.35835470361620103, w0=0.00011883200816169372, w1=-0.03447544093809931\n",
      "Gradient Descent(368/399): loss=0.3582098037108344, w0=0.00011664797076600205, w1=-0.03451543712202497\n",
      "Gradient Descent(369/399): loss=0.35806522680977493, w0=0.00011446896946692838, w1=-0.03455534180746145\n",
      "Gradient Descent(370/399): loss=0.35792097140621193, w0=0.00011229507189879545, w1=-0.03459515546457292\n",
      "Gradient Descent(371/399): loss=0.3577770360049275, w0=0.00011012634478803674, w1=-0.0346348785598597\n",
      "Gradient Descent(372/399): loss=0.3576334191221804, w0=0.00010796285396096098, w1=-0.0346745115561962\n",
      "Gradient Descent(373/399): loss=0.35749011928559127, w0=0.00010580466435147359, w1=-0.034714054912868324\n",
      "Gradient Descent(374/399): loss=0.3573471350340316, w0=0.00010365184000875486, w1=-0.034753509085610555\n",
      "Gradient Descent(375/399): loss=0.3572044649175107, w0=0.00010150444410489473, w1=-0.03479287452664248\n",
      "Gradient Descent(376/399): loss=0.35706210749706624, w0=9.93625389424842e-05, w1=-0.03483215168470491\n",
      "Gradient Descent(377/399): loss=0.35692006134465615, w0=9.7226185962163e-05, w1=-0.03487134100509559\n",
      "Gradient Descent(378/399): loss=0.3567783250430514, w0=9.509544575012354e-05, w1=-0.03491044292970445\n",
      "Gradient Descent(379/399): loss=0.3566368971857292, w0=9.297037804557111e-05, w1=-0.03494945789704846\n",
      "Gradient Descent(380/399): loss=0.35649577637677016, w0=9.08510417481401e-05, w1=-0.03498838634230602\n",
      "Gradient Descent(381/399): loss=0.3563549612307529, w0=8.87374949252661e-05, w1=-0.03502722869735099\n",
      "Gradient Descent(382/399): loss=0.3562144503726548, w0=8.662979481951415e-05, w1=-0.03506598539078629\n",
      "Gradient Descent(383/399): loss=0.35607424243774877, w0=8.452799785586266e-05, w1=-0.035104656847977096\n",
      "Gradient Descent(384/399): loss=0.3559343360715056, w0=8.243215964894322e-05, w1=-0.03514324349108362\n",
      "Gradient Descent(385/399): loss=0.35579472992949523, w0=8.03423350102362e-05, w1=-0.03518174573909358\n",
      "Gradient Descent(386/399): loss=0.35565542267728945, w0=7.825857795522188e-05, w1=-0.03522016400785415\n",
      "Gradient Descent(387/399): loss=0.35551641299036585, w0=7.618094171048759e-05, w1=-0.03525849871010367\n",
      "Gradient Descent(388/399): loss=0.3553776995540149, w0=7.410947872079023e-05, w1=-0.03529675025550286\n",
      "Gradient Descent(389/399): loss=0.3552392810632436, w0=7.204424065607458e-05, w1=-0.03533491905066574\n",
      "Gradient Descent(390/399): loss=0.3551011562226861, w0=6.998527841844737e-05, w1=-0.03537300549919018\n",
      "Gradient Descent(391/399): loss=0.3549633237465095, w0=6.79326421491068e-05, w1=-0.035411010001688024\n",
      "Gradient Descent(392/399): loss=0.3548257823583268, w0=6.58863812352279e-05, w1=-0.03544893295581493\n",
      "Gradient Descent(393/399): loss=0.35468853079110474, w0=6.384654431680353e-05, w1=-0.0354867747562998\n",
      "Gradient Descent(394/399): loss=0.35455156778707786, w0=6.181317929344101e-05, w1=-0.03552453579497392\n",
      "Gradient Descent(395/399): loss=0.3544148920976604, w0=5.9786333331114745e-05, w1=-0.035562216460799705\n",
      "Gradient Descent(396/399): loss=0.3542785024833609, w0=5.776605286887429e-05, w1=-0.035599817139899126\n",
      "Gradient Descent(397/399): loss=0.3541423977136968, w0=5.5752383625508595e-05, w1=-0.0356373382155818\n",
      "Gradient Descent(398/399): loss=0.3540065765671106, w0=5.3745370606165854e-05, w1=-0.035674780068372755\n",
      "Gradient Descent(399/399): loss=0.3538710378308873, w0=5.174505810892944e-05, w1=-0.035712143076039864\n",
      "++++ gamma = 0.00193069772888\n",
      "ciaociaociao\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/399): loss=0.4999999999999887, w0=-1.32971637681253e-06, w1=-0.0006921790679257304\n",
      "Gradient Descent(1/399): loss=0.49566123808114854, w0=-4.905803824638662e-07, w1=-0.001365668719898139\n",
      "Gradient Descent(2/399): loss=0.4915511696237397, w0=2.2990377644345445e-06, w1=-0.0020211746622378884\n",
      "Gradient Descent(3/399): loss=0.48765336170872337, w0=6.837563907575084e-06, w1=-0.002659371046936368\n",
      "Gradient Descent(4/399): loss=0.48395273629627084, w0=1.2938990702200896e-05, w1=-0.0032809021000257117\n",
      "Gradient Descent(5/399): loss=0.48043544493667995, w0=2.0431739455360104e-05, w1=-0.0038863836538890273\n",
      "Gradient Descent(6/399): loss=0.47708875652669624, w0=2.915760613530035e-05, w1=-0.004476404589849278\n",
      "Gradient Descent(7/399): loss=0.47390095658510434, w0=3.897078516547981e-05, w1=-0.0050515281969221275\n",
      "Gradient Descent(8/399): loss=0.47086125671835877, w0=4.973696511369489e-05, w1=-0.005612293452199048\n",
      "Gradient Descent(9/399): loss=0.46795971311667955, w0=6.133249084289614e-05, w1=-0.006159216227939294\n",
      "Gradient Descent(10/399): loss=0.46518715306738956, w0=7.364358711047254e-05, w1=-0.006692790430090781\n",
      "Gradient Descent(11/399): loss=0.462535108598659, w0=8.656563898997719e-05, w1=-0.007213489072627992\n",
      "Gradient Descent(12/399): loss=0.4599957564761299, w0=0.0001000025248460624, w1=-0.00772176529178786\n",
      "Gradient Descent(13/399): loss=0.4575618638695408, w0=0.00011386599792220072, w1=-0.008218053304000184\n",
      "Gradient Descent(14/399): loss=0.4552267390885413, w0=0.00012807511290380722, w1=-0.008702769311045834\n",
      "Gradient Descent(15/399): loss=0.45298418685816566, w0=0.00014255569409869014, w1=-0.009176312355732037\n",
      "Gradient Descent(16/399): loss=0.4508284676664165, w0=0.00015723984213421684, w1=-0.009639065131148126\n",
      "Gradient Descent(17/399): loss=0.44875426077039765, w0=0.00017206547630792485, w1=-0.01009139474635574\n",
      "Gradient Descent(18/399): loss=0.44675663049455133, w0=0.00018697590994711774, w1=-0.010533653451173328\n",
      "Gradient Descent(19/399): loss=0.44483099549569866, w0=0.00020191945633473278, w1=-0.010966179322534969\n",
      "Gradient Descent(20/399): loss=0.4429731007056302, w0=0.0002168490629447961, w1=-0.011389296914736552\n",
      "Gradient Descent(21/399): loss=0.44117899169356367, w0=0.0002317219719023356, w1=-0.011803317875727647\n"
     ]
    }
   ],
   "source": [
    "\n",
    "OUT_FOLDER = 'output/'\n",
    "name = 'Gradient_descent.csv'\n",
    "degs=range(4,8)\n",
    "gammas=np.logspace(-4,-1,8);\n",
    "k_fold=5\n",
    "max_iters=400\n",
    "\n",
    "\n",
    "mask_jets_train = split_jets_mask(tx_train)\n",
    "mask_jets_test = split_jets_mask(tx_test)\n",
    "len_mask = len(mask_jets_train)\n",
    "\n",
    "y_preds_train = np.zeros(len(y_train))\n",
    "y_preds_test = np.zeros(tx_test.shape[0])\n",
    "\n",
    "\n",
    "\n",
    "for mask_jet_id in range(len_mask):\n",
    "    print('**** Starting Jet ', mask_jet_id, '*****')\n",
    "    tx_single_jet_train = tx_train[mask_jets_train[mask_jet_id]]\n",
    "    tx_single_jet_test =  tx_test[mask_jets_test[mask_jet_id]]\n",
    "    y_single_jet_train =   y_train[mask_jets_train[mask_jet_id]]\n",
    "    \n",
    "    \n",
    "    \n",
    "    [best_deg, gamma_best, acc_max] =cross_validation_GD(y_single_jet_train, tx_single_jet_train, k_fold, max_iters, degs,gammas)\n",
    "    tx_single_jet_train, tx_single_jet_test,len_init_data = prepare_data(tx_single_jet_train,tx_single_jet_test, best_deg)\n",
    "    \n",
    "    initial_w = np.zeros(tx_single_jet_train.shape[1]);\n",
    "    w, loss = func_GD (y=y_single_jet_train, tx=tx_single_jet_train, test_set=tx_single_jet_test, max_iters=max_iters,gamma=best_gamma, initial_w=initial_w);\n",
    "    \n",
    "    y_pred_train = predict_labels(w, tx_single_jet_train)\n",
    "    y_pred_test = predict_labels(w, tx_single_jet_test)\n",
    "    y_preds_train[mask_jets_train[mask_jet_id]] = y_pred_train\n",
    "    y_preds_test[mask_jets_test[mask_jet_id]] = y_pred_test\n",
    "    \n",
    "    right_train = np.sum(y_single_jet_train == y_pred_train)/len(y_single_jet_train )*100\n",
    "    \n",
    "    print('**** Jet ', mask_jet_id, '*****accuracy jet',right_train)\n",
    "    \n",
    "\n",
    "create_csv_submission(ids_test, y_preds_test, OUT_FOLDER+name)\n",
    "right_train_total = np.sum(y_train == y_preds_train)/len(y_train)*100\n",
    "\n",
    "print('>>>>>>>> Accuracy TOTAL ',right_train_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "deg=5;\n",
    "lambda_=1e-5\n",
    "mask_jets_train = split_jets_mask(tx_train)\n",
    "mask_jets_test = split_jets_mask(tx_test)\n",
    "len_mask = len(mask_jets_train)\n",
    "for mask_jet_id in range(len_mask):\n",
    "    print('**** Starting Jet ', mask_jet_id, '*****')\n",
    "    tx_single_jet_train = tx_train[mask_jets_train[mask_jet_id]]\n",
    "    tx_single_jet_test =  tx_test[mask_jets_test[mask_jet_id]]\n",
    "    y_single_jet_train =   y_train[mask_jets_train[mask_jet_id]]\n",
    "    tx_single_jet_train, tx_single_jet_test, len_init = prepare_data(tx_single_jet_train, tx_single_jet_test, deg);\n",
    "    \n",
    "    [w,loss]=func_ridge_regression (y=y_single_jet_train, tx=tx_single_jet_train, test_set=tx_single_jet_test, lambda_=lambda_);    \n",
    "    \n",
    "    y_pred_train = predict_labels(w, tx_single_jet_train)\n",
    "    y_pred_test = predict_labels(w, tx_single_jet_test)\n",
    "    y_preds_train[mask_jets_train[mask_jet_id]] = y_pred_train\n",
    "    y_preds_test[mask_jets_test[mask_jet_id]] = y_pred_test\n",
    "    \n",
    "    right_train = np.sum(y_single_jet_train == y_pred_train)/len(y_single_jet_train )*100\n",
    "    \n",
    "    print('**** Jet ', mask_jet_id, '*****accuracy jet',right_train)\n",
    "    \n",
    "\n",
    "    \n",
    "right_train_total = np.sum(y_train == y_preds_train)/len(y_train )*100\n",
    "print('>>>>>>>> Accuracy TOTAL ',right_train_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##  SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def func_SGD (y, tx, test_set, max_iters, gamma, initial_w, batch_size):\n",
    "    name = 'Stochastic_Gradient_descent'\n",
    "    \n",
    "    w,loss = least_squares_SGD(y, tx, initial_w, max_iters,\\\n",
    "                               gamma, batch_size)\n",
    "    \n",
    "    y_pred = predict_labels(w, test_set)\n",
    "    create_csv_submission(ids_test, y_pred, OUT_FOLDER+name)\n",
    "    \n",
    "    plt.plot(w, 'go')\n",
    "    plt.title('SGD: weights with loss:  '+str(loss),fontsize=15, fontweight='bold');\n",
    "    plt.show()\n",
    "    \n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "deg=5;\n",
    "lambda_=1e-5\n",
    "mask_jets_train = split_jets_mask(tx_train)\n",
    "mask_jets_test = split_jets_mask(tx_test)\n",
    "len_mask = len(mask_jets_train)\n",
    "for mask_jet_id in range(len_mask):\n",
    "    print('**** Starting Jet ', mask_jet_id, '*****')\n",
    "    tx_single_jet_train = tx_train[mask_jets_train[mask_jet_id]]\n",
    "    tx_single_jet_test =  tx_test[mask_jets_test[mask_jet_id]]\n",
    "    y_single_jet_train =   y_train[mask_jets_train[mask_jet_id]]\n",
    "    tx_single_jet_train, tx_single_jet_test, len_init = prepare_data(tx_single_jet_train, tx_single_jet_test, deg);\n",
    "    \n",
    "    initial_w = np.zeros(tx_single_jet_train.shape[1]);\n",
    "    w, loss = func_SGD (y=y_single_jet_train, tx=tx_single_jet_train, test_set=tx_single_jet_test, max_iters=500,gamma=0.00005, initial_w=initial_w,\\\n",
    "                   batch_size=1);    \n",
    "   \n",
    "    y_pred_train = predict_labels(w, tx_single_jet_train)\n",
    "    y_pred_test = predict_labels(w, tx_single_jet_test)\n",
    "    y_preds_train[mask_jets_train[mask_jet_id]] = y_pred_train\n",
    "    y_preds_test[mask_jets_test[mask_jet_id]] = y_pred_test\n",
    "    \n",
    "    right_train = np.sum(y_single_jet_train == y_pred_train)/len(y_single_jet_train )*100\n",
    "    \n",
    "    print('**** Jet ', mask_jet_id, '*****accuracy jet',right_train)\n",
    "    \n",
    "\n",
    "    \n",
    "right_train_total = np.sum(y_train == y_preds_train)/len(y_train )*100\n",
    "print('>>>>>>>> Accuracy TOTAL ',right_train_total)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "def cross_validation_logistic_regression(y,tx, k_fold, max_iters, gammas):\n",
    "    \n",
    "    accuracies_test=np.zeros(len(gammas))\n",
    "    accuracies_train=np.zeros(len(gammas))\n",
    "    acc_max=0;\n",
    "    gamma_best=0;\n",
    "    \n",
    "    for i, single_gamma in enumerate(gammas):\n",
    "        \n",
    "        seed=1;\n",
    "\n",
    "        # get k'th subgroup in test, others in train\n",
    "        k_indices = build_k_indices(y, k_fold, seed)\n",
    "        accuracy_train = np.zeros(k_fold)\n",
    "        accuracy_test = np.zeros(k_fold)\n",
    "\n",
    "\n",
    "        for k in range(k_fold):\n",
    "            #print('----- FOLD', k, '-----')\n",
    "            k_index = k_indices[k]\n",
    "            test_y = y[k_index]\n",
    "            test_tx = tx[k_index,:]\n",
    "            mask = np.ones(len(y), dtype=bool) # set all elements to True\n",
    "            mask[k_index] = False              # set test elements to False\n",
    "            train_tx = tx[mask,:]              # select only True elements (ie train elements)\n",
    "            train_y = y[mask]\n",
    "            initial_w = np.zeros(train_tx.shape[1]);\n",
    "            weights,loss = logistic_regression(train_y, train_tx, initial_w, max_iters, single_gamma)\n",
    "            # Compute the predictions\n",
    "            y_pred_train = predict_labels(weights, train_tx)\n",
    "            y_pred_test = predict_labels(weights, test_tx);\n",
    "            accuracy_train[k] = np.sum(y_pred_train == train_y)/len(train_y)\n",
    "            accuracy_test[k] = np.sum(y_pred_test == test_y)/len(test_y)\n",
    "            \n",
    "        accuracies_test[i]=np.mean(accuracy_test);\n",
    "        accuracies_train[i]=np.mean(accuracy_train);\n",
    "        \n",
    "        print('GAMMA:', single_gamma, '---','ACCURANCY TEST:',accuracies_test[i], '---','ACCURANCY TRAIN:',accuracies_train[i])\n",
    "        if (accuracies_test[i]>acc_max):\n",
    "                gamma_best=gammas[i];\n",
    "                acc_max=accuracies_test[i];\n",
    "    \n",
    "    return [gamma_best,acc_max]\n",
    "               \n",
    "        # Compute accuracy of the predictions\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def func_logistic (y, tx, test_set, max_iters, gamma, initial_w):\n",
    "    name = 'Logistic regression'\n",
    "    \n",
    "    #w,loss = logistic_regression(y, tx, initial_w, max_iters, gamma)\n",
    "    w,loss = logistic_regression(y, tx, initial_w, max_iters, gamma)\n",
    "    \n",
    "    loss=loss/len(y);\n",
    "    y_pred = predict_labels(w, test_set)\n",
    "    create_csv_submission(ids_test, y_pred, OUT_FOLDER+name)  \n",
    "    \n",
    "    plt.plot(w, 'go')\n",
    "    plt.title('logistic reg: weights with loss:  '+str(loss),fontsize=15, fontweight='bold');\n",
    "    plt.show()\n",
    "    \n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gammas=np.linspace(0.00001,0.9,10);\n",
    "k_fold=3;\n",
    "max_iters=1000;\n",
    "deg=5;\n",
    "\n",
    "\n",
    "mask_jets_train = split_jets_mask(tx_train)\n",
    "mask_jets_test = split_jets_mask(tx_test)\n",
    "len_mask = len(mask_jets_train)\n",
    "for mask_jet_id in range(len_mask):\n",
    "    print('**** Starting Jet ', mask_jet_id, '*****')\n",
    "    tx_single_jet_train = tx_train[mask_jets_train[mask_jet_id]]\n",
    "    tx_single_jet_test =  tx_test[mask_jets_test[mask_jet_id]]\n",
    "    y_single_jet_train =   y_train[mask_jets_train[mask_jet_id]]\n",
    "    tx_single_jet_train, tx_single_jet_test, len_init = prepare_data(tx_single_jet_train, tx_single_jet_test, deg);\n",
    "    \n",
    "    \n",
    "    \n",
    "    [best_gamma,acc_max]=cross_validation_logistic_regression(y_single_jet_train, tx_single_jet_train, k_fold, max_iters, gammas)\n",
    "    \n",
    "    print('BEST GAMMA:', best_gamma, '---','ACCURANCY:',acc_max)\n",
    "\n",
    "    initial_w = np.zeros(tx_single_jet_train.shape[1]);\n",
    "    w, loss = func_logistic (y=y_single_jet_train, tx=tx_single_jet_train, test_set=tx_single_jet_test, max_iters=1000,\\\n",
    "                         gamma=best_gamma, initial_w=initial_w);\n",
    "    \n",
    "    y_pred_train = predict_labels(w, tx_single_jet_train)\n",
    "    y_pred_test = predict_labels(w, tx_single_jet_test)\n",
    "    y_preds_train[mask_jets_train[mask_jet_id]] = y_pred_train\n",
    "    y_preds_test[mask_jets_test[mask_jet_id]] = y_pred_test\n",
    "    \n",
    "    right_train = np.sum(y_single_jet_train == y_pred_train)/len(y_single_jet_train )*100\n",
    "    \n",
    "    print('**** Jet ', mask_jet_id, '*****accuracy jet',right_train)\n",
    "    \n",
    "\n",
    "    \n",
    "right_train_total = np.sum(y_train == y_preds_train)/len(y_train )*100\n",
    "print('>>>>>>>> Accuracy TOTAL ',right_train_total)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOGISTIC REGULARIZED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "def cross_validation_logistic_regression_reg(y,tx, k_fold, max_iters, gammas,lambdas):\n",
    "    \n",
    "    acc_max=0;\n",
    "    accuracies_test=np.zeros([len(gammas),len(lambdas)]);\n",
    "    \n",
    "    accuracies_train=np.zeros([len(gammas),len(lambdas)]);\n",
    "    \n",
    "    \n",
    "    for i, single_gamma in enumerate(gammas):\n",
    "        for j, single_lambda in enumerate(lambdas):\n",
    "            seed=1;\n",
    "        \n",
    "            # get k'th subgroup in test, others in train\n",
    "            k_indices = build_k_indices(y, k_fold, seed)\n",
    "            accuracy_train = np.zeros(k_fold)\n",
    "            accuracy_test = np.zeros(k_fold)\n",
    "\n",
    "\n",
    "            for k in range(k_fold):\n",
    "                #print('----- FOLD', k, '-----')\n",
    "                k_index = k_indices[k]\n",
    "                test_y = y[k_index]\n",
    "                test_tx = tx[k_index,:]\n",
    "\n",
    "                mask = np.ones(len(y), dtype=bool) # set all elements to True\n",
    "                mask[k_index] = False              # set test elements to False\n",
    "                train_tx = tx[mask,:]              # select only True elements (ie train elements)\n",
    "                train_y = y[mask]\n",
    "                initial_w = np.zeros(train_tx.shape[1]);\n",
    "                weights,loss = reg_logistic_regression(train_y, train_tx,single_lambda, initial_w, max_iters, single_gamma)\n",
    "                # Compute the predictions\n",
    "                y_pred_train = predict_labels(weights, train_tx)\n",
    "                y_pred_test = predict_labels(weights, test_tx)\n",
    "                predictions=True;\n",
    "                accuracy_train[k] = np.sum(y_pred_train == train_y)/len(train_y)\n",
    "                accuracy_test[k] = np.sum(y_pred_test == test_y)/len(test_y)\n",
    "                \n",
    "                \n",
    "            accuracies_test[i,j]= np.mean(accuracy_test);\n",
    "            accuracies_train[i,j]= np.mean(accuracy_train);\n",
    "            print('GAMMA', single_gamma, '---','LAMBDA', single_lambda, '---ACCURANCY TEST:',accuracies_test[i,j],'---ACCURANCY TRAIN:',accuracies_train[i,j])\n",
    "            \n",
    "            if (accuracies_test[i,j]>acc_max):\n",
    "                gamma_best=gammas[i];\n",
    "                lambda_best=lambdas[j];\n",
    "                acc_max=accuracies_test[i,j];\n",
    "            \n",
    "            \n",
    "    return [gamma_best,lambda_best,acc_max]\n",
    "     \n",
    "    \n",
    "    \n",
    "            # Compute accuracy of the predictions\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def func_logistic_reg (y, tx, lambda_, test_set, max_iters, gamma, initial_w):\n",
    "    name = 'Logistic regression regularized'\n",
    "    \n",
    "    #w,loss = logistic_regression(y, tx, initial_w, max_iters, gamma)\n",
    "    w,loss = reg_logistic_regression(y, tx, lambda_, initial_w, max_iters, gamma)\n",
    "    \n",
    "    loss=loss/len(y);\n",
    "    y_pred = predict_labels(w, test_set)\n",
    "    create_csv_submission(ids_test, y_pred, OUT_FOLDER+name)\n",
    "    \n",
    "    plt.plot(w, 'go')\n",
    "    plt.title('logistic reg: weights with the normalized log-like:  '+str(loss),fontsize=15, fontweight='bold');\n",
    "    plt.show()\n",
    "    \n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test cross_validation\n",
    "\n",
    "gammas=np.linspace(0.000001,0.0001,2);\n",
    "lambdas=np.linspace(0.001,0.3,2);\n",
    "k_fold=6;\n",
    "max_iters=50;\n",
    "\n",
    "deg=2;\n",
    "\n",
    "mask_jets_train = split_jets_mask(tx_train)\n",
    "mask_jets_test = split_jets_mask(tx_test)\n",
    "len_mask = len(mask_jets_train)\n",
    "for mask_jet_id in range(len_mask):\n",
    "    print('**** Starting Jet ', mask_jet_id, '*****')\n",
    "    tx_single_jet_train = tx_train[mask_jets_train[mask_jet_id]]\n",
    "    tx_single_jet_test =  tx_test[mask_jets_test[mask_jet_id]]\n",
    "    y_single_jet_train =   y_train[mask_jets_train[mask_jet_id]]\n",
    "    tx_single_jet_train, tx_single_jet_test, len_init\n",
    "    = prepare_data(tx_single_jet_train, tx_single_jet_test, deg);\n",
    "    \n",
    "    [gamma_best,lambda_best,acc_max]= cross_validation_logistic_regression_reg(y_single_jet_train, tx_single_jet_train, k_fold, max_iters, gammas,lambdas)\n",
    "    print('BEST GAMMA', gamma_best, '---','BEST LAMBDA', lambda_best, '---ACCURANCY:',acc_max)\n",
    "    \n",
    "    initial_w = np.zeros(tx_single_jet_train.shape[1]);\n",
    "    w, loss = func_logistic_reg (y=y_single_jet_train, tx=tx_single_jet_train, lambda_=lambda_best, test_set=tx_single_jet_test , max_iters=max_iters,\\\n",
    "                         gamma=gamma_best, initial_w=initial_w);\n",
    "\n",
    "\n",
    "    \n",
    "    y_pred_train = predict_labels(w, tx_single_jet_train)\n",
    "    y_pred_test = predict_labels(w, tx_single_jet_test)\n",
    "    y_preds_train[mask_jets_train[mask_jet_id]] = y_pred_train\n",
    "    y_preds_test[mask_jets_test[mask_jet_id]] = y_pred_test\n",
    "    \n",
    "    right_train = np.sum(y_single_jet_train == y_pred_train)/len(y_single_jet_train )*100\n",
    "    \n",
    "    print('**** Jet ', mask_jet_id, '*****accuracy jet',right_train)\n",
    "    \n",
    "\n",
    "    \n",
    "right_train_total = np.sum(y_train == y_preds_train)/len(y_train )*100\n",
    "print('>>>>>>>> Accuracy TOTAL ',right_train_total)\n",
    "\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
