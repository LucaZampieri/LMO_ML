{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Project 1\n",
    "\n",
    "    We begin by loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from preprocessing_functions import *\n",
    "%matplotlib inline \n",
    "import numpy as np   # generic stuff\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#### REMOVE THIS LINE BEFORE SUBMISSION\n",
    "import pandas as pd\n",
    "#######################################################################\n",
    "\n",
    "from lib.proj1_helpers import * #the helper provided for the project\n",
    "from lib.costs import *\n",
    "\n",
    "# choose which implementations you would like\n",
    "from lib.implementations import *\n",
    "#from implementations import * #our implementations of the functions done by us\n",
    "\n",
    "\n",
    "import datetime\n",
    "from helpers import * #helpers of exo 2\n",
    "# Useful starting lines\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_FOLDER = '../data/' # get rid of the ..\n",
    "\n",
    "y_train, tx_train, ids_train = load_csv_data(DATA_FOLDER+'train.csv',sub_sample=False)\n",
    "\n",
    "y_test, tx_test, ids_test = load_csv_data(DATA_FOLDER+'test.csv',sub_sample=False)\n",
    "AAA = 0;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note that here we are only considering a sub_sample as the \"True\" value indicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 30)\n",
      "(568238, 30)\n",
      "(250000,)\n"
     ]
    }
   ],
   "source": [
    "print(tx_train.shape)\n",
    "print(tx_test.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>138.470</td>\n",
       "      <td>51.655</td>\n",
       "      <td>97.827</td>\n",
       "      <td>27.980</td>\n",
       "      <td>0.91</td>\n",
       "      <td>124.711</td>\n",
       "      <td>2.666</td>\n",
       "      <td>3.064</td>\n",
       "      <td>41.928</td>\n",
       "      <td>197.760</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.277</td>\n",
       "      <td>258.733</td>\n",
       "      <td>2.0</td>\n",
       "      <td>67.435</td>\n",
       "      <td>2.150</td>\n",
       "      <td>0.444</td>\n",
       "      <td>46.062</td>\n",
       "      <td>1.24</td>\n",
       "      <td>-2.475</td>\n",
       "      <td>113.497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>160.937</td>\n",
       "      <td>68.768</td>\n",
       "      <td>103.235</td>\n",
       "      <td>48.146</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.473</td>\n",
       "      <td>2.078</td>\n",
       "      <td>125.157</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.916</td>\n",
       "      <td>164.546</td>\n",
       "      <td>1.0</td>\n",
       "      <td>46.226</td>\n",
       "      <td>0.725</td>\n",
       "      <td>1.158</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>46.226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-999.000</td>\n",
       "      <td>162.172</td>\n",
       "      <td>125.953</td>\n",
       "      <td>35.635</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.148</td>\n",
       "      <td>9.336</td>\n",
       "      <td>197.814</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.186</td>\n",
       "      <td>260.414</td>\n",
       "      <td>1.0</td>\n",
       "      <td>44.251</td>\n",
       "      <td>2.053</td>\n",
       "      <td>-2.028</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>44.251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>143.905</td>\n",
       "      <td>81.417</td>\n",
       "      <td>80.943</td>\n",
       "      <td>0.414</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.310</td>\n",
       "      <td>0.414</td>\n",
       "      <td>75.968</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060</td>\n",
       "      <td>86.062</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>175.864</td>\n",
       "      <td>16.915</td>\n",
       "      <td>134.805</td>\n",
       "      <td>16.405</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.891</td>\n",
       "      <td>16.405</td>\n",
       "      <td>57.983</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.871</td>\n",
       "      <td>53.131</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0        1        2       3       4        5        6      7       8   \\\n",
       "0  138.470   51.655   97.827  27.980    0.91  124.711    2.666  3.064  41.928   \n",
       "1  160.937   68.768  103.235  48.146 -999.00 -999.000 -999.000  3.473   2.078   \n",
       "2 -999.000  162.172  125.953  35.635 -999.00 -999.000 -999.000  3.148   9.336   \n",
       "3  143.905   81.417   80.943   0.414 -999.00 -999.000 -999.000  3.310   0.414   \n",
       "4  175.864   16.915  134.805  16.405 -999.00 -999.000 -999.000  3.891  16.405   \n",
       "\n",
       "        9    ...        20       21   22       23       24       25       26  \\\n",
       "0  197.760   ...    -0.277  258.733  2.0   67.435    2.150    0.444   46.062   \n",
       "1  125.157   ...    -1.916  164.546  1.0   46.226    0.725    1.158 -999.000   \n",
       "2  197.814   ...    -2.186  260.414  1.0   44.251    2.053   -2.028 -999.000   \n",
       "3   75.968   ...     0.060   86.062  0.0 -999.000 -999.000 -999.000 -999.000   \n",
       "4   57.983   ...    -0.871   53.131  0.0 -999.000 -999.000 -999.000 -999.000   \n",
       "\n",
       "       27       28       29  \n",
       "0    1.24   -2.475  113.497  \n",
       "1 -999.00 -999.000   46.226  \n",
       "2 -999.00 -999.000   44.251  \n",
       "3 -999.00 -999.000    0.000  \n",
       "4 -999.00 -999.000    0.000  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# illegal: \n",
    "pd.DataFrame(tx_train).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for methods: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OUT_FOLDER = 'output/'\n",
    "\n",
    "def func_least_squares (y, tx, test_set, fct='mse'):\n",
    "    #name = 'least_squares'\n",
    "    w,loss = least_squares(y,tx,fct)\n",
    "    #y_pred = predict_labels(w, test_set)\n",
    "    #create_csv_submission(ids_test, y_pred, OUT_FOLDER+name)\n",
    "    \n",
    "    plt.plot(w, 'go')\n",
    "    plt.title('least squares weights for loss:  '+str(loss),fontsize=15, fontweight='bold');\n",
    "    plt.show()\n",
    "    \n",
    "    return w, loss\n",
    "\n",
    "def func_GD (y, tx, test_set, max_iters, gamma, initial_w):\n",
    "    name = 'Gradient_descent'\n",
    "    w,loss = least_squares_GD(y, tx, initial_w, max_iters, gamma,fct='mse');\n",
    "    y_pred = predict_labels(w, test_set)\n",
    "    create_csv_submission(ids_test, y_pred, OUT_FOLDER+name)\n",
    "    \n",
    "    plt.plot(w, 'go')\n",
    "    plt.title('GD: weights with loss:  '+str(loss),fontsize=15, fontweight='bold');\n",
    "    plt.show()\n",
    "    \n",
    "    return w, loss\n",
    "\n",
    "########################  RIDGE REGRESSION #######################################################\n",
    "def func_ridge_regression (y, tx, test_set, lambda_):\n",
    "    name = 'Ridge_regression'\n",
    "    \n",
    "    w,loss = ridge_regression(y, tx, lambda_=lambda_, fct='mse');\n",
    "    \n",
    "    y_pred = predict_labels(w, test_set)\n",
    "    create_csv_submission(ids_test, y_pred, OUT_FOLDER+name)\n",
    "    \n",
    "    plt.plot(w, 'go')\n",
    "    plt.title('Ridge regression: weights ;loss:  '+str(loss),fontsize=15, fontweight='bold');\n",
    "    plt.show()\n",
    "    \n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LEAST-SQUARES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#todo: cross_validation su degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "def cross_validation_least_square(y, tx, k_fold, degs):            \n",
    "    seed=1;\n",
    "\n",
    "    # get k'th subgroup in test, others in train\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    accuracy_train = np.zeros([k_fold, len(degs)])\n",
    "    accuracy_test = np.zeros([k_fold, len(degs)])\n",
    "               \n",
    "    for k in range(k_fold):\n",
    "        print('----- FOLD', k, '-----')\n",
    "        k_index = k_indices[k]\n",
    "        test_y = y[k_index]\n",
    "        test_tx = tx[k_index,:]\n",
    "        mask = np.ones(len(y), dtype=bool) # set all elements to True\n",
    "        mask[k_index] = False              # set test elements to False\n",
    "        train_tx = tx[mask,:]              # select only True elements (ie train elements)\n",
    "        train_y = y[mask]\n",
    "\n",
    "        len_init_data = 0\n",
    "\n",
    "        for i, single_deg in enumerate(degs):\n",
    "            \n",
    "            print('++++ Deg =', single_deg)\n",
    "            if i==0:\n",
    "                train_tx, test_tx, len_init_data = prepare_data(train_tx, test_tx, single_deg);\n",
    "            else:\n",
    "                train_tx = add_powers(train_tx, single_deg, 0, len_init_data, features='x', current_max_deg=single_deg-1)\n",
    "                test_tx = add_powers(test_tx, single_deg, 0, len_init_data, features='x', current_max_deg=single_deg-1)\n",
    "            weights,loss = least_squares(train_y,train_tx,fct='mse');\n",
    "\n",
    "            # Compute the predictions\n",
    "            y_pred_train = predict_labels(weights, train_tx)\n",
    "            y_pred_test = predict_labels(weights, test_tx);\n",
    "            accuracy_train[k, i] = np.sum(y_pred_train == train_y)/len(train_y)\n",
    "            accuracy_test[k, i] = np.sum(y_pred_test == test_y)/len(test_y)\n",
    "\n",
    "    accuracies_test=np.mean(accuracy_test, axis=0); # mean on the k's\n",
    "    accuracies_train=np.mean(accuracy_train, axis=0);\n",
    "\n",
    "    max_index, acc_max = max(enumerate(accuracies_test), key=operator.itemgetter(1))\n",
    "    deg_best = degs[max_index]\n",
    "    \n",
    "    return deg_best, acc_max;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Starting Jet  0 *****\n",
      "----- FOLD 0 -----\n",
      "++++ Deg = 2\n",
      "++++ Deg = 3\n",
      "++++ Deg = 4\n",
      "++++ Deg = 5\n",
      "++++ Deg = 6\n",
      "++++ Deg = 7\n",
      "++++ Deg = 8\n",
      "++++ Deg = 9\n",
      "++++ Deg = 10\n",
      "++++ Deg = 11\n",
      "++++ Deg = 12\n",
      "++++ Deg = 13\n",
      "----- FOLD 1 -----\n",
      "++++ Deg = 2\n",
      "++++ Deg = 3\n",
      "++++ Deg = 4\n",
      "++++ Deg = 5\n",
      "++++ Deg = 6\n",
      "++++ Deg = 7\n",
      "++++ Deg = 8\n",
      "++++ Deg = 9\n",
      "++++ Deg = 10\n",
      "++++ Deg = 11\n",
      "++++ Deg = 12\n",
      "++++ Deg = 13\n",
      "----- FOLD 2 -----\n",
      "++++ Deg = 2\n",
      "++++ Deg = 3\n",
      "++++ Deg = 4\n",
      "++++ Deg = 5\n",
      "++++ Deg = 6\n",
      "++++ Deg = 7\n",
      "++++ Deg = 8\n",
      "++++ Deg = 9\n",
      "++++ Deg = 10\n",
      "++++ Deg = 11\n",
      "++++ Deg = 12\n",
      "++++ Deg = 13\n",
      "----- FOLD 3 -----\n",
      "++++ Deg = 2\n",
      "++++ Deg = 3\n",
      "++++ Deg = 4\n",
      "++++ Deg = 5\n",
      "++++ Deg = 6\n",
      "++++ Deg = 7\n",
      "++++ Deg = 8\n",
      "++++ Deg = 9\n",
      "++++ Deg = 10\n",
      "++++ Deg = 11\n",
      "++++ Deg = 12\n",
      "++++ Deg = 13\n",
      "----- FOLD 4 -----\n",
      "++++ Deg = 2\n",
      "++++ Deg = 3\n",
      "++++ Deg = 4\n",
      "++++ Deg = 5\n",
      "++++ Deg = 6\n",
      "++++ Deg = 7\n",
      "++++ Deg = 8\n",
      "++++ Deg = 9\n",
      "++++ Deg = 10\n",
      "++++ Deg = 11\n",
      "++++ Deg = 12\n",
      "++++ Deg = 13\n",
      "12\n",
      "12\n",
      "[ 0.63061756  0.61141027  0.57356621  0.54820338  0.54161746  0.50752677\n",
      "  0.57925133  0.57876088  0.52059854  0.57772996  0.52022821  0.49412471]\n",
      "Best degree =  2\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbIAAAEKCAYAAAB36tAEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUmj/H7LeeSFpiYmaAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fea57557668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Jet  0 ***** Accuracy jet 83.4135698057\n",
      "**** Starting Jet  1 *****\n",
      "----- FOLD 0 -----\n",
      "++++ Deg = 2\n",
      "++++ Deg = 3\n",
      "++++ Deg = 4\n",
      "++++ Deg = 5\n",
      "++++ Deg = 6\n",
      "++++ Deg = 7\n",
      "++++ Deg = 8\n",
      "++++ Deg = 9\n",
      "++++ Deg = 10\n",
      "++++ Deg = 11\n",
      "++++ Deg = 12\n",
      "++++ Deg = 13\n",
      "----- FOLD 1 -----\n",
      "++++ Deg = 2\n",
      "++++ Deg = 3\n",
      "++++ Deg = 4\n",
      "++++ Deg = 5\n",
      "++++ Deg = 6\n",
      "++++ Deg = 7\n",
      "++++ Deg = 8\n",
      "++++ Deg = 9\n",
      "++++ Deg = 10\n",
      "++++ Deg = 11\n",
      "++++ Deg = 12\n",
      "++++ Deg = 13\n",
      "----- FOLD 2 -----\n",
      "++++ Deg = 2\n",
      "++++ Deg = 3\n",
      "++++ Deg = 4\n",
      "++++ Deg = 5\n",
      "++++ Deg = 6\n",
      "++++ Deg = 7\n",
      "++++ Deg = 8\n",
      "++++ Deg = 9\n",
      "++++ Deg = 10\n",
      "++++ Deg = 11\n",
      "++++ Deg = 12\n",
      "++++ Deg = 13\n",
      "----- FOLD 3 -----\n",
      "++++ Deg = 2\n",
      "++++ Deg = 3\n",
      "++++ Deg = 4\n",
      "++++ Deg = 5\n",
      "++++ Deg = 6\n",
      "++++ Deg = 7\n",
      "++++ Deg = 8\n",
      "++++ Deg = 9\n",
      "++++ Deg = 10\n",
      "++++ Deg = 11\n",
      "++++ Deg = 12\n",
      "++++ Deg = 13\n",
      "----- FOLD 4 -----\n",
      "++++ Deg = 2\n",
      "++++ Deg = 3\n",
      "++++ Deg = 4\n",
      "++++ Deg = 5\n",
      "++++ Deg = 6\n",
      "++++ Deg = 7\n",
      "++++ Deg = 8\n",
      "++++ Deg = 9\n",
      "++++ Deg = 10\n",
      "++++ Deg = 11\n",
      "++++ Deg = 12\n",
      "++++ Deg = 13\n",
      "12\n",
      "12\n",
      "[ 0.54350013  0.54740779  0.54806551  0.55459118  0.55474594  0.55686097\n",
      "  0.55203766  0.5567707   0.55433325  0.57225948  0.52580604  0.53687129]\n",
      "Best degree =  11\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbMAAAEKCAYAAACYKLs6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFMxERCTzFMxERCTzFMxERCTz/gcGwm3fLej4vgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fea0d5b6400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Jet  1 ***** Accuracy jet 80.7992881461\n",
      "**** Starting Jet  2 *****\n",
      "----- FOLD 0 -----\n",
      "++++ Deg = 2\n",
      "++++ Deg = 3\n",
      "++++ Deg = 4\n",
      "++++ Deg = 5\n",
      "++++ Deg = 6\n",
      "++++ Deg = 7\n",
      "++++ Deg = 8\n",
      "++++ Deg = 9\n",
      "++++ Deg = 10\n",
      "++++ Deg = 11\n",
      "++++ Deg = 12\n",
      "++++ Deg = 13\n",
      "----- FOLD 1 -----\n",
      "++++ Deg = 2\n",
      "++++ Deg = 3\n",
      "++++ Deg = 4\n",
      "++++ Deg = 5\n",
      "++++ Deg = 6\n",
      "++++ Deg = 7\n",
      "++++ Deg = 8\n",
      "++++ Deg = 9\n",
      "++++ Deg = 10\n",
      "++++ Deg = 11\n",
      "++++ Deg = 12\n",
      "++++ Deg = 13\n",
      "----- FOLD 2 -----\n",
      "++++ Deg = 2\n",
      "++++ Deg = 3\n",
      "++++ Deg = 4\n",
      "++++ Deg = 5\n",
      "++++ Deg = 6\n",
      "++++ Deg = 7\n",
      "++++ Deg = 8\n",
      "++++ Deg = 9\n",
      "++++ Deg = 10\n",
      "++++ Deg = 11\n",
      "++++ Deg = 12\n",
      "++++ Deg = 13\n",
      "----- FOLD 3 -----\n",
      "++++ Deg = 2\n",
      "++++ Deg = 3\n",
      "++++ Deg = 4\n",
      "++++ Deg = 5\n",
      "++++ Deg = 6\n",
      "++++ Deg = 7\n",
      "++++ Deg = 8\n",
      "++++ Deg = 9\n",
      "++++ Deg = 10\n",
      "++++ Deg = 11\n",
      "++++ Deg = 12\n",
      "++++ Deg = 13\n",
      "----- FOLD 4 -----\n",
      "++++ Deg = 2\n",
      "++++ Deg = 3\n",
      "++++ Deg = 4\n",
      "++++ Deg = 5\n",
      "++++ Deg = 6\n",
      "++++ Deg = 7\n",
      "++++ Deg = 8\n",
      "++++ Deg = 9\n",
      "++++ Deg = 10\n",
      "++++ Deg = 11\n",
      "++++ Deg = 12\n",
      "++++ Deg = 13\n",
      "12\n",
      "12\n",
      "[ 0.52015881  0.51406452  0.51136476  0.51396526  0.51390571  0.51660546\n",
      "  0.51408437  0.51644665  0.51868983  0.50989578  0.51134491  0.49725062]\n",
      "Best degree =  2\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAawAAAEKCAYAAABOjWFfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fea1672f748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Jet  2 ***** Accuracy jet 80.5831794994\n",
      "**** Starting Jet  3 *****\n",
      "----- FOLD 0 -----\n",
      "++++ Deg = 2\n",
      "++++ Deg = 3\n",
      "++++ Deg = 4\n",
      "++++ Deg = 5\n",
      "++++ Deg = 6\n",
      "++++ Deg = 7\n",
      "++++ Deg = 8\n",
      "++++ Deg = 9\n",
      "++++ Deg = 10\n",
      "++++ Deg = 11\n",
      "++++ Deg = 12\n",
      "++++ Deg = 13\n",
      "----- FOLD 1 -----\n",
      "++++ Deg = 2\n",
      "++++ Deg = 3\n",
      "++++ Deg = 4\n",
      "++++ Deg = 5\n",
      "++++ Deg = 6\n",
      "++++ Deg = 7\n",
      "++++ Deg = 8\n",
      "++++ Deg = 9\n",
      "++++ Deg = 10\n",
      "++++ Deg = 11\n",
      "++++ Deg = 12\n",
      "++++ Deg = 13\n",
      "----- FOLD 2 -----\n",
      "++++ Deg = 2\n",
      "++++ Deg = 3\n",
      "++++ Deg = 4\n",
      "++++ Deg = 5\n",
      "++++ Deg = 6\n",
      "++++ Deg = 7\n",
      "++++ Deg = 8\n",
      "++++ Deg = 9\n",
      "++++ Deg = 10\n",
      "++++ Deg = 11\n",
      "++++ Deg = 12\n",
      "++++ Deg = 13\n",
      "----- FOLD 3 -----\n",
      "++++ Deg = 2\n",
      "++++ Deg = 3\n",
      "++++ Deg = 4\n",
      "++++ Deg = 5\n",
      "++++ Deg = 6\n",
      "++++ Deg = 7\n",
      "++++ Deg = 8\n",
      "++++ Deg = 9\n",
      "++++ Deg = 10\n",
      "++++ Deg = 11\n",
      "++++ Deg = 12\n",
      "++++ Deg = 13\n",
      "----- FOLD 4 -----\n",
      "++++ Deg = 2\n",
      "++++ Deg = 3\n",
      "++++ Deg = 4\n",
      "++++ Deg = 5\n",
      "++++ Deg = 6\n",
      "++++ Deg = 7\n",
      "++++ Deg = 8\n",
      "++++ Deg = 9\n",
      "++++ Deg = 10\n",
      "++++ Deg = 11\n",
      "++++ Deg = 12\n",
      "++++ Deg = 13\n",
      "12\n",
      "12\n",
      "[ 0.51326715  0.50672383  0.50857401  0.50898014  0.50866426  0.50812274\n",
      "  0.51028881  0.51168773  0.51123646  0.49905235  0.55126354  0.51380866]\n",
      "Best degree =  12\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbgAAAEKCAYAAABgyEDNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fea1a916b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Jet  3 ***** Accuracy jet 84.8853997473\n",
      ">>>>>>>> Accuracy TOTAL  82.1628\n"
     ]
    }
   ],
   "source": [
    "OUT_FOLDER = 'output/'\n",
    "name = 'least_squares.csv'\n",
    "\n",
    "degs=range(2,14)\n",
    "k_fold=5\n",
    "\n",
    "mask_jets_train = split_jets_mask(tx_train)\n",
    "mask_jets_test = split_jets_mask(tx_test)\n",
    "len_mask = len(mask_jets_train)\n",
    "y_preds_train = np.zeros(len(y_train))\n",
    "y_preds_test = np.zeros(tx_test.shape[0])\n",
    "\n",
    "\n",
    "for mask_jet_id in range(len_mask):\n",
    "    print('**** Starting Jet ', mask_jet_id, '*****')\n",
    "    tx_single_jet_train = tx_train[mask_jets_train[mask_jet_id]]\n",
    "    tx_single_jet_test = tx_test[mask_jets_test[mask_jet_id]]\n",
    "    y_single_jet_train = y_train[mask_jets_train[mask_jet_id]]\n",
    "    \n",
    "    best_deg, best_acc = cross_validation_least_square(y_single_jet_train, tx_single_jet_train, k_fold, degs)\n",
    "    print('Best degree = ', best_deg)\n",
    "    \n",
    "    tx_single_jet_train, tx_single_jet_test,len_init_data = prepare_data(tx_single_jet_train,tx_single_jet_test, best_deg)\n",
    "    w, loss = func_least_squares(y=y_single_jet_train, tx=tx_single_jet_train, test_set=tx_single_jet_test, fct='mse')\n",
    "    \n",
    "    y_pred_train = predict_labels(w, tx_single_jet_train)\n",
    "    y_pred_test = predict_labels(w, tx_single_jet_test)\n",
    "    y_preds_train[mask_jets_train[mask_jet_id]] = y_pred_train\n",
    "    y_preds_test[mask_jets_test[mask_jet_id]] = y_pred_test\n",
    "    \n",
    "    right_train = np.sum(y_single_jet_train == y_pred_train)/len(y_single_jet_train)*100\n",
    "    \n",
    "    print('**** Jet ', mask_jet_id, '***** Accuracy jet', right_train)\n",
    "    \n",
    "\n",
    "create_csv_submission(ids_test, y_preds_test, OUT_FOLDER+name)\n",
    "right_train_total = np.sum(y_train == y_preds_train)/len(y_train)*100\n",
    "print('>>>>>>>> Accuracy TOTAL ',right_train_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_preds_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5b7597dc3927>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_preds_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_preds_test\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'y_preds_test' is not defined"
     ]
    }
   ],
   "source": [
    "len(y_preds_test[y_preds_test==-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "246651"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_preds_test[y_preds_test==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_preds_test[y_preds_test==-1])+len(y_preds_test[y_preds_test==1])==tx_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1., -1.,  1.,  1., -1.,  1.,  1., -1., -1., -1., -1.,  1.,  1.,\n",
       "       -1., -1.,  1., -1.,  1., -1., -1., -1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1., -1., -1., -1., -1., -1., -1., -1.,  1.,  1.,\n",
       "       -1., -1., -1.,  1.,  1.,  1.,  1., -1.,  1., -1.,  1.,  1., -1.,\n",
       "        1., -1., -1., -1.,  1., -1.,  1., -1., -1., -1.,  1., -1., -1.,\n",
       "       -1.,  1.,  1.,  1., -1.,  1., -1.,  1.,  1.,  1., -1.,  1.,  1.,\n",
       "        1.,  1., -1., -1., -1.,  1., -1., -1., -1.,  1., -1.,  1.,  1.,\n",
       "       -1., -1., -1., -1., -1., -1.,  1., -1.,  1., -1., -1., -1., -1.,\n",
       "       -1., -1., -1., -1., -1.,  1., -1.,  1., -1., -1.,  1., -1., -1.,\n",
       "       -1.,  1., -1., -1.,  1., -1., -1., -1.,  1., -1., -1., -1., -1.,\n",
       "       -1., -1.,  1.,  1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "       -1.,  1.,  1., -1., -1., -1.,  1.,  1., -1., -1., -1., -1., -1.,\n",
       "       -1., -1., -1., -1., -1.,  1., -1.,  1.,  1.,  1., -1., -1., -1.,\n",
       "       -1., -1., -1., -1., -1., -1.,  1.,  1., -1.,  1., -1., -1.,  1.,\n",
       "       -1., -1., -1., -1., -1., -1., -1., -1., -1.,  1.,  1.,  1., -1.,\n",
       "       -1.,  1.,  1., -1.,  1.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds_test[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cross validation su gamma e degree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def cross_validation_GD(y,tx, k_fold, max_iters, gammas):\n",
    "    \n",
    "    accuracies_test=np.zeros(len(gammas))\n",
    "    accuracies_train=np.zeros(len(gammas))\n",
    "    acc_max=0;\n",
    "    gamma_best=0;\n",
    "    \n",
    "    for i, single_gamma in enumerate(gammas):\n",
    "        \n",
    "        seed=1;\n",
    "\n",
    "        # get k'th subgroup in test, others in train\n",
    "        k_indices = build_k_indices(y, k_fold, seed)\n",
    "        accuracy_train = np.zeros(k_fold)\n",
    "        accuracy_test = np.zeros(k_fold)\n",
    "\n",
    "\n",
    "        for k in range(k_fold):\n",
    "            #print('----- FOLD', k, '-----')\n",
    "            k_index = k_indices[k]\n",
    "            test_y = y[k_index]\n",
    "            test_tx = tx[k_index,:]\n",
    "            mask = np.ones(len(y), dtype=bool) # set all elements to True\n",
    "            mask[k_index] = False              # set test elements to False\n",
    "            train_tx = tx[mask,:]              # select only True elements (ie train elements)\n",
    "            train_y = y[mask]\n",
    "            initial_w = np.zeros(train_tx.shape[1]);\n",
    "            weights,loss = logistic_regression(train_y, train_tx, initial_w, max_iters, single_gamma)\n",
    "            # Compute the predictions\n",
    "            y_pred_train = predict_labels(weights, train_tx)\n",
    "            y_pred_test = predict_labels(weights, test_tx);\n",
    "            accuracy_train[k] = np.sum(y_pred_train == train_y)/len(train_y)\n",
    "            accuracy_test[k] = np.sum(y_pred_test == test_y)/len(test_y)\n",
    "            \n",
    "        accuracies_test[i]=np.mean(accuracy_test);\n",
    "        accuracies_train[i]=np.mean(accuracy_train);\n",
    "        \n",
    "        print('GAMMA:', single_gamma, '---','ACCURANCY TEST:',accuracies_test[i], '---','ACCURANCY TRAIN:',accuracies_train[i])\n",
    "        if (accuracies_test[i]>acc_max):\n",
    "                gamma_best=gammas[i];\n",
    "                acc_max=accuracies_test[i];\n",
    "    \n",
    "    return [gamma_best,acc_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Starting Jet  0 *****\n",
      "Gradient Descent(0/499): loss=0.49999999999999784, w0=4.318881558209673e-06, w1=-0.00360114905465818\n",
      "Gradient Descent(1/499): loss=0.47984092459190764, w0=5.3194282256529834e-05, w1=-0.006710262173664215\n",
      "Gradient Descent(2/499): loss=0.4645713617677788, w0=0.00011887164466863113, w1=-0.009420834375421301\n",
      "Gradient Descent(3/499): loss=0.452604307467966, w0=0.00018513724341014546, w1=-0.011805594078871936\n",
      "Gradient Descent(4/499): loss=0.44292196291615404, w0=0.000242991769469354, w1=-0.013921787263083287\n",
      "Gradient Descent(5/499): loss=0.4348546694989052, w0=0.00028790284728711046, w1=-0.01581494114974068\n",
      "Gradient Descent(6/499): loss=0.4279533363483794, w0=0.0003180631955061308, w1=-0.0175215886442407\n",
      "Gradient Descent(7/499): loss=0.42191219070426056, w0=0.00033328939320437337, w1=-0.019071272564084807\n",
      "Gradient Descent(8/499): loss=0.41652026564905853, w0=0.00033432870998928264, w1=-0.020488043214210185\n",
      "Gradient Descent(9/499): loss=0.41163013287140016, w0=0.00032242578969605984, w1=-0.02179159388389409\n",
      "Gradient Descent(10/499): loss=0.40713738373289426, w0=0.00029905467696869317, w1=-0.02299813339317375\n",
      "Gradient Descent(11/499): loss=0.40296699353656096, w0=0.0002657558883035946, w1=-0.02412106461903506\n",
      "Gradient Descent(12/499): loss=0.3990641768564748, w0=0.00022404003938682012, w1=-0.02517151766331015\n",
      "Gradient Descent(13/499): loss=0.3953882106996055, w0=0.00017533345427780635, w1=-0.02615877256180632\n",
      "Gradient Descent(14/499): loss=0.39190823639830996, w0=0.00012095006513908331, w1=-0.02709059696669872\n",
      "Gradient Descent(15/499): loss=0.38860038942768416, w0=6.207958747630985e-05, w1=-0.027973517627508257\n",
      "Gradient Descent(16/499): loss=0.38544582506699665, w0=-2.1441377742422493e-07, w1=-0.028813039814778462\n",
      "Gradient Descent(17/499): loss=0.38242935121141464, w0=-6.499062802956367e-05, w1=-0.029613825460918668\n",
      "Gradient Descent(18/499): loss=0.3795384745147702, w0=-0.0001314228444457462, w1=-0.030379838328406187\n",
      "Gradient Descent(19/499): loss=0.37676272922116466, w0=-0.00019879115634205316, w1=-0.031114462685385473\n",
      "Gradient Descent(20/499): loss=0.3740932003079464, w0=-0.00026647252988278795, w1=-0.03182060058955996\n",
      "Gradient Descent(21/499): loss=0.37152218094025813, w0=-0.00033393135021320987, w1=-0.032500751828007136\n",
      "Gradient Descent(22/499): loss=0.3690429233507083, w0=-0.00040071030722896176, w1=-0.033157079746469385\n",
      "Gradient Descent(23/499): loss=0.3666494551694276, w0=-0.0004664218232019075, w1=-0.033791465565833706\n",
      "Gradient Descent(24/499): loss=0.3643364419781439, w0=-0.0005307401231302073, w1=-0.03440555328239556\n",
      "Gradient Descent(25/499): loss=0.36209908280821745, w0=-0.0005933939851676169, w1=-0.035000786850541234\n",
      "Gradient Descent(26/499): loss=0.3599330293582301, w0=-0.00065416016940691, w1=-0.03557844102842575\n",
      "Gradient Descent(27/499): loss=0.3578343224834033, w0=-0.0007128574999796127, w1=-0.0361396470116824\n",
      "Gradient Descent(28/499): loss=0.3557993414181879, w0=-0.0007693415622736544, w1=-0.036685413773993654\n",
      "Gradient Descent(29/499): loss=0.3538247625120923, w0=-0.0008234999704141325, w1=-0.037216645866360296\n",
      "Gradient Descent(30/499): loss=0.35190752517451485, w0=-0.0008752481576247434, w1=-0.03773415829127094\n",
      "Gradient Descent(31/499): loss=0.35004480336386573, w0=-0.0009245256421388724, w1=-0.038238688957539575\n",
      "Gradient Descent(32/499): loss=0.34823398140575135, w0=-0.0009712927229479899, w1=-0.0387309091314789\n",
      "Gradient Descent(33/499): loss=0.3464726332431386, w0=-0.0010155275622004663, w1=-0.03921143222644105\n",
      "Gradient Descent(34/499): loss=0.34475850444831135, w0=-0.0010572236140666083, w1=-0.03968082121248685\n",
      "Gradient Descent(35/499): loss=0.3430894964896054, w0=-0.00109638736308792, w1=-0.04013959487854986\n",
      "Gradient Descent(36/499): loss=0.34146365286428326, w0=-0.0011330363382531664, w1=-0.04058823313893288\n",
      "Gradient Descent(37/499): loss=0.33987914679564685, w0=-0.0011671973721801854, w1=-0.0410271815426851\n",
      "Gradient Descent(38/499): loss=0.3383342702566391, w0=-0.0011989050777642268, w1=-0.04145685511703735\n",
      "Gradient Descent(39/499): loss=0.3368274241301841, w0=-0.0012282005174438513, w1=-0.04187764165354774\n",
      "Gradient Descent(40/499): loss=0.33535710935280727, w0=-0.0012551300428165146, w1=-0.042289904527054646\n",
      "Gradient Descent(41/499): loss=0.3339219189158373, w0=-0.001279744284702902, w1=-0.04269398512223553\n",
      "Gradient Descent(42/499): loss=0.33252053061998127, w0=-0.001302097275915164, w1=-0.04309020492994461\n",
      "Gradient Descent(43/499): loss=0.3311517004958944, w0=-0.0013222456909377587, w1=-0.0434788673650738\n",
      "Gradient Descent(44/499): loss=0.3298142568167149, w0=-0.001340248188492087, w1=-0.04386025934905895\n",
      "Gradient Descent(45/499): loss=0.32850709463922445, w0=-0.001356164844540571, w1=-0.04423465269301679\n",
      "Gradient Descent(46/499): loss=0.32722917081898983, w0=-0.0013700566647060145, w1=-0.04460230531158681\n",
      "Gradient Descent(47/499): loss=0.32597949945196203, w0=-0.0013819851663518941, w1=-0.04496346229264871\n",
      "Gradient Descent(48/499): loss=0.32475714770091757, w0=-0.0013920120217020702, w1=-0.045318356844016326\n",
      "Gradient Descent(49/499): loss=0.32356123197008335, w0=-0.00140019875438724, w1=-0.04566721113482548\n",
      "Gradient Descent(50/499): loss=0.3223909143954705, w0=-0.0014066064827023407, w1=-0.04601023704651822\n",
      "Gradient Descent(51/499): loss=0.3212453996220214, w0=-0.0014112957036553563, w1=-0.04634763684598059\n",
      "Gradient Descent(52/499): loss=0.32012393184174937, w0=-0.0014143261125939776, w1=-0.0466796037914346\n",
      "Gradient Descent(53/499): loss=0.31902579206971005, w0=-0.0014157564538218226, w1=-0.04700632268005092\n",
      "Gradient Descent(54/499): loss=0.31795029563697896, w0=-0.0014156443981691268, w1=-0.047327970344882\n",
      "Gradient Descent(55/499): loss=0.3168967898818458, w0=-0.0014140464439717996, w1=-0.04764471610757038\n",
      "Gradient Descent(56/499): loss=0.31586465202223396, w0=-0.001411017838344628, w1=-0.04795672219232689\n",
      "Gradient Descent(57/499): loss=0.31485328719396516, w0=-0.0014066125160155463, w1=-0.0482641441058664\n",
      "Gradient Descent(58/499): loss=0.31386212664089297, w0=-0.0014008830533240365, w1=-0.04856713098731024\n",
      "Gradient Descent(59/499): loss=0.3128906260442187, w0=-0.0013938806352829817, w1=-0.04886582593149222\n",
      "Gradient Descent(60/499): loss=0.3119382639794307, w0=-0.001385655033864242, w1=-0.049160366288622345\n",
      "Gradient Descent(61/499): loss=0.3110045404903401, w0=-0.001376254595897958, w1=-0.04945088394285401\n",
      "Gradient Descent(62/499): loss=0.310088975770609, w0=-0.0013657262391777193, w1=-0.04973750557195443\n",
      "Gradient Descent(63/499): loss=0.3091911089440016, w0=-0.0013541154555415011, w1=-0.050020352889984666\n",
      "Gradient Descent(64/499): loss=0.30831049693534657, w0=-0.0013414663198545395, w1=-0.05029954287464593\n",
      "Gradient Descent(65/499): loss=0.3074467134248826, w0=-0.001327821503957614, w1=-0.050575187980736194\n",
      "Gradient Descent(66/499): loss=0.3065993478792849, w0=-0.0013132222947647937, w1=-0.0508473963409798\n",
      "Gradient Descent(67/499): loss=0.30576800465323384, w0=-0.0012977086158005543, w1=-0.05111627195533738\n",
      "Gradient Descent(68/499): loss=0.3049523021559035, w0=-0.0012813190515590656, w1=-0.051381914869770416\n",
      "Gradient Descent(69/499): loss=0.30415187207721905, w0=-0.0012640908741499222, w1=-0.05164442134532061\n",
      "Gradient Descent(70/499): loss=0.3033663586691582, w0=-0.0012460600717660144, w1=-0.05190388401826568\n",
      "Gradient Descent(71/499): loss=0.3025954180777627, w0=-0.0012272613785718305, w1=-0.052160392052028506\n",
      "Gradient Descent(72/499): loss=0.3018387177218892, w0=-0.0012077283056653118, w1=-0.052414031281443035\n",
      "Gradient Descent(73/499): loss=0.30109593571504445, w0=-0.0011874931728143735, w1=-0.05266488434991688\n",
      "Gradient Descent(74/499): loss=0.30036676032696047, w0=-0.0011665871407112016, w1=-0.05291303083997513\n",
      "Gradient Descent(75/499): loss=0.29965088948182794, w0=-0.001145040243524167, w1=-0.05315854739762189\n",
      "Gradient Descent(76/499): loss=0.2989480302903659, w0=-0.0011228814215592917, w1=-0.05340150785091388\n",
      "Gradient Descent(77/499): loss=0.29825789861312546, w0=-0.001100138553871237, w1=-0.053641983323103805\n",
      "Gradient Descent(78/499): loss=0.29758021865264406, w0=-0.0010768384906882451, w1=-0.05388004234067859\n",
      "Gradient Descent(79/499): loss=0.2969147225722525, w0=-0.0010530070855368074, w1=-0.05411575093658951\n",
      "Gradient Descent(80/499): loss=0.2962611501395158, w0=-0.001028669226970411, w1=-0.054349172748945754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(81/499): loss=0.29561924839245224, w0=-0.001003848869822909, w1=-0.054580369115421\n",
      "Gradient Descent(82/499): loss=0.2949887713268207, w0=-0.000978569065921109, w1=-0.05480939916360275\n",
      "Gradient Descent(83/499): loss=0.29436947960290394, w0=-0.0009528519942034092, w1=-0.0550363198974965\n",
      "Gradient Descent(84/499): loss=0.29376114027033845, w0=-0.000926718990201887, w1=-0.05526118628038136\n",
      "Gradient Descent(85/499): loss=0.2931635265096622, w0=-0.0009001905748544322, w1=-0.05548405131419941\n",
      "Gradient Descent(86/499): loss=0.2925764173893466, w0=-0.0008732864826214343, w1=-0.05570496611564868\n",
      "Gradient Descent(87/499): loss=0.29199959763718736, w0=-0.0008460256888883737, w1=-0.05592397998913814\n",
      "Gradient Descent(88/499): loss=0.2914328574250079, w0=-0.0008184264366415564, w1=-0.05614114049675281\n",
      "Gradient Descent(89/499): loss=0.29087599216571736, w0=-0.0007905062624092724, w1=-0.05635649352536787\n",
      "Gradient Descent(90/499): loss=0.2903288023218375, w0=-0.0007622820214649892, w1=-0.05657008335104199\n",
      "Gradient Descent(91/499): loss=0.28979109322468, w0=-0.0007337699122928708, w1=-0.056781952700812675\n",
      "Gradient Descent(92/499): loss=0.28926267490342555, w0=-0.0007049855003190451, w1=-0.056992142812009004\n",
      "Gradient Descent(93/499): loss=0.2887433619234035, w0=-0.0006759437409146823, w1=-0.057200693489190886\n",
      "Gradient Descent(94/499): loss=0.28823297323293895, w0=-0.0006466590016791697, w1=-0.057407643158817995\n",
      "Gradient Descent(95/499): loss=0.28773133201816814, w0=-0.0006171450840135227, w1=-0.05761302892174593\n",
      "Gradient Descent(96/499): loss=0.28723826556528065, w0=-0.0005874152439956953, w1=-0.057816886603642155\n",
      "Gradient Descent(97/499): loss=0.2867536051296824, w0=-0.0005574822125707155, w1=-0.05801925080340953\n",
      "Gradient Descent(98/499): loss=0.28627718581161155, w0=-0.0005273582150695738, w1=-0.058220154939700924\n",
      "Gradient Descent(99/499): loss=0.28580884643778115, w0=-0.0004970549900715961, w1=-0.05841963129560421\n",
      "Gradient Descent(100/499): loss=0.2853484294486437, w0=-0.0004665838076256501, w1=-0.058617711061573236\n",
      "Gradient Descent(101/499): loss=0.2848957807909164, w0=-0.0004359554868459999, w1=-0.05881442437667683\n",
      "Gradient Descent(102/499): loss=0.2844507498150214, w0=-0.0004051804128989418, w1=-0.05900980036823442\n",
      "Gradient Descent(103/499): loss=0.2840131891771285, w0=-0.00037426855339657144, w1=-0.05920386718990383\n",
      "Gradient Descent(104/499): loss=0.28358295474550804, w0=-0.00034322947421413824, w1=-0.05939665205828388\n",
      "Gradient Descent(105/499): loss=0.283159905510923, w0=-0.0003120723547474691, w1=-0.05958818128809156\n",
      "Gradient Descent(106/499): loss=0.282743903500813, w0=-0.000280806002626893, w1=-0.05977848032597097\n",
      "Gradient Descent(107/499): loss=0.28233481369703406, w0=-0.0002494388679039914, w1=-0.059967573782988684\n",
      "Gradient Descent(108/499): loss=0.2819325039569434, w0=-0.00021797905672733326, w1=-0.060155485465868\n",
      "Gradient Descent(109/499): loss=0.2815368449376283, w0=-0.00018643434452314913, w1=-0.06034223840701204\n",
      "Gradient Descent(110/499): loss=0.28114771002309275, w0=-0.00015481218869665985, w1=-0.06052785489336392\n",
      "Gradient Descent(111/499): loss=0.28076497525423283, w0=-0.00012311974086949505, w1=-0.06071235649414975\n",
      "Gradient Descent(112/499): loss=0.28038851926144054, w0=-9.136385866834984e-05, w1=-0.06089576408754883\n",
      "Gradient Descent(113/499): loss=0.28001822319968767, w0=-5.955111707970429e-05, w1=-0.06107809788633303\n",
      "Gradient Descent(114/499): loss=0.27965397068595205, w0=-2.768781938510503e-05, w1=-0.061259377462516196\n",
      "Gradient Descent(115/499): loss=0.27929564773885895, w0=4.219992308842438e-06, w1=-0.061439621771052166\n",
      "Gradient Descent(116/499): loss=0.27894314272041787, w0=3.6166526931965765e-05, w1=-0.061618849172619\n",
      "Gradient Descent(117/499): loss=0.27859634627974506, w0=6.814623469005897e-05, w1=-0.061797077455525\n",
      "Gradient Descent(118/499): loss=0.27825515129866696, w0=0.00010015379766937721, w1=-0.06197432385677101\n",
      "Gradient Descent(119/499): loss=0.27791945283910907, w0=0.00013218412079337095, w1=-0.06215060508230199\n",
      "Gradient Descent(120/499): loss=0.2775891480921812, w0=0.00016423232311930453, w1=-0.0623259373264794\n",
      "Gradient Descent(121/499): loss=0.27726413632887426, w0=0.00019629372946276222, w1=-0.06250033629080502\n",
      "Gradient Descent(122/499): loss=0.2769443188522911, w0=0.00022836386233840417, w1=-0.06267381720192522\n",
      "Gradient Descent(123/499): loss=0.2766295989513388, w0=0.0002604384342056925, w1=-0.06284639482894382\n",
      "Gradient Descent(124/499): loss=0.27631988185581224, w0=0.00029251334000864916, w1=-0.06301808350007045\n",
      "Gradient Descent(125/499): loss=0.2760150746928074, w0=0.0003245846499990613, w1=-0.06318889711863032\n",
      "Gradient Descent(126/499): loss=0.27571508644440346, w0=0.00035664860283287763, w1=-0.06335884917846024\n",
      "Gradient Descent(127/499): loss=0.27541982790655706, w0=0.0003887015989298825, w1=-0.06352795277871466\n",
      "Gradient Descent(128/499): loss=0.2751292116491579, w0=0.0004207401940870478, w1=-0.06369622063810493\n",
      "Gradient Descent(129/499): loss=0.27484315197719394, w0=0.00045276109333628503, w1=-0.06386366510859354\n",
      "Gradient Descent(130/499): loss=0.2745615648929831, w0=0.00048476114503763544, w1=-0.06403029818856473\n",
      "Gradient Descent(131/499): loss=0.2742843680594239, w0=0.0005167373351992223, w1=-0.0641961315354917\n",
      "Gradient Descent(132/499): loss=0.2740114807642287, w0=0.0005486867820156048, w1=-0.06436117647811997\n",
      "Gradient Descent(133/499): loss=0.27374282388509635, w0=0.0005806067306164405, w1=-0.06452544402818586\n",
      "Gradient Descent(134/499): loss=0.2734783198557925, w0=0.0006124945480176548, w1=-0.06468894489168785\n",
      "Gradient Descent(135/499): loss=0.27321789263309854, w0=0.0006443477182675796, w1=-0.06485168947972844\n",
      "Gradient Descent(136/499): loss=0.272961467664602, w0=0.000676163837780789, w1=-0.06501368791894306\n",
      "Gradient Descent(137/499): loss=0.2727089718572936, w0=0.0007079406108526183, w1=-0.06517495006153204\n",
      "Gradient Descent(138/499): loss=0.2724603335469439, w0=0.0007396758453475905, w1=-0.06533548549491111\n",
      "Gradient Descent(139/499): loss=0.272215482468233, w0=0.000771367448555225, w1=-0.06549530355099524\n",
      "Gradient Descent(140/499): loss=0.27197434972560547, w0=0.0008030134232069317, w1=-0.06565441331512989\n",
      "Gradient Descent(141/499): loss=0.2717368677648284, w0=0.0008346118636479149, w1=-0.06581282363468356\n",
      "Gradient Descent(142/499): loss=0.27150297034522697, w0=0.0008661609521582372, w1=-0.06597054312731462\n",
      "Gradient Descent(143/499): loss=0.27127259251257596, w0=0.0008976589554173976, w1=-0.06612758018892513\n",
      "Gradient Descent(144/499): loss=0.27104567057262896, w0=0.000929104221106987, w1=-0.06628394300131368\n",
      "Gradient Descent(145/499): loss=0.270822142065261, w0=0.0009604951746461781, w1=-0.06643963953953913\n",
      "Gradient Descent(146/499): loss=0.27060194573920804, w0=0.0009918303160549985, w1=-0.06659467757900613\n",
      "Gradient Descent(147/499): loss=0.2703850215273859, w0=0.0010231082169405254, w1=-0.06674906470228349\n",
      "Gradient Descent(148/499): loss=0.2701713105227687, w0=0.0010543275176013072, w1=-0.0669028083056657\n",
      "Gradient Descent(149/499): loss=0.2699607549548134, w0=0.001085486924245501, w1=-0.06705591560548727\n",
      "Gradient Descent(150/499): loss=0.26975329816641186, w0=0.0011165852063183752, w1=-0.0672083936441999\n",
      "Gradient Descent(151/499): loss=0.26954888459135706, w0=0.0011476211939349882, w1=-0.06736024929622132\n",
      "Gradient Descent(152/499): loss=0.2693474597323092, w0=0.0011785937754140079, w1=-0.06751148927356476\n",
      "Gradient Descent(153/499): loss=0.2691489701392457, w0=0.001209501894908789, w1=-0.06766212013125755\n",
      "Gradient Descent(154/499): loss=0.26895336338838494, w0=0.0012403445501319688, w1=-0.067812148272557\n",
      "Gradient Descent(155/499): loss=0.2687605880615681, w0=0.0012711207901699771, w1=-0.06796157995397131\n",
      "Gradient Descent(156/499): loss=0.2685705937260894, w0=0.001301829713383999, w1=-0.0681104212900933\n",
      "Gradient Descent(157/499): loss=0.26838333091496, w0=0.0013324704653940476, w1=-0.0682586782582538\n",
      "Gradient Descent(158/499): loss=0.26819875110759844, w0=0.0013630422371429403, w1=-0.06840635670300199\n",
      "Gradient Descent(159/499): loss=0.26801680671093153, w0=0.001393544263037084, w1=-0.06855346234041935\n",
      "Gradient Descent(160/499): loss=0.26783745104090034, w0=0.0014239758191610934, w1=-0.0687000007622734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(161/499): loss=0.26766063830435716, w0=0.001454336221563384, w1=-0.06884597744001768\n",
      "Gradient Descent(162/499): loss=0.26748632358134644, w0=0.0014846248246099777, w1=-0.06899139772864381\n",
      "Gradient Descent(163/499): loss=0.26731446280775817, w0=0.0015148410194038757, w1=-0.06913626687039127\n",
      "Gradient Descent(164/499): loss=0.2671450127583464, w0=0.0015449842322674445, w1=-0.06928058999832049\n",
      "Gradient Descent(165/499): loss=0.26697793103010325, w0=0.0015750539232853632, w1=-0.06942437213975439\n",
      "Gradient Descent(166/499): loss=0.26681317602597954, w0=0.0016050495849057663, w1=-0.06956761821959362\n",
      "Gradient Descent(167/499): loss=0.26665070693894444, w0=0.0016349707405973188, w1=-0.06971033306351018\n",
      "Gradient Descent(168/499): loss=0.2664904837363764, w0=0.0016648169435600298, w1=-0.0698525214010242\n",
      "Gradient Descent(169/499): loss=0.26633246714477676, w0=0.0016945877754877088, w1=-0.06999418786846837\n",
      "Gradient Descent(170/499): loss=0.2661766186347989, w0=0.0017242828453800402, w1=-0.07013533701184427\n",
      "Gradient Descent(171/499): loss=0.26602290040658655, w0=0.001753901788402334, w1=-0.07027597328957484\n",
      "Gradient Descent(172/499): loss=0.2658712753754129, w0=0.0017834442647910805, w1=-0.07041610107515694\n",
      "Gradient Descent(173/499): loss=0.26572170715761445, w0=0.0018129099588035107, w1=-0.07055572465971784\n",
      "Gradient Descent(174/499): loss=0.265574160056813, w0=0.0018422985777094359, w1=-0.07069484825447925\n",
      "Gradient Descent(175/499): loss=0.2654285990504186, w0=0.0018716098508236992, w1=-0.07083347599313262\n",
      "Gradient Descent(176/499): loss=0.265284989776408, w0=0.0019008435285776432, w1=-0.0709716119341289\n",
      "Gradient Descent(177/499): loss=0.2651432985203729, w0=0.0019299993816280555, w1=-0.07110926006288613\n",
      "Gradient Descent(178/499): loss=0.2650034922028298, w0=0.0019590772000021104, w1=-0.07124642429391806\n",
      "Gradient Descent(179/499): loss=0.2648655383667895, w0=0.0019880767922768917, w1=-0.07138310847288669\n",
      "Gradient Descent(180/499): loss=0.2647294051655758, w0=0.0020169979847921233, w1=-0.07151931637858175\n",
      "Gradient Descent(181/499): loss=0.2645950613508936, w0=0.002045840620894799, w1=-0.07165505172482982\n",
      "Gradient Descent(182/499): loss=0.2644624762611348, w0=0.0020746045602144454, w1=-0.07179031816233584\n",
      "Gradient Descent(183/499): loss=0.2643316198099231, w0=0.002103289677967807, w1=-0.07192511928045957\n",
      "Gradient Descent(184/499): loss=0.2642024624748872, w0=0.0021318958642917805, w1=-0.07205945860892939\n",
      "Gradient Descent(185/499): loss=0.2640749752866614, w0=0.0021604230236034853, w1=-0.072193339619496\n",
      "Gradient Descent(186/499): loss=0.26394912981810664, w0=0.002188871073986385, w1=-0.07232676572752819\n",
      "Gradient Descent(187/499): loss=0.2638248981737481, w0=0.002217239946601424, w1=-0.07245974029355277\n",
      "Gradient Descent(188/499): loss=0.26370225297942396, w0=0.0022455295851221916, w1=-0.07259226662474105\n",
      "Gradient Descent(189/499): loss=0.2635811673721433, w0=0.002273739945193143, w1=-0.07272434797634367\n",
      "Gradient Descent(190/499): loss=0.2634616149901446, w0=0.0023018709939099688, w1=-0.0728559875530758\n",
      "Gradient Descent(191/499): loss=0.2633435699631549, w0=0.002329922709321225, w1=-0.07298718851045466\n",
      "Gradient Descent(192/499): loss=0.2632270069028427, w0=0.002357895079950372, w1=-0.07311795395609101\n",
      "Gradient Descent(193/499): loss=0.2631119008934612, w0=0.0023857881043374083, w1=-0.07324828695093648\n",
      "Gradient Descent(194/499): loss=0.26299822748267865, w0=0.0024136017905993182, w1=-0.07337819051048838\n",
      "Gradient Descent(195/499): loss=0.2628859626725917, w0=0.0024413361560085708, w1=-0.07350766760595352\n",
      "Gradient Descent(196/499): loss=0.26277508291091634, w0=0.002468991226588957, w1=-0.07363672116537263\n",
      "Gradient Descent(197/499): loss=0.26266556508235533, w0=0.002496567036728062, w1=-0.07376535407470691\n",
      "Gradient Descent(198/499): loss=0.26255738650013705, w0=0.0025240636288057037, w1=-0.07389356917888801\n",
      "Gradient Descent(199/499): loss=0.2624505248977219, w0=0.002551481052837705, w1=-0.0740213692828329\n",
      "Gradient Descent(200/499): loss=0.2623449584206735, w0=0.0025788193661343685, w1=-0.07414875715242497\n",
      "Gradient Descent(201/499): loss=0.262240665618692, w0=0.002606078632973074, w1=-0.07427573551546247\n",
      "Gradient Descent(202/499): loss=0.26213762543780406, w0=0.002633258924284423, w1=-0.0744023070625757\n",
      "Gradient Descent(203/499): loss=0.2620358172127087, w0=0.0026603603173513857, w1=-0.07452847444811392\n",
      "Gradient Descent(204/499): loss=0.2619352206592733, w0=0.0026873828955209216, w1=-0.07465424029100334\n",
      "Gradient Descent(205/499): loss=0.2618358158671802, w0=0.0027143267479275775, w1=-0.07477960717557704\n",
      "Gradient Descent(206/499): loss=0.2617375832927169, w0=0.002741191969228568, w1=-0.0749045776523779\n",
      "Gradient Descent(207/499): loss=0.26164050375170916, w0=0.002767978659349883, w1=-0.07502915423893575\n",
      "Gradient Descent(208/499): loss=0.26154455841259505, w0=0.0027946869232429654, w1=-0.07515333942051933\n",
      "Gradient Descent(209/499): loss=0.2614497287896338, w0=0.002821316870651543, w1=-0.07527713565086436\n",
      "Gradient Descent(210/499): loss=0.26135599673625004, w0=0.002847868615888187, w1=-0.07540054535287834\n",
      "Gradient Descent(211/499): loss=0.26126334443850885, w0=0.002874342277620214, w1=-0.075523570919323\n",
      "Gradient Descent(212/499): loss=0.2611717544087197, w0=0.002900737978664545, w1=-0.07564621471347535\n",
      "Gradient Descent(213/499): loss=0.2610812094791666, w0=0.002927055845791157, w1=-0.07576847906976786\n",
      "Gradient Descent(214/499): loss=0.2609916927959604, w0=0.0029532960095347803, w1=-0.07589036629440876\n",
      "Gradient Descent(215/499): loss=0.2609031878130149, w0=0.0029794586040144994, w1=-0.07601187866598313\n",
      "Gradient Descent(216/499): loss=0.2608156782861383, w0=0.0030055437667609406, w1=-0.07613301843603536\n",
      "Gradient Descent(217/499): loss=0.26072914826724375, w0=0.0030315516385507313, w1=-0.0762537878296338\n",
      "Gradient Descent(218/499): loss=0.2606435820986712, w0=0.0030574823632479363, w1=-0.0763741890459182\n",
      "Gradient Descent(219/499): loss=0.2605589644076236, w0=0.003083336087652183, w1=-0.07649422425863048\n",
      "Gradient Descent(220/499): loss=0.2604752801007095, w0=0.003109112961353202, w1=-0.07661389561662961\n",
      "Gradient Descent(221/499): loss=0.26039251435859584, w0=0.003134813136591522, w1=-0.07673320524439098\n",
      "Gradient Descent(222/499): loss=0.26031065263076225, w0=0.00316043676812506, w1=-0.0768521552424909\n",
      "Gradient Descent(223/499): loss=0.26022968063036206, w0=0.0031859840131013713, w1=-0.0769707476880768\n",
      "Gradient Descent(224/499): loss=0.2601495843291816, w0=0.0032114550309353227, w1=-0.0770889846353236\n",
      "Gradient Descent(225/499): loss=0.26007034995269884, w0=0.0032368499831919636, w1=-0.07720686811587672\n",
      "Gradient Descent(226/499): loss=0.25999196397523905, w0=0.0032621690334743826, w1=-0.07732440013928223\n",
      "Gradient Descent(227/499): loss=0.2599144131152261, w0=0.0032874123473163445, w1=-0.07744158269340459\n",
      "Gradient Descent(228/499): loss=0.2598376843305255, w0=0.003312580092079509, w1=-0.07755841774483248\n",
      "Gradient Descent(229/499): loss=0.2597617648138796, w0=0.003337672436855038, w1=-0.07767490723927296\n",
      "Gradient Descent(230/499): loss=0.25968664198843133, w0=0.0033626895523694166, w1=-0.07779105310193463\n",
      "Gradient Descent(231/499): loss=0.2596123035033368, w0=0.003387631610894305, w1=-0.0779068572378999\n",
      "Gradient Descent(232/499): loss=0.25953873722946186, w0=0.003412498786160261, w1=-0.07802232153248705\n",
      "Gradient Descent(233/499): loss=0.25946593125516487, w0=0.0034372912532741654, w1=-0.07813744785160204\n",
      "Gradient Descent(234/499): loss=0.25939387388215995, w0=0.0034620091886402023, w1=-0.0782522380420809\n",
      "Gradient Descent(235/499): loss=0.2593225536214629, w0=0.0034866527698842417, w1=-0.07836669393202261\n",
      "Gradient Descent(236/499): loss=0.25925195918941424, w0=0.0035112221757814827, w1=-0.07848081733111308\n",
      "Gradient Descent(237/499): loss=0.2591820795037821, w0=0.003535717586187224, w1=-0.07859461003094027\n",
      "Gradient Descent(238/499): loss=0.25911290367993967, w0=0.0035601391819706277, w1=-0.07870807380530115\n",
      "Gradient Descent(239/499): loss=0.25904442102711767, w0=0.0035844871449513516, w1=-0.07882121041050025\n",
      "Gradient Descent(240/499): loss=0.2589766210447298, w0=0.003608761657838932, w1=-0.07893402158564061\n",
      "Gradient Descent(241/499): loss=0.25890949341877006, w0=0.003632962904174797, w1=-0.07904650905290704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(242/499): loss=0.2588430280182805, w0=0.0036570910682768043, w1=-0.079158674517842\n",
      "Gradient Descent(243/499): loss=0.2587772148918869, w0=0.0036811463351861936, w1=-0.07927051966961458\n",
      "Gradient Descent(244/499): loss=0.258712044264403, w0=0.0037051288906168552, w1=-0.07938204618128243\n",
      "Gradient Descent(245/499): loss=0.2586475065334996, w0=0.003729038920906813, w1=-0.07949325571004723\n",
      "Gradient Descent(246/499): loss=0.25858359226644057, w0=0.003752876612971831, w1=-0.07960414989750372\n",
      "Gradient Descent(247/499): loss=0.25852029219687933, w0=0.0037766421542610533, w1=-0.07971473036988251\n",
      "Gradient Descent(248/499): loss=0.2584575972217208, w0=0.003800335732714591, w1=-0.07982499873828697\n",
      "Gradient Descent(249/499): loss=0.25839549839804254, w0=0.003823957536722971, w1=-0.0799349565989243\n",
      "Gradient Descent(250/499): loss=0.25833398694007503, w0=0.0038475077550883716, w1=-0.08004460553333101\n",
      "Gradient Descent(251/499): loss=0.25827305421624375, w0=0.0038709865769875683, w1=-0.080153947108593\n",
      "Gradient Descent(252/499): loss=0.2582126917462645, w0=0.0038943941919365116, w1=-0.0802629828775604\n",
      "Gradient Descent(253/499): loss=0.2581528911982986, w0=0.003917730789756474, w1=-0.08037171437905731\n",
      "Gradient Descent(254/499): loss=0.258093644386162, w0=0.003940996560541695, w1=-0.08048014313808668\n",
      "Gradient Descent(255/499): loss=0.2580349432665878, w0=0.003964191694628461, w1=-0.08058827066603043\n",
      "Gradient Descent(256/499): loss=0.2579767799365432, w0=0.003987316382565562, w1=-0.08069609846084491\n",
      "Gradient Descent(257/499): loss=0.2579191466305977, w0=0.004010370815086054, w1=-0.08080362800725199\n",
      "Gradient Descent(258/499): loss=0.2578620357183425, w0=0.004033355183080293, w1=-0.08091086077692579\n",
      "Gradient Descent(259/499): loss=0.25780543970185965, w0=0.004056269677570156, w1=-0.08101779822867522\n",
      "Gradient Descent(260/499): loss=0.2577493512132411, w0=0.00407911448968443, w1=-0.0811244418086225\n",
      "Gradient Descent(261/499): loss=0.2576937630121543, w0=0.004101889810635293, w1=-0.08123079295037772\n",
      "Gradient Descent(262/499): loss=0.2576386679834567, w0=0.004124595831695852, w1=-0.08133685307520963\n",
      "Gradient Descent(263/499): loss=0.2575840591348546, w0=0.004147232744178689, w1=-0.08144262359221273\n",
      "Gradient Descent(264/499): loss=0.25752992959460863, w0=0.004169800739415374, w1=-0.08154810589847075\n",
      "Gradient Descent(265/499): loss=0.2574762726092833, w0=0.0041923000087369, w1=-0.08165330137921673\n",
      "Gradient Descent(266/499): loss=0.25742308154153937, w0=0.004214730743454999, w1=-0.08175821140798964\n",
      "Gradient Descent(267/499): loss=0.25737034986796975, w0=0.004237093134844305, w1=-0.08186283734678786\n",
      "Gradient Descent(268/499): loss=0.25731807117697686, w0=0.004259387374125322, w1=-0.08196718054621933\n",
      "Gradient Descent(269/499): loss=0.2572662391666906, w0=0.004281613652448165, w1=-0.08207124234564883\n",
      "Gradient Descent(270/499): loss=0.25721484764292685, w0=0.004303772160877041, w1=-0.08217502407334207\n",
      "Gradient Descent(271/499): loss=0.2571638905171861, w0=0.0043258630903754295, w1=-0.08227852704660703\n",
      "Gradient Descent(272/499): loss=0.2571133618046888, w0=0.0043478866317919465, w1=-0.08238175257193242\n",
      "Gradient Descent(273/499): loss=0.25706325562245047, w0=0.004369842975846847, w1=-0.08248470194512346\n",
      "Gradient Descent(274/499): loss=0.25701356618739296, w0=0.0043917323131191476, w1=-0.0825873764514349\n",
      "Gradient Descent(275/499): loss=0.2569642878144925, w0=0.004413554834034333, w1=-0.08268977736570157\n",
      "Gradient Descent(276/499): loss=0.25691541491496195, w0=0.004435310728852636, w1=-0.08279190595246644\n",
      "Gradient Descent(277/499): loss=0.2568669419944717, w0=0.0044570001876578415, w1=-0.0828937634661061\n",
      "Gradient Descent(278/499): loss=0.25681886365139983, w0=0.00447862340034662, w1=-0.082995351150954\n",
      "Gradient Descent(279/499): loss=0.2567711745751201, w0=0.004500180556618338, w1=-0.08309667024142137\n",
      "Gradient Descent(280/499): loss=0.2567238695443205, w0=0.00452167184596535, w1=-0.08319772196211586\n",
      "Gradient Descent(281/499): loss=0.2566769434253551, w0=0.004543097457663732, w1=-0.08329850752795806\n",
      "Gradient Descent(282/499): loss=0.2566303911706277, w0=0.004564457580764442, w1=-0.0833990281442958\n",
      "Gradient Descent(283/499): loss=0.2565842078170052, w0=0.004585752404084898, w1=-0.0834992850070165\n",
      "Gradient Descent(284/499): loss=0.256538388484263, w0=0.004606982116200938, w1=-0.0835992793026575\n",
      "Gradient Descent(285/499): loss=0.25649292837355964, w0=0.004628146905439158, w1=-0.08369901220851429\n",
      "Gradient Descent(286/499): loss=0.25644782276594036, w0=0.004649246959869606, w1=-0.08379848489274709\n",
      "Gradient Descent(287/499): loss=0.2564030670208699, w0=0.004670282467298812, w1=-0.08389769851448534\n",
      "Gradient Descent(288/499): loss=0.25635865657479295, w0=0.004691253615263151, w1=-0.08399665422393049\n",
      "Gradient Descent(289/499): loss=0.2563145869397226, w0=0.004712160591022508, w1=-0.0840953531624571\n",
      "Gradient Descent(290/499): loss=0.25627085370185493, w0=0.004733003581554244, w1=-0.08419379646271206\n",
      "Gradient Descent(291/499): loss=0.2562274525202123, w0=0.0047537827735474455, w1=-0.08429198524871229\n",
      "Gradient Descent(292/499): loss=0.256184379125309, w0=0.004774498353397437, w1=-0.0843899206359407\n",
      "Gradient Descent(293/499): loss=0.2561416293178468, w0=0.004795150507200557, w1=-0.08448760373144068\n",
      "Gradient Descent(294/499): loss=0.25609919896743116, w0=0.0048157394207491815, w1=-0.08458503563390893\n",
      "Gradient Descent(295/499): loss=0.2560570840113152, w0=0.004836265279526974, w1=-0.08468221743378684\n",
      "Gradient Descent(296/499): loss=0.25601528045316585, w0=0.0048567282687043654, w1=-0.0847791502133504\n",
      "Gradient Descent(297/499): loss=0.25597378436185453, w0=0.004877128573134246, w1=-0.08487583504679862\n",
      "Gradient Descent(298/499): loss=0.2559325918702702, w0=0.004897466377347863, w1=-0.08497227300034059\n",
      "Gradient Descent(299/499): loss=0.2558916991741553, w0=0.004917741865550902, w1=-0.08506846513228118\n",
      "Gradient Descent(300/499): loss=0.2558511025309642, w0=0.004937955221619772, w1=-0.08516441249310538\n",
      "Gradient Descent(301/499): loss=0.255810798258743, w0=0.004958106629098048, w1=-0.08526011612556134\n",
      "Gradient Descent(302/499): loss=0.2557707827350312, w0=0.0049781962711930965, w1=-0.08535557706474209\n",
      "Gradient Descent(303/499): loss=0.255731052395783, w0=0.004998224330772851, w1=-0.0854507963381661\n",
      "Gradient Descent(304/499): loss=0.2556916037343109, w0=0.00501819099036275, w1=-0.08554577496585657\n",
      "Gradient Descent(305/499): loss=0.25565243330024845, w0=0.005038096432142813, w1=-0.08564051396041951\n",
      "Gradient Descent(306/499): loss=0.25561353769853223, w0=0.005057940837944862, w1=-0.08573501432712069\n",
      "Gradient Descent(307/499): loss=0.25557491358840423, w0=0.005077724389249873, w1=-0.08582927706396146\n",
      "Gradient Descent(308/499): loss=0.25553655768243255, w0=0.005097447267185452, w1=-0.0859233031617534\n",
      "Gradient Descent(309/499): loss=0.2554984667455509, w0=0.005117109652523437, w1=-0.08601709360419195\n",
      "Gradient Descent(310/499): loss=0.25546063759411636, w0=0.005136711725677608, w1=-0.08611064936792896\n",
      "Gradient Descent(311/499): loss=0.2554230670949846, w0=0.005156253666701515, w1=-0.08620397142264413\n",
      "Gradient Descent(312/499): loss=0.2553857521646031, w0=0.005175735655286404, w1=-0.08629706073111554\n",
      "Gradient Descent(313/499): loss=0.2553486897681221, w0=0.005195157870759245, w1=-0.08638991824928904\n",
      "Gradient Descent(314/499): loss=0.25531187691852, w0=0.005214520492080854, w1=-0.08648254492634687\n",
      "Gradient Descent(315/499): loss=0.2552753106757492, w0=0.0052338236978441074, w1=-0.08657494170477507\n",
      "Gradient Descent(316/499): loss=0.25523898814589424, w0=0.0052530676662722415, w1=-0.08666710952043018\n",
      "Gradient Descent(317/499): loss=0.25520290648034816, w0=0.005272252575217233, w1=-0.08675904930260485\n",
      "Gradient Descent(318/499): loss=0.255167062875004, w0=0.00529137860215826, w1=-0.0868507619740927\n",
      "Gradient Descent(319/499): loss=0.25513145456946107, w0=0.005310445924200237, w1=-0.08694224845125213\n",
      "Gradient Descent(320/499): loss=0.2550960788462467, w0=0.005329454718072418, w1=-0.08703350964406945\n",
      "Gradient Descent(321/499): loss=0.2550609330300523, w0=0.005348405160127076, w1=-0.08712454645622099\n",
      "Gradient Descent(322/499): loss=0.25502601448698436, w0=0.005367297426338235, w1=-0.08721535978513453\n",
      "Gradient Descent(323/499): loss=0.2549913206238288, w0=0.005386131692300473, w1=-0.08730595052204981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(324/499): loss=0.25495684888733017, w0=0.005404908133227781, w1=-0.08739631955207827\n",
      "Gradient Descent(325/499): loss=0.2549225967634836, w0=0.005423626923952479, w1=-0.08748646775426205\n",
      "Gradient Descent(326/499): loss=0.2548885617768401, w0=0.005442288238924178, w1=-0.08757639600163215\n",
      "Gradient Descent(327/499): loss=0.25485474148982584, w0=0.005460892252208809, w1=-0.08766610516126591\n",
      "Gradient Descent(328/499): loss=0.2548211335020727, w0=0.005479439137487681, w1=-0.08775559609434369\n",
      "Gradient Descent(329/499): loss=0.2547877354497633, w0=0.0054979290680566015, w1=-0.08784486965620485\n",
      "Gradient Descent(330/499): loss=0.2547545450049863, w0=0.005516362216825031, w1=-0.087933926696403\n",
      "Gradient Descent(331/499): loss=0.2547215598751056, w0=0.005534738756315286, w1=-0.08802276805876062\n",
      "Gradient Descent(332/499): loss=0.2546887778021403, w0=0.005553058858661779, w1=-0.0881113945814229\n",
      "Gradient Descent(333/499): loss=0.2546561965621562, w0=0.005571322695610295, w1=-0.08819980709691094\n",
      "Gradient Descent(334/499): loss=0.2546238139646697, w0=0.005589530438517309, w1=-0.08828800643217428\n",
      "Gradient Descent(335/499): loss=0.2545916278520619, w0=0.005607682258349339, w1=-0.08837599340864286\n",
      "Gradient Descent(336/499): loss=0.2545596360990038, w0=0.005625778325682324, w1=-0.08846376884227816\n",
      "Gradient Descent(337/499): loss=0.25452783661189304, w0=0.005643818810701047, w1=-0.08855133354362395\n",
      "Gradient Descent(338/499): loss=0.25449622732829985, w0=0.005661803883198578, w1=-0.08863868831785617\n",
      "Gradient Descent(339/499): loss=0.25446480621642453, w0=0.005679733712575747, w1=-0.08872583396483243\n",
      "Gradient Descent(340/499): loss=0.254433571274565, w0=0.005697608467840653, w1=-0.08881277127914075\n",
      "Gradient Descent(341/499): loss=0.25440252053059254, w0=0.005715428317608193, w1=-0.08889950105014782\n",
      "Gradient Descent(342/499): loss=0.2543716520414401, w0=0.005733193430099614, w1=-0.08898602406204659\n",
      "Gradient Descent(343/499): loss=0.25434096389259797, w0=0.005750903973142098, w1=-0.08907234109390336\n",
      "Gradient Descent(344/499): loss=0.25431045419761966, w0=0.005768560114168364, w1=-0.08915845291970431\n",
      "Gradient Descent(345/499): loss=0.2542801210976373, w0=0.005786162020216292, w1=-0.08924436030840147\n",
      "Gradient Descent(346/499): loss=0.25424996276088463, w0=0.005803709857928578, w1=-0.08933006402395809\n",
      "Gradient Descent(347/499): loss=0.2542199773822309, w0=0.0058212037935523936, w1=-0.08941556482539359\n",
      "Gradient Descent(348/499): loss=0.25419016318272236, w0=0.00583864399293908, w1=-0.08950086346682791\n",
      "Gradient Descent(349/499): loss=0.25416051840913123, w0=0.0058560306215438545, w1=-0.08958596069752538\n",
      "Gradient Descent(350/499): loss=0.25413104133351544, w0=0.005873363844425535, w1=-0.08967085726193803\n",
      "Gradient Descent(351/499): loss=0.25410173025278404, w0=0.005890643826246284, w1=-0.0897555538997485\n",
      "Gradient Descent(352/499): loss=0.2540725834882731, w0=0.005907870731271371, w1=-0.08984005134591236\n",
      "Gradient Descent(353/499): loss=0.25404359938532683, w0=0.005925044723368945, w1=-0.08992435033070001\n",
      "Gradient Descent(354/499): loss=0.2540147763128885, w0=0.00594216596600983, w1=-0.09000845157973807\n",
      "Gradient Descent(355/499): loss=0.2539861126630984, w0=0.0059592346222673365, w1=-0.09009235581405033\n",
      "Gradient Descent(356/499): loss=0.25395760685089835, w0=0.005976250854817074, w1=-0.0901760637500982\n",
      "Gradient Descent(357/499): loss=0.25392925731364513, w0=0.005993214825936796, w1=-0.09025957609982078\n",
      "Gradient Descent(358/499): loss=0.2539010625107291, w0=0.006010126697506247, w1=-0.09034289357067436\n",
      "Gradient Descent(359/499): loss=0.25387302092320174, w0=0.0060269866310070285, w1=-0.09042601686567163\n",
      "Gradient Descent(360/499): loss=0.25384513105340867, w0=0.006043794787522472, w1=-0.09050894668342031\n",
      "Gradient Descent(361/499): loss=0.2538173914246302, w0=0.006060551327737529, w1=-0.0905916837181615\n",
      "Gradient Descent(362/499): loss=0.25378980058072803, w0=0.006077256411938675, w1=-0.09067422865980743\n",
      "Gradient Descent(363/499): loss=0.2537623570857987, w0=0.006093910200013817, w1=-0.09075658219397897\n",
      "Gradient Descent(364/499): loss=0.2537350595238333, w0=0.006110512851452226, w1=-0.09083874500204264\n",
      "Gradient Descent(365/499): loss=0.2537079064983839, w0=0.006127064525344461, w1=-0.09092071776114721\n",
      "Gradient Descent(366/499): loss=0.2536808966322348, w0=0.0061435653803823265, w1=-0.09100250114425995\n",
      "Gradient Descent(367/499): loss=0.25365402856708164, w0=0.006160015574858821, w1=-0.09108409582020244\n",
      "Gradient Descent(368/499): loss=0.2536273009632146, w0=0.006176415266668109, w1=-0.09116550245368601\n",
      "Gradient Descent(369/499): loss=0.2536007124992088, w0=0.006192764613305497, w1=-0.09124672170534685\n",
      "Gradient Descent(370/499): loss=0.25357426187161936, w0=0.006209063771867419, w1=-0.09132775423178065\n",
      "Gradient Descent(371/499): loss=0.2535479477946828, w0=0.006225312899051436, w1=-0.09140860068557691\n",
      "Gradient Descent(372/499): loss=0.2535217690000232, w0=0.00624151215115624, w1=-0.09148926171535293\n",
      "Gradient Descent(373/499): loss=0.2534957242363646, w0=0.006257661684081668, w1=-0.0915697379657874\n",
      "Gradient Descent(374/499): loss=0.253469812269247, w0=0.006273761653328733, w1=-0.09165003007765363\n",
      "Gradient Descent(375/499): loss=0.25344403188075, w0=0.00628981221399965, w1=-0.09173013868785238\n",
      "Gradient Descent(376/499): loss=0.253418381869219, w0=0.006305813520797882, w1=-0.09181006442944449\n",
      "Gradient Descent(377/499): loss=0.25339286104899816, w0=0.00632176572802819, w1=-0.09188980793168303\n",
      "Gradient Descent(378/499): loss=0.25336746825016704, w0=0.006337668989596693, w1=-0.09196936982004515\n",
      "Gradient Descent(379/499): loss=0.25334220231828286, w0=0.006353523459010937, w1=-0.09204875071626366\n",
      "Gradient Descent(380/499): loss=0.25331706211412675, w0=0.006369329289379966, w1=-0.09212795123835815\n",
      "Gradient Descent(381/499): loss=0.2532920465134554, w0=0.0063850866334144146, w1=-0.09220697200066597\n",
      "Gradient Descent(382/499): loss=0.2532671544067559, w0=0.0064007956434265914, w1=-0.09228581361387267\n",
      "Gradient Descent(383/499): loss=0.25324238469900673, w0=0.006416456471330586, w1=-0.09236447668504237\n",
      "Gradient Descent(384/499): loss=0.2532177363094419, w0=0.0064320692686423726, w1=-0.0924429618176476\n",
      "Gradient Descent(385/499): loss=0.25319320817131885, w0=0.006447634186479929, w1=-0.09252126961159898\n",
      "Gradient Descent(386/499): loss=0.2531687992316927, w0=0.00646315137556336, w1=-0.09259940066327448\n",
      "Gradient Descent(387/499): loss=0.2531445084511923, w0=0.006478620986215026, w1=-0.0926773555655485\n",
      "Gradient Descent(388/499): loss=0.2531203348038005, w0=0.006494043168359685, w1=-0.09275513490782059\n",
      "Gradient Descent(389/499): loss=0.253096277276641, w0=0.0065094180715246355, w1=-0.09283273927604387\n",
      "Gradient Descent(390/499): loss=0.2530723348697655, w0=0.0065247458448398745, w1=-0.09291016925275318\n",
      "Gradient Descent(391/499): loss=0.25304850659594635, w0=0.006540026637038256, w1=-0.09298742541709297\n",
      "Gradient Descent(392/499): loss=0.25302479148047385, w0=0.006555260596455661, w1=-0.09306450834484485\n",
      "Gradient Descent(393/499): loss=0.25300118856095494, w0=0.006570447871031172, w1=-0.0931414186084549\n",
      "Gradient Descent(394/499): loss=0.2529776968871176, w0=0.006585588608307256, w1=-0.09321815677706073\n",
      "Gradient Descent(395/499): loss=0.2529543155206176, w0=0.006600682955429957, w1=-0.09329472341651823\n",
      "Gradient Descent(396/499): loss=0.25293104353484863, w0=0.006615731059149091, w1=-0.09337111908942805\n",
      "Gradient Descent(397/499): loss=0.2529078800147565, w0=0.006630733065818452, w1=-0.09344734435516186\n",
      "Gradient Descent(398/499): loss=0.25288482405665635, w0=0.006645689121396023, w1=-0.09352339976988829\n",
      "Gradient Descent(399/499): loss=0.2528618747680528, w0=0.006660599371444197, w1=-0.09359928588659866\n",
      "Gradient Descent(400/499): loss=0.2528390312674636, w0=0.00667546396113, w1=-0.09367500325513248\n",
      "Gradient Descent(401/499): loss=0.25281629268424677, w0=0.006690283035225323, w1=-0.09375055242220257\n",
      "Gradient Descent(402/499): loss=0.2527936581584299, w0=0.006705056738107165, w1=-0.09382593393142011\n",
      "Gradient Descent(403/499): loss=0.25277112684054326, w0=0.006719785213757878, w1=-0.0939011483233193\n",
      "Gradient Descent(404/499): loss=0.2527486978914557, w0=0.006734468605765422, w1=-0.09397619613538187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(405/499): loss=0.25272637048221325, w0=0.0067491070573236205, w1=-0.09405107790206128\n",
      "Gradient Descent(406/499): loss=0.2527041437938805, w0=0.006763700711232435, w1=-0.09412579415480676\n",
      "Gradient Descent(407/499): loss=0.2526820170173852, w0=0.006778249709898235, w1=-0.09420034542208705\n",
      "Gradient Descent(408/499): loss=0.2526599893533654, w0=0.006792754195334078, w1=-0.09427473222941393\n",
      "Gradient Descent(409/499): loss=0.25263806001201927, w0=0.006807214309159999, w1=-0.09434895509936554\n",
      "Gradient Descent(410/499): loss=0.25261622821295693, w0=0.006821630192603304, w1=-0.09442301455160947\n",
      "Gradient Descent(411/499): loss=0.25259449318505656, w0=0.0068360019864988734, w1=-0.09449691110292557\n",
      "Gradient Descent(412/499): loss=0.25257285416632125, w0=0.0068503298312894674, w1=-0.09457064526722869\n",
      "Gradient Descent(413/499): loss=0.25255131040373896, w0=0.00686461386702604, w1=-0.09464421755559098\n",
      "Gradient Descent(414/499): loss=0.2525298611531459, w0=0.006878854233368062, w1=-0.09471762847626418\n",
      "Gradient Descent(415/499): loss=0.25250850567909056, w0=0.006893051069583849, w1=-0.09479087853470158\n",
      "Gradient Descent(416/499): loss=0.25248724325470145, w0=0.006907204514550893, w1=-0.09486396823357983\n",
      "Gradient Descent(417/499): loss=0.2524660731615568, w0=0.006921314706756207, w1=-0.09493689807282048\n",
      "Gradient Descent(418/499): loss=0.2524449946895562, w0=0.00693538178429667, w1=-0.09500966854961138\n",
      "Gradient Descent(419/499): loss=0.2524240071367949, w0=0.006949405884879381, w1=-0.09508228015842782\n",
      "Gradient Descent(420/499): loss=0.25240310980944003, w0=0.006963387145822022, w1=-0.09515473339105354\n",
      "Gradient Descent(421/499): loss=0.2523823020216091, w0=0.006977325704053221, w1=-0.09522702873660148\n",
      "Gradient Descent(422/499): loss=0.2523615830952507, w0=0.006991221696112928, w1=-0.09529916668153433\n",
      "Gradient Descent(423/499): loss=0.25234095236002696, w0=0.007005075258152798, w1=-0.09537114770968494\n",
      "Gradient Descent(424/499): loss=0.25232040915319853, w0=0.00701888652593657, w1=-0.0954429723022765\n",
      "Gradient Descent(425/499): loss=0.2522999528195106, w0=0.007032655634840463, w1=-0.09551464093794251\n",
      "Gradient Descent(426/499): loss=0.2522795827110825, w0=0.007046382719853578, w1=-0.09558615409274668\n",
      "Gradient Descent(427/499): loss=0.2522592981872974, w0=0.0070600679155782995, w1=-0.09565751224020243\n",
      "Gradient Descent(428/499): loss=0.252239098614695, w0=0.007073711356230709, w1=-0.09572871585129243\n",
      "Gradient Descent(429/499): loss=0.25221898336686627, w0=0.007087313175641, w1=-0.09579976539448783\n",
      "Gradient Descent(430/499): loss=0.2521989518243488, w0=0.007100873507253905, w1=-0.09587066133576734\n",
      "Gradient Descent(431/499): loss=0.2521790033745255, w0=0.007114392484129126, w1=-0.09594140413863614\n",
      "Gradient Descent(432/499): loss=0.2521591374115233, w0=0.007127870238941769, w1=-0.09601199426414461\n",
      "Gradient Descent(433/499): loss=0.25213935333611553, w0=0.007141306903982785, w1=-0.09608243217090691\n",
      "Gradient Descent(434/499): loss=0.2521196505556242, w0=0.007154702611159424, w1=-0.0961527183151193\n",
      "Gradient Descent(435/499): loss=0.25210002848382485, w0=0.007168057491995683, w1=-0.09622285315057846\n",
      "Gradient Descent(436/499): loss=0.25208048654085335, w0=0.0071813716776327734, w1=-0.09629283712869943\n",
      "Gradient Descent(437/499): loss=0.25206102415311293, w0=0.007194645298829582, w1=-0.09636267069853358\n",
      "Gradient Descent(438/499): loss=0.2520416407531848, w0=0.007207878485963147, w1=-0.09643235430678626\n",
      "Gradient Descent(439/499): loss=0.2520223357797381, w0=0.007221071369029136, w1=-0.09650188839783443\n",
      "Gradient Descent(440/499): loss=0.2520031086774429, w0=0.0072342240776423275, w1=-0.09657127341374401\n",
      "Gradient Descent(441/499): loss=0.25198395889688463, w0=0.007247336741037107, w1=-0.09664050979428714\n",
      "Gradient Descent(442/499): loss=0.2519648858944787, w0=0.0072604094880679565, w1=-0.09670959797695922\n",
      "Gradient Descent(443/499): loss=0.2519458891323877, w0=0.007273442447209962, w1=-0.09677853839699589\n",
      "Gradient Descent(444/499): loss=0.2519269680784404, w0=0.007286435746559319, w1=-0.0968473314873898\n",
      "Gradient Descent(445/499): loss=0.2519081222060501, w0=0.007299389513833842, w1=-0.0969159776789072\n",
      "Gradient Descent(446/499): loss=0.2518893509941366, w0=0.007312303876373491, w1=-0.0969844774001044\n",
      "Gradient Descent(447/499): loss=0.25187065392704844, w0=0.007325178961140891, w1=-0.09705283107734414\n",
      "Gradient Descent(448/499): loss=0.25185203049448657, w0=0.007338014894721862, w1=-0.09712103913481175\n",
      "Gradient Descent(449/499): loss=0.2518334801914285, w0=0.007350811803325957, w1=-0.09718910199453117\n",
      "Gradient Descent(450/499): loss=0.25181500251805566, w0=0.007363569812787001, w1=-0.09725702007638082\n",
      "Gradient Descent(451/499): loss=0.2517965969796798, w0=0.00737628904856364, w1=-0.09732479379810938\n",
      "Gradient Descent(452/499): loss=0.25177826308667256, w0=0.007388969635739892, w1=-0.09739242357535136\n",
      "Gradient Descent(453/499): loss=0.2517600003543939, w0=0.007401611699025701, w1=-0.0974599098216426\n",
      "Gradient Descent(454/499): loss=0.25174180830312476, w0=0.007414215362757506, w1=-0.09752725294843556\n",
      "Gradient Descent(455/499): loss=0.25172368645799775, w0=0.007426780750898805, w1=-0.09759445336511452\n",
      "Gradient Descent(456/499): loss=0.25170563434893145, w0=0.007439307987040727, w1=-0.09766151147901064\n",
      "Gradient Descent(457/499): loss=0.2516876515105638, w0=0.007451797194402614, w1=-0.09772842769541687\n",
      "Gradient Descent(458/499): loss=0.2516697374821882, w0=0.007464248495832603, w1=-0.09779520241760274\n",
      "Gradient Descent(459/499): loss=0.25165189180768965, w0=0.007476662013808213, w1=-0.09786183604682905\n",
      "Gradient Descent(460/499): loss=0.25163411403548275, w0=0.007489037870436939, w1=-0.09792832898236233\n",
      "Gradient Descent(461/499): loss=0.2516164037184494, w0=0.007501376187456854, w1=-0.09799468162148928\n",
      "Gradient Descent(462/499): loss=0.2515987604138791, w0=0.007513677086237209, w1=-0.09806089435953105\n",
      "Gradient Descent(463/499): loss=0.25158118368340976, w0=0.007525940687779041, w1=-0.0981269675898574\n",
      "Gradient Descent(464/499): loss=0.25156367309296784, w0=0.007538167112715789, w1=-0.09819290170390065\n",
      "Gradient Descent(465/499): loss=0.25154622821271244, w0=0.007550356481313911, w1=-0.09825869709116965\n",
      "Gradient Descent(466/499): loss=0.2515288486169778, w0=0.007562508913473506, w1=-0.09832435413926353\n",
      "Gradient Descent(467/499): loss=0.2515115338842175, w0=0.007574624528728942, w1=-0.09838987323388539\n",
      "Gradient Descent(468/499): loss=0.2514942835969507, w0=0.007586703446249488, w1=-0.09845525475885575\n",
      "Gradient Descent(469/499): loss=0.2514770973417069, w0=0.007598745784839951, w1=-0.09852049909612609\n",
      "Gradient Descent(470/499): loss=0.2514599747089742, w0=0.007610751662941319, w1=-0.09858560662579208\n",
      "Gradient Descent(471/499): loss=0.25144291529314633, w0=0.007622721198631402, w1=-0.09865057772610675\n",
      "Gradient Descent(472/499): loss=0.25142591869247144, w0=0.007634654509625488, w1=-0.09871541277349363\n",
      "Gradient Descent(473/499): loss=0.2514089845090022, w0=0.007646551713276993, w1=-0.09878011214255968\n",
      "Gradient Descent(474/499): loss=0.2513921123485453, w0=0.007658412926578122, w1=-0.0988446762061081\n",
      "Gradient Descent(475/499): loss=0.25137530182061274, w0=0.007670238266160535, w1=-0.09890910533515111\n",
      "Gradient Descent(476/499): loss=0.2513585525383749, w0=0.007682027848296007, w1=-0.0989733998989226\n",
      "Gradient Descent(477/499): loss=0.25134186411861137, w0=0.0076937817888971085, w1=-0.09903756026489059\n",
      "Gradient Descent(478/499): loss=0.25132523618166563, w0=0.007705500203517874, w1=-0.09910158679876967\n",
      "Gradient Descent(479/499): loss=0.25130866835139876, w0=0.007717183207354487, w1=-0.09916547986453329\n",
      "Gradient Descent(480/499): loss=0.25129216025514456, w0=0.00772883091524596, w1=-0.099229239824426\n",
      "Gradient Descent(481/499): loss=0.2512757115236649, w0=0.007740443441674825, w1=-0.09929286703897547\n",
      "Gradient Descent(482/499): loss=0.2512593217911062, w0=0.007752020900767826, w1=-0.09935636186700458\n",
      "Gradient Descent(483/499): loss=0.25124299069495665, w0=0.00776356340629661, w1=-0.09941972466564322\n",
      "Gradient Descent(484/499): loss=0.25122671787600315, w0=0.007775071071678434, w1=-0.09948295579034011\n",
      "Gradient Descent(485/499): loss=0.25121050297829045, w0=0.007786544009976859, w1=-0.0995460555948745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(486/499): loss=0.2511943456490799, w0=0.007797982333902466, w1=-0.09960902443136775\n",
      "Gradient Descent(487/499): loss=0.2511782455388088, w0=0.00780938615581356, w1=-0.0996718626502948\n",
      "Gradient Descent(488/499): loss=0.25116220230105113, w0=0.007820755587716886, w1=-0.0997345706004956\n",
      "Gradient Descent(489/499): loss=0.25114621559247796, w0=0.00783209074126835, w1=-0.09979714862918636\n",
      "Gradient Descent(490/499): loss=0.2511302850728192, w0=0.007843391727773737, w1=-0.09985959708197077\n",
      "Gradient Descent(491/499): loss=0.25111441040482596, w0=0.007854658658189434, w1=-0.09992191630285115\n",
      "Gradient Descent(492/499): loss=0.25109859125423295, w0=0.007865891643123162, w1=-0.09998410663423936\n",
      "Gradient Descent(493/499): loss=0.2510828272897213, w0=0.007877090792834707, w1=-0.10004616841696783\n",
      "Gradient Descent(494/499): loss=0.25106711818288335, w0=0.007888256217236652, w1=-0.1001081019903003\n",
      "Gradient Descent(495/499): loss=0.2510514636081866, w0=0.007899388025895116, w1=-0.10016990769194263\n",
      "Gradient Descent(496/499): loss=0.2510358632429381, w0=0.007910486328030495, w1=-0.10023158585805339\n",
      "Gradient Descent(497/499): loss=0.2510203167672506, w0=0.007921551232518208, w1=-0.10029313682325443\n",
      "Gradient Descent(498/499): loss=0.2510048238640088, w0=0.007932582847889443, w1=-0.10035456092064136\n",
      "Gradient Descent(499/499): loss=0.25098938421883465, w0=0.007943581282331908, w1=-0.10041585848179395\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEKCAYAAAAb7IIBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x114592630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'y_preds_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-03b1c2153aa8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0my_pred_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx_single_jet_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0my_pred_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx_single_jet_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0my_preds_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask_jets_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask_jet_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_pred_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0my_preds_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask_jets_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask_jet_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_pred_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_preds_train' is not defined"
     ]
    }
   ],
   "source": [
    "deg=5;\n",
    "mask_jets_train = split_jets_mask(tx_train)\n",
    "mask_jets_test = split_jets_mask(tx_test)\n",
    "len_mask = len(mask_jets_train)\n",
    "\n",
    "y_preds_train = np.zeros(len(y_train))\n",
    "y_preds_test = np.zeros(tx_test.shape[0])\n",
    "\n",
    "\n",
    "\n",
    "for mask_jet_id in range(len_mask):\n",
    "    print('**** Starting Jet ', mask_jet_id, '*****')\n",
    "    tx_single_jet_train = tx_train[mask_jets_train[mask_jet_id]]\n",
    "    tx_single_jet_test =  tx_test[mask_jets_test[mask_jet_id]]\n",
    "    y_single_jet_train =   y_train[mask_jets_train[mask_jet_id]]\n",
    "    tx_single_jet_train, tx_single_jet_test, len_init = prepare_data(tx_single_jet_train, tx_single_jet_test, deg);\n",
    "    initial_w = np.zeros(tx_single_jet_train.shape[1]);\n",
    "    w, loss = func_GD (y=y_single_jet_train, tx=tx_single_jet_train, test_set=tx_single_jet_test, max_iters=500,gamma=0.01, initial_w=initial_w);\n",
    "    \n",
    "    y_pred_train = predict_labels(w, tx_single_jet_train)\n",
    "    y_pred_test = predict_labels(w, tx_single_jet_test)\n",
    "    y_preds_train[mask_jets_train[mask_jet_id]] = y_pred_train\n",
    "    y_preds_test[mask_jets_test[mask_jet_id]] = y_pred_test\n",
    "    \n",
    "    right_train = np.sum(y_single_jet_train == y_pred_train)/len(y_single_jet_train )*100\n",
    "    \n",
    "    print('**** Jet ', mask_jet_id, '*****accuracy jet',right_train)\n",
    "    \n",
    "\n",
    "    \n",
    "right_train_total = np.sum(y_train == y_preds_train)/len(y_train )*100\n",
    "print('>>>>>>>> Accuracy TOTAL ',right_train_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "deg=5;\n",
    "lambda_=1e-5\n",
    "mask_jets_train = split_jets_mask(tx_train)\n",
    "mask_jets_test = split_jets_mask(tx_test)\n",
    "len_mask = len(mask_jets_train)\n",
    "for mask_jet_id in range(len_mask):\n",
    "    print('**** Starting Jet ', mask_jet_id, '*****')\n",
    "    tx_single_jet_train = tx_train[mask_jets_train[mask_jet_id]]\n",
    "    tx_single_jet_test =  tx_test[mask_jets_test[mask_jet_id]]\n",
    "    y_single_jet_train =   y_train[mask_jets_train[mask_jet_id]]\n",
    "    tx_single_jet_train, tx_single_jet_test, len_init = prepare_data(tx_single_jet_train, tx_single_jet_test, deg);\n",
    "    \n",
    "    [w,loss]=func_ridge_regression (y=y_single_jet_train, tx=tx_single_jet_train, test_set=tx_single_jet_test, lambda_=lambda_);    \n",
    "    \n",
    "    y_pred_train = predict_labels(w, tx_single_jet_train)\n",
    "    y_pred_test = predict_labels(w, tx_single_jet_test)\n",
    "    y_preds_train[mask_jets_train[mask_jet_id]] = y_pred_train\n",
    "    y_preds_test[mask_jets_test[mask_jet_id]] = y_pred_test\n",
    "    \n",
    "    right_train = np.sum(y_single_jet_train == y_pred_train)/len(y_single_jet_train )*100\n",
    "    \n",
    "    print('**** Jet ', mask_jet_id, '*****accuracy jet',right_train)\n",
    "    \n",
    "\n",
    "    \n",
    "right_train_total = np.sum(y_train == y_preds_train)/len(y_train )*100\n",
    "print('>>>>>>>> Accuracy TOTAL ',right_train_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##  SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def func_SGD (y, tx, test_set, max_iters, gamma, initial_w, batch_size):\n",
    "    name = 'Stochastic_Gradient_descent'\n",
    "    \n",
    "    w,loss = least_squares_SGD(y, tx, initial_w, max_iters,\\\n",
    "                               gamma, batch_size)\n",
    "    \n",
    "    y_pred = predict_labels(w, test_set)\n",
    "    create_csv_submission(ids_test, y_pred, OUT_FOLDER+name)\n",
    "    \n",
    "    plt.plot(w, 'go')\n",
    "    plt.title('SGD: weights with loss:  '+str(loss),fontsize=15, fontweight='bold');\n",
    "    plt.show()\n",
    "    \n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "deg=5;\n",
    "lambda_=1e-5\n",
    "mask_jets_train = split_jets_mask(tx_train)\n",
    "mask_jets_test = split_jets_mask(tx_test)\n",
    "len_mask = len(mask_jets_train)\n",
    "for mask_jet_id in range(len_mask):\n",
    "    print('**** Starting Jet ', mask_jet_id, '*****')\n",
    "    tx_single_jet_train = tx_train[mask_jets_train[mask_jet_id]]\n",
    "    tx_single_jet_test =  tx_test[mask_jets_test[mask_jet_id]]\n",
    "    y_single_jet_train =   y_train[mask_jets_train[mask_jet_id]]\n",
    "    tx_single_jet_train, tx_single_jet_test, len_init = prepare_data(tx_single_jet_train, tx_single_jet_test, deg);\n",
    "    \n",
    "    initial_w = np.zeros(tx_single_jet_train.shape[1]);\n",
    "    w, loss = func_SGD (y=y_single_jet_train, tx=tx_single_jet_train, test_set=tx_single_jet_test, max_iters=500,gamma=0.00005, initial_w=initial_w,\\\n",
    "                   batch_size=1);    \n",
    "   \n",
    "    y_pred_train = predict_labels(w, tx_single_jet_train)\n",
    "    y_pred_test = predict_labels(w, tx_single_jet_test)\n",
    "    y_preds_train[mask_jets_train[mask_jet_id]] = y_pred_train\n",
    "    y_preds_test[mask_jets_test[mask_jet_id]] = y_pred_test\n",
    "    \n",
    "    right_train = np.sum(y_single_jet_train == y_pred_train)/len(y_single_jet_train )*100\n",
    "    \n",
    "    print('**** Jet ', mask_jet_id, '*****accuracy jet',right_train)\n",
    "    \n",
    "\n",
    "    \n",
    "right_train_total = np.sum(y_train == y_preds_train)/len(y_train )*100\n",
    "print('>>>>>>>> Accuracy TOTAL ',right_train_total)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "def cross_validation_logistic_regression(y,tx, k_fold, max_iters, gammas):\n",
    "    \n",
    "    accuracies_test=np.zeros(len(gammas))\n",
    "    accuracies_train=np.zeros(len(gammas))\n",
    "    acc_max=0;\n",
    "    gamma_best=0;\n",
    "    \n",
    "    for i, single_gamma in enumerate(gammas):\n",
    "        \n",
    "        seed=1;\n",
    "\n",
    "        # get k'th subgroup in test, others in train\n",
    "        k_indices = build_k_indices(y, k_fold, seed)\n",
    "        accuracy_train = np.zeros(k_fold)\n",
    "        accuracy_test = np.zeros(k_fold)\n",
    "\n",
    "\n",
    "        for k in range(k_fold):\n",
    "            #print('----- FOLD', k, '-----')\n",
    "            k_index = k_indices[k]\n",
    "            test_y = y[k_index]\n",
    "            test_tx = tx[k_index,:]\n",
    "            mask = np.ones(len(y), dtype=bool) # set all elements to True\n",
    "            mask[k_index] = False              # set test elements to False\n",
    "            train_tx = tx[mask,:]              # select only True elements (ie train elements)\n",
    "            train_y = y[mask]\n",
    "            initial_w = np.zeros(train_tx.shape[1]);\n",
    "            weights,loss = logistic_regression(train_y, train_tx, initial_w, max_iters, single_gamma)\n",
    "            # Compute the predictions\n",
    "            y_pred_train = predict_labels(weights, train_tx)\n",
    "            y_pred_test = predict_labels(weights, test_tx);\n",
    "            accuracy_train[k] = np.sum(y_pred_train == train_y)/len(train_y)\n",
    "            accuracy_test[k] = np.sum(y_pred_test == test_y)/len(test_y)\n",
    "            \n",
    "        accuracies_test[i]=np.mean(accuracy_test);\n",
    "        accuracies_train[i]=np.mean(accuracy_train);\n",
    "        \n",
    "        print('GAMMA:', single_gamma, '---','ACCURANCY TEST:',accuracies_test[i], '---','ACCURANCY TRAIN:',accuracies_train[i])\n",
    "        if (accuracies_test[i]>acc_max):\n",
    "                gamma_best=gammas[i];\n",
    "                acc_max=accuracies_test[i];\n",
    "    \n",
    "    return [gamma_best,acc_max]\n",
    "               \n",
    "        # Compute accuracy of the predictions\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def func_logistic (y, tx, test_set, max_iters, gamma, initial_w):\n",
    "    name = 'Logistic regression'\n",
    "    \n",
    "    #w,loss = logistic_regression(y, tx, initial_w, max_iters, gamma)\n",
    "    w,loss = logistic_regression(y, tx, initial_w, max_iters, gamma)\n",
    "    \n",
    "    loss=loss/len(y);\n",
    "    y_pred = predict_labels(w, test_set)\n",
    "    create_csv_submission(ids_test, y_pred, OUT_FOLDER+name)  \n",
    "    \n",
    "    plt.plot(w, 'go')\n",
    "    plt.title('logistic reg: weights with loss:  '+str(loss),fontsize=15, fontweight='bold');\n",
    "    plt.show()\n",
    "    \n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gammas=np.linspace(0.00001,0.9,10);\n",
    "k_fold=3;\n",
    "max_iters=1000;\n",
    "deg=5;\n",
    "\n",
    "\n",
    "mask_jets_train = split_jets_mask(tx_train)\n",
    "mask_jets_test = split_jets_mask(tx_test)\n",
    "len_mask = len(mask_jets_train)\n",
    "for mask_jet_id in range(len_mask):\n",
    "    print('**** Starting Jet ', mask_jet_id, '*****')\n",
    "    tx_single_jet_train = tx_train[mask_jets_train[mask_jet_id]]\n",
    "    tx_single_jet_test =  tx_test[mask_jets_test[mask_jet_id]]\n",
    "    y_single_jet_train =   y_train[mask_jets_train[mask_jet_id]]\n",
    "    tx_single_jet_train, tx_single_jet_test, len_init = prepare_data(tx_single_jet_train, tx_single_jet_test, deg);\n",
    "    \n",
    "    \n",
    "    \n",
    "    [best_gamma,acc_max]=cross_validation_logistic_regression(y_single_jet_train, tx_single_jet_train, k_fold, max_iters, gammas)\n",
    "    \n",
    "    print('BEST GAMMA:', best_gamma, '---','ACCURANCY:',acc_max)\n",
    "\n",
    "    initial_w = np.zeros(tx_single_jet_train.shape[1]);\n",
    "    w, loss = func_logistic (y=y_single_jet_train, tx=tx_single_jet_train, test_set=tx_single_jet_test, max_iters=1000,\\\n",
    "                         gamma=best_gamma, initial_w=initial_w);\n",
    "    \n",
    "    y_pred_train = predict_labels(w, tx_single_jet_train)\n",
    "    y_pred_test = predict_labels(w, tx_single_jet_test)\n",
    "    y_preds_train[mask_jets_train[mask_jet_id]] = y_pred_train\n",
    "    y_preds_test[mask_jets_test[mask_jet_id]] = y_pred_test\n",
    "    \n",
    "    right_train = np.sum(y_single_jet_train == y_pred_train)/len(y_single_jet_train )*100\n",
    "    \n",
    "    print('**** Jet ', mask_jet_id, '*****accuracy jet',right_train)\n",
    "    \n",
    "\n",
    "    \n",
    "right_train_total = np.sum(y_train == y_preds_train)/len(y_train )*100\n",
    "print('>>>>>>>> Accuracy TOTAL ',right_train_total)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOGISTIC REGULARIZED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "def cross_validation_logistic_regression_reg(y,tx, k_fold, max_iters, gammas,lambdas):\n",
    "    \n",
    "    acc_max=0;\n",
    "    accuracies_test=np.zeros([len(gammas),len(lambdas)]);\n",
    "    \n",
    "    accuracies_train=np.zeros([len(gammas),len(lambdas)]);\n",
    "    \n",
    "    \n",
    "    for i, single_gamma in enumerate(gammas):\n",
    "        for j, single_lambda in enumerate(lambdas):\n",
    "            seed=1;\n",
    "        \n",
    "            # get k'th subgroup in test, others in train\n",
    "            k_indices = build_k_indices(y, k_fold, seed)\n",
    "            accuracy_train = np.zeros(k_fold)\n",
    "            accuracy_test = np.zeros(k_fold)\n",
    "\n",
    "\n",
    "            for k in range(k_fold):\n",
    "                #print('----- FOLD', k, '-----')\n",
    "                k_index = k_indices[k]\n",
    "                test_y = y[k_index]\n",
    "                test_tx = tx[k_index,:]\n",
    "\n",
    "                mask = np.ones(len(y), dtype=bool) # set all elements to True\n",
    "                mask[k_index] = False              # set test elements to False\n",
    "                train_tx = tx[mask,:]              # select only True elements (ie train elements)\n",
    "                train_y = y[mask]\n",
    "                initial_w = np.zeros(train_tx.shape[1]);\n",
    "                weights,loss = reg_logistic_regression(train_y, train_tx,single_lambda, initial_w, max_iters, single_gamma)\n",
    "                # Compute the predictions\n",
    "                y_pred_train = predict_labels(weights, train_tx)\n",
    "                y_pred_test = predict_labels(weights, test_tx)\n",
    "                predictions=True;\n",
    "                accuracy_train[k] = np.sum(y_pred_train == train_y)/len(train_y)\n",
    "                accuracy_test[k] = np.sum(y_pred_test == test_y)/len(test_y)\n",
    "                \n",
    "                \n",
    "            accuracies_test[i,j]= np.mean(accuracy_test);\n",
    "            accuracies_train[i,j]= np.mean(accuracy_train);\n",
    "            print('GAMMA', single_gamma, '---','LAMBDA', single_lambda, '---ACCURANCY TEST:',accuracies_test[i,j],'---ACCURANCY TRAIN:',accuracies_train[i,j])\n",
    "            \n",
    "            if (accuracies_test[i,j]>acc_max):\n",
    "                gamma_best=gammas[i];\n",
    "                lambda_best=lambdas[j];\n",
    "                acc_max=accuracies_test[i,j];\n",
    "            \n",
    "            \n",
    "    return [gamma_best,lambda_best,acc_max]\n",
    "     \n",
    "    \n",
    "    \n",
    "            # Compute accuracy of the predictions\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def func_logistic_reg (y, tx, lambda_, test_set, max_iters, gamma, initial_w):\n",
    "    name = 'Logistic regression regularized'\n",
    "    \n",
    "    #w,loss = logistic_regression(y, tx, initial_w, max_iters, gamma)\n",
    "    w,loss = reg_logistic_regression(y, tx, lambda_, initial_w, max_iters, gamma)\n",
    "    \n",
    "    loss=loss/len(y);\n",
    "    y_pred = predict_labels(w, test_set)\n",
    "    create_csv_submission(ids_test, y_pred, OUT_FOLDER+name)\n",
    "    \n",
    "    plt.plot(w, 'go')\n",
    "    plt.title('logistic reg: weights with the normalized log-like:  '+str(loss),fontsize=15, fontweight='bold');\n",
    "    plt.show()\n",
    "    \n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test cross_validation\n",
    "\n",
    "gammas=np.linspace(0.000001,0.0001,2);\n",
    "lambdas=np.linspace(0.001,0.3,2);\n",
    "k_fold=6;\n",
    "max_iters=50;\n",
    "\n",
    "deg=2;\n",
    "\n",
    "mask_jets_train = split_jets_mask(tx_train)\n",
    "mask_jets_test = split_jets_mask(tx_test)\n",
    "len_mask = len(mask_jets_train)\n",
    "for mask_jet_id in range(len_mask):\n",
    "    print('**** Starting Jet ', mask_jet_id, '*****')\n",
    "    tx_single_jet_train = tx_train[mask_jets_train[mask_jet_id]]\n",
    "    tx_single_jet_test =  tx_test[mask_jets_test[mask_jet_id]]\n",
    "    y_single_jet_train =   y_train[mask_jets_train[mask_jet_id]]\n",
    "    tx_single_jet_train, tx_single_jet_test, len_init\n",
    "    = prepare_data(tx_single_jet_train, tx_single_jet_test, deg);\n",
    "    \n",
    "    [gamma_best,lambda_best,acc_max]= cross_validation_logistic_regression_reg(y_single_jet_train, tx_single_jet_train, k_fold, max_iters, gammas,lambdas)\n",
    "    print('BEST GAMMA', gamma_best, '---','BEST LAMBDA', lambda_best, '---ACCURANCY:',acc_max)\n",
    "    \n",
    "    initial_w = np.zeros(tx_single_jet_train.shape[1]);\n",
    "    w, loss = func_logistic_reg (y=y_single_jet_train, tx=tx_single_jet_train, lambda_=lambda_best, test_set=tx_single_jet_test , max_iters=max_iters,\\\n",
    "                         gamma=gamma_best, initial_w=initial_w);\n",
    "\n",
    "\n",
    "    \n",
    "    y_pred_train = predict_labels(w, tx_single_jet_train)\n",
    "    y_pred_test = predict_labels(w, tx_single_jet_test)\n",
    "    y_preds_train[mask_jets_train[mask_jet_id]] = y_pred_train\n",
    "    y_preds_test[mask_jets_test[mask_jet_id]] = y_pred_test\n",
    "    \n",
    "    right_train = np.sum(y_single_jet_train == y_pred_train)/len(y_single_jet_train )*100\n",
    "    \n",
    "    print('**** Jet ', mask_jet_id, '*****accuracy jet',right_train)\n",
    "    \n",
    "\n",
    "    \n",
    "right_train_total = np.sum(y_train == y_preds_train)/len(y_train )*100\n",
    "print('>>>>>>>> Accuracy TOTAL ',right_train_total)\n",
    "\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
