{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Project 1\n",
    "\n",
    "    We begin by loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from preprocessing_functions import *\n",
    "%matplotlib inline \n",
    "import numpy as np   # generic stuff\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#### REMOVE THIS LINE BEFORE SUBMISSION\n",
    "import pandas as pd\n",
    "#######################################################################\n",
    "\n",
    "from lib.proj1_helpers import * #the helper provided for the project\n",
    "from lib.costs import *\n",
    "\n",
    "# choose which implementations you would like\n",
    "from lib.implementations import *\n",
    "#from implementations import * #our implementations of the functions done by us\n",
    "\n",
    "\n",
    "import datetime\n",
    "from helpers import * #helpers of exo 2\n",
    "# Useful starting lines\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_FOLDER = '../data/' # get rid of the ..\n",
    "\n",
    "y_train, tx_train, ids_train = load_csv_data(DATA_FOLDER+'train.csv',sub_sample=False)\n",
    "\n",
    "y_test, tx_test, ids_test = load_csv_data(DATA_FOLDER+'test.csv',sub_sample=False)\n",
    "AAA = 0;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note that here we are only considering a sub_sample as the \"True\" value indicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 30)\n",
      "(568238, 30)\n",
      "(250000,)\n"
     ]
    }
   ],
   "source": [
    "print(tx_train.shape)\n",
    "print(tx_test.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>138.470</td>\n",
       "      <td>51.655</td>\n",
       "      <td>97.827</td>\n",
       "      <td>27.980</td>\n",
       "      <td>0.91</td>\n",
       "      <td>124.711</td>\n",
       "      <td>2.666</td>\n",
       "      <td>3.064</td>\n",
       "      <td>41.928</td>\n",
       "      <td>197.760</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.277</td>\n",
       "      <td>258.733</td>\n",
       "      <td>2.0</td>\n",
       "      <td>67.435</td>\n",
       "      <td>2.150</td>\n",
       "      <td>0.444</td>\n",
       "      <td>46.062</td>\n",
       "      <td>1.24</td>\n",
       "      <td>-2.475</td>\n",
       "      <td>113.497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>160.937</td>\n",
       "      <td>68.768</td>\n",
       "      <td>103.235</td>\n",
       "      <td>48.146</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.473</td>\n",
       "      <td>2.078</td>\n",
       "      <td>125.157</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.916</td>\n",
       "      <td>164.546</td>\n",
       "      <td>1.0</td>\n",
       "      <td>46.226</td>\n",
       "      <td>0.725</td>\n",
       "      <td>1.158</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>46.226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-999.000</td>\n",
       "      <td>162.172</td>\n",
       "      <td>125.953</td>\n",
       "      <td>35.635</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.148</td>\n",
       "      <td>9.336</td>\n",
       "      <td>197.814</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.186</td>\n",
       "      <td>260.414</td>\n",
       "      <td>1.0</td>\n",
       "      <td>44.251</td>\n",
       "      <td>2.053</td>\n",
       "      <td>-2.028</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>44.251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>143.905</td>\n",
       "      <td>81.417</td>\n",
       "      <td>80.943</td>\n",
       "      <td>0.414</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.310</td>\n",
       "      <td>0.414</td>\n",
       "      <td>75.968</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060</td>\n",
       "      <td>86.062</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>175.864</td>\n",
       "      <td>16.915</td>\n",
       "      <td>134.805</td>\n",
       "      <td>16.405</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.891</td>\n",
       "      <td>16.405</td>\n",
       "      <td>57.983</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.871</td>\n",
       "      <td>53.131</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0        1        2       3       4        5        6      7       8   \\\n",
       "0  138.470   51.655   97.827  27.980    0.91  124.711    2.666  3.064  41.928   \n",
       "1  160.937   68.768  103.235  48.146 -999.00 -999.000 -999.000  3.473   2.078   \n",
       "2 -999.000  162.172  125.953  35.635 -999.00 -999.000 -999.000  3.148   9.336   \n",
       "3  143.905   81.417   80.943   0.414 -999.00 -999.000 -999.000  3.310   0.414   \n",
       "4  175.864   16.915  134.805  16.405 -999.00 -999.000 -999.000  3.891  16.405   \n",
       "\n",
       "        9    ...        20       21   22       23       24       25       26  \\\n",
       "0  197.760   ...    -0.277  258.733  2.0   67.435    2.150    0.444   46.062   \n",
       "1  125.157   ...    -1.916  164.546  1.0   46.226    0.725    1.158 -999.000   \n",
       "2  197.814   ...    -2.186  260.414  1.0   44.251    2.053   -2.028 -999.000   \n",
       "3   75.968   ...     0.060   86.062  0.0 -999.000 -999.000 -999.000 -999.000   \n",
       "4   57.983   ...    -0.871   53.131  0.0 -999.000 -999.000 -999.000 -999.000   \n",
       "\n",
       "       27       28       29  \n",
       "0    1.24   -2.475  113.497  \n",
       "1 -999.00 -999.000   46.226  \n",
       "2 -999.00 -999.000   44.251  \n",
       "3 -999.00 -999.000    0.000  \n",
       "4 -999.00 -999.000    0.000  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# illegal: \n",
    "pd.DataFrame(tx_train).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for methods: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OUT_FOLDER = 'output/'\n",
    "\n",
    "def func_least_squares (y, tx, test_set, fct='mse'):\n",
    "    #name = 'least_squares'\n",
    "    w,loss = least_squares(y,tx,fct)\n",
    "    #y_pred = predict_labels(w, test_set)\n",
    "    #create_csv_submission(ids_test, y_pred, OUT_FOLDER+name)\n",
    "    \n",
    "    plt.plot(w, 'go')\n",
    "    plt.title('least squares weights for loss:  '+str(loss),fontsize=15, fontweight='bold');\n",
    "    plt.show()\n",
    "    \n",
    "    return w, loss\n",
    "\n",
    "def func_GD (y, tx, test_set, max_iters, gamma, initial_w):\n",
    "    w,loss = least_squares_GD(y, tx, initial_w, max_iters, gamma,fct='mse');\n",
    "    #y_pred = predict_labels(w, test_set)\n",
    "    #create_csv_submission(ids_test, y_pred, OUT_FOLDER+name)\n",
    "    \n",
    "    plt.plot(w, 'go')\n",
    "    plt.title('GD: weights with loss:  '+str(loss),fontsize=15, fontweight='bold');\n",
    "    plt.show()\n",
    "    \n",
    "    return w, loss\n",
    "\n",
    "########################  RIDGE REGRESSION #######################################################\n",
    "def func_ridge_regression (y, tx, test_set, lambda_):\n",
    "    name = 'Ridge_regression'\n",
    "    \n",
    "    w,loss = ridge_regression(y, tx, lambda_=lambda_, fct='mse');\n",
    "    \n",
    "    y_pred = predict_labels(w, test_set)\n",
    "    create_csv_submission(ids_test, y_pred, OUT_FOLDER+name)\n",
    "    \n",
    "    plt.plot(w, 'go')\n",
    "    plt.title('Ridge regression: weights ;loss:  '+str(loss),fontsize=15, fontweight='bold');\n",
    "    plt.show()\n",
    "    \n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LEAST-SQUARES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#todo: cross_validation su degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"Build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "def cross_validation_least_square(y, tx, k_fold, degs, seed):\n",
    "    \"\"\"Finds the best degree for Least Squares method with cross validation.\"\"\"\n",
    "    # Get k'th subgroup in test, others in train\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    \n",
    "    # Initialisation of the matrices of accuracies for all the degrees and different folds\n",
    "    accuracy_train = np.zeros([k_fold, len(degs)])\n",
    "    accuracy_test = np.zeros([k_fold, len(degs)])\n",
    "\n",
    "    # Loop over the different folds\n",
    "    for k in range(k_fold):\n",
    "        print('----- FOLD', k, '-----')\n",
    "        k_index = k_indices[k]\n",
    "        test_y = y[k_index]\n",
    "        test_tx = tx[k_index,:]\n",
    "        mask = np.ones(len(y), dtype=bool) # set all elements to True\n",
    "        mask[k_index] = False              # set test elements to False\n",
    "        train_tx = tx[mask,:]              # select only True elements (ie train elements)\n",
    "        train_y = y[mask]\n",
    "\n",
    "        len_init_data = 0\n",
    "\n",
    "        # Loop over the different degrees to try\n",
    "        for i, single_deg in enumerate(degs):\n",
    "            print('++++ Deg =', single_deg)\n",
    "            if i==0:\n",
    "                # Preprocess the data (cleaning and adding features)\n",
    "                train_tx, test_tx, len_init_data = prepare_data(train_tx, test_tx, single_deg);\n",
    "            else:\n",
    "                nb_cols_tx = train_tx.shape[1]\n",
    "                # Add the supplementary powers of the features with respect to the previous iteration and standardize\n",
    "                train_tx = add_powers(train_tx, single_deg, 0, len_init_data, features='x', current_max_deg=single_deg-1)\n",
    "                test_tx = add_powers(test_tx, single_deg, 0, len_init_data, features='x', current_max_deg=single_deg-1)\n",
    "                train_tx[:,nb_cols_tx:] = standardize(train_tx[:,nb_cols_tx:])[0]\n",
    "                test_tx[:,nb_cols_tx:] = standardize(test_tx[:,nb_cols_tx:])[0]\n",
    "                unique_cols = keep_unique_cols(train_tx)\n",
    "                train_tx = train_tx[:,unique_cols]\n",
    "                test_tx = test_tx[:,unique_cols]\n",
    "                \n",
    "            # Compute the weights with LS\n",
    "            weights, loss = least_squares(train_y, train_tx, fct='mse');\n",
    "\n",
    "            # Compute the predictions\n",
    "            y_pred_train = predict_labels(weights, train_tx)\n",
    "            y_pred_test = predict_labels(weights, test_tx);\n",
    "            accuracy_train[k, i] = np.sum(y_pred_train == train_y)/len(train_y)\n",
    "            accuracy_test[k, i] = np.sum(y_pred_test == test_y)/len(test_y)\n",
    "\n",
    "    accuracies_test = np.mean(accuracy_test, axis=0); # mean on the k's\n",
    "    accuracies_train = np.mean(accuracy_train, axis=0);\n",
    "\n",
    "    max_index, acc_max = max(enumerate(accuracies_test), key=operator.itemgetter(1))\n",
    "    print(accuracy_test)\n",
    "    print(accuracies_test)\n",
    "    print(k_fold)\n",
    "    print(len(degs))\n",
    "    print(max_index)\n",
    "    print(acc_max)\n",
    "    deg_best = degs[max_index]\n",
    "    \n",
    "    return deg_best, acc_max;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "OUT_FOLDER = 'output/'\n",
    "name = 'least_squares.csv'\n",
    "\n",
    "degs=range(2,14)\n",
    "k_fold=5\n",
    "\n",
    "mask_jets_train = split_jets_mask(tx_train)\n",
    "mask_jets_test = split_jets_mask(tx_test)\n",
    "len_mask = len(mask_jets_train)\n",
    "y_preds_train = np.zeros(len(y_train))\n",
    "y_preds_test = np.zeros(tx_test.shape[0])\n",
    "\n",
    "\n",
    "for mask_jet_id in range(len_mask):\n",
    "    print('**** Starting Jet ', mask_jet_id, '*****')\n",
    "    tx_single_jet_train = tx_train[mask_jets_train[mask_jet_id]]\n",
    "    tx_single_jet_test = tx_test[mask_jets_test[mask_jet_id]]\n",
    "    y_single_jet_train = y_train[mask_jets_train[mask_jet_id]]\n",
    "    \n",
    "    best_deg, best_acc = cross_validation_least_square(y_single_jet_train, tx_single_jet_train, k_fold, degs)\n",
    "    print('Best degree = ', best_deg)\n",
    "    \n",
    "    tx_single_jet_train, tx_single_jet_test,len_init_data = prepare_data(tx_single_jet_train,tx_single_jet_test, best_deg)\n",
    "    w, loss = func_least_squares(y=y_single_jet_train, tx=tx_single_jet_train, test_set=tx_single_jet_test, fct='mse')\n",
    "    \n",
    "    y_pred_train = predict_labels(w, tx_single_jet_train)\n",
    "    y_pred_test = predict_labels(w, tx_single_jet_test)\n",
    "    y_preds_train[mask_jets_train[mask_jet_id]] = y_pred_train\n",
    "    y_preds_test[mask_jets_test[mask_jet_id]] = y_pred_test\n",
    "    \n",
    "    right_train = np.sum(y_single_jet_train == y_pred_train)/len(y_single_jet_train)*100\n",
    "    \n",
    "    print('**** Jet ', mask_jet_id, '***** Accuracy jet', right_train)\n",
    "    \n",
    "\n",
    "create_csv_submission(ids_test, y_preds_test, OUT_FOLDER+name)\n",
    "right_train_total = np.sum(y_train == y_preds_train)/len(y_train)*100\n",
    "print('>>>>>>>> Accuracy TOTAL ',right_train_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_preds_test[y_preds_test==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_preds_test[y_preds_test==-1])+len(y_preds_test[y_preds_test==1])==tx_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds_test[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cross validation su gamma e degree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "\n",
    "\n",
    "def cross_validation_GD(y, tx, k_fold, max_iters, degs,lambdas):            \n",
    "    seed=1;\n",
    "    \n",
    "    # get k'th subgroup in test, others in train\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    accuracy_train = np.zeros([k_fold, len(degs),len(gammas)])\n",
    "    accuracy_test = np.zeros([k_fold, len(degs),len(gammas)])\n",
    "\n",
    "    acc_max=0;\n",
    "    deg_best=0;\n",
    "    gammas_best=0;\n",
    "               \n",
    "    for k in range(k_fold):\n",
    "        print('----- FOLD', k, '-----')\n",
    "        k_index = k_indices[k]\n",
    "        test_y = y[k_index]\n",
    "        test_tx = tx[k_index,:]\n",
    "        mask = np.ones(len(y), dtype=bool) # set all elements to True\n",
    "        mask[k_index] = False              # set test elements to False\n",
    "        train_tx = tx[mask,:]              # select only True elements (ie train elements)\n",
    "        train_y = y[mask]\n",
    "\n",
    "        len_init_data = 0\n",
    "\n",
    "        for i, single_deg in enumerate(degs):\n",
    "            print('++++ Deg =', single_deg)\n",
    "            for j , single_gamma_ in  enumerate(gammas):\n",
    "                print('++++ gamma =', single_gamma_)\n",
    "                \n",
    "                if i==0:\n",
    "                    train_tx, test_tx, len_init_data = prepare_data(train_tx, test_tx, single_deg);\n",
    "                else:\n",
    "                    shape_tx=train_tx.shape[1];\n",
    "                    train_tx = add_powers(train_tx, single_deg, 0, len_init_data, features='x', current_max_deg=single_deg-1)\n",
    "                    test_tx = add_powers(test_tx, single_deg, 0, len_init_data, features='x', current_max_deg=single_deg-1)\n",
    "                    train_tx[:,shape_tx:]=standardize(train_tx[:,shape_tx:])[0]\n",
    "                initial_w = np.zeros(train_tx.shape[1]);\n",
    "                \n",
    "                weights,loss = least_squares_GD(train_y,train_tx, initial_w, max_iters, single_gamma_,fct='mse');\n",
    "\n",
    "                # Compute the predictions\n",
    "                y_pred_train = predict_labels(weights, train_tx);\n",
    "                y_pred_test = predict_labels(weights, test_tx);\n",
    "                accuracy_train[k, i,j] = np.sum(y_pred_train == train_y)/len(train_y)\n",
    "                accuracy_test[k, i,j] = np.sum(y_pred_test == test_y)/len(test_y)\n",
    "        \n",
    "      \n",
    "                \n",
    "    accuracies_test=np.mean(accuracy_test, axis=0); # mean on the k's\n",
    "    accuracies_train=np.mean(accuracy_train, axis=0);\n",
    "\n",
    " \n",
    "    for i, single_deg in enumerate(degs):\n",
    "         for j , single_gamma_ in  enumerate(gammas):\n",
    "                if (accuracies_test[i,j]>acc_max):\n",
    "                    deg_best=degs[i];\n",
    "                    gammas_best=gammas[j];\n",
    "                    acc_mac=accuracies_test[i,j];\n",
    "    \n",
    "    \n",
    "                    \n",
    "        \n",
    "                \n",
    "    return [deg_best, gamma_best, acc_max];\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1]\n",
      " [2 2 2]]\n",
      "(2, 2)\n",
      "[[ 1 -1 -1]\n",
      " [ 2  1  1]]\n"
     ]
    }
   ],
   "source": [
    "cubic=np.array([[1,1,1],[2,2,2]])\n",
    "print(cubic)\n",
    "print (cubic[:,1:].shape)\n",
    "\n",
    "cubic[:,1:]=standardize(cubic[:,1:])[0];\n",
    "print(cubic)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Starting Jet  0 *****\n",
      "----- FOLD 0 -----\n",
      "++++ Deg = 2\n",
      "++++ gamma = 0.001\n",
      "Gradient Descent(0/499): loss=0.4999999999999887, w0=-6.887232304259569e-07, w1=-0.00035851239558151824\n",
      "Gradient Descent(1/499): loss=0.49797585286474055, w0=-8.131890305572223e-07, w1=-0.0007123693711819188\n",
      "Gradient Descent(2/499): loss=0.4960032517922736, w0=-4.027966359172623e-07, w1=-0.0010616534560791754\n",
      "Gradient Descent(3/499): loss=0.49408045414178847, w0=5.141394643647856e-07, w1=-0.0014064454675072578\n",
      "Gradient Descent(4/499): loss=0.49220578491254147, w0=1.9103536246219008e-06, w1=-0.0017468245514788362\n",
      "Gradient Descent(5/499): loss=0.4903776337399506, w0=3.7595942829996308e-06, w1=-0.0020828682225219777\n",
      "Gradient Descent(6/499): loss=0.4885944520420077, w0=6.036590309182145e-06, w1=-0.0024146524023619265\n",
      "Gradient Descent(7/499): loss=0.48685475030780184, w0=8.71701842470867e-06, w1=-0.002742251457578074\n",
      "Gradient Descent(8/499): loss=0.4851570955204178, w0=1.177747166218384e-05, w1=-0.003065738236265373\n",
      "Gradient Descent(9/499): loss=0.48350010870692034, w0=1.5195428830738795e-05, w1=-0.003385184103728541\n",
      "Gradient Descent(10/499): loss=0.4818824626085558, w0=1.8949224956120952e-05, w1=-0.003700658977236597\n",
      "Gradient Descent(11/499): loss=0.4803028794646931, w0=2.301802266477952e-05, w1=-0.004012231359864415\n",
      "Gradient Descent(12/499): loss=0.47876012890439956, w0=2.738178448227182e-05, w1=-0.0043199683734472\n",
      "Gradient Descent(13/499): loss=0.4772530259398928, w0=3.202124601724377e-05, w1=-0.004623935790673041\n",
      "Gradient Descent(14/499): loss=0.4757804290564409, w0=3.691789000313702e-05, w1=-0.0049241980663379155\n",
      "Gradient Descent(15/499): loss=0.4743412383935871, w0=4.205392117064597e-05, w1=-0.005220818367786817\n",
      "Gradient Descent(16/499): loss=0.47293439401287385, w0=4.741224192479285e-05, w1=-0.005513858604563982\n",
      "Gradient Descent(17/499): loss=0.471558874247503, w0=5.297642880130473e-05, w1=-0.005803379457294489\n",
      "Gradient Descent(18/499): loss=0.47021369412963504, w0=5.87307096777702e-05, w1=-0.006089440405818863\n",
      "Gradient Descent(19/499): loss=0.46889790389127084, w0=6.465994171581929e-05, w1=-0.006372099756601676\n",
      "Gradient Descent(20/499): loss=0.4676105875348819, w0=7.074959001131388e-05, w1=-0.006651414669434509\n",
      "Gradient Descent(21/499): loss=0.4663508614701768, w0=7.698570693025614e-05, w1=-0.006927441183453031\n",
      "Gradient Descent(22/499): loss=0.4651178732135864, w0=8.335491210881961e-05, w1=-0.0072002342424874015\n",
      "Gradient Descent(23/499): loss=0.46391080014725017, w0=8.984437309658323e-05, w1=-0.007469847719764592\n",
      "Gradient Descent(24/499): loss=0.46272884833445105, w0=9.644178662270383e-05, w1=-0.007736334441980708\n",
      "Gradient Descent(25/499): loss=0.461571251388631, w0=0.00010313536046539582, w1=-0.007999746212760861\n",
      "Gradient Descent(26/499): loss=0.4604372693932619, w0=0.00010991379590570177, w1=-0.008260133835523584\n",
      "Gradient Descent(27/499): loss=0.45932618787001245, w0=0.00011676627074713295, w1=-0.008517547135766346\n",
      "Gradient Descent(28/499): loss=0.4582373167927774, w0=0.00012368242288333443, w1=-0.00877203498278819\n",
      "Gradient Descent(29/499): loss=0.4571699896452802, w0=0.0001306523343964889, w1=-0.009023645310865061\n",
      "Gradient Descent(30/499): loss=0.4561235625200779, w0=0.00013766651616971395, w1=-0.00927242513989295\n",
      "Gradient Descent(31/499): loss=0.455097413256916, w0=0.0001447158929972319, w1=-0.009518420595513513\n",
      "Gradient Descent(32/499): loss=0.4540909406184988, w0=0.00015179178917659844, w1=-0.009761676928736415\n",
      "Gradient Descent(33/499): loss=0.4531035635018345, w0=0.00015888591456776982, w1=-0.010002238535072226\n",
      "Gradient Descent(34/499): loss=0.45213472018342227, w0=0.00016599035110426285, w1=-0.0102401489731893\n",
      "Gradient Descent(35/499): loss=0.4511838675966362, w0=0.0001730975397421253, w1=-0.010475450983107643\n",
      "Gradient Descent(36/499): loss=0.45025048063975326, w0=0.00018020026783288067, w1=-0.010708186503942474\n",
      "Gradient Descent(37/499): loss=0.4493340515131503, w0=0.00018729165690704484, w1=-0.010938396691209712\n",
      "Gradient Descent(38/499): loss=0.44843408908427673, w0=0.00019436515085523187, w1=-0.011166121933705354\n",
      "Gradient Descent(39/499): loss=0.44755011827908286, w0=0.00020141450449427282, w1=-0.011391401869970315\n",
      "Gradient Descent(40/499): loss=0.4466816794986508, w0=0.00020843377250616524, w1=-0.011614275404351988\n",
      "Gradient Descent(41/499): loss=0.4458283280598442, w0=0.00021541729873805277, w1=-0.011834780722673424\n",
      "Gradient Descent(42/499): loss=0.444989633658853, w0=0.00022235970585180413, w1=-0.012052955307520772\n",
      "Gradient Descent(43/499): loss=0.44416517985656573, w0=0.00022925588531211833, w1=-0.012268835953159248\n",
      "Gradient Descent(44/499): loss=0.44335456358476166, w0=0.00023610098770243058, w1=-0.012482458780087668\n",
      "Gradient Descent(45/499): loss=0.44255739467216554, w0=0.00024289041335822926, w1=-0.012693859249241228\n",
      "Gradient Descent(46/499): loss=0.441773295389451, w0=0.00024961980330771947, w1=-0.012903072175851994\n",
      "Gradient Descent(47/499): loss=0.44100190001233763, w0=0.00025628503051008494, w1=-0.01311013174297626\n",
      "Gradient Descent(48/499): loss=0.4402428544019569, w0=0.00026288219138190493, w1=-0.013315071514697659\n",
      "Gradient Descent(49/499): loss=0.4394958156017136, w0=0.0002694075976025794, w1=-0.01351792444901471\n",
      "Gradient Descent(50/499): loss=0.43876045144990483, w0=0.0002758577681899017, w1=-0.013718722910421154\n",
      "Gradient Descent(51/499): loss=0.4380364402073923, w0=0.0002822294218371971, w1=-0.01391749868218729\n",
      "Gradient Descent(52/499): loss=0.4373234701996675, w0=0.0002885194695037135, w1=-0.014114282978350183\n",
      "Gradient Descent(53/499): loss=0.43662123947267384, w0=0.00029472500725021125, w1=-0.014309106455420492\n",
      "Gradient Descent(54/499): loss=0.4359294554617841, w0=0.00030084330931195357, w1=-0.01450199922381337\n",
      "Gradient Descent(55/499): loss=0.43524783467336314, w0=0.0003068718214015409, w1=-0.014692990859010716\n",
      "Gradient Descent(56/499): loss=0.4345761023783725, w0=0.00031280815423427225, w1=-0.01488211041246183\n",
      "Gradient Descent(57/499): loss=0.43391399231749717, w0=0.00031865007726894365, w1=-0.015069386422229353\n",
      "Gradient Descent(58/499): loss=0.4332612464173058, w0=0.00032439551265721936, w1=-0.015254846923387123\n",
      "Gradient Descent(59/499): loss=0.43261761451697295, w0=0.0003300425293949237, w1=-0.015438519458176479\n",
      "Gradient Descent(60/499): loss=0.43198285410512094, w0=0.00033558933766881306, w1=-0.015620431085927249\n",
      "Gradient Descent(61/499): loss=0.43135673006635256, w0=0.00034103428339258787, w1=-0.015800608392749584\n",
      "Gradient Descent(62/499): loss=0.4307390144370748, w0=0.0003463758429261012, w1=-0.015979077501002557\n",
      "Gradient Descent(63/499): loss=0.430129486170225, w0=0.0003516126179719107, w1=-0.016155864078545325\n",
      "Gradient Descent(64/499): loss=0.4295279309085349, w0=0.00035674333064350316, w1=-0.01633099334777646\n",
      "Gradient Descent(65/499): loss=0.4289341407659796, w0=0.00036176681869970137, w1=-0.016504490094466882\n",
      "Gradient Descent(66/499): loss=0.42834791411708145, w0=0.00036668203093993286, w1=-0.01667637867639177\n",
      "Gradient Descent(67/499): loss=0.4277690553937472, w0=0.00037148802275520967, w1=-0.01684668303176653\n",
      "Gradient Descent(68/499): loss=0.42719737488933796, w0=0.00037618395182982854, w1=-0.017015426687491866\n",
      "Gradient Descent(69/499): loss=0.42663268856967984, w0=0.0003807690739889587, w1=-0.017182632767212835\n",
      "Gradient Descent(70/499): loss=0.42607481789073953, w0=0.00038524273918743684, w1=-0.017348323999196634\n",
      "Gradient Descent(71/499): loss=0.4255235896227021, w0=0.00038960438763523385, w1=-0.01751252272403367\n",
      "Gradient Descent(72/499): loss=0.4249788356801966, w0=0.0003938535460552043, w1=-0.017675250902166446\n",
      "Gradient Descent(73/499): loss=0.4244403929584314, w0=0.0003979898240688639, w1=-0.017836530121250617\n",
      "Gradient Descent(74/499): loss=0.42390810317500527, w0=0.0004020129107060768, w1=-0.017996381603352395\n",
      "Gradient Descent(75/499): loss=0.4233818127171795, w0=0.000405922571034663, w1=-0.018154826211986483\n",
      "Gradient Descent(76/499): loss=0.4228613724943959, w0=0.00040971864290606204, w1=-0.018311884458998517\n",
      "Gradient Descent(77/499): loss=0.42234663779584236, w0=0.00041340103381331113, w1=-0.01846757651129588\n",
      "Gradient Descent(78/499): loss=0.4218374681528728, w0=0.00041696971785771283, w1=-0.018621922197430734\n",
      "Gradient Descent(79/499): loss=0.42133372720609685, w0=0.00042042473282068323, w1=-0.01877494101403889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(80/499): loss=0.42083528257696634, w0=0.0004237661773373804, w1=-0.01892665213213816\n",
      "Gradient Descent(81/499): loss=0.42034200574368386, w0=0.00042699420816882136, w1=-0.019077074403289603\n",
      "Gradient Descent(82/499): loss=0.41985377192127993, w0=0.00043010903756929904, w1=-0.01922622636562512\n",
      "Gradient Descent(83/499): loss=0.41937045994569533, w0=0.00043311093074601173, w1=-0.019374126249744668\n",
      "Gradient Descent(84/499): loss=0.41889195216172925, w0=0.0004360002034079138, w1=-0.019520791984486273\n",
      "Gradient Descent(85/499): loss=0.41841813431470437, w0=0.00043877721940089297, w1=-0.019666241202572016\n",
      "Gradient Descent(86/499): loss=0.4179488954457171, w0=0.0004414423884264684, w1=-0.019810491246132975\n",
      "Gradient Descent(87/499): loss=0.4174841277903405, w0=0.0004439961638412945, w1=-0.019953559172116112\n",
      "Gradient Descent(88/499): loss=0.4170237266806549, w0=0.00044643904053483935, w1=-0.02009546175757597\n",
      "Gradient Descent(89/499): loss=0.4165675904504881, w0=0.00044877155288269193, w1=-0.020236215504853952\n",
      "Gradient Descent(90/499): loss=0.4161156203437467, w0=0.0004509942727730303, w1=-0.020375836646647945\n",
      "Gradient Descent(91/499): loss=0.4156677204257328, w0=0.0004531078077038631, w1=-0.020514341150974894\n",
      "Gradient Descent(92/499): loss=0.4152237974973342, w0=0.00045511279894873077, w1=-0.02065174472602895\n",
      "Gradient Descent(93/499): loss=0.41478376101199155, w0=0.0004570099197886274, w1=-0.02078806282493763\n",
      "Gradient Descent(94/499): loss=0.4143475229953423, w0=0.00045879987380797343, w1=-0.020923310650418536\n",
      "Gradient Descent(95/499): loss=0.4139149979674472, w0=0.0004604833932525396, w1=-0.02105750315933887\n",
      "Gradient Descent(96/499): loss=0.41348610286750975, w0=0.0004620612374472884, w1=-0.021190655067180212\n",
      "Gradient Descent(97/499): loss=0.4130607569810022, w0=0.00046353419127216335, w1=-0.02132278085241068\n",
      "Gradient Descent(98/499): loss=0.41263888186911546, w0=0.00046490306369392, w1=-0.02145389476076674\n",
      "Gradient Descent(99/499): loss=0.4122204013004491, w0=0.00046616868635215164, w1=-0.021584010809446785\n",
      "Gradient Descent(100/499): loss=0.41180524118487005, w0=0.0004673319121977222, w1=-0.021713142791218547\n",
      "Gradient Descent(101/499): loss=0.4113933295094613, w0=0.00046839361418187577, w1=-0.02184130427844238\n",
      "Gradient Descent(102/499): loss=0.41098459627649053, w0=0.0004693546839943457, w1=-0.021968508627012363\n",
      "Gradient Descent(103/499): loss=0.410578973443333, w0=0.0004702160308488419, w1=-0.02209476898021719\n",
      "Gradient Descent(104/499): loss=0.4101763948642769, w0=0.0004709785803143435, w1=-0.022220098272522647\n",
      "Gradient Descent(105/499): loss=0.4097767962341534, w0=0.0004716432731906771, w1=-0.02234450923327756\n",
      "Gradient Descent(106/499): loss=0.4093801150337262, w0=0.0004722110644269067, w1=-0.02246801439034493\n",
      "Gradient Descent(107/499): loss=0.408986290476785, w0=0.0004726829220811099, w1=-0.022590626073660053\n",
      "Gradient Descent(108/499): loss=0.4085952634588813, w0=0.00047305982632015993, w1=-0.022712356418717203\n",
      "Gradient Descent(109/499): loss=0.40820697650765925, w0=0.0004733427684581761, w1=-0.022833217369986634\n",
      "Gradient Descent(110/499): loss=0.4078213737347197, w0=0.0004735327500323496, w1=-0.022953220684263417\n",
      "Gradient Descent(111/499): loss=0.40743840078897586, w0=0.0004736307819148918, w1=-0.023072377933949676\n",
      "Gradient Descent(112/499): loss=0.407058004811444, w0=0.00047363788345989173, w1=-0.023190700510271778\n",
      "Gradient Descent(113/499): loss=0.4066801343914279, w0=0.00047355508168390954, w1=-0.023308199626433913\n",
      "Gradient Descent(114/499): loss=0.406304739524047, w0=0.00047338341047916874, w1=-0.023424886320709524\n",
      "Gradient Descent(115/499): loss=0.4059317715690684, w0=0.00047312390985824765, w1=-0.02354077145947197\n",
      "Gradient Descent(116/499): loss=0.40556118321099577, w0=0.00047277762522920474, w1=-0.02365586574016582\n",
      "Gradient Descent(117/499): loss=0.40519292842038057, w0=0.00047234560670010703, w1=-0.02377017969422007\n",
      "Gradient Descent(118/499): loss=0.40482696241631017, w0=0.00047182890841196374, w1=-0.023883723689904622\n",
      "Gradient Descent(119/499): loss=0.404463241630037, w0=0.00047122858789909943, w1=-0.02399650793513125\n",
      "Gradient Descent(120/499): loss=0.40410172366971475, w0=0.0004705457054760313, w1=-0.02410854248020032\n",
      "Gradient Descent(121/499): loss=0.4037423672862031, w0=0.00046978132364994604, w1=-0.024219837220494447\n",
      "Gradient Descent(122/499): loss=0.40338513233990547, w0=0.00046893650655790064, w1=-0.024330401899120262\n",
      "Gradient Descent(123/499): loss=0.4030299797686134, w0=0.0004680123194278987, w1=-0.02444024610949945\n",
      "Gradient Descent(124/499): loss=0.40267687155631693, w0=0.00046700982806302245, w1=-0.024549379297910137\n",
      "Gradient Descent(125/499): loss=0.40232577070295694, w0=0.0004659300983478261, w1=-0.024657810765979768\n",
      "Gradient Descent(126/499): loss=0.40197664119508825, w0=0.0004647741957762224, w1=-0.02476554967313047\n",
      "Gradient Descent(127/499): loss=0.401629447977421, w0=0.00046354318500011784, w1=-0.02487260503897799\n",
      "Gradient Descent(128/499): loss=0.40128415692521774, w0=0.00046223812939807815, w1=-0.024978985745685182\n",
      "Gradient Descent(129/499): loss=0.40094073481751763, w0=0.00046086009066332613, w1=-0.02508470054027104\n",
      "Gradient Descent(130/499): loss=0.40059914931115925, w0=0.00045941012841039884, w1=-0.02518975803687624\n",
      "Gradient Descent(131/499): loss=0.4002593689155809, w0=0.0004578892997998116, w1=-0.025294166718986107\n",
      "Gradient Descent(132/499): loss=0.39992136296837183, w0=0.0004562986591800978, w1=-0.02539793494161193\n",
      "Gradient Descent(133/499): loss=0.3995851016115519, w0=0.0004546392577466136, w1=-0.025501070933431555\n",
      "Gradient Descent(134/499): loss=0.3992505557685566, w0=0.00045291214321651707, w1=-0.025603582798890023\n",
      "Gradient Descent(135/499): loss=0.3989176971219052, w0=0.00045111835951934986, w1=-0.025705478520261237\n",
      "Gradient Descent(136/499): loss=0.39858649809153285, w0=0.00044925894650266847, w1=-0.02580676595967136\n",
      "Gradient Descent(137/499): loss=0.39825693181376304, w0=0.0004473349396521896, w1=-0.025907452861084822\n",
      "Gradient Descent(138/499): loss=0.3979289721209032, w0=0.0004453473698259323, w1=-0.026007546852253725\n",
      "Gradient Descent(139/499): loss=0.39760259352144395, w0=0.0004432972630018563, w1=-0.02610705544663137\n",
      "Gradient Descent(140/499): loss=0.3972777711808415, w0=0.0004411856400385107, w1=-0.0262059860452507\n",
      "Gradient Descent(141/499): loss=0.3969544809028681, w0=0.0004390135164482266, w1=-0.02630434593856837\n",
      "Gradient Descent(142/499): loss=0.396632699111511, w0=0.0004367819021823976, w1=-0.026402142308275162\n",
      "Gradient Descent(143/499): loss=0.3963124028334038, w0=0.0004344918014284121, w1=-0.026499382229073467\n",
      "Gradient Descent(144/499): loss=0.3959935696807756, w0=0.00043214421241781156, w1=-0.02659607267042246\n",
      "Gradient Descent(145/499): loss=0.39567617783489906, w0=0.00042974012724526523, w1=-0.026692220498251696\n",
      "Gradient Descent(146/499): loss=0.39536020603002386, w0=0.0004272805316979643, w1=-0.02678783247664374\n",
      "Gradient Descent(147/499): loss=0.39504563353778216, w0=0.0004247664050950518, w1=-0.02688291526948648\n",
      "Gradient Descent(148/499): loss=0.39473244015204956, w0=0.00042219872013671717, w1=-0.02697747544209573\n",
      "Gradient Descent(149/499): loss=0.3944206061742481, w0=0.00041957844276259676, w1=-0.027071519462808745\n",
      "Gradient Descent(150/499): loss=0.39411011239908, w0=0.00041690653201913245, w1=-0.02716505370454924\n",
      "Gradient Descent(151/499): loss=0.39380094010067535, w0=0.00041418393993555397, w1=-0.02725808444636445\n",
      "Gradient Descent(152/499): loss=0.3934930710191464, w0=0.00041141161140815965, w1=-0.027350617874934875\n",
      "Gradient Descent(153/499): loss=0.39318648734753026, w0=0.000408590484092582, w1=-0.027442660086057166\n",
      "Gradient Descent(154/499): loss=0.3928811717191142, w0=0.0004057214883037353, w1=-0.027534217086100776\n",
      "Gradient Descent(155/499): loss=0.39257710719512695, w0=0.00040280554692315076, w1=-0.02762529479343885\n",
      "Gradient Descent(156/499): loss=0.3922742772527904, w0=0.0003998435753134178, w1=-0.02771589903985388\n",
      "Gradient Descent(157/499): loss=0.3919726657737128, w0=0.0003968364812394551, w1=-0.027806035571918627\n",
      "Gradient Descent(158/499): loss=0.3916722570326249, w0=0.0003937851647963482, w1=-0.02789571005235283\n",
      "Gradient Descent(159/499): loss=0.3913730356864371, w0=0.0003906905183434972, w1=-0.027984928061356128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(160/499): loss=0.3910749867636141, w0=0.00038755342644482683, w1=-0.0280736950979177\n",
      "Gradient Descent(161/499): loss=0.39077809565386007, w0=0.0003843747658148198, w1=-0.0281620165811031\n",
      "Gradient Descent(162/499): loss=0.3904823480980966, w0=0.0003811554052701428, w1=-0.028249897851318664\n",
      "Gradient Descent(163/499): loss=0.39018773017873365, w0=0.0003778962056866413, w1=-0.02833734417155401\n",
      "Gradient Descent(164/499): loss=0.3898942283102193, w0=0.0003745980199614876, w1=-0.028424360728603017\n",
      "Gradient Descent(165/499): loss=0.38960182922986214, w0=0.0003712616929802736, w1=-0.02851095263426369\n",
      "Gradient Descent(166/499): loss=0.38931051998891775, w0=0.0003678880615888467, w1=-0.028597124926517347\n",
      "Gradient Descent(167/499): loss=0.38902028794393084, w0=0.00036447795456969454, w1=-0.028682882570687542\n",
      "Gradient Descent(168/499): loss=0.3887311207483282, w0=0.00036103219262269054, w1=-0.02876823046057905\n",
      "Gradient Descent(169/499): loss=0.3884430063442496, w0=0.0003575515883500179, w1=-0.028853173419597367\n",
      "Gradient Descent(170/499): loss=0.3881559329546168, w0=0.0003540369462450981, w1=-0.028937716201849078\n",
      "Gradient Descent(171/499): loss=0.3878698890754271, w0=0.00035048906268535305, w1=-0.029021863493223436\n",
      "Gradient Descent(172/499): loss=0.38758486346826887, w0=0.0003469087259286384, w1=-0.029105619912455544\n",
      "Gradient Descent(173/499): loss=0.3873008451530506, w0=0.000343296716113189, w1=-0.029188990012171463\n",
      "Gradient Descent(174/499): loss=0.38701782340093815, w0=0.00033965380526092563, w1=-0.029271978279915605\n",
      "Gradient Descent(175/499): loss=0.38673578772749473, w0=0.0003359807572839733, w1=-0.029354589139160746\n",
      "Gradient Descent(176/499): loss=0.38645472788601476, w0=0.00033227832799425137, w1=-0.02943682695030099\n",
      "Gradient Descent(177/499): loss=0.38617463386104967, w0=0.00032854726511599686, w1=-0.02951869601162798\n",
      "Gradient Descent(178/499): loss=0.3858954958621168, w0=0.0003247883083010889, w1=-0.029600200560290713\n",
      "Gradient Descent(179/499): loss=0.38561730431758856, w0=0.00032100218914704614, w1=-0.02968134477323922\n",
      "Gradient Descent(180/499): loss=0.38534004986875586, w0=0.0003171896312175745, w1=-0.029762132768152462\n",
      "Gradient Descent(181/499): loss=0.38506372336405814, w0=0.0003133513500655454, w1=-0.029842568604350684\n",
      "Gradient Descent(182/499): loss=0.3847883158534798, w0=0.0003094880532582898, w1=-0.029922656283692565\n",
      "Gradient Descent(183/499): loss=0.38451381858310496, w0=0.0003056004404050977, w1=-0.030002399751457406\n",
      "Gradient Descent(184/499): loss=0.3842402229898259, w0=0.0003016892031868157, w1=-0.030081802897212656\n",
      "Gradient Descent(185/499): loss=0.3839675206962029, w0=0.0002977550253874395, w1=-0.030160869555667044\n",
      "Gradient Descent(186/499): loss=0.3836957035054677, w0=0.0002937985829276024, w1=-0.030239603507509573\n",
      "Gradient Descent(187/499): loss=0.38342476339667136, w0=0.0002898205438998632, w1=-0.030318008480234648\n",
      "Gradient Descent(188/499): loss=0.38315469251996626, w0=0.00028582156860570144, w1=-0.03039608814895357\n",
      "Gradient Descent(189/499): loss=0.38288548319202376, w0=0.0002818023095941313, w1=-0.030473846137192676\n",
      "Gradient Descent(190/499): loss=0.38261712789158, w0=0.00027776341170184714, w1=-0.030551286017678342\n",
      "Gradient Descent(191/499): loss=0.3823496192551077, w0=0.00027370551209481855, w1=-0.0306284113131091\n",
      "Gradient Descent(192/499): loss=0.3820829500726101, w0=0.0002696292403112554, w1=-0.030705225496915102\n",
      "Gradient Descent(193/499): loss=0.38181711328353385, w0=0.00026553521830586515, w1=-0.030781731994005163\n",
      "Gradient Descent(194/499): loss=0.381552101972796, w0=0.000261424060495329, w1=-0.030857934181501588\n",
      "Gradient Descent(195/499): loss=0.38128790936692397, w0=0.0002572963738049254, w1=-0.030933835389463028\n",
      "Gradient Descent(196/499): loss=0.3810245288303031, w0=0.0002531527577162315, w1=-0.03100943890159557\n",
      "Gradient Descent(197/499): loss=0.3807619538615297, w0=0.00024899380431583776, w1=-0.031084747955952265\n",
      "Gradient Descent(198/499): loss=0.38050017808986714, w0=0.00024482009834501055, w1=-0.031159765745621307\n",
      "Gradient Descent(199/499): loss=0.3802391952718, w0=0.0002406322172502421, w1=-0.031234495419403076\n",
      "Gradient Descent(200/499): loss=0.37997899928768775, w0=0.00023643073123462869, w1=-0.031308940082476214\n",
      "Gradient Descent(201/499): loss=0.37971958413850815, w0=0.00023221620331002023, w1=-0.031383102797052954\n",
      "Gradient Descent(202/499): loss=0.3794609439426943, w0=0.0002279891893498864, w1=-0.03145698658302389\n",
      "Gradient Descent(203/499): loss=0.37920307293306005, w0=0.00022375023814284718, w1=-0.031530594418592385\n",
      "Gradient Descent(204/499): loss=0.37894596545381043, w0=0.0002194998914468164, w1=-0.03160392924089873\n",
      "Gradient Descent(205/499): loss=0.37868961595763656, w0=0.00021523868404371078, w1=-0.031676993946634334\n",
      "Gradient Descent(206/499): loss=0.37843401900289053, w0=0.0002109671437946765, w1=-0.03174979139264609\n",
      "Gradient Descent(207/499): loss=0.3781791692508395, w0=0.00020668579169578943, w1=-0.031822324396531015\n",
      "Gradient Descent(208/499): loss=0.37792506146299687, w0=0.00020239514193418515, w1=-0.031894595737221434\n",
      "Gradient Descent(209/499): loss=0.37767169049852667, w0=0.00019809570194457752, w1=-0.03196660815556082\n",
      "Gradient Descent(210/499): loss=0.3774190513117211, w0=0.00019378797246612592, w1=-0.03203836435487048\n",
      "Gradient Descent(211/499): loss=0.37716713894954734, w0=0.000189472447599613, w1=-0.03210986700150719\n",
      "Gradient Descent(212/499): loss=0.37691594854926336, w0=0.000185149614864896, w1=-0.03218111872541201\n",
      "Gradient Descent(213/499): loss=0.37666547533609873, w0=0.0001808199552585967, w1=-0.03225212212065039\n",
      "Gradient Descent(214/499): loss=0.3764157146210009, w0=0.0001764839433119958, w1=-0.0323228797459437\n",
      "Gradient Descent(215/499): loss=0.37616666179844316, w0=0.00017214204714909972, w1=-0.03239339412519241\n",
      "Gradient Descent(216/499): loss=0.37591831234429357, w0=0.00016779472854484847, w1=-0.03246366774799095\n",
      "Gradient Descent(217/499): loss=0.3756706618137425, w0=0.00016344244298343463, w1=-0.03253370307013447\n",
      "Gradient Descent(218/499): loss=0.3754237058392881, w0=0.00015908563971670547, w1=-0.03260350251411765\n",
      "Gradient Descent(219/499): loss=0.3751774401287763, w0=0.0001547247618226201, w1=-0.03267306846962562\n",
      "Gradient Descent(220/499): loss=0.37493186046349586, w0=0.00015036024626373604, w1=-0.0327424032940172\n",
      "Gradient Descent(221/499): loss=0.3746869626963255, w0=0.0001459925239456999, w1=-0.03281150931280056\n",
      "Gradient Descent(222/499): loss=0.37444274274993256, w0=0.0001416220197757183, w1=-0.0328803888201014\n",
      "Gradient Descent(223/499): loss=0.3741991966150195, w0=0.0001372491527209857, w1=-0.03294904407912386\n",
      "Gradient Descent(224/499): loss=0.3739563203486212, w0=0.00013287433586704785, w1=-0.03301747732260417\n",
      "Gradient Descent(225/499): loss=0.37371411007244665, w0=0.00012849797647607953, w1=-0.03308569075325728\n",
      "Gradient Descent(226/499): loss=0.373472561971268, w0=0.00012412047604505648, w1=-0.033153686544216465\n",
      "Gradient Descent(227/499): loss=0.373231672291352, w0=0.0001197422303638027, w1=-0.033221466839466154\n",
      "Gradient Descent(228/499): loss=0.37299143733893625, w0=0.00011536362957289468, w1=-0.03328903375426798\n",
      "Gradient Descent(229/499): loss=0.37275185347874523, w0=0.00011098505822140529, w1=-0.033356389375580234\n",
      "Gradient Descent(230/499): loss=0.3725129171325487, w0=0.00010660689532447079, w1=-0.03342353576247084\n",
      "Gradient Descent(231/499): loss=0.372274624777758, w0=0.00010222951442066514, w1=-0.03349047494652388\n",
      "Gradient Descent(232/499): loss=0.37203697294606186, w0=9.785328362916657e-05, w1=-0.033557208932239874\n",
      "Gradient Descent(233/499): loss=0.3717999582220979, w0=9.347856570670224e-05, w1=-0.033623739697429864\n",
      "Gradient Descent(234/499): loss=0.3715635772421626, w0=8.910571810425727e-05, w1=-0.0336900691936034\n",
      "Gradient Descent(235/499): loss=0.37132782669295333, w0=8.473509302353555e-05, w1=-0.033756199346350574\n",
      "Gradient Descent(236/499): loss=0.3710927033103476, w0=8.036703747315957e-05, w1=-0.033822132055718136\n",
      "Gradient Descent(237/499): loss=0.37085820387821344, w0=7.60018933245984e-05, w1=-0.033887869196579865\n",
      "Gradient Descent(238/499): loss=0.3706243252272522, w0=7.163999736781217e-05, w1=-0.033953412619001214\n",
      "Gradient Descent(239/499): loss=0.3703910642338744, w0=6.728168136660315e-05, w1=-0.034018764148598384\n",
      "Gradient Descent(240/499): loss=0.370158417819104, w0=6.292727211366316e-05, w1=-0.03408392558689187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(241/499): loss=0.36992638294751284, w0=5.857709148530863e-05, w1=-0.03414889871165462\n",
      "Gradient Descent(242/499): loss=0.36969495662618473, w0=5.4231456495893864e-05, w1=-0.03421368527725486\n",
      "Gradient Descent(243/499): loss=0.3694641359037063, w0=4.989067935189491e-05, w1=-0.03427828701499366\n",
      "Gradient Descent(244/499): loss=0.3692339178691861, w0=4.5555067505655796e-05, w1=-0.03434270563343737\n",
      "Gradient Descent(245/499): loss=0.36900429965129955, w0=4.122492370879035e-05, w1=-0.03440694281874501\n",
      "Gradient Descent(246/499): loss=0.3687752784173589, w0=3.690054606523216e-05, w1=-0.0344710002349906\n",
      "Gradient Descent(247/499): loss=0.3685468513724103, w0=3.258222808392695e-05, w1=-0.034534879524480644\n",
      "Gradient Descent(248/499): loss=0.36831901575835224, w0=2.827025873116089e-05, w1=-0.034598582308066804\n",
      "Gradient Descent(249/499): loss=0.36809176885308015, w0=2.3964922482519686e-05, w1=-0.034662110185453766\n",
      "Gradient Descent(250/499): loss=0.3678651079696523, w0=1.9666499374472903e-05, w1=-0.03472546473550248\n",
      "Gradient Descent(251/499): loss=0.3676390304554791, w0=1.5375265055579066e-05, w1=-0.03478864751652879\n",
      "Gradient Descent(252/499): loss=0.36741353369153285, w0=1.109149083730693e-05, w1=-0.03485166006659753\n",
      "Gradient Descent(253/499): loss=0.36718861509157935, w0=6.815443744468858e-06, w1=-0.03491450390381223\n",
      "Gradient Descent(254/499): loss=0.36696427210143034, w0=2.547386565262537e-06, w1=-0.03497718052660035\n",
      "Gradient Descent(255/499): loss=0.36674050219821475, w0=-1.712422099082206e-06, w1=-0.03503969141399429\n",
      "Gradient Descent(256/499): loss=0.3665173028896707, w0=-5.9637277850549035e-06, w1=-0.03510203802590812\n",
      "Gradient Descent(257/499): loss=0.3662946717134544, w0=-1.0206280118016895e-05, w1=-0.03516422180341016\n",
      "Gradient Descent(258/499): loss=0.36607260623646953, w0=-1.4439832763757338e-05, w1=-0.03522624416899146\n",
      "Gradient Descent(259/499): loss=0.36585110405421295, w0=-1.866414338045485e-05, w1=-0.0352881065268302\n",
      "Gradient Descent(260/499): loss=0.3656301627901378, w0=-2.2878973571047155e-05, w1=-0.03534981026305219\n",
      "Gradient Descent(261/499): loss=0.36540978009503416, w0=-2.7084088836010237e-05, w1=-0.035411356745987374\n",
      "Gradient Descent(262/499): loss=0.3651899536464253, w0=-3.127925852654839e-05, w1=-0.03547274732642258\n",
      "Gradient Descent(263/499): loss=0.3649706811479806, w0=-3.546425579819686e-05, w1=-0.03553398333785041\n",
      "Gradient Descent(264/499): loss=0.36475196032894275, w0=-3.963885756483754e-05, w1=-0.03559506609671443\n",
      "Gradient Descent(265/499): loss=0.36453378894357136, w0=-4.3802844453129275e-05, w1=-0.035655996902650686\n",
      "Gradient Descent(266/499): loss=0.3643161647705999, w0=-4.795600075735267e-05, w1=-0.03571677703872563\n",
      "Gradient Descent(267/499): loss=0.3640990856127082, w0=-5.209811439467057e-05, w1=-0.03577740777167049\n",
      "Gradient Descent(268/499): loss=0.36388254929600683, w0=-5.622897686080381e-05, w1=-0.0358378903521121\n",
      "Gradient Descent(269/499): loss=0.3636665536695377, w0=-6.0348383186122676e-05, w1=-0.035898226014800354\n",
      "Gradient Descent(270/499): loss=0.3634510966047845, w0=-6.445613189215397e-05, w1=-0.035958415978832224\n",
      "Gradient Descent(271/499): loss=0.36323617599519875, w0=-6.855202494850335e-05, w1=-0.03601846144787248\n",
      "Gradient Descent(272/499): loss=0.3630217897557361, w0=-7.26358677301925e-05, w1=-0.03607836361037109\n",
      "Gradient Descent(273/499): loss=0.36280793582240584, w0=-7.67074689754105e-05, w1=-0.03613812363977742\n",
      "Gradient Descent(274/499): loss=0.36259461215183175, w0=-8.076664074367905e-05, w1=-0.036197742694751255\n",
      "Gradient Descent(275/499): loss=0.36238181672082415, w0=-8.481319837443004e-05, w1=-0.03625722191937066\n",
      "Gradient Descent(276/499): loss=0.36216954752596286, w0=-8.884696044599523e-05, w1=-0.0363165624433368\n",
      "Gradient Descent(277/499): loss=0.361957802583192, w0=-9.286774873500636e-05, w1=-0.0363757653821757\n",
      "Gradient Descent(278/499): loss=0.36174657992742376, w0=-9.687538817620477e-05, w1=-0.036434831837437016\n",
      "Gradient Descent(279/499): loss=0.36153587761215267, w0=-0.00010086970682265912, w1=-0.03649376289688991\n",
      "Gradient Descent(280/499): loss=0.36132569370908185, w0=-0.00010485053580638967, w1=-0.036552559634715966\n",
      "Gradient Descent(281/499): loss=0.3611160263077541, w0=-0.00010881770929939779, w1=-0.03661122311169932\n",
      "Gradient Descent(282/499): loss=0.36090687351519923, w0=-0.00011277106447509883, w1=-0.03666975437541395\n",
      "Gradient Descent(283/499): loss=0.3606982334555827, w0=-0.00011671044147015672, w1=-0.0367281544604082\n",
      "Gradient Descent(284/499): loss=0.36049010426987027, w0=-0.00012063568334671847, w1=-0.03678642438838662\n",
      "Gradient Descent(285/499): loss=0.3602824841154958, w0=-0.00012454663605504672, w1=-0.036844565168389104\n",
      "Gradient Descent(286/499): loss=0.36007537116604144, w0=-0.00012844314839654825, w1=-0.036902577796967394\n",
      "Gradient Descent(287/499): loss=0.35986876361092274, w0=-0.00013232507198719635, w1=-0.036960463258359\n",
      "Gradient Descent(288/499): loss=0.35966265965508404, w0=-0.00013619226122134516, w1=-0.03701822252465858\n",
      "Gradient Descent(289/499): loss=0.35945705751870005, w0=-0.00014004457323593355, w1=-0.03707585655598675\n",
      "Gradient Descent(290/499): loss=0.35925195543688615, w0=-0.0001438818678750763, w1=-0.0371333663006565\n",
      "Gradient Descent(291/499): loss=0.35904735165941554, w0=-0.0001477040076550403, w1=-0.03719075269533711\n",
      "Gradient Descent(292/499): loss=0.3588432444504426, w0=-0.00015151085772960362, w1=-0.03724801666521567\n",
      "Gradient Descent(293/499): loss=0.35863963208823396, w0=-0.0001553022858557944, w1=-0.03730515912415627\n",
      "Gradient Descent(294/499): loss=0.35843651286490746, w0=-0.00015907816236000792, w1=-0.03736218097485686\n",
      "Gradient Descent(295/499): loss=0.35823388508617504, w0=-0.00016283836010449855, w1=-0.037419083109003756\n",
      "Gradient Descent(296/499): loss=0.35803174707109403, w0=-0.00016658275445424463, w1=-0.037475866407423994\n",
      "Gradient Descent(297/499): loss=0.35783009715182407, w0=-0.00017031122324418313, w1=-0.03753253174023537\n",
      "Gradient Descent(298/499): loss=0.35762893367338966, w0=-0.00017402364674681182, w1=-0.037589079966994315\n",
      "Gradient Descent(299/499): loss=0.3574282549934492, w0=-0.00017771990764015635, w1=-0.03764551193684167\n",
      "Gradient Descent(300/499): loss=0.35722805948206937, w0=-0.0001813998909760989, w1=-0.03770182848864627\n",
      "Gradient Descent(301/499): loss=0.35702834552150486, w0=-0.00018506348414906655, w1=-0.03775803045114645\n",
      "Gradient Descent(302/499): loss=0.3568291115059843, w0=-0.0001887105768650759, w1=-0.03781411864308956\n",
      "Gradient Descent(303/499): loss=0.35663035584150016, w0=-0.0001923410611111313, w1=-0.03787009387336935\n",
      "Gradient Descent(304/499): loss=0.35643207694560575, w0=-0.0001959548311249743, w1=-0.03792595694116143\n",
      "Gradient Descent(305/499): loss=0.3562342732472143, w0=-0.00019955178336518068, w1=-0.037981708636056766\n",
      "Gradient Descent(306/499): loss=0.35603694318640644, w0=-0.00020313181648160322, w1=-0.03803734973819319\n",
      "Gradient Descent(307/499): loss=0.3558400852142386, w0=-0.0002066948312861563, w1=-0.03809288101838506\n",
      "Gradient Descent(308/499): loss=0.35564369779255955, w0=-0.00021024073072394012, w1=-0.038148303238251004\n",
      "Gradient Descent(309/499): loss=0.355447779393829, w0=-0.0002137694198447015, w1=-0.03820361715033983\n",
      "Gradient Descent(310/499): loss=0.35525232850094157, w0=-0.00021728080577462823, w1=-0.03825882349825462\n",
      "Gradient Descent(311/499): loss=0.3550573436070541, w0=-0.00022077479768847408, w1=-0.03831392301677502\n",
      "Gradient Descent(312/499): loss=0.3548628232154192, w0=-0.00022425130678201162, w1=-0.03836891643197777\n",
      "Gradient Descent(313/499): loss=0.3546687658392197, w0=-0.0002277102462448099, w1=-0.03842380446135548\n",
      "Gradient Descent(314/499): loss=0.3544751700014098, w0=-0.00023115153123333388, w1=-0.03847858781393374\n",
      "Gradient Descent(315/499): loss=0.35428203423455834, w0=-0.000234575078844363, w1=-0.03853326719038646\n",
      "Gradient Descent(316/499): loss=0.3540893570806968, w0=-0.00023798080808872551, w1=-0.038587843283149625\n",
      "Gradient Descent(317/499): loss=0.35389713709116943, w0=-0.0002413686398653461, w1=-0.03864231677653338\n",
      "Gradient Descent(318/499): loss=0.3537053728264892, w0=-0.0002447384969356033, w1=-0.03869668834683249\n",
      "Gradient Descent(319/499): loss=0.353514062856195, w0=-0.00024809030389799446, w1=-0.03875095866243524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(320/499): loss=0.353323205758713, w0=-0.00025142398716310454, w1=-0.03880512838393074\n",
      "Gradient Descent(321/499): loss=0.3531328001212217, w0=-0.00025473947492887637, w1=-0.03885919816421473\n",
      "Gradient Descent(322/499): loss=0.3529428445395192, w0=-0.00025803669715617915, w1=-0.0389131686485938\n",
      "Gradient Descent(323/499): loss=0.3527533376178945, w0=-0.0002613155855446725, w1=-0.03896704047488821\n",
      "Gradient Descent(324/499): loss=0.3525642779690009, w0=-0.00026457607350896284, w1=-0.03902081427353312\n",
      "Gradient Descent(325/499): loss=0.35237566421373323, w0=-0.0002678180961550493, w1=-0.039074490667678485\n",
      "Gradient Descent(326/499): loss=0.35218749498110663, w0=-0.0002710415902570566, w1=-0.03912807027328744\n",
      "Gradient Descent(327/499): loss=0.35199976890813933, w0=-0.0002742464942342511, w1=-0.039181553699233304\n",
      "Gradient Descent(328/499): loss=0.3518124846397386, w0=-0.00027743274812833834, w1=-0.039234941547395194\n",
      "Gradient Descent(329/499): loss=0.3516256408285864, w0=-0.0002806002935810384, w1=-0.03928823441275229\n",
      "Gradient Descent(330/499): loss=0.35143923613503125, w0=-0.0002837490738119359, w1=-0.03934143288347671\n",
      "Gradient Descent(331/499): loss=0.3512532692269801, w0=-0.0002868790335966032, w1=-0.039394537541025114\n",
      "Gradient Descent(332/499): loss=0.35106773877979336, w0=-0.00028999011924499187, w1=-0.039447548960228965\n",
      "Gradient Descent(333/499): loss=0.35088264347618314, w0=-0.00029308227858009206, w1=-0.039500467709383494\n",
      "Gradient Descent(334/499): loss=0.3506979820061118, w0=-0.00029615546091685453, w1=-0.039553294350335444\n",
      "Gradient Descent(335/499): loss=0.3505137530666952, w0=-0.0002992096170413743, w1=-0.039606029438569515\n",
      "Gradient Descent(336/499): loss=0.3503299553621055, w0=-0.0003022446991903324, w1=-0.0396586735232936\n",
      "Gradient Descent(337/499): loss=0.35014658760347894, w0=-0.0003052606610306928, w1=-0.03971122714752282\n",
      "Gradient Descent(338/499): loss=0.349963648508823, w0=-0.0003082574576396526, w1=-0.039763690848162336\n",
      "Gradient Descent(339/499): loss=0.349781136802927, w0=-0.0003112350454848414, w1=-0.039816065156089014\n",
      "Gradient Descent(340/499): loss=0.34959905121727497, w0=-0.0003141933824047685, w1=-0.0398683505962319\n",
      "Gradient Descent(341/499): loss=0.34941739048995923, w0=-0.00031713242758951433, w1=-0.03992054768765159\n",
      "Gradient Descent(342/499): loss=0.3492361533655968, w0=-0.000320052141561664, w1=-0.03997265694361843\n",
      "Gradient Descent(343/499): loss=0.3490553385952473, w0=-0.0003229524861574796, w1=-0.040024678871689645\n",
      "Gradient Descent(344/499): loss=0.3488749449363327, w0=-0.00032583342450830935, w1=-0.04007661397378536\n",
      "Gradient Descent(345/499): loss=0.3486949711525584, w0=-0.00032869492102223047, w1=-0.04012846274626354\n",
      "Gradient Descent(346/499): loss=0.34851541601383673, w0=-0.0003315369413659236, w1=-0.040180225679993874\n",
      "Gradient Descent(347/499): loss=0.3483362782962115, w0=-0.00033435945244677536, w1=-0.04023190326043063\n",
      "Gradient Descent(348/499): loss=0.34815755678178417, w0=-0.00033716242239520774, w1=-0.040283495967684435\n",
      "Gradient Descent(349/499): loss=0.34797925025864207, w0=-0.00033994582054723045, w1=-0.04033500427659311\n",
      "Gradient Descent(350/499): loss=0.3478013575207876, w0=-0.00034270961742721446, w1=-0.040386428656791436\n",
      "Gradient Descent(351/499): loss=0.34762387736806855, w0=-0.0003454537847308842, w1=-0.040437769572779975\n",
      "Gradient Descent(352/499): loss=0.3474468086061117, w0=-0.0003481782953085256, w1=-0.040489027483992905\n",
      "Gradient Descent(353/499): loss=0.34727015004625517, w0=-0.00035088312314840783, w1=-0.04054020284486492\n",
      "Gradient Descent(354/499): loss=0.3470939005054844, w0=-0.0003535682433604157, w1=-0.04059129610489716\n",
      "Gradient Descent(355/499): loss=0.3469180588063685, w0=-0.00035623363215989134, w1=-0.04064230770872222\n",
      "Gradient Descent(356/499): loss=0.346742623776997, w0=-0.0003588792668516816, w1=-0.040693238096168274\n",
      "Gradient Descent(357/499): loss=0.3465675942509205, w0=-0.00036150512581438937, w1=-0.04074408770232223\n",
      "Gradient Descent(358/499): loss=0.3463929690670892, w0=-0.0003641111884848266, w1=-0.04079485695759209\n",
      "Gradient Descent(359/499): loss=0.3462187470697953, w0=-0.0003666974353426662, w1=-0.04084554628776835\n",
      "Gradient Descent(360/499): loss=0.3460449271086159, w0=-0.0003692638478952906, w1=-0.04089615611408456\n",
      "Gradient Descent(361/499): loss=0.34587150803835565, w0=-0.0003718104086628352, w1=-0.0409466868532771\n",
      "Gradient Descent(362/499): loss=0.34569848871899245, w0=-0.0003743371011634236, w1=-0.04099713891764401\n",
      "Gradient Descent(363/499): loss=0.3455258680156228, w0=-0.00037684390989859275, w1=-0.04104751271510308\n",
      "Gradient Descent(364/499): loss=0.34535364479840996, w0=-0.00037933082033890606, w1=-0.04109780864924908\n",
      "Gradient Descent(365/499): loss=0.34518181794253083, w0=-0.00038179781890975166, w1=-0.0411480271194102\n",
      "Gradient Descent(366/499): loss=0.34501038632812525, w0=-0.0003842448929773239, w1=-0.04119816852070374\n",
      "Gradient Descent(367/499): loss=0.34483934884024625, w0=-0.0003866720308347861, w1=-0.04124823324409093\n",
      "Gradient Descent(368/499): loss=0.3446687043688118, w0=-0.00038907922168861155, w1=-0.04129822167643109\n",
      "Gradient Descent(369/499): loss=0.344498451808555, w0=-0.00039146645564510236, w1=-0.04134813420053495\n",
      "Gradient Descent(370/499): loss=0.3443285900589787, w0=-0.00039383372369708165, w1=-0.04139797119521729\n",
      "Gradient Descent(371/499): loss=0.3441591180243082, w0=-0.0003961810177107592, w1=-0.04144773303534879\n",
      "Gradient Descent(372/499): loss=0.34399003461344646, w0=-0.0003985083304127668, w1=-0.04149742009190717\n",
      "Gradient Descent(373/499): loss=0.3438213387399296, w0=-0.0004008156553773627, w1=-0.041547032732027665\n",
      "Gradient Descent(374/499): loss=0.3436530293218826, w0=-0.0004031029870138015, w1=-0.041596571319052734\n",
      "Gradient Descent(375/499): loss=0.34348510528197695, w0=-0.0004053703205538691, w1=-0.04164603621258112\n",
      "Gradient Descent(376/499): loss=0.34331756554738857, w0=-0.00040761765203957965, w1=-0.04169542776851618\n",
      "Gradient Descent(377/499): loss=0.3431504090497564, w0=-0.0004098449783110327, w1=-0.04174474633911363\n",
      "Gradient Descent(378/499): loss=0.342983634725142, w0=-0.0004120522969944287, w1=-0.04179399227302851\n",
      "Gradient Descent(379/499): loss=0.3428172415139894, w0=-0.0004142396064902414, w1=-0.0418431659153616\n",
      "Gradient Descent(380/499): loss=0.34265122836108636, w0=-0.00041640690596154393, w1=-0.041892267607705126\n",
      "Gradient Descent(381/499): loss=0.3424855942155259, w0=-0.00041855419532248834, w1=-0.041941297688187856\n",
      "Gradient Descent(382/499): loss=0.3423203380306681, w0=-0.0004206814752269353, w1=-0.041990256491519586\n",
      "Gradient Descent(383/499): loss=0.3421554587641039, w0=-0.0004227887470572327, w1=-0.04203914434903498\n",
      "Gradient Descent(384/499): loss=0.34199095537761814, w0=-0.0004248760129131417, w1=-0.042087961588736816\n",
      "Gradient Descent(385/499): loss=0.34182682683715404, w0=-0.00042694327560090754, w1=-0.04213670853533863\n",
      "Gradient Descent(386/499): loss=0.34166307211277763, w0=-0.0004289905386224739, w1=-0.04218538551030679\n",
      "Gradient Descent(387/499): loss=0.34149969017864423, w0=-0.0004310178061648391, w1=-0.04223399283190194\n",
      "Gradient Descent(388/499): loss=0.3413366800129629, w0=-0.0004330250830895518, w1=-0.04228253081521995\n",
      "Gradient Descent(389/499): loss=0.34117404059796513, w0=-0.00043501237492234526, w1=-0.04233099977223221\n",
      "Gradient Descent(390/499): loss=0.34101177091986984, w0=-0.0004369796878429076, w1=-0.04237940001182542\n",
      "Gradient Descent(391/499): loss=0.34084986996885364, w0=-0.00043892702867478713, w1=-0.042427731839840846\n",
      "Gradient Descent(392/499): loss=0.3406883367390162, w0=-0.00044085440487543035, w1=-0.04247599555911298\n",
      "Gradient Descent(393/499): loss=0.3405271702283519, w0=-0.00044276182452635167, w1=-0.04252419146950773\n",
      "Gradient Descent(394/499): loss=0.340366369438717, w0=-0.0004446492963234326, w1=-0.042572319867960014\n",
      "Gradient Descent(395/499): loss=0.3402059333758011, w0=-0.00044651682956734927, w1=-0.042620381048510916\n",
      "Gradient Descent(396/499): loss=0.3400458610490966, w0=-0.000448364434154126, w1=-0.042668375302344255\n",
      "Gradient Descent(397/499): loss=0.3398861514718693, w0=-0.00045019212056581414, w1=-0.04271630291782272\n",
      "Gradient Descent(398/499): loss=0.3397268036611304, w0=-0.0004519998998612943, w1=-0.04276416418052344\n",
      "Gradient Descent(399/499): loss=0.3395678166376079, w0=-0.0004537877836671998, w1=-0.04281195937327312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(400/499): loss=0.33940918942571896, w0=-0.0004555557841689611, w1=-0.04285968877618268\n",
      "Gradient Descent(401/499): loss=0.33925092105354204, w0=-0.0004573039141019681, w1=-0.04290735266668141\n",
      "Gradient Descent(402/499): loss=0.33909301055279073, w0=-0.0004590321867428504, w1=-0.04295495131955064\n",
      "Gradient Descent(403/499): loss=0.33893545695878685, w0=-0.00046074061590087285, w1=-0.04300248500695701\n",
      "Gradient Descent(404/499): loss=0.33877825931043365, w0=-0.0004624292159094453, w1=-0.04304995399848521\n",
      "Gradient Descent(405/499): loss=0.3386214166501915, w0=-0.0004640980016177455, w1=-0.043097358561170335\n",
      "Gradient Descent(406/499): loss=0.3384649280240516, w0=-0.00046574698838245304, w1=-0.04314469895952973\n",
      "Gradient Descent(407/499): loss=0.3383087924815107, w0=-0.00046737619205959336, w1=-0.04319197545559448\n",
      "Gradient Descent(408/499): loss=0.33815300907554824, w0=-0.00046898562899649025, w1=-0.043239188308940406\n",
      "Gradient Descent(409/499): loss=0.3379975768626001, w0=-0.00047057531602382564, w1=-0.04328633777671867\n",
      "Gradient Descent(410/499): loss=0.33784249490253626, w0=-0.0004721452704478051, w1=-0.043333424113685945\n",
      "Gradient Descent(411/499): loss=0.3376877622586361, w0=-0.0004736955100424276, w1=-0.04338044757223419\n",
      "Gradient Descent(412/499): loss=0.33753337799756644, w0=-0.00047522605304185897, w1=-0.04342740840242002\n",
      "Gradient Descent(413/499): loss=0.33737934118935775, w0=-0.0004767369181329063, w1=-0.04347430685199363\n",
      "Gradient Descent(414/499): loss=0.33722565090738293, w0=-0.00047822812444759343, w1=-0.04352114316642741\n",
      "Gradient Descent(415/499): loss=0.33707230622833306, w0=-0.0004796996915558356, w1=-0.04356791758894409\n",
      "Gradient Descent(416/499): loss=0.3369193062321976, w0=-0.00048115163945821167, w1=-0.04361463036054457\n",
      "Gradient Descent(417/499): loss=0.3367666500022418, w0=-0.0004825839885788335, w1=-0.043661281720035285\n",
      "Gradient Descent(418/499): loss=0.336614336624986, w0=-0.00048399675975831024, w1=-0.043707871904055305\n",
      "Gradient Descent(419/499): loss=0.33646236519018385, w0=-0.0004853899742468072, w1=-0.043754401147102995\n",
      "Gradient Descent(420/499): loss=0.3363107347908025, w0=-0.0004867636536971976, w1=-0.04380086968156232\n",
      "Gradient Descent(421/499): loss=0.33615944452300167, w0=-0.0004881178201583059, w1=-0.04384727773772882\n",
      "Gradient Descent(422/499): loss=0.3360084934861143, w0=-0.0004894524960682422, w1=-0.04389362554383522\n",
      "Gradient Descent(423/499): loss=0.33585788078262535, w0=-0.0004907677042478254, w1=-0.043939913326076684\n",
      "Gradient Descent(424/499): loss=0.33570760551815343, w0=-0.0004920634678940957, w1=-0.04398614130863575\n",
      "Gradient Descent(425/499): loss=0.3355576668014317, w0=-0.0004933398105739127, w1=-0.0440323097137069\n",
      "Gradient Descent(426/499): loss=0.3354080637442869, w0=-0.000494596756217641, w1=-0.044078418761520806\n",
      "Gradient Descent(427/499): loss=0.3352587954616233, w0=-0.0004958343291129199, w1=-0.044124468670368296\n",
      "Gradient Descent(428/499): loss=0.3351098610714016, w0=-0.0004970525538985172, w1=-0.0441704596566239\n",
      "Gradient Descent(429/499): loss=0.3349612596946222, w0=-0.0004982514555582655, w1=-0.044216391934769186\n",
      "Gradient Descent(430/499): loss=0.33481299045530655, w0=-0.0004994310594150801, w1=-0.04426226571741568\n",
      "Gradient Descent(431/499): loss=0.33466505248047895, w0=-0.000500591391125058, w1=-0.04430808121532757\n",
      "Gradient Descent(432/499): loss=0.3345174449001497, w0=-0.0005017324766716559, w1=-0.04435383863744405\n",
      "Gradient Descent(433/499): loss=0.33437016684729726, w0=-0.0005028543423599469, w1=-0.04439953819090136\n",
      "Gradient Descent(434/499): loss=0.33422321745785066, w0=-0.0005039570148109546, w1=-0.04444518008105456\n",
      "Gradient Descent(435/499): loss=0.334076595870673, w0=-0.000505040520956064, w1=-0.044490764511499\n",
      "Gradient Descent(436/499): loss=0.3339303012275445, w0=-0.0005061048880315074, w1=-0.04453629168409148\n",
      "Gradient Descent(437/499): loss=0.33378433267314617, w0=-0.0005071501435729255, w1=-0.04458176179897116\n",
      "Gradient Descent(438/499): loss=0.3336386893550428, w0=-0.0005081763154100008, w1=-0.044627175054580186\n",
      "Gradient Descent(439/499): loss=0.33349337042366733, w0=-0.0005091834316611652, w1=-0.044672531647684005\n",
      "Gradient Descent(440/499): loss=0.33334837503230447, w0=-0.000510171520728378, w1=-0.04471783177339144\n",
      "Gradient Descent(441/499): loss=0.3332037023370753, w0=-0.0005111406112919749, w1=-0.04476307562517449\n",
      "Gradient Descent(442/499): loss=0.333059351496921, w0=-0.0005120907323055874, w1=-0.04480826339488786\n",
      "Gradient Descent(443/499): loss=0.3329153216735883, w0=-0.0005130219129911298, w1=-0.04485339527278825\n",
      "Gradient Descent(444/499): loss=0.33277161203161254, w0=-0.0005139341828338557, w1=-0.044898471447553344\n",
      "Gradient Descent(445/499): loss=0.3326282217383051, w0=-0.0005148275715774802, w1=-0.04494349210630059\n",
      "Gradient Descent(446/499): loss=0.332485149963736, w0=-0.0005157021092193693, w1=-0.0449884574346057\n",
      "Gradient Descent(447/499): loss=0.3323423958807207, w0=-0.000516557826005794, w1=-0.045033367616520936\n",
      "Gradient Descent(448/499): loss=0.33219995866480434, w0=-0.0005173947524272493, w1=-0.0450782228345931\n",
      "Gradient Descent(449/499): loss=0.33205783749424844, w0=-0.0005182129192138365, w1=-0.04512302326988133\n",
      "Gradient Descent(450/499): loss=0.3319160315500159, w0=-0.0005190123573307085, w1=-0.04516776910197467\n",
      "Gradient Descent(451/499): loss=0.3317745400157568, w0=-0.0005197930979735772, w1=-0.04521246050900934\n",
      "Gradient Descent(452/499): loss=0.33163336207779487, w0=-0.0005205551725642818, w1=-0.04525709766768585\n",
      "Gradient Descent(453/499): loss=0.3314924969251128, w0=-0.0005212986127464176, w1=-0.04530168075328584\n",
      "Gradient Descent(454/499): loss=0.33135194374934007, w0=-0.0005220234503810246, w1=-0.04534620993968873\n",
      "Gradient Descent(455/499): loss=0.3312117017447376, w0=-0.0005227297175423345, w1=-0.0453906853993881\n",
      "Gradient Descent(456/499): loss=0.33107177010818556, w0=-0.0005234174465135759, w1=-0.045435107303507916\n",
      "Gradient Descent(457/499): loss=0.3309321480391696, w0=-0.0005240866697828368, w1=-0.04547947582181851\n",
      "Gradient Descent(458/499): loss=0.33079283473976817, w0=-0.0005247374200389839, w1=-0.045523791122752316\n",
      "Gradient Descent(459/499): loss=0.33065382941463867, w0=-0.0005253697301676368, w1=-0.04556805337341948\n",
      "Gradient Descent(460/499): loss=0.33051513127100524, w0=-0.0005259836332471984, w1=-0.045612262739623176\n",
      "Gradient Descent(461/499): loss=0.33037673951864593, w0=-0.0005265791625449389, w1=-0.04565641938587477\n",
      "Gradient Descent(462/499): loss=0.3302386533698802, w0=-0.0005271563515131331, w1=-0.0457005234754088\n",
      "Gradient Descent(463/499): loss=0.33010087203955546, w0=-0.0005277152337852519, w1=-0.045744575170197685\n",
      "Gradient Descent(464/499): loss=0.32996339474503644, w0=-0.0005282558431722043, w1=-0.04578857463096632\n",
      "Gradient Descent(465/499): loss=0.3298262207061912, w0=-0.0005287782136586324, w1=-0.04583252201720644\n",
      "Gradient Descent(466/499): loss=0.32968934914538006, w0=-0.0005292823793992563, w1=-0.045876417487190804\n",
      "Gradient Descent(467/499): loss=0.3295527792874431, w0=-0.0005297683747152698, w1=-0.045920261197987186\n",
      "Gradient Descent(468/499): loss=0.3294165103596882, w0=-0.0005302362340907849, w1=-0.04596405330547218\n",
      "Gradient Descent(469/499): loss=0.32928054159188025, w0=-0.000530685992169326, w1=-0.04600779396434485\n",
      "Gradient Descent(470/499): loss=0.3291448722162277, w0=-0.0005311176837503718, w1=-0.04605148332814018\n",
      "Gradient Descent(471/499): loss=0.32900950146737207, w0=-0.0005315313437859445, w1=-0.046095121549242314\n",
      "Gradient Descent(472/499): loss=0.32887442858237703, w0=-0.000531927007377247, w1=-0.04613870877889772\n",
      "Gradient Descent(473/499): loss=0.3287396528007155, w0=-0.0005323047097713455, w1=-0.046182245167228055\n",
      "Gradient Descent(474/499): loss=0.32860517336425976, w0=-0.0005326644863578978, w1=-0.046225730863242984\n",
      "Gradient Descent(475/499): loss=0.32847098951726944, w0=-0.0005330063726659277, w1=-0.046269166014852744\n",
      "Gradient Descent(476/499): loss=0.32833710050638076, w0=-0.0005333304043606427, w1=-0.04631255076888059\n",
      "Gradient Descent(477/499): loss=0.3282035055805955, w0=-0.0005336366172402965, w1=-0.046355885271075044\n",
      "Gradient Descent(478/499): loss=0.3280702039912706, w0=-0.0005339250472330949, w1=-0.04639916966612203\n",
      "Gradient Descent(479/499): loss=0.3279371949921063, w0=-0.0005341957303941435, w1=-0.04644240409765681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(480/499): loss=0.3278044778391364, w0=-0.000534448702902439, w1=-0.04648558870827579\n",
      "Gradient Descent(481/499): loss=0.32767205179071757, w0=-0.0005346840010579012, w1=-0.046528723639548145\n",
      "Gradient Descent(482/499): loss=0.3275399161075182, w0=-0.0005349016612784462, w1=-0.046571809032027345\n",
      "Gradient Descent(483/499): loss=0.3274080700525087, w0=-0.0005351017200971012, w1=-0.04661484502526247\n",
      "Gradient Descent(484/499): loss=0.32727651289095094, w0=-0.0005352842141591577, w1=-0.04665783175780941\n",
      "Gradient Descent(485/499): loss=0.3271452438903877, w0=-0.0005354491802193655, w1=-0.04670076936724193\n",
      "Gradient Descent(486/499): loss=0.327014262320633, w0=-0.0005355966551391654, w1=-0.04674365799016255\n",
      "Gradient Descent(487/499): loss=0.3268835674537617, w0=-0.0005357266758839601, w1=-0.046786497762213365\n",
      "Gradient Descent(488/499): loss=0.3267531585640996, w0=-0.0005358392795204232, w1=-0.046829288818086616\n",
      "Gradient Descent(489/499): loss=0.32662303492821354, w0=-0.0005359345032138459, w1=-0.04687203129153521\n",
      "Gradient Descent(490/499): loss=0.32649319582490144, w0=-0.0005360123842255203, w1=-0.046914725315383064\n",
      "Gradient Descent(491/499): loss=0.32636364053518296, w0=-0.0005360729599101598, w1=-0.04695737102153535\n",
      "Gradient Descent(492/499): loss=0.32623436834228925, w0=-0.0005361162677133544, w1=-0.046999968540988556\n",
      "Gradient Descent(493/499): loss=0.32610537853165367, w0=-0.0005361423451690631, w1=-0.047042518003840475\n",
      "Gradient Descent(494/499): loss=0.3259766703909023, w0=-0.00053615122989714, w1=-0.047085019539300006\n",
      "Gradient Descent(495/499): loss=0.3258482432098441, w0=-0.0005361429596008954, w1=-0.04712747327569689\n",
      "Gradient Descent(496/499): loss=0.32572009628046256, w0=-0.0005361175720646916, w1=-0.047169879340491286\n",
      "Gradient Descent(497/499): loss=0.3255922288969047, w0=-0.0005360751051515719, w1=-0.04721223786028321\n",
      "Gradient Descent(498/499): loss=0.3254646403554735, w0=-0.0005360155968009226, w1=-0.047254548960821895\n",
      "Gradient Descent(499/499): loss=0.32533732995461806, w0=-0.0005359390850261683, w1=-0.04729681276701501\n",
      "++++ gamma = 0.0294285714286\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-b7761b3c537e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0;34m[\u001b[0m\u001b[0mbest_deg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma_best\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_max\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mcross_validation_GD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_single_jet_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx_single_jet_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_fold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgammas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mtx_single_jet_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx_single_jet_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen_init_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx_single_jet_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtx_single_jet_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_deg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-4c7c255048b5>\u001b[0m in \u001b[0;36mcross_validation_GD\u001b[0;34m(y, tx, k_fold, max_iters, degs, lambdas)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                     \u001b[0mtrain_tx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_tx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen_init_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_tx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_tx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msingle_deg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                     \u001b[0mshape_tx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_tx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/matteociprian/Documents/GitHub/LMO_ML/project1/A_final/preprocessing_functions.py\u001b[0m in \u001b[0;36mprepare_data\u001b[0;34m(train_tx, test_tx, deg)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;31m#print('Cross products')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0mtrain_tx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_all_cross_prod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_tx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m     \u001b[0mtest_tx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_all_cross_prod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_tx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/matteociprian/Documents/GitHub/LMO_ML/project1/A_final/preprocessing_functions.py\u001b[0m in \u001b[0;36madd_all_cross_prod\u001b[0;34m(tx)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mtx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_cross_prod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/matteociprian/Documents/GitHub/LMO_ML/project1/A_final/preprocessing_functions.py\u001b[0m in \u001b[0;36madd_cross_prod\u001b[0;34m(tx, i, j)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0madd_cross_prod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0madd_all_cross_prod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "OUT_FOLDER = 'output/'\n",
    "name = 'Gradient_descent.csv'\n",
    "degs=range(2,5)\n",
    "gammas=np.linspace(0.001,0.2,4);\n",
    "k_fold=5\n",
    "max_iters=20\n",
    "\n",
    "\n",
    "mask_jets_train = split_jets_mask(tx_train)\n",
    "mask_jets_test = split_jets_mask(tx_test)\n",
    "len_mask = len(mask_jets_train)\n",
    "\n",
    "y_preds_train = np.zeros(len(y_train))\n",
    "y_preds_test = np.zeros(tx_test.shape[0])\n",
    "\n",
    "\n",
    "\n",
    "for mask_jet_id in range(len_mask):\n",
    "    print('**** Starting Jet ', mask_jet_id, '*****')\n",
    "    tx_single_jet_train = tx_train[mask_jets_train[mask_jet_id]]\n",
    "    tx_single_jet_test =  tx_test[mask_jets_test[mask_jet_id]]\n",
    "    y_single_jet_train =   y_train[mask_jets_train[mask_jet_id]]\n",
    "    \n",
    "    \n",
    "    \n",
    "    [best_deg, gamma_best, acc_max] =cross_validation_GD(y_single_jet_train, tx_single_jet_train, k_fold, max_iters, degs,gammas)\n",
    "    tx_single_jet_train, tx_single_jet_test,len_init_data = prepare_data(tx_single_jet_train,tx_single_jet_test, best_deg)\n",
    "    \n",
    "    initial_w = np.zeros(tx_single_jet_train.shape[1]);\n",
    "    w, loss = func_GD (y=y_single_jet_train, tx=tx_single_jet_train, test_set=tx_single_jet_test, max_iters=max_iters,gamma=best_gamma, initial_w=initial_w);\n",
    "    \n",
    "    y_pred_train = predict_labels(w, tx_single_jet_train)\n",
    "    y_pred_test = predict_labels(w, tx_single_jet_test)\n",
    "    y_preds_train[mask_jets_train[mask_jet_id]] = y_pred_train\n",
    "    y_preds_test[mask_jets_test[mask_jet_id]] = y_pred_test\n",
    "    \n",
    "    right_train = np.sum(y_single_jet_train == y_pred_train)/len(y_single_jet_train )*100\n",
    "    \n",
    "    print('**** Jet ', mask_jet_id, '*****accuracy jet',right_train)\n",
    "    \n",
    "\n",
    "create_csv_submission(ids_test, y_preds_test, OUT_FOLDER+name)\n",
    "right_train_total = np.sum(y_train == y_preds_train)/len(y_train)*100\n",
    "\n",
    "print('>>>>>>>> Accuracy TOTAL ',right_train_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "deg=5;\n",
    "lambda_=1e-5\n",
    "mask_jets_train = split_jets_mask(tx_train)\n",
    "mask_jets_test = split_jets_mask(tx_test)\n",
    "len_mask = len(mask_jets_train)\n",
    "for mask_jet_id in range(len_mask):\n",
    "    print('**** Starting Jet ', mask_jet_id, '*****')\n",
    "    tx_single_jet_train = tx_train[mask_jets_train[mask_jet_id]]\n",
    "    tx_single_jet_test =  tx_test[mask_jets_test[mask_jet_id]]\n",
    "    y_single_jet_train =   y_train[mask_jets_train[mask_jet_id]]\n",
    "    tx_single_jet_train, tx_single_jet_test, len_init = prepare_data(tx_single_jet_train, tx_single_jet_test, deg);\n",
    "    \n",
    "    [w,loss]=func_ridge_regression (y=y_single_jet_train, tx=tx_single_jet_train, test_set=tx_single_jet_test, lambda_=lambda_);    \n",
    "    \n",
    "    y_pred_train = predict_labels(w, tx_single_jet_train)\n",
    "    y_pred_test = predict_labels(w, tx_single_jet_test)\n",
    "    y_preds_train[mask_jets_train[mask_jet_id]] = y_pred_train\n",
    "    y_preds_test[mask_jets_test[mask_jet_id]] = y_pred_test\n",
    "    \n",
    "    right_train = np.sum(y_single_jet_train == y_pred_train)/len(y_single_jet_train )*100\n",
    "    \n",
    "    print('**** Jet ', mask_jet_id, '*****accuracy jet',right_train)\n",
    "    \n",
    "\n",
    "    \n",
    "right_train_total = np.sum(y_train == y_preds_train)/len(y_train )*100\n",
    "print('>>>>>>>> Accuracy TOTAL ',right_train_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##  SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def func_SGD (y, tx, test_set, max_iters, gamma, initial_w, batch_size):\n",
    "    name = 'Stochastic_Gradient_descent'\n",
    "    \n",
    "    w,loss = least_squares_SGD(y, tx, initial_w, max_iters,\\\n",
    "                               gamma, batch_size)\n",
    "    \n",
    "    y_pred = predict_labels(w, test_set)\n",
    "    create_csv_submission(ids_test, y_pred, OUT_FOLDER+name)\n",
    "    \n",
    "    plt.plot(w, 'go')\n",
    "    plt.title('SGD: weights with loss:  '+str(loss),fontsize=15, fontweight='bold');\n",
    "    plt.show()\n",
    "    \n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "deg=5;\n",
    "lambda_=1e-5\n",
    "mask_jets_train = split_jets_mask(tx_train)\n",
    "mask_jets_test = split_jets_mask(tx_test)\n",
    "len_mask = len(mask_jets_train)\n",
    "for mask_jet_id in range(len_mask):\n",
    "    print('**** Starting Jet ', mask_jet_id, '*****')\n",
    "    tx_single_jet_train = tx_train[mask_jets_train[mask_jet_id]]\n",
    "    tx_single_jet_test =  tx_test[mask_jets_test[mask_jet_id]]\n",
    "    y_single_jet_train =   y_train[mask_jets_train[mask_jet_id]]\n",
    "    tx_single_jet_train, tx_single_jet_test, len_init = prepare_data(tx_single_jet_train, tx_single_jet_test, deg);\n",
    "    \n",
    "    initial_w = np.zeros(tx_single_jet_train.shape[1]);\n",
    "    w, loss = func_SGD (y=y_single_jet_train, tx=tx_single_jet_train, test_set=tx_single_jet_test, max_iters=500,gamma=0.00005, initial_w=initial_w,\\\n",
    "                   batch_size=1);    \n",
    "   \n",
    "    y_pred_train = predict_labels(w, tx_single_jet_train)\n",
    "    y_pred_test = predict_labels(w, tx_single_jet_test)\n",
    "    y_preds_train[mask_jets_train[mask_jet_id]] = y_pred_train\n",
    "    y_preds_test[mask_jets_test[mask_jet_id]] = y_pred_test\n",
    "    \n",
    "    right_train = np.sum(y_single_jet_train == y_pred_train)/len(y_single_jet_train )*100\n",
    "    \n",
    "    print('**** Jet ', mask_jet_id, '*****accuracy jet',right_train)\n",
    "    \n",
    "\n",
    "    \n",
    "right_train_total = np.sum(y_train == y_preds_train)/len(y_train )*100\n",
    "print('>>>>>>>> Accuracy TOTAL ',right_train_total)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "def cross_validation_logistic_regression(y,tx, k_fold, max_iters, gammas):\n",
    "    \n",
    "    accuracies_test=np.zeros(len(gammas))\n",
    "    accuracies_train=np.zeros(len(gammas))\n",
    "    acc_max=0;\n",
    "    gamma_best=0;\n",
    "    \n",
    "    for i, single_gamma in enumerate(gammas):\n",
    "        \n",
    "        seed=1;\n",
    "\n",
    "        # get k'th subgroup in test, others in train\n",
    "        k_indices = build_k_indices(y, k_fold, seed)\n",
    "        accuracy_train = np.zeros(k_fold)\n",
    "        accuracy_test = np.zeros(k_fold)\n",
    "\n",
    "\n",
    "        for k in range(k_fold):\n",
    "            #print('----- FOLD', k, '-----')\n",
    "            k_index = k_indices[k]\n",
    "            test_y = y[k_index]\n",
    "            test_tx = tx[k_index,:]\n",
    "            mask = np.ones(len(y), dtype=bool) # set all elements to True\n",
    "            mask[k_index] = False              # set test elements to False\n",
    "            train_tx = tx[mask,:]              # select only True elements (ie train elements)\n",
    "            train_y = y[mask]\n",
    "            initial_w = np.zeros(train_tx.shape[1]);\n",
    "            weights,loss = logistic_regression(train_y, train_tx, initial_w, max_iters, single_gamma)\n",
    "            # Compute the predictions\n",
    "            y_pred_train = predict_labels(weights, train_tx)\n",
    "            y_pred_test = predict_labels(weights, test_tx);\n",
    "            accuracy_train[k] = np.sum(y_pred_train == train_y)/len(train_y)\n",
    "            accuracy_test[k] = np.sum(y_pred_test == test_y)/len(test_y)\n",
    "            \n",
    "        accuracies_test[i]=np.mean(accuracy_test);\n",
    "        accuracies_train[i]=np.mean(accuracy_train);\n",
    "        \n",
    "        print('GAMMA:', single_gamma, '---','ACCURANCY TEST:',accuracies_test[i], '---','ACCURANCY TRAIN:',accuracies_train[i])\n",
    "        if (accuracies_test[i]>acc_max):\n",
    "                gamma_best=gammas[i];\n",
    "                acc_max=accuracies_test[i];\n",
    "    \n",
    "    return [gamma_best,acc_max]\n",
    "               \n",
    "        # Compute accuracy of the predictions\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def func_logistic (y, tx, test_set, max_iters, gamma, initial_w):\n",
    "    name = 'Logistic regression'\n",
    "    \n",
    "    #w,loss = logistic_regression(y, tx, initial_w, max_iters, gamma)\n",
    "    w,loss = logistic_regression(y, tx, initial_w, max_iters, gamma)\n",
    "    \n",
    "    loss=loss/len(y);\n",
    "    y_pred = predict_labels(w, test_set)\n",
    "    create_csv_submission(ids_test, y_pred, OUT_FOLDER+name)  \n",
    "    \n",
    "    plt.plot(w, 'go')\n",
    "    plt.title('logistic reg: weights with loss:  '+str(loss),fontsize=15, fontweight='bold');\n",
    "    plt.show()\n",
    "    \n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gammas=np.linspace(0.00001,0.9,10);\n",
    "k_fold=3;\n",
    "max_iters=1000;\n",
    "deg=5;\n",
    "\n",
    "\n",
    "mask_jets_train = split_jets_mask(tx_train)\n",
    "mask_jets_test = split_jets_mask(tx_test)\n",
    "len_mask = len(mask_jets_train)\n",
    "for mask_jet_id in range(len_mask):\n",
    "    print('**** Starting Jet ', mask_jet_id, '*****')\n",
    "    tx_single_jet_train = tx_train[mask_jets_train[mask_jet_id]]\n",
    "    tx_single_jet_test =  tx_test[mask_jets_test[mask_jet_id]]\n",
    "    y_single_jet_train =   y_train[mask_jets_train[mask_jet_id]]\n",
    "    tx_single_jet_train, tx_single_jet_test, len_init = prepare_data(tx_single_jet_train, tx_single_jet_test, deg);\n",
    "    \n",
    "    \n",
    "    \n",
    "    [best_gamma,acc_max]=cross_validation_logistic_regression(y_single_jet_train, tx_single_jet_train, k_fold, max_iters, gammas)\n",
    "    \n",
    "    print('BEST GAMMA:', best_gamma, '---','ACCURANCY:',acc_max)\n",
    "\n",
    "    initial_w = np.zeros(tx_single_jet_train.shape[1]);\n",
    "    w, loss = func_logistic (y=y_single_jet_train, tx=tx_single_jet_train, test_set=tx_single_jet_test, max_iters=1000,\\\n",
    "                         gamma=best_gamma, initial_w=initial_w);\n",
    "    \n",
    "    y_pred_train = predict_labels(w, tx_single_jet_train)\n",
    "    y_pred_test = predict_labels(w, tx_single_jet_test)\n",
    "    y_preds_train[mask_jets_train[mask_jet_id]] = y_pred_train\n",
    "    y_preds_test[mask_jets_test[mask_jet_id]] = y_pred_test\n",
    "    \n",
    "    right_train = np.sum(y_single_jet_train == y_pred_train)/len(y_single_jet_train )*100\n",
    "    \n",
    "    print('**** Jet ', mask_jet_id, '*****accuracy jet',right_train)\n",
    "    \n",
    "\n",
    "    \n",
    "right_train_total = np.sum(y_train == y_preds_train)/len(y_train )*100\n",
    "print('>>>>>>>> Accuracy TOTAL ',right_train_total)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOGISTIC REGULARIZED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "def cross_validation_logistic_regression_reg(y,tx, k_fold, max_iters, gammas,lambdas):\n",
    "    \n",
    "    acc_max=0;\n",
    "    accuracies_test=np.zeros([len(gammas),len(lambdas)]);\n",
    "    \n",
    "    accuracies_train=np.zeros([len(gammas),len(lambdas)]);\n",
    "    \n",
    "    \n",
    "    for i, single_gamma in enumerate(gammas):\n",
    "        for j, single_lambda in enumerate(lambdas):\n",
    "            seed=1;\n",
    "        \n",
    "            # get k'th subgroup in test, others in train\n",
    "            k_indices = build_k_indices(y, k_fold, seed)\n",
    "            accuracy_train = np.zeros(k_fold)\n",
    "            accuracy_test = np.zeros(k_fold)\n",
    "\n",
    "\n",
    "            for k in range(k_fold):\n",
    "                #print('----- FOLD', k, '-----')\n",
    "                k_index = k_indices[k]\n",
    "                test_y = y[k_index]\n",
    "                test_tx = tx[k_index,:]\n",
    "\n",
    "                mask = np.ones(len(y), dtype=bool) # set all elements to True\n",
    "                mask[k_index] = False              # set test elements to False\n",
    "                train_tx = tx[mask,:]              # select only True elements (ie train elements)\n",
    "                train_y = y[mask]\n",
    "                initial_w = np.zeros(train_tx.shape[1]);\n",
    "                weights,loss = reg_logistic_regression(train_y, train_tx,single_lambda, initial_w, max_iters, single_gamma)\n",
    "                # Compute the predictions\n",
    "                y_pred_train = predict_labels(weights, train_tx)\n",
    "                y_pred_test = predict_labels(weights, test_tx)\n",
    "                predictions=True;\n",
    "                accuracy_train[k] = np.sum(y_pred_train == train_y)/len(train_y)\n",
    "                accuracy_test[k] = np.sum(y_pred_test == test_y)/len(test_y)\n",
    "                \n",
    "                \n",
    "            accuracies_test[i,j]= np.mean(accuracy_test);\n",
    "            accuracies_train[i,j]= np.mean(accuracy_train);\n",
    "            print('GAMMA', single_gamma, '---','LAMBDA', single_lambda, '---ACCURANCY TEST:',accuracies_test[i,j],'---ACCURANCY TRAIN:',accuracies_train[i,j])\n",
    "            \n",
    "            if (accuracies_test[i,j]>acc_max):\n",
    "                gamma_best=gammas[i];\n",
    "                lambda_best=lambdas[j];\n",
    "                acc_max=accuracies_test[i,j];\n",
    "            \n",
    "            \n",
    "    return [gamma_best,lambda_best,acc_max]\n",
    "     \n",
    "    \n",
    "    \n",
    "            # Compute accuracy of the predictions\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def func_logistic_reg (y, tx, lambda_, test_set, max_iters, gamma, initial_w):\n",
    "    name = 'Logistic regression regularized'\n",
    "    \n",
    "    #w,loss = logistic_regression(y, tx, initial_w, max_iters, gamma)\n",
    "    w,loss = reg_logistic_regression(y, tx, lambda_, initial_w, max_iters, gamma)\n",
    "    \n",
    "    loss=loss/len(y);\n",
    "    y_pred = predict_labels(w, test_set)\n",
    "    create_csv_submission(ids_test, y_pred, OUT_FOLDER+name)\n",
    "    \n",
    "    plt.plot(w, 'go')\n",
    "    plt.title('logistic reg: weights with the normalized log-like:  '+str(loss),fontsize=15, fontweight='bold');\n",
    "    plt.show()\n",
    "    \n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test cross_validation\n",
    "\n",
    "gammas=np.linspace(0.000001,0.0001,2);\n",
    "lambdas=np.linspace(0.001,0.3,2);\n",
    "k_fold=6;\n",
    "max_iters=50;\n",
    "\n",
    "deg=2;\n",
    "\n",
    "mask_jets_train = split_jets_mask(tx_train)\n",
    "mask_jets_test = split_jets_mask(tx_test)\n",
    "len_mask = len(mask_jets_train)\n",
    "for mask_jet_id in range(len_mask):\n",
    "    print('**** Starting Jet ', mask_jet_id, '*****')\n",
    "    tx_single_jet_train = tx_train[mask_jets_train[mask_jet_id]]\n",
    "    tx_single_jet_test =  tx_test[mask_jets_test[mask_jet_id]]\n",
    "    y_single_jet_train =   y_train[mask_jets_train[mask_jet_id]]\n",
    "    tx_single_jet_train, tx_single_jet_test, len_init\n",
    "    = prepare_data(tx_single_jet_train, tx_single_jet_test, deg);\n",
    "    \n",
    "    [gamma_best,lambda_best,acc_max]= cross_validation_logistic_regression_reg(y_single_jet_train, tx_single_jet_train, k_fold, max_iters, gammas,lambdas)\n",
    "    print('BEST GAMMA', gamma_best, '---','BEST LAMBDA', lambda_best, '---ACCURANCY:',acc_max)\n",
    "    \n",
    "    initial_w = np.zeros(tx_single_jet_train.shape[1]);\n",
    "    w, loss = func_logistic_reg (y=y_single_jet_train, tx=tx_single_jet_train, lambda_=lambda_best, test_set=tx_single_jet_test , max_iters=max_iters,\\\n",
    "                         gamma=gamma_best, initial_w=initial_w);\n",
    "\n",
    "\n",
    "    \n",
    "    y_pred_train = predict_labels(w, tx_single_jet_train)\n",
    "    y_pred_test = predict_labels(w, tx_single_jet_test)\n",
    "    y_preds_train[mask_jets_train[mask_jet_id]] = y_pred_train\n",
    "    y_preds_test[mask_jets_test[mask_jet_id]] = y_pred_test\n",
    "    \n",
    "    right_train = np.sum(y_single_jet_train == y_pred_train)/len(y_single_jet_train )*100\n",
    "    \n",
    "    print('**** Jet ', mask_jet_id, '*****accuracy jet',right_train)\n",
    "    \n",
    "\n",
    "    \n",
    "right_train_total = np.sum(y_train == y_preds_train)/len(y_train )*100\n",
    "print('>>>>>>>> Accuracy TOTAL ',right_train_total)\n",
    "\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
