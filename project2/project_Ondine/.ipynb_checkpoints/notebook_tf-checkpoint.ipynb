{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ondine/anaconda/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Baseline for machine learning project on road segmentation.\n",
    "This simple baseline consits of a CNN with two convolutional+pooling layers with a soft-max loss\n",
    "\n",
    "Credits: Aurelien Lucchi, ETH ZÃ¼rich\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# import libraries\n",
    "import gzip\n",
    "import os\n",
    "import sys\n",
    "import urllib\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image\n",
    "\n",
    "# to plot images in the notebook\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import code\n",
    "\n",
    "import tensorflow.python.platform\n",
    "\n",
    "import numpy\n",
    "import tensorflow as tf\n",
    "\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import math\n",
    "\n",
    "from helper_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM_CHANNELS:  3\n",
      "PIXEL_DEPTH:  255\n",
      "NUM_LABELS:  2\n",
      "IMG_PATCH_SIZE:  16\n"
     ]
    }
   ],
   "source": [
    "TRAINING_SIZE = 20\n",
    "VALIDATION_SIZE = 5  # Size of the validation set.\n",
    "SEED = 50  # Set to None for random seed.\n",
    "BATCH_SIZE = 16 # 64\n",
    "NUM_EPOCHS = 8 # how many as you like\n",
    "RESTORE_MODEL = False # If True, restore existing model instead of training a new one\n",
    "RECORDING_STEP = 50\n",
    "TEST = False  # if we want to predict test image as well\n",
    "TESTING_SIZE = 50 # number of test images i.e. 50\n",
    "print('NUM_CHANNELS: ',NUM_CHANNELS )\n",
    "print('PIXEL_DEPTH: ',PIXEL_DEPTH)\n",
    "print ('NUM_LABELS: ',NUM_LABELS)\n",
    "print ('IMG_PATCH_SIZE: ',IMG_PATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where to save stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.app.flags.DEFINE_string('train_dir', '/tmp/mnist/test1',\n",
    "                           \"\"\"Directory where to write event logs \"\"\"\n",
    "                           \"\"\"and checkpoint.\"\"\")\n",
    "FLAGS = tf.app.flags.FLAGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Directories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = 'training/'\n",
    "train_data_filename = data_dir + 'images/'\n",
    "train_labels_filename = data_dir + 'groundtruth/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convolutional Layer 1.\n",
    "filter_size1 = 5          # Convolution filters are 5 x 5 pixels.\n",
    "num_filters1 = 16         # There are 16 of these filters.\n",
    "\n",
    "# Convolutional Layer 2.\n",
    "filter_size2 = 5          # Convolution filters are 5 x 5 pixels.\n",
    "num_filters2 = 36         # There are 36 of these filters.\n",
    "\n",
    "# Fully-connected layer.\n",
    "fc_size = 128             # Number of neurons in fully-connected layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "some additional functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make an image summary for 4d tensor image with index idx\n",
    "def get_image_summary(img, idx = 0):\n",
    "    V = tf.slice(img, (0, 0, 0, idx), (1, -1, -1, 1))\n",
    "    img_w = img.get_shape().as_list()[1]\n",
    "    img_h = img.get_shape().as_list()[2]\n",
    "    min_value = tf.reduce_min(V)\n",
    "    V = V - min_value\n",
    "    max_value = tf.reduce_max(V)\n",
    "    V = V / (max_value*PIXEL_DEPTH)\n",
    "    V = tf.reshape(V, (img_w, img_h, 1))\n",
    "    V = tf.transpose(V, (2, 0, 1))\n",
    "    V = tf.reshape(V, (-1, img_w, img_h, 1))\n",
    "    return V\n",
    "\n",
    "# Make an image summary for 3d tensor image with index idx\n",
    "def get_image_summary_3d(img):\n",
    "    V = tf.slice(img, (0, 0, 0), (1, -1, -1))\n",
    "    img_w = img.get_shape().as_list()[1]\n",
    "    img_h = img.get_shape().as_list()[2]\n",
    "    V = tf.reshape(V, (img_w, img_h, 1))\n",
    "    V = tf.transpose(V, (2, 0, 1))\n",
    "    V = tf.reshape(V, (-1, img_w, img_h, 1))\n",
    "    return V\n",
    "\n",
    "# Get a concatenation of the prediction and groundtruth for given input file\n",
    "def get_prediction_with_groundtruth(filename, image_idx):\n",
    "    imageid = \"satImage_%.3d\" % image_idx\n",
    "    image_filename = filename + imageid + \".png\"\n",
    "    img = mpimg.imread(image_filename)\n",
    "\n",
    "    img_prediction = get_prediction(img)\n",
    "    cimg = concatenate_images(img, img_prediction)\n",
    "\n",
    "    return cimg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the data into numpy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training/images/satImage_001.png\n",
      "Loading training/images/satImage_002.png\n",
      "Loading training/images/satImage_003.png\n",
      "Loading training/images/satImage_004.png\n",
      "Loading training/images/satImage_005.png\n",
      "Loading training/images/satImage_006.png\n",
      "Loading training/images/satImage_007.png\n",
      "Loading training/images/satImage_008.png\n",
      "Loading training/images/satImage_009.png\n",
      "Loading training/images/satImage_010.png\n",
      "Loading training/images/satImage_011.png\n",
      "Loading training/images/satImage_012.png\n",
      "Loading training/images/satImage_013.png\n",
      "Loading training/images/satImage_014.png\n",
      "Loading training/images/satImage_015.png\n",
      "Loading training/images/satImage_016.png\n",
      "Loading training/images/satImage_017.png\n",
      "Loading training/images/satImage_018.png\n",
      "Loading training/images/satImage_019.png\n",
      "Loading training/images/satImage_020.png\n",
      "Loading training/groundtruth/satImage_001.png\n",
      "Loading training/groundtruth/satImage_002.png\n",
      "Loading training/groundtruth/satImage_003.png\n",
      "Loading training/groundtruth/satImage_004.png\n",
      "Loading training/groundtruth/satImage_005.png\n",
      "Loading training/groundtruth/satImage_006.png\n",
      "Loading training/groundtruth/satImage_007.png\n",
      "Loading training/groundtruth/satImage_008.png\n",
      "Loading training/groundtruth/satImage_009.png\n",
      "Loading training/groundtruth/satImage_010.png\n",
      "Loading training/groundtruth/satImage_011.png\n",
      "Loading training/groundtruth/satImage_012.png\n",
      "Loading training/groundtruth/satImage_013.png\n",
      "Loading training/groundtruth/satImage_014.png\n",
      "Loading training/groundtruth/satImage_015.png\n",
      "Loading training/groundtruth/satImage_016.png\n",
      "Loading training/groundtruth/satImage_017.png\n",
      "Loading training/groundtruth/satImage_018.png\n",
      "Loading training/groundtruth/satImage_019.png\n",
      "Loading training/groundtruth/satImage_020.png\n"
     ]
    }
   ],
   "source": [
    "# Extract it into numpy arrays.\n",
    "train_data = extract_data(train_data_filename, TRAINING_SIZE)\n",
    "train_labels = extract_labels(train_labels_filename, TRAINING_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Balance the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points per class: c0 = 9450 c1 = 3050\n",
      "Balancing training data...\n",
      "len(new_indices):  6100\n",
      "train_data.shape:  (12500, 16, 16, 3)\n",
      "new train_data.shape:  (6100, 16, 16, 3)\n",
      "Number of data points per class: c0 = 3050 c1 = 3050\n"
     ]
    }
   ],
   "source": [
    "num_epochs = NUM_EPOCHS\n",
    "# Now check the size of both classes and balance ###############################\n",
    "c0 = 0\n",
    "c1 = 0\n",
    "for i in range(len(train_labels)):\n",
    "    if train_labels[i][0] == 1:\n",
    "        c0 = c0 + 1\n",
    "    else:\n",
    "        c1 = c1 + 1\n",
    "print ('Number of data points per class: c0 = ' + str(c0) + ' c1 = ' + str(c1))\n",
    "# balance to take the same number of patches with c0 and c1 classes\n",
    "print ('Balancing training data...')\n",
    "min_c = min(c0, c1)\n",
    "idx0 = [i for i, j in enumerate(train_labels) if j[0] == 1]\n",
    "idx1 = [i for i, j in enumerate(train_labels) if j[1] == 1]\n",
    "new_indices = idx0[0:min_c] + idx1[0:min_c]\n",
    "print ('len(new_indices): ',len(new_indices))\n",
    "print ('train_data.shape: ',train_data.shape)\n",
    "train_data = train_data[new_indices,:,:,:]\n",
    "train_labels = train_labels[new_indices]\n",
    "print ('new train_data.shape: ',train_data.shape)\n",
    "train_size = train_labels.shape[0]\n",
    "\n",
    "c0 = 0\n",
    "c1 = 0\n",
    "for i in range(len(train_labels)):\n",
    "    if train_labels[i][0] == 1:\n",
    "        c0 = c0 + 1\n",
    "    else:\n",
    "        c1 = c1 + 1\n",
    "print ('Number of data points per class: c0 = ' + str(c0) + ' c1 = ' + str(c1))\n",
    "# END of balancing #############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_patch(array):\n",
    "    plt.imshow(array, interpolation='nearest')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_4_patch(images):\n",
    "    if len(images) != 4:\n",
    "        print (\"Error, not 4 images\")\n",
    "    fig, axes = plt.subplots(2, 2)\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.imshow(images[i], cmap='binary')\n",
    "        xlabel = \"image {0}\".format(i)\n",
    "        ax.set_xlabel(xlabel)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAS4AAAD5CAYAAACZDNhgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nHSUuEYmOEpeIREeJS0Si8/8AaM75YmKthFcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11feeba20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(16, 16, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = train_data[0,:,:,:] # a lot of these\n",
    "\n",
    "#plot_patch(array=data)\n",
    "data = train_data[0:4,:,:,:]\n",
    "plot_4_patch(data)\n",
    "train_data[0,:,:,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is where training samples and labels are fed to the graph.\n",
    "# These placeholder nodes will be fed a batch of training data at each\n",
    "# training step using the {feed_dict} argument to the Run() call below.\n",
    "with tf.name_scope('input'):\n",
    "    train_data_node = tf.placeholder(\n",
    "        tf.float32,\n",
    "        shape=(BATCH_SIZE, IMG_PATCH_SIZE, IMG_PATCH_SIZE, NUM_CHANNELS))\n",
    "    train_labels_node = tf.placeholder(tf.float32,\n",
    "                                       shape=(BATCH_SIZE, NUM_LABELS))\n",
    "    \n",
    "    train_all_data_node = tf.constant(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The variables below hold all the trainable weights. They are passed an\n",
    "# initial value which will be assigned when when we call:\n",
    "# {tf.initialize_all_variables().run()}\n",
    "def new_weights(shape,stddev_ = 0.05):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=stddev_,seed=SEED),name=\"W\")\n",
    "\n",
    "def new_biases(length):\n",
    "    return tf.Variable(tf.constant(0.1, shape=[length]),name=\"B\")\n",
    "\n",
    "with tf.name_scope(\"conv1\"):\n",
    "    conv1_weights = new_weights([5, 5, NUM_CHANNELS, 32]) # 5x5 filter, depth 32.\n",
    "    conv1_biases = tf.Variable(tf.zeros([32]), name = \"B\")\n",
    "\n",
    "with tf.name_scope(\"conv2\"):\n",
    "    conv2_weights = new_weights([5, 5, 32, 64])\n",
    "    conv2_biases = new_biases(length=64)\n",
    "\n",
    "with tf.name_scope(\"fc1\"):\n",
    "    fc1_weights = new_weights(shape= [int(IMG_PATCH_SIZE / 4 * IMG_PATCH_SIZE / 4 * 64), 512],\n",
    "                                stddev_ = 0.1) # fully connected, depth 512.\n",
    "    fc1_biases = new_biases(length=512)\n",
    "\n",
    "with tf.name_scope(\"fc2\"):\n",
    "    fc2_weights = new_weights(shape= [512, NUM_LABELS],stddev_ = 0.1)\n",
    "    fc2_biases = new_biases(length=NUM_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def new_conv_layer(input,              # The previous layer.\n",
    "                   num_input_channels, # Num. channels in prev. layer.\n",
    "                   filter_size,        # Width and height of each filter.\n",
    "                   num_filters,        # Number of filters.\n",
    "                   use_pooling=True,name=\"conv\"):  # Use 2x2 max-pooling.\n",
    "    with tf.name_scope(name):\n",
    "        # Shape of the filter-weights for the convolution.\n",
    "        # This format is determined by the TensorFlow API.\n",
    "        shape = [filter_size, filter_size, num_input_channels, num_filters]\n",
    "\n",
    "        # Create new weights aka. filters with the given shape.\n",
    "        weights = new_weights(shape=shape)\n",
    "\n",
    "        # Create new biases, one for each filter.\n",
    "        biases = new_biases(length=num_filters)\n",
    "\n",
    "        # Create the TensorFlow operation for convolution.\n",
    "        # Note the strides are set to 1 in all dimensions.\n",
    "        # The first and last stride must always be 1,\n",
    "        # because the first is for the image-number and\n",
    "        # the last is for the input-channel.\n",
    "        # But e.g. strides=[1, 2, 2, 1] would mean that the filter\n",
    "        # is moved 2 pixels across the x- and y-axis of the image.\n",
    "        # The padding is set to 'SAME' which means the input image\n",
    "        # is padded with zeroes so the size of the output is the same.\n",
    "        layer = tf.nn.conv2d(input=input,\n",
    "                             filter=weights,\n",
    "                             strides=[1, 1, 1, 1],\n",
    "                             padding='SAME')\n",
    "\n",
    "        # Add the biases to the results of the convolution.\n",
    "        # A bias-value is added to each filter-channel.\n",
    "        layer += biases\n",
    "\n",
    "        # Use pooling to down-sample the image resolution?\n",
    "        if use_pooling:\n",
    "            # This is 2x2 max-pooling, which means that we\n",
    "            # consider 2x2 windows and select the largest value\n",
    "            # in each window. Then we move 2 pixels to the next window.\n",
    "            layer = tf.nn.max_pool(value=layer,\n",
    "                                   ksize=[1, 2, 2, 1],\n",
    "                                   strides=[1, 2, 2, 1],\n",
    "                                   padding='SAME')\n",
    "\n",
    "        # Rectified Linear Unit (ReLU).\n",
    "        # It calculates max(x, 0) for each input pixel x.\n",
    "        # This adds some non-linearity to the formula and allows us\n",
    "        # to learn more complicated functions.\n",
    "        layer = tf.nn.relu(layer)\n",
    "\n",
    "        # Note that ReLU is normally executed before the pooling,\n",
    "        # but since relu(max_pool(x)) == max_pool(relu(x)) we can\n",
    "        # save 75% of the relu-operations by max-pooling first.\n",
    "\n",
    "        # We return both the resulting layer and the filter-weights\n",
    "        # because we will plot the weights later.\n",
    "    return layer, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flatten_layer(layer):\n",
    "    # Get the shape of the input layer.\n",
    "    layer_shape = layer.get_shape()\n",
    "\n",
    "    # The shape of the input layer is assumed to be:\n",
    "    # layer_shape == [num_images, img_height, img_width, num_channels]\n",
    "\n",
    "    # The number of features is: img_height * img_width * num_channels\n",
    "    # We can use a function from TensorFlow to calculate this.\n",
    "    num_features = layer_shape[1:4].num_elements()\n",
    "    \n",
    "    # Reshape the layer to [num_images, num_features].\n",
    "    # Note that we just set the size of the second dimension\n",
    "    # to num_features and the size of the first dimension to -1\n",
    "    # which means the size in that dimension is calculated\n",
    "    # so the total size of the tensor is unchanged from the reshaping.\n",
    "    layer_flat = tf.reshape(layer, [-1, num_features])\n",
    "\n",
    "    # The shape of the flattened layer is now:\n",
    "    # [num_images, img_height * img_width * num_channels]\n",
    "\n",
    "    # Return both the flattened layer and the number of features.\n",
    "    return layer_flat, num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def new_fc_layer(input,          # The previous layer.\n",
    "                 num_inputs,     # Num. inputs from prev. layer.\n",
    "                 num_outputs,    # Num. outputs.\n",
    "                 use_relu=True,name=\"fc\"): # Use Rectified Linear Unit (ReLU)?\n",
    "    with tf.name_scope(name):\n",
    "        # Create new weights and biases.\n",
    "        weights = new_weights(shape=[num_inputs, num_outputs])\n",
    "        biases = new_biases(length=num_outputs)\n",
    "\n",
    "        # Calculate the layer as the matrix multiplication of\n",
    "        # the input and weights, and then add the bias-values.\n",
    "        layer = tf.matmul(input, weights) + biases\n",
    "        \n",
    "        # Use ReLU?\n",
    "        if use_relu:\n",
    "            layer = tf.nn.relu(layer)\n",
    "\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAYER CREATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We create convolutional layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'conv/Relu:0' shape=(16, 8, 8, 16) dtype=float32>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_conv1, weights_conv1 = \\\n",
    "    new_conv_layer(input=train_data_node,\n",
    "                   num_input_channels=NUM_CHANNELS,\n",
    "                   filter_size=filter_size1,\n",
    "                   num_filters=num_filters1,\n",
    "                   use_pooling=True)\n",
    "layer_conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'conv_1/Relu:0' shape=(16, 4, 4, 36) dtype=float32>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_conv2, weights_conv2 = \\\n",
    "    new_conv_layer(input=layer_conv1,\n",
    "                   num_input_channels=num_filters1,\n",
    "                   filter_size=filter_size2,\n",
    "                   num_filters=num_filters2,\n",
    "                   use_pooling=True)\n",
    "layer_conv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flatten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape:0' shape=(16, 576) dtype=float32>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_flat, num_features = flatten_layer(layer_conv2)\n",
    "layer_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "576"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fully connected layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'fc/Relu:0' shape=(16, 128) dtype=float32>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_fc1 = new_fc_layer(input=layer_flat,\n",
    "                         num_inputs=num_features,\n",
    "                         num_outputs=fc_size,\n",
    "                         use_relu=True)\n",
    "layer_fc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'fc_1/add:0' shape=(16, 2) dtype=float32>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_fc2 = new_fc_layer(input=layer_fc1,\n",
    "                         num_inputs=fc_size,\n",
    "                         num_outputs=NUM_LABELS,\n",
    "                         use_relu=False)\n",
    "layer_fc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rewrite layer creation in a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model (data = train_data_node,train=False):\n",
    "    # conv 1\n",
    "    layer_conv1, weights_conv1 = \\\n",
    "    new_conv_layer(input=data,\n",
    "                   num_input_channels=NUM_CHANNELS,\n",
    "                   filter_size=filter_size1,\n",
    "                   num_filters=num_filters1,\n",
    "                   use_pooling=True)\n",
    "    # conv 2\n",
    "    layer_conv2, weights_conv2 = \\\n",
    "    new_conv_layer(input=layer_conv1,\n",
    "                   num_input_channels=num_filters1,\n",
    "                   filter_size=filter_size2,\n",
    "                   num_filters=num_filters2,\n",
    "                   use_pooling=True)\n",
    "    # flatten\n",
    "    layer_flat, num_features = flatten_layer(layer_conv2)\n",
    "    \n",
    "    # fully connected 1\n",
    "    layer_fc1 = new_fc_layer(input=layer_flat,\n",
    "                         num_inputs=num_features,\n",
    "                         num_outputs=fc_size,\n",
    "                         use_relu=True)\n",
    "    # dropout?\n",
    "    if train:\n",
    "        layer_fc1 = tf.nn.dropout(layer_fc1, 0.5, seed=SEED)\n",
    "    \n",
    "    # fully connected 2\n",
    "    layer_fc2 = new_fc_layer(input=layer_fc1,\n",
    "                         num_inputs=fc_size,\n",
    "                         num_outputs=NUM_LABELS,\n",
    "                         use_relu=False)\n",
    "    \n",
    "    \n",
    "    out = layer_fc2\n",
    "    return out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cost function to be optimized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# logits = layer_fc2 \n",
    "logits = model(data = train_data_node, train=True)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits,\n",
    "                                                        labels=train_labels_node)\n",
    "\n",
    "with tf.name_scope(\"xent\"): \n",
    "    loss = tf.reduce_mean(cross_entropy)\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    \n",
    "    # to use the rest, the weights should be defined outside the model\n",
    "    \n",
    "    #regularizers = (tf.nn.l2_loss(fc1_weights) + tf.nn.l2_loss(fc1_biases) +\n",
    "    #                tf.nn.l2_loss(fc2_weights) + tf.nn.l2_loss(fc2_biases))\n",
    "    # Add the regularization term to the loss.\n",
    "    #loss += 5e-4 * regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adam = False\n",
    "batch = tf.Variable(0)\n",
    "learning_rate = tf.train.exponential_decay(\n",
    "        0.01,                # Base learning rate.\n",
    "        batch * BATCH_SIZE,  # Current index into the dataset.\n",
    "        train_size,          # Decay step.\n",
    "        0.95,                # Decay rate.\n",
    "        staircase=True,name=\"learning_rate\")\n",
    "# write it down to tensorboard\n",
    "tf.summary.scalar('learning_rate', learning_rate)\n",
    "\n",
    "if adam:\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss,global_step=batch)\n",
    "else:\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate,\n",
    "                                           0.0).minimize(loss,\n",
    "                                                         global_step=batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicted Class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = tf.nn.softmax(logits)\n",
    "\n",
    "y_pred_class = tf.argmax(y_pred, axis=1) # not usefull I think"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We'll compute them only once in a while by calling their {eval()} method.\n",
    "train_all_prediction = tf.nn.softmax(model(train_all_data_node))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add ops to save and restore all the variables.\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### measure of performances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(y_pred, train_labels_node)\n",
    "# or arg max of each??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helpers for the optimisation:\n",
    "\n",
    "We do it on a small batch so that it is faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batch_size = BATCH_SIZE\n",
    "BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Counter for total number of iterations performed so far.\n",
    "total_iterations = 0\n",
    "\n",
    "def optimize(num_iterations = 0):\n",
    "    # Ensure we update the global variable rather than a local copy.\n",
    "    global total_iterations\n",
    "    # Build the summary operation based on the TF collection of Summaries.\n",
    "    \n",
    "\n",
    "    print ('Initialized!')\n",
    "    # Loop through training steps.\n",
    "    print ('Total number of iterations = ' + str(int(num_epochs * train_size / BATCH_SIZE)))\n",
    "\n",
    "    training_indices = range(train_size)\n",
    "    \n",
    "    # Start-time used for printing time-usage below.\n",
    "    start_time = time.time()\n",
    "    \n",
    "    \n",
    "    for iepoch in range(num_epochs):\n",
    "\n",
    "        # Permute training indices\n",
    "        perm_indices = numpy.random.permutation(training_indices)\n",
    "\n",
    "        for step in range (int(train_size / BATCH_SIZE)):\n",
    "\n",
    "            offset = (step * BATCH_SIZE) % (train_size - BATCH_SIZE)\n",
    "            batch_indices = perm_indices[offset:(offset + BATCH_SIZE)]\n",
    "\n",
    "            # Compute the offset of the current minibatch in the data.\n",
    "            # Note that we could use better randomization across epochs.\n",
    "            batch_data = train_data[batch_indices, :, :, :]\n",
    "            batch_labels = train_labels[batch_indices]\n",
    "            # This dictionary maps the batch data (as a numpy array) to the\n",
    "            # node in the graph is should be fed to.\n",
    "            feed_dict = {train_data_node: batch_data,\n",
    "                         train_labels_node: batch_labels}\n",
    "\n",
    "            if step % RECORDING_STEP == 0:\n",
    "\n",
    "                summary_str, _, l, lr, predictions = s.run(\n",
    "                    [summary_op, optimizer, loss, learning_rate, y_pred],\n",
    "                    feed_dict=feed_dict)\n",
    "\n",
    "                summary_str = s.run(summary_op, feed_dict=feed_dict)\n",
    "                summary_writer.add_summary(summary_str, step)\n",
    "                summary_writer.flush()\n",
    "\n",
    "                print ('global step:', iepoch*int(train_size / BATCH_SIZE)+step,\\\n",
    "                        ' over ',num_epochs*int(train_size / BATCH_SIZE))\n",
    "                print ('Epoch: ', iepoch, '   || Step',float(step))\n",
    "                print ('Minibatch loss: %.3f, learning rate: %.6f' % (l, lr))\n",
    "                print ('Minibatch error: %.1f%%' % error_rate(predictions,\n",
    "                                                             batch_labels))\n",
    "\n",
    "                sys.stdout.flush()\n",
    "            else:\n",
    "                # Run the graph and fetch some of the nodes.\n",
    "                _, l, lr, predictions = s.run(\n",
    "                    [optimizer, loss, learning_rate, y_pred],\n",
    "                    feed_dict=feed_dict)\n",
    "\n",
    "    # Ending time.\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Difference between start and end-times.\n",
    "    time_dif = end_time - start_time\n",
    "\n",
    "    # Print the time-usage.\n",
    "    print(\"Time usage: \" + str(timedelta(seconds=int(round(time_dif)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialise variables and optimise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized!\n",
      "Total number of iterations = 3050\n",
      "global step: 0  over  3048\n",
      "Epoch:  0    || Step 0.0\n",
      "Minibatch loss: 0.681, learning rate: 0.010000\n",
      "Minibatch error: 43.8%\n",
      "global step: 50  over  3048\n",
      "Epoch:  0    || Step 50.0\n",
      "Minibatch loss: 0.686, learning rate: 0.010000\n",
      "Minibatch error: 50.0%\n",
      "global step: 100  over  3048\n",
      "Epoch:  0    || Step 100.0\n",
      "Minibatch loss: 0.690, learning rate: 0.010000\n",
      "Minibatch error: 43.8%\n",
      "global step: 150  over  3048\n",
      "Epoch:  0    || Step 150.0\n",
      "Minibatch loss: 0.715, learning rate: 0.010000\n",
      "Minibatch error: 56.2%\n",
      "global step: 200  over  3048\n",
      "Epoch:  0    || Step 200.0\n",
      "Minibatch loss: 0.689, learning rate: 0.010000\n",
      "Minibatch error: 31.2%\n",
      "global step: 250  over  3048\n",
      "Epoch:  0    || Step 250.0\n",
      "Minibatch loss: 0.698, learning rate: 0.010000\n",
      "Minibatch error: 50.0%\n",
      "global step: 300  over  3048\n",
      "Epoch:  0    || Step 300.0\n",
      "Minibatch loss: 0.717, learning rate: 0.010000\n",
      "Minibatch error: 62.5%\n",
      "global step: 350  over  3048\n",
      "Epoch:  0    || Step 350.0\n",
      "Minibatch loss: 0.683, learning rate: 0.010000\n",
      "Minibatch error: 50.0%\n",
      "global step: 381  over  3048\n",
      "Epoch:  1    || Step 0.0\n",
      "Minibatch loss: 0.683, learning rate: 0.010000\n",
      "Minibatch error: 50.0%\n",
      "global step: 431  over  3048\n",
      "Epoch:  1    || Step 50.0\n",
      "Minibatch loss: 0.667, learning rate: 0.009500\n",
      "Minibatch error: 31.2%\n",
      "global step: 481  over  3048\n",
      "Epoch:  1    || Step 100.0\n",
      "Minibatch loss: 0.703, learning rate: 0.009500\n",
      "Minibatch error: 43.8%\n",
      "global step: 531  over  3048\n",
      "Epoch:  1    || Step 150.0\n",
      "Minibatch loss: 0.711, learning rate: 0.009500\n",
      "Minibatch error: 68.8%\n",
      "global step: 581  over  3048\n",
      "Epoch:  1    || Step 200.0\n",
      "Minibatch loss: 0.693, learning rate: 0.009500\n",
      "Minibatch error: 56.2%\n",
      "global step: 631  over  3048\n",
      "Epoch:  1    || Step 250.0\n",
      "Minibatch loss: 0.708, learning rate: 0.009500\n",
      "Minibatch error: 62.5%\n",
      "global step: 681  over  3048\n",
      "Epoch:  1    || Step 300.0\n",
      "Minibatch loss: 0.691, learning rate: 0.009500\n",
      "Minibatch error: 50.0%\n",
      "global step: 731  over  3048\n",
      "Epoch:  1    || Step 350.0\n",
      "Minibatch loss: 0.708, learning rate: 0.009500\n",
      "Minibatch error: 43.8%\n",
      "global step: 762  over  3048\n",
      "Epoch:  2    || Step 0.0\n",
      "Minibatch loss: 0.682, learning rate: 0.009500\n",
      "Minibatch error: 50.0%\n",
      "global step: 812  over  3048\n",
      "Epoch:  2    || Step 50.0\n",
      "Minibatch loss: 0.695, learning rate: 0.009025\n",
      "Minibatch error: 50.0%\n",
      "global step: 862  over  3048\n",
      "Epoch:  2    || Step 100.0\n",
      "Minibatch loss: 0.723, learning rate: 0.009025\n",
      "Minibatch error: 62.5%\n",
      "global step: 912  over  3048\n",
      "Epoch:  2    || Step 150.0\n",
      "Minibatch loss: 0.661, learning rate: 0.009025\n",
      "Minibatch error: 31.2%\n",
      "global step: 962  over  3048\n",
      "Epoch:  2    || Step 200.0\n",
      "Minibatch loss: 0.672, learning rate: 0.009025\n",
      "Minibatch error: 37.5%\n",
      "global step: 1012  over  3048\n",
      "Epoch:  2    || Step 250.0\n",
      "Minibatch loss: 0.668, learning rate: 0.009025\n",
      "Minibatch error: 43.8%\n",
      "global step: 1062  over  3048\n",
      "Epoch:  2    || Step 300.0\n",
      "Minibatch loss: 0.713, learning rate: 0.009025\n",
      "Minibatch error: 50.0%\n",
      "global step: 1112  over  3048\n",
      "Epoch:  2    || Step 350.0\n",
      "Minibatch loss: 0.633, learning rate: 0.009025\n",
      "Minibatch error: 37.5%\n",
      "global step: 1143  over  3048\n",
      "Epoch:  3    || Step 0.0\n",
      "Minibatch loss: 0.719, learning rate: 0.009025\n",
      "Minibatch error: 56.2%\n",
      "global step: 1193  over  3048\n",
      "Epoch:  3    || Step 50.0\n",
      "Minibatch loss: 0.626, learning rate: 0.008574\n",
      "Minibatch error: 37.5%\n",
      "global step: 1243  over  3048\n",
      "Epoch:  3    || Step 100.0\n",
      "Minibatch loss: 0.626, learning rate: 0.008574\n",
      "Minibatch error: 37.5%\n",
      "global step: 1293  over  3048\n",
      "Epoch:  3    || Step 150.0\n",
      "Minibatch loss: 0.688, learning rate: 0.008574\n",
      "Minibatch error: 43.8%\n",
      "global step: 1343  over  3048\n",
      "Epoch:  3    || Step 200.0\n",
      "Minibatch loss: 0.656, learning rate: 0.008574\n",
      "Minibatch error: 50.0%\n",
      "global step: 1393  over  3048\n",
      "Epoch:  3    || Step 250.0\n",
      "Minibatch loss: 0.794, learning rate: 0.008574\n",
      "Minibatch error: 56.2%\n",
      "global step: 1443  over  3048\n",
      "Epoch:  3    || Step 300.0\n",
      "Minibatch loss: 0.654, learning rate: 0.008574\n",
      "Minibatch error: 43.8%\n",
      "global step: 1493  over  3048\n",
      "Epoch:  3    || Step 350.0\n",
      "Minibatch loss: 0.653, learning rate: 0.008574\n",
      "Minibatch error: 43.8%\n",
      "global step: 1524  over  3048\n",
      "Epoch:  4    || Step 0.0\n",
      "Minibatch loss: 0.650, learning rate: 0.008574\n",
      "Minibatch error: 31.2%\n",
      "global step: 1574  over  3048\n",
      "Epoch:  4    || Step 50.0\n",
      "Minibatch loss: 0.694, learning rate: 0.008145\n",
      "Minibatch error: 43.8%\n",
      "global step: 1624  over  3048\n",
      "Epoch:  4    || Step 100.0\n",
      "Minibatch loss: 0.660, learning rate: 0.008145\n",
      "Minibatch error: 37.5%\n",
      "global step: 1674  over  3048\n",
      "Epoch:  4    || Step 150.0\n",
      "Minibatch loss: 0.569, learning rate: 0.008145\n",
      "Minibatch error: 18.8%\n",
      "global step: 1724  over  3048\n",
      "Epoch:  4    || Step 200.0\n",
      "Minibatch loss: 0.649, learning rate: 0.008145\n",
      "Minibatch error: 43.8%\n",
      "global step: 1774  over  3048\n",
      "Epoch:  4    || Step 250.0\n",
      "Minibatch loss: 0.699, learning rate: 0.008145\n",
      "Minibatch error: 43.8%\n",
      "global step: 1824  over  3048\n",
      "Epoch:  4    || Step 300.0\n",
      "Minibatch loss: 0.663, learning rate: 0.008145\n",
      "Minibatch error: 43.8%\n",
      "global step: 1874  over  3048\n",
      "Epoch:  4    || Step 350.0\n",
      "Minibatch loss: 0.681, learning rate: 0.008145\n",
      "Minibatch error: 43.8%\n",
      "global step: 1905  over  3048\n",
      "Epoch:  5    || Step 0.0\n",
      "Minibatch loss: 0.773, learning rate: 0.008145\n",
      "Minibatch error: 50.0%\n",
      "global step: 1955  over  3048\n",
      "Epoch:  5    || Step 50.0\n",
      "Minibatch loss: 0.574, learning rate: 0.007738\n",
      "Minibatch error: 12.5%\n",
      "global step: 2005  over  3048\n",
      "Epoch:  5    || Step 100.0\n",
      "Minibatch loss: 0.641, learning rate: 0.007738\n",
      "Minibatch error: 37.5%\n",
      "global step: 2055  over  3048\n",
      "Epoch:  5    || Step 150.0\n",
      "Minibatch loss: 0.650, learning rate: 0.007738\n",
      "Minibatch error: 37.5%\n",
      "global step: 2105  over  3048\n",
      "Epoch:  5    || Step 200.0\n",
      "Minibatch loss: 0.569, learning rate: 0.007738\n",
      "Minibatch error: 31.2%\n",
      "global step: 2155  over  3048\n",
      "Epoch:  5    || Step 250.0\n",
      "Minibatch loss: 0.671, learning rate: 0.007738\n",
      "Minibatch error: 37.5%\n",
      "global step: 2205  over  3048\n",
      "Epoch:  5    || Step 300.0\n",
      "Minibatch loss: 0.583, learning rate: 0.007738\n",
      "Minibatch error: 25.0%\n",
      "global step: 2255  over  3048\n",
      "Epoch:  5    || Step 350.0\n",
      "Minibatch loss: 0.590, learning rate: 0.007738\n",
      "Minibatch error: 25.0%\n",
      "global step: 2286  over  3048\n",
      "Epoch:  6    || Step 0.0\n",
      "Minibatch loss: 0.524, learning rate: 0.007738\n",
      "Minibatch error: 18.8%\n",
      "global step: 2336  over  3048\n",
      "Epoch:  6    || Step 50.0\n",
      "Minibatch loss: 0.620, learning rate: 0.007351\n",
      "Minibatch error: 18.8%\n",
      "global step: 2386  over  3048\n",
      "Epoch:  6    || Step 100.0\n",
      "Minibatch loss: 0.666, learning rate: 0.007351\n",
      "Minibatch error: 43.8%\n",
      "global step: 2436  over  3048\n",
      "Epoch:  6    || Step 150.0\n",
      "Minibatch loss: 0.481, learning rate: 0.007351\n",
      "Minibatch error: 12.5%\n",
      "global step: 2486  over  3048\n",
      "Epoch:  6    || Step 200.0\n",
      "Minibatch loss: 0.611, learning rate: 0.007351\n",
      "Minibatch error: 31.2%\n",
      "global step: 2536  over  3048\n",
      "Epoch:  6    || Step 250.0\n",
      "Minibatch loss: 0.660, learning rate: 0.007351\n",
      "Minibatch error: 31.2%\n",
      "global step: 2586  over  3048\n",
      "Epoch:  6    || Step 300.0\n",
      "Minibatch loss: 0.644, learning rate: 0.007351\n",
      "Minibatch error: 31.2%\n",
      "global step: 2636  over  3048\n",
      "Epoch:  6    || Step 350.0\n",
      "Minibatch loss: 0.716, learning rate: 0.007351\n",
      "Minibatch error: 43.8%\n",
      "global step: 2667  over  3048\n",
      "Epoch:  7    || Step 0.0\n",
      "Minibatch loss: 0.670, learning rate: 0.007351\n",
      "Minibatch error: 43.8%\n",
      "global step: 2717  over  3048\n",
      "Epoch:  7    || Step 50.0\n",
      "Minibatch loss: 0.501, learning rate: 0.006983\n",
      "Minibatch error: 12.5%\n",
      "global step: 2767  over  3048\n",
      "Epoch:  7    || Step 100.0\n",
      "Minibatch loss: 0.642, learning rate: 0.006983\n",
      "Minibatch error: 31.2%\n",
      "global step: 2817  over  3048\n",
      "Epoch:  7    || Step 150.0\n",
      "Minibatch loss: 0.705, learning rate: 0.006983\n",
      "Minibatch error: 43.8%\n",
      "global step: 2867  over  3048\n",
      "Epoch:  7    || Step 200.0\n",
      "Minibatch loss: 0.696, learning rate: 0.006983\n",
      "Minibatch error: 43.8%\n",
      "global step: 2917  over  3048\n",
      "Epoch:  7    || Step 250.0\n",
      "Minibatch loss: 0.594, learning rate: 0.006983\n",
      "Minibatch error: 37.5%\n",
      "global step: 2967  over  3048\n",
      "Epoch:  7    || Step 300.0\n",
      "Minibatch loss: 0.559, learning rate: 0.006983\n",
      "Minibatch error: 31.2%\n",
      "global step: 3017  over  3048\n",
      "Epoch:  7    || Step 350.0\n",
      "Minibatch loss: 0.591, learning rate: 0.006983\n",
      "Minibatch error: 25.0%\n",
      "Time usage: 0:00:24\n",
      "Model saved in file: /tmp/mnist/test1/model.ckpt\n",
      "Running prediction on training set\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'get_predicted_groundtruth' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-85ebdea4f000>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_training_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTRAINING_SIZE\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mpred_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_predicted_groundtruth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_training_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"predicted_groundtruth_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_predicted_groundtruth' is not defined"
     ]
    }
   ],
   "source": [
    "with tf.Session() as s:\n",
    "    \n",
    "    if RESTORE_MODEL:\n",
    "        # Restore variables from disk.\n",
    "        saver.restore(s, FLAGS.train_dir + \"/model.ckpt\")\n",
    "        print(\"Model restored.\")\n",
    "\n",
    "    else:\n",
    "        s.run(tf.global_variables_initializer())\n",
    "\n",
    "        summary_op = tf.summary.merge_all()\n",
    "        summary_writer = tf.summary.FileWriter(FLAGS.train_dir)\n",
    "        summary_writer.add_graph(s.graph)\n",
    "\n",
    "        # optimize\n",
    "        optimize(num_iterations=1)\n",
    "\n",
    "        # Save the variables to disk.\n",
    "        save_path = saver.save(s, FLAGS.train_dir + \"/model.ckpt\")\n",
    "        print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "    print (\"Running prediction on training set\")\n",
    "    prediction_training_dir = \"predictions_training/\"\n",
    "    if not os.path.isdir(prediction_training_dir):\n",
    "        os.mkdir(prediction_training_dir)\n",
    "    for i in range(1, TRAINING_SIZE+1):\n",
    "        pred_img = get_predicted_groundtruth(train_data_filename, i)\n",
    "        Image.fromarray(pred_img).save(prediction_training_dir + \"predicted_groundtruth_\" + str(i) + \".png\")\n",
    "\n",
    "        #pimg = get_prediction_with_groundtruth(train_data_filename, i)\n",
    "        #Image.fromarray(pimg).save(prediction_training_dir + \"prediction_\" + str(i) + \".png\")\n",
    "        oimg = get_prediction_with_overlay(train_data_filename, i)\n",
    "        oimg.save(prediction_training_dir + \"overlay_\" + str(i) + \".png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run on training images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_idx = 3\n",
    "filename =train_data_filename\n",
    "imageid = \"satImage_%.3d\" % image_idx\n",
    "image_filename = filename + imageid + \".png\"\n",
    "img = mpimg.imread(image_filename)\n",
    "data = numpy.asarray(img_crop(img, IMG_PATCH_SIZE, IMG_PATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_node = tf.constant(data)\n",
    "output = tf.nn.softmax(model(data_node))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Attempted to use a closed Session.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-b5e9366e75ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0moutput_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ondine/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ondine/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1045\u001b[0m     \u001b[0;31m# Check session.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_closed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Attempted to use a closed Session.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m       raise RuntimeError('The Session graph is empty.  Add operations to the '\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Attempted to use a closed Session."
     ]
    }
   ],
   "source": [
    "s.run(tf.global_variables_initializer())\n",
    "output_prediction = s.run(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_prediction = label_to_img(img.shape[0], img.shape[1], IMG_PATCH_SIZE, IMG_PATCH_SIZE, output_prediction)\n",
    "max_ = img_prediction.max()\n",
    "min_ = img_prediction.min()\n",
    "print (min_,max_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def img_float_to_uint8(img):\n",
    "    rimg = img - numpy.min(img)\n",
    "    rimg = (rimg / numpy.max(rimg) * PIXEL_DEPTH).round().astype(numpy.uint8)\n",
    "    return rimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gt_img8 = img_float_to_uint8(output_prediction)\n",
    "gt_img8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_pred(array):\n",
    "    N = array.shape[0]\n",
    "    G = np.zeros((N,N,1))\n",
    "    G[Z>0.5] = [1]\n",
    "    G[Z<0.5] = [0]\n",
    "    plt.imshow(G, 'gray')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_prediction.shape\n",
    "print(output_prediction[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plot_pred(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get prediction for given input image\n",
    "def get_prediction(img):\n",
    "    data = numpy.asarray(img_crop(img, IMG_PATCH_SIZE, IMG_PATCH_SIZE))\n",
    "    data_node = tf.constant(data)\n",
    "    output = tf.nn.softmax(model(data_node))\n",
    "    #s.run(tf.global_variables_initializer()) # to remoove??\n",
    "    output_prediction = s.run(output)\n",
    "    img_prediction = label_to_img(img.shape[0], img.shape[1], IMG_PATCH_SIZE, IMG_PATCH_SIZE, output_prediction)\n",
    "\n",
    "    return img_prediction\n",
    "\n",
    "\n",
    "# Get a concatenation of the prediction and groundtruth for given input file\n",
    "def get_predicted_groundtruth(filename, image_idx):\n",
    "\n",
    "    imageid = \"satImage_%.3d\" % image_idx\n",
    "    image_filename = filename + imageid + \".png\"\n",
    "    img = mpimg.imread(image_filename)\n",
    "\n",
    "    img_prediction = get_prediction(img)\n",
    "    cimg = False_concatenate_images(img_prediction)\n",
    "    return cimg\n",
    "\n",
    "# Get prediction overlaid on the original image for given input file\n",
    "def get_prediction_with_overlay(filename, image_idx):\n",
    "\n",
    "    imageid = \"satImage_%.3d\" % image_idx\n",
    "    image_filename = filename + imageid + \".png\"\n",
    "    img = mpimg.imread(image_filename)\n",
    "\n",
    "    img_prediction = get_prediction(img)\n",
    "    oimg = make_img_overlay(img, img_prediction)\n",
    "\n",
    "    return oimg\n",
    "################### end other functions ####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (\"Running prediction on training set\")\n",
    "prediction_training_dir = \"predictions_training/\"\n",
    "if not os.path.isdir(prediction_training_dir):\n",
    "    os.mkdir(prediction_training_dir)\n",
    "for i in range(1, TRAINING_SIZE+1):\n",
    "    pred_img = get_predicted_groundtruth(train_data_filename, i)\n",
    "    Image.fromarray(pred_img).save(prediction_training_dir + \"predicted_groundtruth_\" + str(i) + \".png\")\n",
    "\n",
    "    #pimg = get_prediction_with_groundtruth(train_data_filename, i)\n",
    "    #Image.fromarray(pimg).save(prediction_training_dir + \"prediction_\" + str(i) + \".png\")\n",
    "    oimg = get_prediction_with_overlay(train_data_filename, i)\n",
    "    oimg.save(prediction_training_dir + \"overlay_\" + str(i) + \".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END SESSION:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# comment the next line if you don't wanna restart he whole notebook\n",
    "s.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
